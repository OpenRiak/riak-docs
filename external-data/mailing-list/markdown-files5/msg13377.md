---
title: "Re: Single node causing cluster to be extremely slow (leveldb)"
description: ""
project: community
lastmod: 2014-01-10T06:59:28-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg13377"
mailinglist_parent_id: "msg13376"
author_name: "Martin May"
project_section: "mailinglistitem"
sent_date: 2014-01-10T06:59:28-08:00
---


Matthew,

Thanks for the help and suggestions, we really appreciate it. We’re planning on 
giving Riak 2.0 a shot as soon as it’s released, and are looking forward to the 
new features.

Best,
Martin

On Jan 10, 2014, at 7:51 AM, Matthew Von-Maszewski  wrote:

&gt; Martin,
&gt; 
&gt; Assuming your business continues to grow, this problem will come back under 
&gt; 1.4 … but not for a while. We can push the cache\\_size as far down as 8Mbytes 
&gt; to make room for a little more file cache space if needed. 
&gt; 
&gt; The manual tunings I gave you and the subsequent block\\_size tuning I 
&gt; mentioned are all automated in the leveldb for Riak 2.0. You should consider 
&gt; that upgrade once its available (we are code complete and testing now).
&gt; 
&gt; The cache sizing considerations are discussed here: 
&gt; 
&gt; https://github.com/basho/leveldb/wiki/mv-flexcache
&gt; 
&gt; The block size considerations are discussed here:
&gt; 
&gt; https://github.com/basho/leveldb/wiki/mv-dynamic-block-size
&gt; 
&gt; And sooner or later you are going to be asking why deletes do not free up 
&gt; space (which implies freeing up file cache space):
&gt; 
&gt; https://github.com/basho/leveldb/wiki/mv-aggressive-delete
&gt; 
&gt; 
&gt; Let me know if you have further questions or concerns.
&gt; 
&gt; Matthew
&gt; 
&gt; 
&gt; 
&gt; 
&gt; On Jan 10, 2014, at 9:41 AM, Martin May  wrote:
&gt; 
&gt;&gt; Hi Matthew,
&gt;&gt; 
&gt;&gt; We applied this change to node 4, started it up, and it seems much happier 
&gt;&gt; (no crazy CPU). We’re going to keep an eye on it for a little while, and 
&gt;&gt; then apply this setting to all the other nodes as well.
&gt;&gt; 
&gt;&gt; Is there anything we can do to prevent this scenario in the future, or 
&gt;&gt; should the settings you suggested take care of that?
&gt;&gt; 
&gt;&gt; Thanks,
&gt;&gt; Martin
&gt;&gt; 
&gt;&gt; On Jan 10, 2014, at 6:42 AM, Matthew Von-Maszewski  
&gt;&gt; wrote:
&gt;&gt; 
&gt;&gt;&gt; Sean,
&gt;&gt;&gt; 
&gt;&gt;&gt; I did some math based upon the app.config and LOG files. I am guessing 
&gt;&gt;&gt; that you are starting to thrash your file cache.
&gt;&gt;&gt; 
&gt;&gt;&gt; This theory should be easy to prove / disprove. On that one node, change 
&gt;&gt;&gt; the cache\\_size and max\\_open\\_files to:
&gt;&gt;&gt; 
&gt;&gt;&gt; cache\\_size 68435456
&gt;&gt;&gt; max\\_open\\_files 425
&gt;&gt;&gt; 
&gt;&gt;&gt; If I am correct, the node should come up and not cause problems. We are 
&gt;&gt;&gt; trading block cache space for file cache space. A miss in the file cache 
&gt;&gt;&gt; is far more costly than a miss in the block cache.
&gt;&gt;&gt; 
&gt;&gt;&gt; Let me know how this works for you. It is possible that we might want to 
&gt;&gt;&gt; talk about raising your block size slightly to reduce file cache overhead.
&gt;&gt;&gt; 
&gt;&gt;&gt; Matthew
&gt;&gt;&gt; 
&gt;&gt;&gt; On Jan 9, 2014, at 9:33 PM, Sean McKibben  wrote:
&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; We have a 5 node cluster using elevelDB (1.4.2) and 2i, and this afternoon 
&gt;&gt;&gt;&gt; it started responding extremely slowly. CPU on member 4 was extremely high 
&gt;&gt;&gt;&gt; and we restarted that process, but it didn’t help. We temporarily shut 
&gt;&gt;&gt;&gt; down member 4 and cluster speed returned to normal, but as soon as we boot 
&gt;&gt;&gt;&gt; member 4 back up, the cluster performance goes to shit.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; We’ve run in to this before but were able to just start with a fresh set 
&gt;&gt;&gt;&gt; of data after wiping machines as it was before we migrated to this 
&gt;&gt;&gt;&gt; bare-metal cluster. Now it is causing some pretty significant issues and 
&gt;&gt;&gt;&gt; we’re not sure what we can do to get it back to normal, many of our queues 
&gt;&gt;&gt;&gt; are filling up and we’ll probably have to take node 4 off again just so we 
&gt;&gt;&gt;&gt; can provide a regular quality of service.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; We’ve turned off AAE on node 4 but it hasn’t helped. We have some 
&gt;&gt;&gt;&gt; transfers that need to happen but they are going very slowly.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 'riak-admin top’ on node 4 reports this:
&gt;&gt;&gt;&gt; Load: cpu 610 Memory: total 503852 binary 
&gt;&gt;&gt;&gt; 231544
&gt;&gt;&gt;&gt; procs 804 processes 179850 code 
&gt;&gt;&gt;&gt; 11588
&gt;&gt;&gt;&gt; runq 134 atom 533 ets 
&gt;&gt;&gt;&gt; 4581
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Pid Name or Initial Func Time Reds 
&gt;&gt;&gt;&gt; Memory MsgQ Current Function
&gt;&gt;&gt;&gt; -------------------------------------------------------------------------------------------------------------------------------
&gt;&gt;&gt;&gt; &lt;6175.29048.3&gt; proc\\_lib:init\\_p/5 '-' 462231 
&gt;&gt;&gt;&gt; 51356760 0 mochijson2:json\\_bin\\_is\\_safe/1
&gt;&gt;&gt;&gt; &lt;6175.12281.6&gt; proc\\_lib:init\\_p/5 '-' 307183 
&gt;&gt;&gt;&gt; 64195856 1 gen\\_fsm:loop/7
&gt;&gt;&gt;&gt; &lt;6175.1581.5&gt; proc\\_lib:init\\_p/5 '-' 286143 
&gt;&gt;&gt;&gt; 41085600 0 mochijson2:json\\_bin\\_is\\_safe/1
&gt;&gt;&gt;&gt; &lt;6175.6659.0&gt; proc\\_lib:init\\_p/5 '-' 281845 
&gt;&gt;&gt;&gt; 13752 0 sext:decode\\_binary/3
&gt;&gt;&gt;&gt; &lt;6175.6666.0&gt; proc\\_lib:init\\_p/5 '-' 209113 
&gt;&gt;&gt;&gt; 21648 0 sext:decode\\_binary/3
&gt;&gt;&gt;&gt; &lt;6175.12219.6&gt; proc\\_lib:init\\_p/5 '-' 168832 
&gt;&gt;&gt;&gt; 16829200 0 riak\\_client:wait\\_for\\_query\\_results/4
&gt;&gt;&gt;&gt; &lt;6175.8403.0&gt; proc\\_lib:init\\_p/5 '-' 133333 
&gt;&gt;&gt;&gt; 13880 1 eleveldb:iterator\\_move/2
&gt;&gt;&gt;&gt; &lt;6175.8813.0&gt; proc\\_lib:init\\_p/5 '-' 119548 
&gt;&gt;&gt;&gt; 9000 1 eleveldb:iterator/3
&gt;&gt;&gt;&gt; &lt;6175.8411.0&gt; proc\\_lib:init\\_p/5 '-' 115759 
&gt;&gt;&gt;&gt; 34472 0 riak\\_kv\\_vnode:'-result\\_fun\\_ack/2-fun-0-'
&gt;&gt;&gt;&gt; &lt;6175.5679.0&gt; proc\\_lib:init\\_p/5 '-' 109577 
&gt;&gt;&gt;&gt; 8952 0 riak\\_kv\\_vnode:'-result\\_fun\\_ack/2-fun-0-'
&gt;&gt;&gt;&gt; Output server crashed: connection\\_lost
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Based on that, is there anything anyone can think to do to try to bring 
&gt;&gt;&gt;&gt; performance back in to the land of usability? Does this thing appear to be 
&gt;&gt;&gt;&gt; something that may have been resolved in 1.4.6 or 1.4.7?
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Only thing we can think of at this point might be to remove or force 
&gt;&gt;&gt;&gt; remove the member and join in a new freshly built one, but last time we 
&gt;&gt;&gt;&gt; attempted that (on a different cluster) our secondary indexes got 
&gt;&gt;&gt;&gt; irreparably damaged and only regained consistency when we copied every 
&gt;&gt;&gt;&gt; individual key to (this) new cluster! Not a good experience :( but i’m 
&gt;&gt;&gt;&gt; hopeful that 1.4.6 may have addressed some of our issues.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Any help is appreciated.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Thank you,
&gt;&gt;&gt;&gt; Sean McKibben
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt; 
&gt; 


\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

