---
title: "Re: Durable writes and parallel reads"
description: ""
project: community
lastmod: 2011-11-04T06:40:35-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg05441"
mailinglist_parent_id: "msg05439"
author_name: "Erik Søe Sørensen"
project_section: "mailinglistitem"
sent_date: 2011-11-04T06:40:35-07:00
---



(inline replies)

On 04-11-2011 00:21, Jon Meredith wrote:

Hi Erik,

Apologies it's taken so long to get back to you.

No worries; a few days is not long to wait for a clear and complete answer.
(Besides, the theme \\*was\\* latency... ;-) )


Durable writes:
 You're interpreting it correctly. DW means that the storage backend 
has accepted the write. As the backends are pluggable and 
configurable so that affects what durable means. For bitcask you can 
control the sync strategy and there are similar controls for innodb. 
 For the memory backend there is no durable write. With hindsight it 
would have been better to have used something like accepted write (AW) 
and write (W) rather than W/DW, but we're fairly stuck with it now.
I agree - naming is important though. I'm afraid I'm guilty of 
propagating my initial misconceptions of the semantics of DW, which were 
mostly due to how I though it "must be"...


Could you perhaps clarify on the Basho wiki that "commit to durable 
storage" means that the backend has received the data, but that whether 
this means that the data have also been persisted is up to the backend 
in question - that the data might still be in the hands of software, 
rather than hardware?

(e.g. at http://wiki.basho.com/Basic-Riak-API-Operations.html)

 Combining writes/acceptance is a very interesting idea going forward, 
but doesn't fit well with the sync nature of the backend API we have 
currently.

No, I understand that.

Even so, I'll hazard the following suggestion as to how a 
backwards-compatible API change might be:


 To the present put/5 signature:
 -spec put(riak\\_object:bucket(), riak\\_object:key(), [index\\_spec()],
 binary(), state()) -&gt;
 {ok, state()} |
 {error, term(), state()}.
 add the return option
 {ok, state(), buffered | persisted}
 and add a flushing function:
 -spec persist(state()) -&gt;
 {ok, state()}.

 This function is only ever called if put() returns {ok, \\_, buffered}.

With this change, if riak\\_kv\\_vnode ever receives an {ok, \\_, buffered} 
return value, it can set up a persist timer.
(I say riak\\_kv\\_vnode, but in the first iteration it could in principle 
just be a wrapper backend.)



Parallel reads:
 With 1.0.0 we've introduced a thread pool to increase the 
concurrency vnodes can use for listing keys. I'd like to improve on 
read concurrency. The current architecture ensures that a key is only 
updated by a single thread which makes writing backend drivers 
simpler. We either need to add support to the k/v vnode to ensure the 
property is true when being updated in parallel or change the backend 
drivers to be tolerant of it.


The performance numbers are interesting. How many vnodes were you 
simulating?
For the 30-45% numbers, I had three "vnodes", each accessing a separate 
1GB+ file.


Elaborating a bit:
Results from a Ubuntu Linux (2.6.32) laptop - note that results are 
rather I/O scheduler dependent:


 /---- Scheduler=CFQ ("Completely fair queueing")
 serial : 24.7593 seconds total
 serial\\_sorted : 16.5154 seconds total // 50% tp
 improvement
 parallel : 24.5351 seconds total// 1% tp
 improvement
 // Note: performance seems to be hindered by the "Fair" part of the
 scheduling algorithm,
 // which tries to give separate processes equal bandwidth to the disk.

 /---- Scheduler=Anticipatory
 serial : 24.7583 seconds total
 serial\\_sorted : 16.3723 seconds total // 51% tp
 improvement
 parallel : 19.3194 seconds total // 28% tp
 improvement

 /---- Scheduler=Deadline
 serial : 21.7606 seconds total
 serial\\_sorted : 17.5556 seconds total // 24% tp
 improvement
 parallel : 16.3663 seconds total// 33% tp
 improvement

 /---- Scheduler=Noop
 serial : 21.7646 seconds total
 serial\\_sorted : 17.4545 seconds total// 25% tp
 improvement
 parallel : 15.7670 seconds total// 38% tp
 improvement

These are from a single-run test; I note that the 'parallel' numbers are 
in the lower end of the range I quoted.


Test code is attached. (Much of the code isn't used, as there was a bit 
of exploration involved.)

The above results were produced by running

 parread:multi\\_vnode\\_test\\_main(["/tmp/erk1G","/tmp/erk1Gb","/tmp/erk1Gc"]).

where the erkXX are separate files of each 1GB. (Each is used by one 
"vnode", so this is a 3-vnode-test.)


The "clear\\_disk\\_cache\\_for\\_testing\\_purposes" script is of course system 
dependent; this one needs to be suid root (or the test program run as 
root, but don't do that unless you have reviewed the code well).


/Erik


Jon.

On Mon, Oct 31, 2011 at 10:38 AM, Erik Søe Sørensen &gt; wrote:


 The following is a couple of questions (and suggestions) regarding the
 technical sides of Riak performance and reliability.

 The questions have been prompted by reading Riak source code and by
 discussions within our company.

 I suppose the common thread here is "latency hiding"...

 Durable Writes.
 ---------------
 The default for bitcask's 'sync\\_strategy' setting is not to flush
 to disk explicitly at all.

 This means that a 'Durable Write' isn't actually durable; the
 difference between 'W' and 'DW' replies is whether the write has
 made it past Riak, to the OS - but not through the OS and down to
 disk.

 Is this correct?

 What I'd have expected, as a reaonably-performing alternative, is
 that Riak would flush periodically - say, after at most W\\_flush
 writes or MS\\_flush milliseconds, and send 'dw' replies for all of
 the relevant requests (those written since last flush) at once
 after the flush has been completed.
 This would combine 'real' DW semantics with reasonable performance
 (and is how I have handled a similar problem; my conceptions about
 what is right and proper may of course be influenced by my own
 coding history...).

 (For kicks, MS\\_flush might even be dynamically determined by how
 long the flush operations tend to take; the typical value of a
 flush duration, multiplied by a small constant, would probably be
 a fitting value.)


 Parallel Reads.
 ---------------
 Within a vnode, bitcask read operations happen in serial.
 Is there any reason for reads not happening in parallel?

 For map/reduce operations, in particular, I imagine this might
 make a difference, by giving the OS the opportunity to schedule
 disk accesses so as to reduce seek time.

 (Unless of course Riak itself reorders the keys before reading,
 but I don't believe this is the case - especially since the order
 would depend on the backend: for bitcask, by time; for innostore,
 by key order, for instance.)

 Of course, if each host has multiple vnodes, there will be some
 parallellity even with serialized reads within each bitcask.

 Regards,
 Erik Søe Sørensen


 \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
 riak-users mailing list
 riak-users@lists.basho.com 
 http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com




--
Jon Meredith
Platform Engineering Manager
Basho Technologies, Inc.
jmered...@basho.com 



#!/bin/bash
echo 3 | sudo tee /proc/sys/vm/drop\\_caches || echo "ERROR: $?" &gt;&2
-module(parread).
-compile(export\\_all).

%%% To use this test, you need a script called
%%% clear\\_disk\\_cache\\_for\\_testing\\_purposes
%%% which clears the disk caches so that all reads are actual disk reads.

multi\\_vnode\\_test\\_main(FileNames) -&gt;
 BlockSize = 1024,
 N = 1000,
 TestFuns = [{serial, fun test/3},
 {serial\\_sorted, fun stest/3},
 {parallel, fun p2test/3}],
 R = [begin
 io:format("Trying ~s...\\n", [Name]),
 clear\\_disk\\_cache(),
 Before = os:timestamp(),
 multi\\_vnode\\_test(TestFun, FileNames, BlockSize, N),
 After = os:timestamp(),
 {Name, timer:now\\_diff(After, Before) \\* 1.0e-6}
 end
 || {Name,TestFun} &lt;- TestFuns],
 [io:format("~-30s: ~8.4f seconds total\\n", [Name,TotalSecs])
 || {Name, TotalSecs} &lt;- R],
 ok.

clear\\_disk\\_cache() -&gt;
 os:cmd("./clear\\_disk\\_cache\\_for\\_testing\\_purposes").


%%====================
test2(FileName, BlockSize, N, P) -&gt;
 [spawn(fun() -&gt;
 {A,B,C} = now(),
 random:seed(A,B,C),
 io:format("~p\\n", [test(FileName, BlockSize, N)])
 end)
 || \\_ &lt;- lists:seq(1,P)]. 

rtest2(FileName, BlockSize, N, P) -&gt;
 [spawn(fun() -&gt;
 {A,B,C} = now(),
 random:seed(A,B,C),
 io:format("~p\\n", [rtest(FileName, BlockSize, N)])
 end)
 || \\_ &lt;- lists:seq(1,P)]. 

p2test2(FileName, BlockSize, N, P) -&gt;
 [spawn(fun() -&gt;
 {A,B,C} = now(),
 random:seed(A,B,C),
 io:format("~p\\n", [p2test(FileName, BlockSize, N)])
 end)
 || \\_ &lt;- lists:seq(1,P)].

multi\\_vnode\\_test(TestFun, FileNames, BlockSize, N) -&gt;
 R = parmap(fun(FileName) -&gt;
 {A,B,C} = now(),
 random:seed(A,B,C),
 TestFun(FileName, BlockSize, N)
 end,
 FileNames),
 R.
 

parmap(Fun, Args) -&gt;
 Parent = self(),
 Pids = [spawn(fun() -&gt; Parent ! {self(), Fun(A)} end) || A &lt;- Args],
 Res = [receive {P,R} -&gt; R end || P &lt;- Pids],
 io:format("parmap(~p,~p) -&gt;\\n ~p\\n", [Fun, Args, Res]),
 Res.
 

%%====================
test(FileName, BlockSize, N) -&gt; % Test sync. non-raw reads.
 test(FileName, BlockSize, N, fun loop/3, [], sync\\_non\\_raw).

rtest(FileName, BlockSize, N) -&gt; % Test sync. raw reads.
 test(FileName, BlockSize, N, fun loop/3, [raw], sync\\_raw).

ptest(FileName, BlockSize, N) -&gt; % Test async. non-raw reads.
 test(FileName, BlockSize, N, fun ploop/3, [], async\\_non\\_raw).

stest(FileName, BlockSize, N) -&gt; % Test async. non-raw reads.
 test(FileName, BlockSize, N, fun sloop/3, [], async\\_non\\_raw).

gtest(FileName, BlockSize, N) -&gt; % Test async. non-raw reads.
 test(FileName, BlockSize, N, fun gloop/3, [], grouped).

test(FileName, BlockSize, N, LoopFun, OpenOpts, Tag) -&gt;
 {ok, Fd} = file:open(FileName, [read, binary | OpenOpts]),
 {ok, Sz} = file:position(Fd, eof),
 Reqs = request\\_set(Sz-BlockSize, N),

 Before = os:timestamp(),
 LoopFun(Fd, BlockSize, Reqs),
 After = os:timestamp(),

 file:close(Fd),
 Elapsed = timer:now\\_diff(After,Before),
 {result, Tag, Elapsed / 1.0e6, float(Elapsed) / N}.

p2test(FileName, BlockSize, N) -&gt; % Test async. non-raw reads.
 test\\_by\\_filename(FileName, BlockSize, N, fun p2loop/3, [], parallel\\_by\\_filename).

test\\_by\\_filename(FileName, BlockSize, N, LoopFun, OpenOpts, Tag) -&gt;
 {ok, Fd} = file:open(FileName, [read, binary | OpenOpts]),
 {ok, Sz} = file:position(Fd, eof),
 Reqs = request\\_set(Sz-BlockSize, N),

 Before = os:timestamp(),
 LoopFun(FileName, BlockSize, Reqs),
 After = os:timestamp(),

 file:close(Fd),
 Elapsed = timer:now\\_diff(After,Before),
 {result, Tag, Elapsed / 1.0e6, float(Elapsed) / N}.

%%==========
sloop(Fd, BlockSize, Reqs) -&gt;
 loop(Fd, BlockSize, lists:sort(Reqs)).

loop(\\_Fd, \\_BlockSize, []) -&gt; ok;
loop(Fd, BlockSize, [Pos|Rest]) -&gt;
 read\\_block\\_synchronously(Fd, Pos, BlockSize),
 loop(Fd, BlockSize, Rest).

%%==========
gloop(\\_Fd, \\_BlockSize, []) -&gt; ok;
gloop(Fd, BlockSize, Reqs) -&gt;
 {Group, Rest} = try lists:split(50, Reqs)
 catch error:badarg -&gt; {Reqs, []}
 end,
 read\\_block\\_group(Fd, [{Pos, BlockSize} || Pos &lt;- Group]),
 gloop(Fd, BlockSize, Rest).

%%==========
ploop(Fd, BlockSize, Reqs) -&gt;
 ploop(Fd, BlockSize, 0, Reqs).

ploop(\\_Fd, \\_BlockSize, 0, []) -&gt; ok;
ploop(Fd, BlockSize, Outstanding, []) -&gt;
 wait\\_for\\_outstanding(),
 ploop(Fd, BlockSize, Outstanding-1, []);
ploop(Fd, BlockSize, Outstanding, [Pos|Rest]=Reqs) -&gt;
 if Outstanding &gt;= 50 -&gt;
 %% No slots free.
 wait\\_for\\_outstanding(),
 ploop(Fd, BlockSize, Outstanding-1, Reqs);
 true -&gt;
 read\\_block\\_asynchronously(Fd, Pos, BlockSize),
 ploop(Fd, BlockSize, Outstanding+1, Rest)
 end.

%%==========
p2loop(Filename, BlockSize, Reqs) -&gt;
 p2loop(Filename, BlockSize, 0, Reqs).

p2loop(\\_Filename, \\_BlockSize, 0, []) -&gt; ok;
p2loop(Filename, BlockSize, Outstanding, []) -&gt;
 wait\\_for\\_outstanding(),
 p2loop(Filename, BlockSize, Outstanding-1, []);
p2loop(Filename, BlockSize, Outstanding, [Pos|Rest]=Reqs) -&gt;
 if Outstanding &gt;= 50 -&gt;
 %% No slots free.
 wait\\_for\\_outstanding(),
 p2loop(Filename, BlockSize, Outstanding-1, Reqs);
 true -&gt;
 read\\_block\\_asynchronously\\_from\\_filename(Filename, Pos, BlockSize),
 p2loop(Filename, BlockSize, Outstanding+1, Rest)
 end.

%%==========
request\\_set(StartLimit, N) -&gt;
 [random:uniform(StartLimit) || \\_ &lt;- lists:seq(1,N)].

read\\_block\\_synchronously(Fd, Pos, BlockSize) -&gt;
 {ok, \\_Data} = file:pread(Fd, Pos, BlockSize).

read\\_block\\_group(Fd, Group) -&gt;
 io:format("DB| group size: ~p\\n", [length(Group)]),
 {ok, \\_Datas} = file:pread(Fd, Group).

read\\_block\\_asynchronously(Fd, Pos, BlockSize) -&gt;
 Owner = self(),
 Pid = spawn(fun() -&gt;
%% io:format("DB| started: ~p @ ~p\\n", [self(), Pos]),
 try file:pread(Fd, Pos, BlockSize)
 of {ok, Data} -&gt;
%% io:format("DB| done: ~p @ ~p\\n", [self(), Pos]),
 Owner ! {read\\_done, self(), Data};
 Error1 -&gt; io:format("DB| error: ~p :: ~p\\n", [self(), Error1])
 catch \\_:Error -&gt;
 io:format("DB| error: ~p :: ~p\\n", [self(), Error])
 end
 end),
 Pid.

read\\_block\\_asynchronously\\_from\\_filename(FileName, Pos, BlockSize) -&gt;
 Owner = self(),
 Pid = spawn(fun() -&gt;
 {ok, Fd} = file:open(FileName, [read, binary, raw]),
%% io:format("DB| started: ~p @ ~p\\n", [self(), Pos]),
 try file:pread(Fd, Pos, BlockSize)
 of {ok, Data} -&gt;
%% io:format("DB| done: ~p @ ~p\\n", [self(), Pos]),
 Owner ! {read\\_done, self(), Data};
 Error1 -&gt; io:format("DB| error: ~p :: ~p\\n", [self(), Error1])
 catch \\_:Error -&gt;
 io:format("DB| error: ~p :: ~p\\n", [self(), Error])
 end
 end),
 Pid.

wait\\_for\\_outstanding() -&gt;
 receive {read\\_done, \\_Ref, \\_Data} -&gt; ok
 after 5000 -&gt; error(timeout)
 end.
 


\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

