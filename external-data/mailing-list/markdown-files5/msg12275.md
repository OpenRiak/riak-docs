---
title: "Re: Deleting data from bitcask backend"
description: ""
project: community
lastmod: 2013-09-16T12:13:53-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg12275"
mailinglist_parent_id: "msg12274"
author_name: "Evan Vigil-McClanahan"
project_section: "mailinglistitem"
sent_date: 2013-09-16T12:13:53-07:00
---


riak-admin vnode-status can be used to get information about the
number of bitcask files, their fragmentation and dead bytes, but since
it uses a lot of blocking vnode commands, it can spike latencies, so
should only be used off-peak.

On Mon, Sep 16, 2013 at 7:36 AM, Alex Moore  wrote:
&gt; Hi Charl,
&gt;
&gt; The problem is that even though documents seem to no longer be
&gt; available (doing a GET on a deleted document returns an expected 404)
&gt; the disk usage is not seeming reducing much and has currently been at
&gt; ~80% utilisation across all nodes for almost a week.
&gt;
&gt; When you delete a document, a tombstone record is written to bitcask, and
&gt; the reference to the key is removed from memory (which is why you get
&gt; 404's). The old entry isn't actually removed until the next bitcask merge.
&gt;
&gt; At first I though the large amount of deletes being performed might be
&gt; causing fragmentation of the merge index so I've been regularly
&gt; running forced compaction as documented here:
&gt; https://gist.github.com/rzezeski/3996286.
&gt;
&gt; That merge index is for Riak Search, not bitcask.
&gt;
&gt; There are ways of forcing a merge, but let's double check your settings/logs
&gt; first. Can you send me your app.config and a console.log from one of your
&gt; nodes?
&gt;
&gt; Thanks,
&gt; Alex
&gt;
&gt; --
&gt; Alex Moore
&gt; Sent with Airmail
&gt;
&gt; On September 16, 2013 at 4:43:07 AM, Charl Matthee (ch...@ntrippy.net)
&gt; wrote:
&gt;
&gt; Hi,
&gt;
&gt; We have a 8-node riak v1.4.0 cluster writing data to bitcask backends.
&gt;
&gt; We've recently started running out of disk across all nodes and so
&gt; implemented a 30-day sliding window data retention policy. This policy
&gt; is enforced by a go app that concurrently deletes documents outside
&gt; the window.
&gt;
&gt; The problem is that even though documents seem to no longer be
&gt; available (doing a GET on a deleted document returns an expected 404)
&gt; the disk usage is not seeming reducing much and has currently been at
&gt; ~80% utilisation across all nodes for almost a week.
&gt;
&gt; At first I though the large amount of deletes being performed might be
&gt; causing fragmentation of the merge index so I've been regularly
&gt; running forced compaction as documented here:
&gt; https://gist.github.com/rzezeski/3996286.
&gt;
&gt; This has helped somewhat but I suspect it has reached the limits of
&gt; what can be done so I wonder if there is not further fragmentation
&gt; elsewhere that is not being compacted.
&gt;
&gt; Could this be an issue? How can I tell whether merge indexes or
&gt; something else needs compaction/attention?
&gt;
&gt; Our nodes were initially configured to run with the default settings
&gt; for the bitcask backend but when this all started I switched to the
&gt; following to try and see if I can trigger compaction more frequently:
&gt;
&gt; {bitcask, [
&gt; %% Configure how Bitcask writes data to disk.
&gt; %% erlang: Erlang's built-in file API
&gt; %% nif: Direct calls to the POSIX C API
&gt; %%
&gt; %% The NIF mode provides higher throughput for certain
&gt; %% workloads, but has the potential to negatively impact
&gt; %% the Erlang VM, leading to higher worst-case latencies
&gt; %% and possible throughput collapse.
&gt; {io\\_mode, erlang},
&gt;
&gt; {data\\_root, "/var/lib/riak/bitcask"},
&gt;
&gt; {frag\\_merge\\_trigger, 40}, %% trigger merge if
&gt; framentation is &gt; 40% default is 60%
&gt; {dead\\_bytes\\_merge\\_trigger, 67108864}, %% trigger if dead
&gt; bytes for keys &gt; 64MB default is 512MB
&gt; {frag\\_threshold, 20}, %% framentation &gt;= 20% default is 40
&gt; {dead\\_bytes\\_threshold, 67108864} %% trigger if dead bytes
&gt; for data &gt; 64MB default is 128MB
&gt; ]},
&gt;
&gt; From my observations this change did not make much of a difference.
&gt;
&gt; The data we're inserting is hierarchical JSON data that roughly falls
&gt; into the following size (in bytes) profile:
&gt;
&gt; Max: 10320
&gt; Min: 1981
&gt; Avg: 3707
&gt; Med: 2905
&gt;
&gt; --
&gt; Ciao
&gt;
&gt; Charl
&gt;
&gt; "I will either find a way, or make one." -- Hannibal
&gt;
&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;
&gt;
&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;

\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

