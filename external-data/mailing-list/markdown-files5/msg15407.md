---
title: "Re: Weird RIAK behavior"
description: ""
project: community
lastmod: 2014-12-23T01:01:04-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg15407"
mailinglist_parent_id: "msg15404"
author_name: "Sargun Dhillon"
project_section: "mailinglistitem"
sent_date: 2014-12-23T01:01:04-08:00
---


It sounds like allow\\_mult is off.
1. What's your last\\_write\\_wins set to?
2. Are the clocks on your nodes accurately synced to one another (if
they're all NTP peers from one another, what's the deltas?)? I know
drift is a common problem in virtual machines on VMWare -- or clocks
plain lying.
3. Are you using vector clocks for all operations?
4. Are you making concurrent updates?

Are you familiar with AP Riak, the consistency model, and conflict
resolution? There is some excellent documentation here:
http://docs.basho.com/riak/2.0.2/dev/using/conflict-resolution/



On Mon, Dec 22, 2014 at 7:15 PM, Claudio Cesar Sanchez Tejeda
 wrote:
&gt; Hi,
&gt;
&gt; Sorry, I forgot to mention that it is RIAK 1.4.10. They are configured
&gt; with multibackend. We are using, memory, bitcask and elevelDB
&gt; backends.
&gt;
&gt; On the buckets where we are having issues, the siblings are disabled
&gt; (allow\\_multi = false).
&gt;
&gt; Regards.
&gt;
&gt; On Mon, Dec 22, 2014 at 9:17 PM, Sargun Dhillon  wrote:
&gt;&gt; What versions of Riak are you using? And are these CRDT sets?
&gt;&gt;
&gt;&gt; Sent from my iPhone
&gt;&gt;
&gt;&gt;&gt; On Dec 22, 2014, at 16:04, Claudio Cesar Sanchez Tejeda 
&gt;&gt;&gt;  wrote:
&gt;&gt;&gt;
&gt;&gt;&gt; I'm a sysadmin and I managing 5 cluster of RIAK:
&gt;&gt;&gt;
&gt;&gt;&gt; - two of them are LXC containers on the same physical machine (3 nodes
&gt;&gt;&gt; per cluster)
&gt;&gt;&gt; - one of them are LXC containers located on different physical
&gt;&gt;&gt; machines (6 nodes)
&gt;&gt;&gt; - one of them are LXC containers located on different physical
&gt;&gt;&gt; machines and XEN VMs (6 nodes)
&gt;&gt;&gt; - and the last of them are VMware ESX VMs (3 nodes)
&gt;&gt;&gt;
&gt;&gt;&gt; Our application works correctly on the first four clusters, but it
&gt;&gt;&gt; doesn't work as we expected on the last one.
&gt;&gt;&gt;
&gt;&gt;&gt; When we update a key and we retrieve this key in order to write it
&gt;&gt;&gt; again, it has an old value (it doesn't have the first value that we
&gt;&gt;&gt; wrote), for example:
&gt;&gt;&gt;
&gt;&gt;&gt; The key has: lalala
&gt;&gt;&gt; We retrieve the key, and add lololo, so it should be lalala,lololo
&gt;&gt;&gt; We retrieve the key again, and try to add lelele, so it should be now:
&gt;&gt;&gt; lalala,lololo,lelele, but when we retrieve it again, we only have:
&gt;&gt;&gt; lalala,lelele
&gt;&gt;&gt;
&gt;&gt;&gt; In the second write action, when we retrieve the key, we obtained a
&gt;&gt;&gt; key with the old value. We set r, w, pr and rw to 3 to the REST
&gt;&gt;&gt; requests, but it doesn't help.
&gt;&gt;&gt;
&gt;&gt;&gt; All the configuration files are very similiar and we don't have any
&gt;&gt;&gt; major differences in the disk I/O and network performance of the nodes
&gt;&gt;&gt; of the clusters.
&gt;&gt;&gt;
&gt;&gt;&gt; Did anyone have a similar issue?
&gt;&gt;&gt;
&gt;&gt;&gt; Regards.
&gt;&gt;&gt;
&gt;&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

