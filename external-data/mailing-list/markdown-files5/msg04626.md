---
title: "Re: Riak Clustering Changes in 1.0"
description: ""
project: community
lastmod: 2011-09-08T08:42:41-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg04626"
mailinglist_parent_id: "msg04611"
author_name: "Joseph Blomstedt"
project_section: "mailinglistitem"
sent_date: 2011-09-08T08:42:41-07:00
---


&gt; Out of curiousity, what was the reason for the 'join' command behaviour to
&gt; change?

1. Existing bugs/limitations. For example, joining two entire clusters
together was not an entirely safe operation. In some cases, the newly
formed cluster would not correctly converge, leaving the ring/cluster
in flux. Likewise, we realized that many users were often joining two
clusters together by accident and would prefer additional safety. In
particular, joining two clusters together with overlapping data but no
common vector clock relationship could result in data loss as
unintended siblings were reconciled.

2. It was necessary consequence of how the new cluster code works. In
the new cluster, the cluster state / ring is only ever mutated by a
single node at a time. This is done by having a cluster-wide claimant,
as mentioned in my original email. Given the claimant approach, all
cluster state / ring changes are totally ordered. When a new node
joins an existing cluster, it throws away it's existing ring and
replaces it with a copy of the ring from the target cluster, thus
joining into the same cluster history. If you were to join two
clusters together, we would need to deterministically merge two
independent cluster histories and elect a single new claimant for the
new cluster. This is easy in cases where there are no node failures or
net-splits during joining, but less trivial when there are errors. The
entire new cluster code was heavily modeled before implementation, and
in the modeling work several corner cases related to failures were
found that were hard to address in a cluster/cluster join but easy to
fix in a node/cluster join. Thus, I went with the simple and correct
approach.

-Joe

-- 
Joseph Blomstedt 
Software Engineer
Basho Technologies, Inc.
http://www.basho.com/

On Thu, Sep 8, 2011 at 5:19 AM, Jens Rantil  wrote:
&gt; Out of curiousity, what was the reason for the 'join' command behaviour to
&gt; change?
&gt;
&gt;
&gt;
&gt; Regards,
&gt;
&gt; Jens
&gt;
&gt;
&gt;
&gt; -----------------------------------------------------------
&gt;
&gt; Date: Wed, 7 Sep 2011 18:12:40 -0600
&gt;
&gt; From: Joseph Blomstedt 
&gt;
&gt; To: riak-users Users 
&gt;
&gt; Subject: Riak Clustering Changes in 1.0
&gt;
&gt; Message-ID:
&gt;
&gt;
&gt; 
&gt;
&gt; Content-Type: text/plain; charset=ISO-8859-1
&gt;
&gt;
&gt;
&gt; Given that 1.0 prerelease packages are now available, I wanted to
&gt;
&gt; mention some changes to Riak's clustering capabilities in 1.0. In
&gt;
&gt; particular, there are some subtle semantic differences in the
&gt;
&gt; riak-admin commands. More complete docs will be updated in the near
&gt;
&gt; future, but I hope a quick email suffices for now.
&gt;
&gt;
&gt;
&gt; [nodeB/riak-admin join nodeA] is now strictly one-way. It joins nodeB
&gt;
&gt; to the cluster that nodeA is a member of. This is semantically
&gt;
&gt; different than pre-1.0 Riak in which join essentially joined clusters
&gt;
&gt; together rather than joined a node to a cluster. As part of this
&gt;
&gt; change, the joining node (nodeB in this case) must be a singleton
&gt;
&gt; (1-node) cluster.
&gt;
&gt;
&gt;
&gt; In pre-1.0, leave and remove were essentially the same operation, with
&gt;
&gt; leave just being an alias for 'remove this-node'. This has changed.
&gt;
&gt; Leave and remove are now very different operations.
&gt;
&gt;
&gt;
&gt; [nodeB/riak-admin leave] is the only safe way to have a node leave the
&gt;
&gt; cluster, and it must be executed by the node that you want to remove.
&gt;
&gt; In this case, nodeB will start leaving the cluster, and will not leave
&gt;
&gt; the cluster until after it has handed off all its data. Even if nodeB
&gt;
&gt; is restarted (crashed/shutdown/whatever), it will remain in the leave
&gt;
&gt; state and continue handing off partitions until done. After handoff,
&gt;
&gt; it will leave the cluster, and eventually shutdown.
&gt;
&gt;
&gt;
&gt; [nodeA/riak-admin remove nodeB] immediately removes nodeB from the
&gt;
&gt; cluster, without handing off its data. All replicas held by nodeB are
&gt;
&gt; therefore lost, and will need to be re-generated through read-repair.
&gt;
&gt; Use this command carefully. It's intended for nodes that are
&gt;
&gt; permanently unrecoverable and therefore for which handoff doesn't make
&gt;
&gt; sense. By the final 1.0 release, this command may be renamed
&gt;
&gt; "force-remove" just to make the distinction clear.
&gt;
&gt;
&gt;
&gt; There are now two new commands that provide additional insight into
&gt;
&gt; the cluster. [riak-admin member\\_status] and [riak-admin ring\\_status].
&gt;
&gt;
&gt;
&gt; Underneath, the clustering protocol has been mostly re-written. The
&gt;
&gt; new approach has the following advantages:
&gt;
&gt; 1. It is no longer necessary to wait on [riak-admin ringready] in
&gt;
&gt; between adding/removing nodes from the cluster, and adding/removing is
&gt;
&gt; also much more sound/graceful. Starting up 16 nodes and issuing
&gt;
&gt; [nodeX: riak-admin join node1] for X=1:16 should just work.
&gt;
&gt;
&gt;
&gt; 2. Data is first transferred to new partition owners before handing
&gt;
&gt; over partition ownership. This change fixes numerous bugs, such as
&gt;
&gt; 404s/not\\_founds during ownership changes. The Ring/Pending columns in
&gt;
&gt; [riak-admin member\\_status] visualize this at a high-level, and the
&gt;
&gt; full transfer status in [riak-admin ring\\_status] provide additional
&gt;
&gt; insight.
&gt;
&gt;
&gt;
&gt; 3. All partition ownership decisions are now made by a single node in
&gt;
&gt; the cluster (the claimant). Any node can be the claimant, and the duty
&gt;
&gt; is automatically taken over if the previous claimant is removed from
&gt;
&gt; the cluster. [riak-admin member\\_status] will list the current
&gt;
&gt; claimant.
&gt;
&gt;
&gt;
&gt; 4. Handoff related to ownership changes can now occur under load;
&gt;
&gt; hinted handoff still only occurs when a vnode is inactive. This change
&gt;
&gt; allows a cluster to scale up/down under load, although this needs to
&gt;
&gt; be further benchmarked and tuned before 1.0.
&gt;
&gt;
&gt;
&gt; To support all of the above, a new limitation has been introduced.
&gt;
&gt; Cluster changes (member addition/removal, ring rebalance, etc) can
&gt;
&gt; only occur when all nodes are up and reachable. [riak-admin
&gt;
&gt; ring\\_status] will complain when this is not the case. If a node is
&gt;
&gt; down, you must issue [riak-admin down ] to mark the node as
&gt;
&gt; down, and the remaining nodes will then proceed to converge as usual.
&gt;
&gt; Once the down node comes back online, it will automatically
&gt;
&gt; re-integrate into the cluster. However, there is nothing preventing
&gt;
&gt; client requests being served by a down node before it re-integrates.
&gt;
&gt; Before issuing [down ], make sure to update your load balancers
&gt;
&gt; / connection pools to not include this node. Future releases of Riak
&gt;
&gt; may make offlining a node an automatic operation, but it's a
&gt;
&gt; user-initiated action in 1.0.
&gt;
&gt;
&gt;
&gt; -Joe
&gt;
&gt;
&gt;
&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;
&gt;

\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

