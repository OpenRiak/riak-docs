---
title: "Re: Issues with search (2.0)"
description: ""
project: community
lastmod: 2014-08-11T20:48:07-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg14645"
mailinglist_parent_id: "msg14643"
author_name: "Chaim Solomon"
project_section: "mailinglistitem"
sent_date: 2014-08-11T20:48:07-07:00
---


The nodes have 8G - so well more then the recommended value.
The configuration was at the default of 1G - I now changed it to 2G.

Chaim Solomon



On Mon, Aug 11, 2014 at 6:14 PM, Eric Redmond  wrote:

&gt; If Solr is stumbling over bad data, your node's solr.log should be filled
&gt; up. If Yokozuna is stumbling over bad data that it's trying to send Solr in
&gt; a loop, the console.log should be full. If yokozuna is going ahead and
&gt; indexing bad values (such as unparsable json), it will go ahead and index a
&gt; blank object with \\_yz\\_err (just search for existence). If you have a case
&gt; of sibling explosion, you'll have many duplicates of the same object with
&gt; different \\_yz\\_vtag fields (again search for existence).
&gt;
&gt; You said it's not a resource issue, but just to rule that out, how much
&gt; RAM does each node have? Also, how much is made available to Solr? You can
&gt; adjust the max heap size given to Solr in riak.conf, by
&gt; changing search.solr.jvm\\_options max heap size values from -Xmx1g to -Xmx2g
&gt; or more.
&gt;
&gt; Eric
&gt;
&gt;
&gt; On Aug 11, 2014, at 8:03 AM, Chaim Solomon 
&gt; wrote:
&gt;
&gt; Hi,
&gt;
&gt; I don't think that it is a resource issue now.
&gt;
&gt; After removing the data, the other nodes had low load and are handling the
&gt; workload just fine.
&gt; And the Java process - when it crashed - was really dead, on shutting down
&gt; Riak it stayed around and needed a -9 to go away.
&gt;
&gt; I don't think the disks are a problem but rather suspect that a crash may
&gt; have caused Solr to stumble over bad data and then crash.
&gt;
&gt; Chaim Solomon
&gt;
&gt;
&gt;
&gt; On Mon, Aug 11, 2014 at 5:47 PM, Jordan West  wrote:
&gt;
&gt;&gt; Chaim,
&gt;&gt;
&gt;&gt; Some comments inline:
&gt;&gt;
&gt;&gt; On Mon, Aug 11, 2014 at 4:14 AM, Chaim Solomon &lt;
&gt;&gt; ch...@itcentralstation.com&gt; wrote:
&gt;&gt;
&gt;&gt;&gt; Hi,
&gt;&gt;&gt;
&gt;&gt;&gt; I've been running into an issue with the yz search acting up.
&gt;&gt;&gt;
&gt;&gt;&gt; I've been getting a lot of these:
&gt;&gt;&gt;
&gt;&gt;&gt; 2014-08-11 06:45:22.005 [error] &lt;0.913.0&gt;@yz\\_kv:index:206 failed to
&gt;&gt;&gt; index object {&lt;&lt;"bucketname"&gt;&gt;,&lt;&lt;"123"&gt;&gt;} with error {"Failed to index
&gt;&gt;&gt; docs",{error,req\\_timedout}} because [{yz\\_solr,index,3,[{file,"s
&gt;&gt;&gt;
&gt;&gt;&gt; rc/yz\\_solr.erl"},{line,192}]},{yz\\_kv,index,7,[{file,"src/yz\\_kv.erl"},{line,258}]},{yz\\_kv,index,3,[{file,
&gt;&gt;&gt;
&gt;&gt;&gt; "src/yz\\_kv.erl"},{line,193}]},{riak\\_kv\\_vnode,actual\\_put,6,[{file,"src/riak\\_kv\\_vnode.erl"},{line,1416}]},
&gt;&gt;&gt;
&gt;&gt;&gt; {riak\\_kv\\_vnode,perform\\_put,3,[{file,"src/riak\\_kv\\_vnode.erl"},{line,1404}]},{riak\\_kv\\_vnode,do\\_put,7,[{fil
&gt;&gt;&gt;
&gt;&gt;&gt; e,"src/riak\\_kv\\_vnode.erl"},{line,1199}]},{riak\\_kv\\_vnode,handle\\_command,3,[{file,"src/riak\\_kv\\_vnode.erl"}
&gt;&gt;&gt;
&gt;&gt;&gt; ,{line,485}]},{riak\\_core\\_vnode,vnode\\_command,3,[{file,"src/riak\\_core\\_vnode.erl"},{line,345}]}]
&gt;&gt;&gt;
&gt;&gt;&gt; and the Java process uses a lot of CPU and eventually runs out of memory
&gt;&gt;&gt; or something like that and gets stuck. Killing the process gets the cluster
&gt;&gt;&gt; back up and running.
&gt;&gt;&gt;
&gt;&gt;&gt; I am guessing that it may be data corruption on the yz data on one node.
&gt;&gt;&gt;
&gt;&gt;&gt; Clearing away the yz data on that node and restarting riak makes the
&gt;&gt;&gt; system work again - and I guess AAE will rebuild the index.
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt; This sounds very similar to the issue last week. I would certainly like
&gt;&gt; to rule out any sort of data corruption (are you thinking your disks are
&gt;&gt; corrupting the data or are you assuming Solr is?).
&gt;&gt;
&gt;&gt; However, it is also possible, like the last issue, that the node/cluster
&gt;&gt; simply does not have enough memory. When you delete the data Solr no longer
&gt;&gt; has anything to cache in-memory thus using significantly less. As
&gt;&gt; discussed, the recommended minimum
&gt;&gt;
&gt;&gt;
&gt;&gt;&gt; But I'm wondering why a crashing Java on one node practically takes down
&gt;&gt;&gt; the search on the cluster. Shouldn't Riak be more resilient than that?
&gt;&gt;&gt;
&gt;&gt;
&gt;&gt; The hard part here is, at least initially, the Java process doesn't
&gt;&gt; crash, it just starts to timeout. In distributed systems a slow-node is
&gt;&gt; often worse than a down node. Riak, prior to 1.4 had something called
&gt;&gt; "health check" that would mark a node down in this situation. Unfortunately
&gt;&gt; in some workloads, and I believe given your cluster's limited resources it
&gt;&gt; would happen here, this often results in excessive work being offloaded to
&gt;&gt; another node, which also does not have sufficient resources and around we
&gt;&gt; go until the entire cluster falls over. A capacity problem, typically, can
&gt;&gt; only be solved by adding more capacity.
&gt;&gt;
&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; Is there a explicit reindex command for the full text search subsystem?
&gt;&gt;&gt;
&gt;&gt;&gt; Could Riak keep an eye on the java process and restart it if it crashes
&gt;&gt;&gt; or runs away?
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt; Riak does manage the JVM process (starting/stopping/restarting) .I agree
&gt;&gt; that if we could include run-away process, like in your case, that would be
&gt;&gt; even better. I would have to think a bit more about how this would work (to
&gt;&gt; prevent the same problems mentioned above with the old-style health check)
&gt;&gt;
&gt;&gt; Jordan
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt;&gt; Chaim Solomon
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;
&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;
&gt;
&gt;
\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

