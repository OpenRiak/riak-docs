---
title: "Re: Rebalancing issue -- failure to hand-off partitions"
description: ""
project: community
lastmod: 2013-03-29T15:31:41-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg10626"
author_name: "Giri Iyengar"
project_section: "mailinglistitem"
sent_date: 2013-03-29T15:31:41-07:00
---


Evan,

As recommended by you, I disabled the TTL on the memory backends and did a
rolling restart of the cluster. Now, things are rebalancing quite nicely.
Do you think I can turn the TTL back on once the rebalancing completes? I'd
like to ensure that the vnodes in memory don't keep growing forever.

-giri

On Thu, Mar 28, 2013 at 6:50 PM, Giri Iyengar wrote:

&gt; Evan,
&gt;
&gt; This has been happening for a while now (about 3.5 weeks now), even prior
&gt; to our upgrade to 1.3.
&gt;
&gt; -giri
&gt;
&gt; On Thu, Mar 28, 2013 at 6:36 PM, Evan Vigil-McClanahan &lt;
&gt; emcclana...@basho.com&gt; wrote:
&gt;
&gt;&gt; No. AAE is unrelated to the handoff subsystem. I am not familiar
&gt;&gt; enough with the lowest level of it's working to know if it'd reproduce
&gt;&gt; the TTL stuff across on nodes that don't have it.
&gt;&gt;
&gt;&gt; I am not totally sure about your timeline here.
&gt;&gt;
&gt;&gt; When did you start seeing these errors, before or after your upgrade
&gt;&gt; to 1.3? When did you start your cluster transition? What cluster
&gt;&gt; transitions have you initiated?
&gt;&gt;
&gt;&gt; If these errors started with 1.3, an interesting experiment would be
&gt;&gt; to disable AAE and do a rolling restart of the cluster, which should
&gt;&gt; lead to empty memory backends that won't be populated by AAE with
&gt;&gt; anything suspicious. That said: if you've had cluster balance
&gt;&gt; problems for a while, it's possible that these messages (even this
&gt;&gt; whole issue) is just masking some other problem.
&gt;&gt;
&gt;&gt; On Thu, Mar 28, 2013 at 3:24 PM, Giri Iyengar
&gt;&gt;  wrote:
&gt;&gt; &gt; Evan,
&gt;&gt; &gt;
&gt;&gt; &gt; All nodes have been restarted (more than once, in fact) after the config
&gt;&gt; &gt; changes. Using riak-admin aae-status, I noticed that the anti-entropy
&gt;&gt; repair
&gt;&gt; &gt; is still proceeding across the cluster.
&gt;&gt; &gt; It has been less than 24 hours since I upgraded to 1.3 and maybe I have
&gt;&gt; to
&gt;&gt; &gt; wait till the first complete build of the index trees happens for the
&gt;&gt; &gt; cluster to start rebalancing itself.
&gt;&gt; &gt; Could that be the case?
&gt;&gt; &gt;
&gt;&gt; &gt; -giri
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt; &gt; On Thu, Mar 28, 2013 at 5:49 PM, Evan Vigil-McClanahan
&gt;&gt; &gt;  wrote:
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; Giri,
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; if all of the nodes are using identical app.config files (including
&gt;&gt; &gt;&gt; the joining node) and have been restarted since those files changed,
&gt;&gt; &gt;&gt; it may be some other, related issue.
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; On Thu, Mar 28, 2013 at 2:46 PM, Giri Iyengar
&gt;&gt; &gt;&gt;  wrote:
&gt;&gt; &gt;&gt; &gt; Evan,
&gt;&gt; &gt;&gt; &gt;
&gt;&gt; &gt;&gt; &gt; I reconfirmed that all the servers are using identical app.configs.
&gt;&gt; They
&gt;&gt; &gt;&gt; &gt; all
&gt;&gt; &gt;&gt; &gt; use multi-backend schema. Are you saying that some of the vnodes are
&gt;&gt; in
&gt;&gt; &gt;&gt; &gt; memory backend in one physical node and in eleveldb backend in
&gt;&gt; another
&gt;&gt; &gt;&gt; &gt; physical node?
&gt;&gt; &gt;&gt; &gt; If so, how can I fix the offending vnodes?
&gt;&gt; &gt;&gt; &gt;
&gt;&gt; &gt;&gt; &gt; Thanks,
&gt;&gt; &gt;&gt; &gt;
&gt;&gt; &gt;&gt; &gt; -giri
&gt;&gt; &gt;&gt; &gt;
&gt;&gt; &gt;&gt; &gt; On Thu, Mar 28, 2013 at 5:18 PM, Evan Vigil-McClanahan
&gt;&gt; &gt;&gt; &gt;  wrote:
&gt;&gt; &gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; it would if some of the nodes weren't migrated to the new
&gt;&gt; &gt;&gt; &gt;&gt; multi-backend schema; if a memory node was trying to hand off to a
&gt;&gt; &gt;&gt; &gt;&gt; eleveldb backed node, you'd see this.
&gt;&gt; &gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; On Thu, Mar 28, 2013 at 2:05 PM, Giri Iyengar
&gt;&gt; &gt;&gt; &gt;&gt;  wrote:
&gt;&gt; &gt;&gt; &gt;&gt; &gt; Evan,
&gt;&gt; &gt;&gt; &gt;&gt; &gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt; I verified that all of the memory backends have the same ttl
&gt;&gt; settings
&gt;&gt; &gt;&gt; &gt;&gt; &gt; and
&gt;&gt; &gt;&gt; &gt;&gt; &gt; have done rolling restarts but it doesn't seem to make a
&gt;&gt; difference.
&gt;&gt; &gt;&gt; &gt;&gt; &gt; One
&gt;&gt; &gt;&gt; &gt;&gt; &gt; thing to note though -- I remember this problem starting roughly
&gt;&gt; &gt;&gt; &gt;&gt; &gt; around
&gt;&gt; &gt;&gt; &gt;&gt; &gt; the
&gt;&gt; &gt;&gt; &gt;&gt; &gt; time I migrated a bucket from being backed by leveldb to being
&gt;&gt; backed
&gt;&gt; &gt;&gt; &gt;&gt; &gt; by
&gt;&gt; &gt;&gt; &gt;&gt; &gt; memory. I did this by setting the bucket properties via curl and
&gt;&gt; let
&gt;&gt; &gt;&gt; &gt;&gt; &gt; Riak do
&gt;&gt; &gt;&gt; &gt;&gt; &gt; the migration of the objects in that bucket. Would that cause such
&gt;&gt; &gt;&gt; &gt;&gt; &gt; issues?
&gt;&gt; &gt;&gt; &gt;&gt; &gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt; Thanks for your help.
&gt;&gt; &gt;&gt; &gt;&gt; &gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt; -giri
&gt;&gt; &gt;&gt; &gt;&gt; &gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt; On Thu, Mar 28, 2013 at 4:55 PM, Evan Vigil-McClanahan
&gt;&gt; &gt;&gt; &gt;&gt; &gt;  wrote:
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Giri, I've seen similar issues in the past when someone was
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; adjusting
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; their ttl setting on the memory backend. Because one memory
&gt;&gt; backend
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; has it and the other does not, it fails on handoff. The
&gt;&gt; solution
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; then was to make sure that all memory backend settings are the
&gt;&gt; same
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; and then do a rolling restart of the cluster (ignoring a lot of
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; errors
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; along the way). I am not sure that this is applicable to your
&gt;&gt; case,
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; but it's something to look at.
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; On Thu, Mar 28, 2013 at 10:22 AM, Giri Iyengar
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;  wrote:
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; Godefroy:
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; Thanks. Your email exchange on the mailing list was what
&gt;&gt; prompted
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; me
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; to
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; consider switching to Riak 1.3. I do see repair messages in the
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; console
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; logs
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; and so some healing is happening. However, there are a bunch of
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; hinted
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; handoffs and ownership handoffs that are simply not proceeding
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; because
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; the
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; same vnodes keep coming up for transfer and fail. Perhaps
&gt;&gt; there is
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; a
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; manual
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; way to forcibly repair and push the vnodes around.
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; -giri
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; On Thu, Mar 28, 2013 at 1:19 PM, Godefroy de Compreignac
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;  wrote:
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; I have exactly the same problem with my cluster. If anyone
&gt;&gt; knows
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; what
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; those errors mean... :-)
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Godefroy
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 2013/3/28 Giri Iyengar 
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; Hello,
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; We are running a 6-node Riak 1.3.0 cluster in production. We
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; recently
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; upgraded to 1.3. Prior to this, we were running Riak 1.2 on
&gt;&gt; the
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; same
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; 6-node
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; cluster.
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; We are finding that the nodes are not balanced. For instance:
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; ================================= Membership
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; ==================================
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; Status Ring Pending Node
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; -------------------------------------------------------------------------------
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; valid 0.0% 0.0% 'riak@172.16.25.106'
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; valid 34.4% 20.3% 'riak@172.16.25.107'
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; valid 21.9% 20.3% 'riak@172.16.25.113'
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; valid 19.5% 20.3% 'riak@172.16.25.114'
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; valid 8.6% 19.5% 'riak@172.16.25.121'
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; valid 15.6% 19.5% 'riak@172.16.25.122'
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; -------------------------------------------------------------------------------
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; Valid:6 / Leaving:0 / Exiting:0 / Joining:0 / Down:0
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; When we look at the logs in the largest node
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; (riak@172.16.25.107),
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; we
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; see
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; error messages that look like this:
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; 2013-03-28 13:04:16.957 [error]
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &lt;0.10957.1462&gt;@riak\\_core\\_handoff\\_sender:start\\_fold:226
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; hinted\\_handoff
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; transfer of riak\\_kv\\_vnode from 'riak@172.16.25.107'
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; 148433760041419827630061740822747494183805648896 to
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; 'riak@172.16.25.121'
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; 148433760041419827630061740822747494183805648896 failed
&gt;&gt; because
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; of
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; error:{badmatch,{error,{worker\\_crash,{function\\_clause,[{riak\\_core\\_pb,encode,[{ts,{1364,476737,222223}},{{ts,{1364,476737,222223}},&lt;&lt;131,104,7,100,0,8,114,95,111,98,106,101,99,116,109,0,0,0,11,69,78,84,73,84,89,95,83,69,83,83,109,0,0,0,36,67,54,57,95,48,48,51,56,100,56,102,50,52,49,52,99,97,97,54,102,99,52,56,53,52,99,99,101,51,98,50,48,102,53,98,52,108,0,0,0,1,104,3,100,0,9,114,95,99,111,110,116,101,110,116,104,9,100,0,4,100,105,99,116,97,5,97,16,97,16,97,8,97,80,97,48,104,16,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,104,1,104,16,106,106,106,106,106,106,106,106,106,106,108,0,0,0,2,108,0,0,0,11,109,0,0,0,12,99,111,110,116,101,110,116,45,116,121,112,101,97,116,97,101,97,120,97,116,97,47,97,112,97,108,97,97,97,105,97,110,106,108,0,0,0,23,109,0,0,0,11,88,45,82,105,97,107,45,86,84,97,103,97,51,97,120,97,105,97,101,97,120,97,66,97,120,97,107,97,119,97,101,97,75,97,117,97,122,97,111,97,55,97,85,97,104,97,85,97,107,97,112,97,120,97,107,106,106,108,0,0,0,1,108,0,0,0,1,109,0,0,0,5,105,110,100,101,120,106,106,106,108,0,0,0,1,108,0,0,0,1,109,0,0,0,20,88,45,82,105,97,107,45,76,97,115,116,45,77,111,100,105,102,105,101,100,104,3,98,0,0,5,84,98,0,7,70,65,98,0,3,99,115,106,106,108,0,0,0,1,108,0,0,0,6,109,0,0,0,7,99,104,97,114,115,101,116,97,85,97,84,97,70,97,45,97,56,106,106,109,0,0,0,36,52,54,55,98,54,51,98,50,45,50,99,56,52,45,52,56,50,99,45,97,48,99,54,45,56,53,50,100,53,99,57,97,98,98,53,101,106,108,0,0,0,1,104,2,109,0,0,0,8,0,69,155,215,81,84,63,31,104,2,97,1,110,5,0,65,191,200,202,14,106,104,9,100,0,4,100,105,99,116,97,1,97,16,97,16,97,8,97,80,97,48,104,16,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,104,1,104,16,106,106,106,106,106,106,106,106,106,106,106,106,106,106,108,0,0,0,1,108,0,0,0,1,100,0,5,99,108,101,97,110,100,0,4,116,114,117,101,106,106,100,0,9,117,110,100,101,102,105,110,101,100&gt;&gt;}],[{file,"src/riak\\_core\\_pb.erl"},{line,40}]},{riak\\_core\\_pb,pack,5,...},...]},...}}}
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; [{riak\\_core\\_handoff\\_sender,start\\_fold,5,[{file,"src/riak\\_core\\_handoff\\_sender.erl"},{line,170}]}]
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; 2013-03-28 13:04:16.961 [error] &lt;0.29352.909&gt; CRASH REPORT
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; Process
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &lt;0.29352.909&gt; with 0 neighbours exited with reason: no
&gt;&gt; function
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; clause
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; matching riak\\_core\\_pb:encode({ts,{1364,476737,222223}},
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; {{ts,{1364,476737,222223}},&lt;&lt;131,104,7,100,0,8,114,95,111,98,106,101,99,116,109,0,0,0,11,69,78,...&gt;&gt;})
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; line 40 in gen\\_server:terminate/6 line 747
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; 2013-03-28 13:04:13.888 [error]
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &lt;0.12680.1435&gt;@riak\\_core\\_handoff\\_sender:start\\_fold:226
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; ownership\\_handoff
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; transfer of riak\\_kv\\_vnode from 'riak@172.16.25.107'
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; 11417981541647679048466287755595961091061972992 to
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; 'riak@172.16.25.113'
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; 11417981541647679048466287755595961091061972992 failed
&gt;&gt; because
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; of
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; error:{badmatch,{error,{worker\\_crash,{function\\_clause,[{riak\\_core\\_pb,encode,[{ts,{1364,458917,232318}},{{ts,{1364,458917,232318}},&lt;&lt;131,104,7,100,0,8,114,95,111,98,106,101,99,116,109,0,0,0,11,69,78,84,73,84,89,95,83,69,83,83,109,0,0,0,36,67,54,57,95,48,48,48,54,52,98,99,52,53,51,49,52,55,101,50,101,53,97,102,101,102,49,57,99,50,55,99,97,49,53,54,99,108,0,0,0,1,104,3,100,0,9,114,95,99,111,110,116,101,110,116,104,9,100,0,4,100,105,99,116,97,5,97,16,97,16,97,8,97,80,97,48,104,16,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,104,1,104,16,106,106,106,106,106,106,106,106,106,106,108,0,0,0,2,108,0,0,0,11,109,0,0,0,12,99,111,110,116,101,110,116,45,116,121,112,101,97,116,97,101,97,120,97,116,97,47,97,112,97,108,97,97,97,105,97,110,106,108,0,0,0,23,109,0,0,0,11,88,45,82,105,97,107,45,86,84,97,103,97,54,97,88,97,76,97,66,97,69,97,69,97,116,97,73,97,104,97,118,97,77,97,86,97,48,97,81,97,103,97,110,97,119,97,73,97,51,97,85,97,72,97,53,106,106,108,0,0,0,1,108,0,0,0,1,109,0,0,0,5,105,110,100,101,120,106,106,106,108,0,0,0,1,108,0,0,0,1,109,0,0,0,20,88,45,82,105,97,107,45,76,97,115,116,45,77,111,100,105,102,105,101,100,104,3,98,0,0,5,84,98,0,7,0,165,98,0,3,138,179,106,106,108,0,0,0,1,108,0,0,0,6,109,0,0,0,7,99,104,97,114,115,101,116,97,85,97,84,97,70,97,45,97,56,106,106,109,0,0,0,36,55,102,98,52,50,54,54,53,45,57,100,56,48,45,52,54,98,97,45,98,53,97,100,45,56,55,52,52,54,54,97,97,50,56,53,99,106,108,0,0,0,1,104,2,109,0,0,0,8,0,69,155,215,81,59,179,219,104,2,97,1,110,5,0,165,121,200,202,14,106,104,9,100,0,4,100,105,99,116,97,1,97,16,97,16,97,8,97,80,97,48,104,16,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,104,1,104,16,106,106,106,106,106,106,106,106,106,106,106,106,106,106,108,0,0,0,1,108,0,0,0,1,100,0,5,99,108,101,97,110,100,0,4,116,114,117,101,106,106,100,0,9,117,110,100,101,102,105,110,101,100&gt;&gt;}],[{file,"src/riak\\_core\\_pb.erl"},{line,40}]},{riak\\_core\\_pb,pack,5,[{...},...]},...]},...}}}
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; [{riak\\_core\\_handoff\\_sender,start\\_fold,5,[{file,"src/riak\\_core\\_handoff\\_sender.erl"},{line,170}]}]
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; 2013-03-28 13:04:14.255 [error] &lt;0.1120.0&gt; CRASH REPORT
&gt;&gt; Process
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &lt;0.1120.0&gt; with 0 neighbours exited with reason: no function
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; clause
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; matching
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; riak\\_core\\_pb:encode({ts,{1364,458917,232318}},
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; {{ts,{1364,458917,232318}},&lt;&lt;131,104,7,100,0,8,114,95,111,98,106,101,99,116,109,0,0,0,11,69,78,...&gt;&gt;})
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; line 40 in gen\\_server:terminate/6 line 747
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; This has been going on for days and the cluster doesn't seem
&gt;&gt; to
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; be
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; rebalancing itself. We see this issue with both
&gt;&gt; hinted\\_handoffs
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; and
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; ownership\\_handoffs. Looks like we have some corrupt data in
&gt;&gt; our
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; cluster. I
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; checked through the leveldb LOGs and did not see any
&gt;&gt; compaction
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; errors.
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; I was hoping that upgrading to 1.3.0 will slowly start
&gt;&gt; repairing
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; the
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; cluster. However, that doesn't seem to be happening.
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; Any help/hints would be much appreciated.
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; -giri
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; --
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; GIRI IYENGAR, CTO
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; SOCIOCAST
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; Simple. Powerful. Predictions.
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; 36 WEST 25TH STREET, 7TH FLOOR NEW YORK CITY, NY 10010
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; O: 917.525.2466x104 M: 914.924.7935 F: 347.943.6281
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; E: giri.iyen...@sociocast.com W: www.sociocast.com
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; Facebook's Ad Guru Joins Sociocast - http://bit.ly/NjPQBQ
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; riak-users mailing list
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; --
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; GIRI IYENGAR, CTO
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; SOCIOCAST
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; Simple. Powerful. Predictions.
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; 36 WEST 25TH STREET, 7TH FLOOR NEW YORK CITY, NY 10010
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; O: 917.525.2466x104 M: 914.924.7935 F: 347.943.6281
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; E: giri.iyen...@sociocast.com W: www.sociocast.com
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; Facebook's Ad Guru Joins Sociocast - http://bit.ly/NjPQBQ
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; riak-users mailing list
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; riak-users@lists.basho.com
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;
&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt; --
&gt;&gt; &gt;&gt; &gt;&gt; &gt; GIRI IYENGAR, CTO
&gt;&gt; &gt;&gt; &gt;&gt; &gt; SOCIOCAST
&gt;&gt; &gt;&gt; &gt;&gt; &gt; Simple. Powerful. Predictions.
&gt;&gt; &gt;&gt; &gt;&gt; &gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt; 36 WEST 25TH STREET, 7TH FLOOR NEW YORK CITY, NY 10010
&gt;&gt; &gt;&gt; &gt;&gt; &gt; O: 917.525.2466x104 M: 914.924.7935 F: 347.943.6281
&gt;&gt; &gt;&gt; &gt;&gt; &gt; E: giri.iyen...@sociocast.com W: www.sociocast.com
&gt;&gt; &gt;&gt; &gt;&gt; &gt;
&gt;&gt; &gt;&gt; &gt;&gt; &gt; Facebook's Ad Guru Joins Sociocast - http://bit.ly/NjPQBQ
&gt;&gt; &gt;&gt; &gt;
&gt;&gt; &gt;&gt; &gt;
&gt;&gt; &gt;&gt; &gt;
&gt;&gt; &gt;&gt; &gt;
&gt;&gt; &gt;&gt; &gt; --
&gt;&gt; &gt;&gt; &gt; GIRI IYENGAR, CTO
&gt;&gt; &gt;&gt; &gt; SOCIOCAST
&gt;&gt; &gt;&gt; &gt; Simple. Powerful. Predictions.
&gt;&gt; &gt;&gt; &gt;
&gt;&gt; &gt;&gt; &gt; 36 WEST 25TH STREET, 7TH FLOOR NEW YORK CITY, NY 10010
&gt;&gt; &gt;&gt; &gt; O: 917.525.2466x104 M: 914.924.7935 F: 347.943.6281
&gt;&gt; &gt;&gt; &gt; E: giri.iyen...@sociocast.com W: www.sociocast.com
&gt;&gt; &gt;&gt; &gt;
&gt;&gt; &gt;&gt; &gt; Facebook's Ad Guru Joins Sociocast - http://bit.ly/NjPQBQ
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt; &gt; --
&gt;&gt; &gt; GIRI IYENGAR, CTO
&gt;&gt; &gt; SOCIOCAST
&gt;&gt; &gt; Simple. Powerful. Predictions.
&gt;&gt; &gt;
&gt;&gt; &gt; 36 WEST 25TH STREET, 7TH FLOOR NEW YORK CITY, NY 10010
&gt;&gt; &gt; O: 917.525.2466x104 M: 914.924.7935 F: 347.943.6281
&gt;&gt; &gt; E: giri.iyen...@sociocast.com W: www.sociocast.com
&gt;&gt; &gt;
&gt;&gt; &gt; Facebook's Ad Guru Joins Sociocast - http://bit.ly/NjPQBQ
&gt;&gt;
&gt;
&gt;
&gt;
&gt; --
&gt; GIRI IYENGAR, CTO
&gt; SOCIOCAST
&gt; Simple. Powerful. Predictions.
&gt;
&gt; 36 WEST 25TH STREET, 7TH FLOOR NEW YORK CITY, NY 10010
&gt; O: 917.525.2466x104 M: 914.924.7935 F: 347.943.6281
&gt; E: \\*giri.iyen...@sociocast.com\\* W: \\*www.sociocast.com\\*
&gt;
&gt; Facebook's Ad Guru Joins Sociocast - http://bit.ly/NjPQBQ
&gt;



-- 
GIRI IYENGAR, CTO
SOCIOCAST
Simple. Powerful. Predictions.

36 WEST 25TH STREET, 7TH FLOOR NEW YORK CITY, NY 10010
O: 917.525.2466x104 M: 914.924.7935 F: 347.943.6281
E: \\*giri.iyen...@sociocast.com\\* W: \\*www.sociocast.com\\*

Facebook's Ad Guru Joins Sociocast - http://bit.ly/NjPQBQ
\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

