---
title: "Re: Slow write performance for Riak CS"
description: ""
project: community
lastmod: 2014-07-03T18:10:25-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg14458"
mailinglist_parent_id: "msg14457"
author_name: "Toby Corkindale"
project_section: "mailinglistitem"
sent_date: 2014-07-03T18:10:25-07:00
---


On 4 July 2014 10:20, Matthew MacClary  wrote:
&gt; Hi all, a Riak CS user named Toby started this discussion about write
&gt; performance. I am seeing the exact same behavior in terms of idle CPUs,
&gt; network, and disks, but low throughput. Toby do you happen to have any
&gt; follow up about settings to improve the raw Riak throughput and/or Riak CS
&gt; throughput?
&gt;
&gt; http://lists.basho.com/pipermail/riak-users\\_lists.basho.com/2013-May/012177.html

Hi Matt,
I'm still running the settings I mentioned a year ago; I also setup
RiakCS to talk to a per-machine haproxy that proxies out to the Riak
CS port on all machines, rather than just talking to 127.0.0.01

Performance isn't great per client for putting data into the store,
but it's fast enough for our purposes, and does parallelize OK. (We're
a read-heavy workload and also writes are naturally split over a bunch
of clients writing smaller files, rather than fewer with large files)

Toby

&gt; On 29/05/13 06:12, Reid Draper wrote:
&gt;&gt; Hey Toby,
&gt;&gt;
&gt;&gt; Another option is to explore some of the Riak CS PUT configuration
&gt;&gt; parameters, like the internal write concurrency and the buffer size. These
&gt;&gt; can either be changed by editing the app.config, or changing them at
&gt;&gt; run-time in an Erlang shell (via riak-cs attach). The two parameters are
&gt;&gt; `put\\_concurrency` and `put\\_buffer\\_factor`.
&gt;&gt;
&gt;&gt; They both default to 1. The `put\\_concurrency` controls the number of
&gt;&gt; threads inside of Riak CS that are used to write blocks to Riak. The
&gt;&gt; `put\\_buffer\\_factor` controls the number of blocks that will be buffered
&gt;&gt; in-memory in Riak CS before it starts to slow down reading from the HTTP
&gt;&gt; client. I suggesting trying to raise these values to get higher
&gt;&gt; single-client throughput. If you wish to edit the app.config, add lines like
&gt;&gt; in the `riak\\_cs` section:
&gt;&gt;
&gt;&gt; {put\\_concurrency, 8},
&gt;&gt; {put\\_buffer\\_factor, 16},
&gt;
&gt;
&gt; Ah, thanks -- that's interesting. Bumping up those up a bit did improve
&gt; things a bit -- I went for a conservative concurrency of 4,
&gt; buffer\\_factor of 8, and that took speeds on 500M files from 8-9mb/s to
&gt; 12-13mb/sec.
&gt;
&gt; Might play with other values when I next get time, but for now that's a
&gt; good cheap win.
&gt;
&gt;&gt; In increasing these values, you might also find it useful to run a load
&gt;&gt; balancer between the Riak CS and Riak nodes, instead of having Riak CS just
&gt;&gt; communicate with the local Riak node. We intend to have this behavior built
&gt;&gt; into Riak CS in the future, but for now a load balancer will suffice.
&gt;
&gt; Ah, ta. Can do.
&gt;
&gt;&gt; Furthermore, please make sure you've looked through the Linux Tuning page
&gt;&gt; for Riak [1]. And as for +zdbbl, you can try even higher, like 128MB: +zdbbl
&gt;&gt; 131072.
&gt;
&gt; Thanks.
&gt; I'd been through there already, and applied things like filesystem and
&gt; scheduler options.
&gt; I haven't touched the networking sysctls since they came with a warning
&gt; that they should only be messed with if networking \\*was\\* the bottleneck.
&gt;
&gt; Given what I've mentioned so far, do you think it's likely?
&gt; I guess it can't hurt to adjust them and see..
&gt;
&gt;&gt; [1]
&gt;&gt; http://docs.basho.com/riak/1.3.1/cookbooks/Linux-Performance-Tuning/#Linux-Tuning
&gt;
&gt;
&gt; Thanks for your advice,
&gt; Toby
&gt;
&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt; Reid
&gt;&gt;
&gt;&gt; On May 27, 2013, at 11:50 PM, Toby Corkindale &gt; strategicdata.com.au&gt; wrote:
&gt;&gt;
&gt;&gt;&gt; On 28/05/13 13:11, Jared Morrow wrote:
&gt;&gt;&gt;&gt; Toby,
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Can you trying putting data with a second client simultaneously? When
&gt;&gt;&gt;&gt; people have slow benchmarking, lots of times just using multiple
&gt;&gt;&gt;&gt; worker/clients helps. Also, what client library are you using?
&gt;&gt;&gt;
&gt;&gt;&gt; Running up three S3 clients (on separate machines) simultaneously saw
&gt;&gt;&gt; them return 8, 8, 6 MB/sec. Interesting to note that the performance hasn't
&gt;&gt;&gt; dropped threefold, but still, it'd be really nice if an individual transfer
&gt;&gt;&gt; would run faster, given the performance of the underlying hardware.
&gt;&gt;&gt;
&gt;&gt;&gt; The nodes hardly get utilised during a write operation. I've uploading a
&gt;&gt;&gt; total of 1500M from the three nodes, and yet CPUs are 90% idle, and there's
&gt;&gt;&gt; no real disk activity going on, apart from a couple of times when several
&gt;&gt;&gt; hundred get flushed out over the course of a second.
&gt;&gt;&gt;
&gt;&gt;&gt; I feel like something isn't quite right. What is the system \\*waiting\\*
&gt;&gt;&gt; for? There's plenty of CPU and IO to go around.
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; In the case of the current benchmarking, I'm using either s3cmd or curl.
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Also I meant to mention in my first reply, but Boundary
&gt;&gt;&gt;&gt; http://boundary.com/ worked wonders for us being able to see how much
&gt;&gt;&gt;&gt; data was really moving around. They have a free trial as far as I know.
&gt;&gt;&gt;&gt; It might be worth it to see if there are any obvious bottlenecks.
&gt;&gt;&gt;
&gt;&gt;&gt; Thanks, I'll have a look and see if the effort of setting it all up looks
&gt;&gt;&gt; worthwhile.
&gt;&gt;&gt;
&gt;&gt;&gt; Cheers,
&gt;&gt;&gt; Toby
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt;&gt; On Mon, May 27, 2013 at 8:46 PM, Toby Corkindale
&gt;&gt;&gt;&gt; &gt;&gt;&gt; &gt; wrote:
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; On 28/05/13 01:41, Jared Morrow wrote:
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Toby,
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; If you write with multiple clients does it still stick to 9mb/s
&gt;&gt;&gt;&gt; or
&gt;&gt;&gt;&gt; does it increase? What is the network link between your client
&gt;&gt;&gt;&gt; and
&gt;&gt;&gt;&gt; the Riak CS cluster? On our internal CS cluster we were seeing
&gt;&gt;&gt;&gt; around 2gb/s read+write at the network level so I know CS can
&gt;&gt;&gt;&gt; take
&gt;&gt;&gt;&gt; the speeds, so my gut thinks you single client might have a slow
&gt;&gt;&gt;&gt; link. That is just a guess.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; The network links are all Ethernet, and appear to be functioning OK.
&gt;&gt;&gt;&gt; iperf reports:
&gt;&gt;&gt;&gt; bandwidth from client to loadbalancer: 2.20 gbit/sec
&gt;&gt;&gt;&gt; bandwidth from loadbalancer to a riak node: 941 mbit/sec
&gt;&gt;&gt;&gt; bandwidth from one riak node to another node: 942 mbit/s
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; I've tested going direct from a client to a riak node rather than
&gt;&gt;&gt;&gt; via the loadbalancer, but it doesn't seem to make any difference.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Having tested a bit further now, I'd guess that the problem lies
&gt;&gt;&gt;&gt; with Riak rather than Riak CS.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; I've noticed that if I try to push files directly into Riak, they go
&gt;&gt;&gt;&gt; fairly slowly too - around 10-20mbyte/sec.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; I've tried 3, 10 and 50 MB files, against bitcask, leveldb and even
&gt;&gt;&gt;&gt; memory backends, and in all cases I get fairly consistent transfer
&gt;&gt;&gt;&gt; rates in that range. (just using curl for testing here)
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; I've tried reducing n\\_val to 1, there was a small but not
&gt;&gt;&gt;&gt; significant improvement.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; I'm a bit stumped.. However I do note that the log files seem to
&gt;&gt;&gt;&gt; have a lot of "monitor busy\\_dist\\_port" messages in them.. I'm
&gt;&gt;&gt;&gt; wondering if that might be related somehow?
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt; riak-users at lists.basho.com
&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;
&gt;
&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;



-- 
Turning and turning in the widening gyre
The falcon cannot hear the falconer
Things fall apart; the center cannot hold
Mere anarchy is loosed upon the world

\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

