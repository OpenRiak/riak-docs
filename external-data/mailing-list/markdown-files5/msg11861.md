---
title: "Re: performance"
description: ""
project: community
lastmod: 2013-08-01T20:15:07-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg11861"
mailinglist_parent_id: "msg11860"
author_name: "Jeremy Ong"
project_section: "mailinglistitem"
sent_date: 2013-08-01T20:15:07-07:00
---


What erlang version did you build with? How are you load balancing
between the nodes? What kind of disks are you using?

On Thu, Aug 1, 2013 at 7:53 PM, Paul Ingalls  wrote:
&gt; FYI, 2 more nodes died with the end of the last test. Storm, which I'm
&gt; using to put data in, kills the topology a bit abruptly, perhaps the nodes
&gt; don't like a client going away like that?
&gt;
&gt; log from one of the nodes:
&gt;
&gt; 2013-08-02 02:27:23 =ERROR REPORT====
&gt; Error in process &lt;0.4959.0&gt; on node 'riak@riak004' with exit value:
&gt; {badarg,[{riak\\_core\\_stat,vnodeq\\_len,1,[{file,"src/riak\\_core\\_stat.erl"},{line,181}]},{riak\\_core\\_stat,'-vnodeq\\_stats/0-lc$^0/1-0-',1,[{file,"src/riak\\_core\\_stat.erl"},{line,172}]},{riak\\_core\\_stat,'-vnodeq\\_stats/0-lc$^0/1-0-',1,[...
&gt;
&gt; 2013-08-02 02:27:33 =ERROR REPORT====
&gt; Error in process &lt;0.5055.0&gt; on node 'riak@riak004' with exit value:
&gt; {badarg,[{riak\\_core\\_stat,vnodeq\\_len,1,[{file,"src/riak\\_core\\_stat.erl"},{line,181}]},{riak\\_core\\_stat,'-vnodeq\\_stats/0-lc$^0/1-0-',1,[{file,"src/riak\\_core\\_stat.erl"},{line,172}]},{riak\\_core\\_stat,'-vnodeq\\_stats/0-lc$^0/1-0-',1,[...
&gt;
&gt; 2013-08-02 02:27:51 =ERROR REPORT====
&gt; Error in process &lt;0.5228.0&gt; on node 'riak@riak004' with exit value:
&gt; {badarg,[{riak\\_core\\_stat,vnodeq\\_len,1,[{file,"src/riak\\_core\\_stat.erl"},{line,181}]},{riak\\_core\\_stat,'-vnodeq\\_stats/0-lc$^0/1-0-',1,[{file,"src/riak\\_core\\_stat.erl"},{line,172}]},{riak\\_core\\_stat,'-vnodeq\\_stats/0-lc$^0/1-0-',1,[...
&gt;
&gt; and the log from the other node:
&gt;
&gt; 2013-08-02 00:09:39 =ERROR REPORT====
&gt; Error in process &lt;0.4952.0&gt; on node 'riak@riak007' with exit value:
&gt; {badarg,[{riak\\_core\\_stat,vnodeq\\_len,1,[{file,"src/riak\\_core\\_stat.erl"},{line,181}]},{riak\\_core\\_stat,'-vnodeq\\_stats/0-lc$^0/1-0-',1,[{file,"src/riak\\_core\\_stat.erl"},{line,172}]},{riak\\_core\\_stat,'-vnodeq\\_stats/0-lc$^0/1-0-',1,[...
&gt;
&gt; 2013-08-02 00:09:44 =ERROR REPORT====
&gt; \\*\\* State machine &lt;0.2368.0&gt; terminating
&gt; \\*\\* Last event in was unregistered
&gt; \\*\\* When State == active
&gt; \\*\\* Data ==
&gt; {state,114179815416476790484662877555959610910619729920,riak\\_kv\\_vnode,{deleted,{state,114179815416476790484662877555959610910619729920,riak\\_kv\\_eleveldb\\_backend,{state,&lt;&lt;&gt;&gt;,"/mnt/datadrive/riak/data/leveldb/114179815416476790484662877555959610910619729920",[{create\\_if\\_missing,true},{max\\_open\\_files,128},{use\\_bloomfilter,true},{write\\_buffer\\_size,58858594}],[{add\\_paths,[]},{allow\\_strfun,false},{anti\\_entropy,{on,[]}},{anti\\_entropy\\_build\\_limit,{1,3600000}},{anti\\_entropy\\_concurrency,2},{anti\\_entropy\\_data\\_dir,"/mnt/datadrive/riak/data/anti\\_entropy"},{anti\\_entropy\\_expire,604800000},{anti\\_entropy\\_leveldb\\_opts,[{write\\_buffer\\_size,4194304},{max\\_open\\_files,20}]},{anti\\_entropy\\_tick,15000},{create\\_if\\_missing,true},{data\\_root,"/mnt/datadrive/riak/data/leveldb"},{fsm\\_limit,50000},{hook\\_js\\_vm\\_count,2},{http\\_url\\_encoding,on},{included\\_applications,[]},{js\\_max\\_vm\\_mem,8},{js\\_thread\\_stack,16},{legacy\\_stats,true},{listkeys\\_backpressure,true},{map\\_js\\_vm\\_count,8},{mapred\\_2i\\_pipe,true},{mapred\\_name,"mapred"},{max\\_open\\_files,128},{object\\_format,v1},{reduce\\_js\\_vm\\_count,6},{stats\\_urlpath,"stats"},{storage\\_backend,riak\\_kv\\_eleveldb\\_backend},{use\\_bloomfilter,true},{vnode\\_vclocks,true},{write\\_buffer\\_size,58858594}],[],[],[{fill\\_cache,false}],true,false},{dict,0,16,16,8,80,48,{[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]},{{[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]}}},undefined,3000,1000,100,100,true,true,undefined}},riak@riak003,none,undefined,undefined,undefined,{pool,riak\\_kv\\_worker,10,[]},undefined,107615}
&gt; \\*\\* Reason for termination =
&gt; \\*\\*
&gt; {badarg,[{eleveldb,close,[&lt;&lt;&gt;&gt;],[]},{riak\\_kv\\_eleveldb\\_backend,stop,1,[{file,"src/riak\\_kv\\_eleveldb\\_backend.erl"},{line,149}]},{riak\\_kv\\_vnode,terminate,2,[{file,"src/riak\\_kv\\_vnode.erl"},{line,836}]},{riak\\_core\\_vnode,terminate,3,[{file,"src/riak\\_core\\_vnode.erl"},{line,847}]},{gen\\_fsm,terminate,7,[{file,"gen\\_fsm.erl"},{line,586}]},{proc\\_lib,init\\_p\\_do\\_apply,3,[{file,"proc\\_lib.erl"},{line,227}]}]}
&gt; 2013-08-02 00:09:44 =CRASH REPORT====
&gt; crasher:
&gt; initial call: riak\\_core\\_vnode:init/1
&gt; pid: &lt;0.2368.0&gt;
&gt; registered\\_name: []
&gt; exception exit:
&gt; {{badarg,[{eleveldb,close,[&lt;&lt;&gt;&gt;],[]},{riak\\_kv\\_eleveldb\\_backend,stop,1,[{file,"src/riak\\_kv\\_eleveldb\\_backend.erl"},{line,149}]},{riak\\_kv\\_vnode,terminate,2,[{file,"src/riak\\_kv\\_vnode.erl"},{line,836}]},{riak\\_core\\_vnode,terminate,3,[{file,"src/riak\\_core\\_vnode.erl"},{line,847}]},{gen\\_fsm,terminate,7,[{file,"gen\\_fsm.erl"},{line,586}]},{proc\\_lib,init\\_p\\_do\\_apply,3,[{file,"proc\\_lib.erl"},{line,227}]}]},[{gen\\_fsm,terminate,7,[{file,"gen\\_fsm.erl"},{line,589}]},{proc\\_lib,init\\_p\\_do\\_apply,3,[{file,"proc\\_lib.erl"},{line,227}]}]}
&gt; ancestors: [riak\\_core\\_vnode\\_sup,riak\\_core\\_sup,&lt;0.139.0&gt;]
&gt; messages: []
&gt; links: [&lt;0.142.0&gt;]
&gt; dictionary: [{random\\_seed,{8115,23258,22987}}]
&gt; trap\\_exit: true
&gt; status: running
&gt; heap\\_size: 196418
&gt; stack\\_size: 24
&gt; reductions: 12124
&gt; neighbours:
&gt; 2013-08-02 00:09:44 =SUPERVISOR REPORT====
&gt; Supervisor: {local,riak\\_core\\_vnode\\_sup}
&gt; Context: child\\_terminated
&gt; Reason:
&gt; {badarg,[{eleveldb,close,[&lt;&lt;&gt;&gt;],[]},{riak\\_kv\\_eleveldb\\_backend,stop,1,[{file,"src/riak\\_kv\\_eleveldb\\_backend.erl"},{line,149}]},{riak\\_kv\\_vnode,terminate,2,[{file,"src/riak\\_kv\\_vnode.erl"},{line,836}]},{riak\\_core\\_vnode,terminate,3,[{file,"src/riak\\_core\\_vnode.erl"},{line,847}]},{gen\\_fsm,terminate,7,[{file,"gen\\_fsm.erl"},{line,586}]},{proc\\_lib,init\\_p\\_do\\_apply,3,[{file,"proc\\_lib.erl"},{line,227}]}]}
&gt; Offender:
&gt; [{pid,&lt;0.2368.0&gt;},{name,undefined},{mfargs,{riak\\_core\\_vnode,start\\_link,undefined}},{restart\\_type,temporary},{shutdown,300000},{child\\_type,worker}]
&gt;
&gt;
&gt;
&gt; Paul Ingalls
&gt; Founder & CEO Fanzo
&gt; p...@fanzo.me
&gt; @paulingalls
&gt; http://www.linkedin.com/in/paulingalls
&gt;
&gt;
&gt;
&gt; On Aug 1, 2013, at 7:49 PM, Paul Ingalls  wrote:
&gt;
&gt; I should say that I build riak from the master branch on the git repository.
&gt; Perhaps that was a bad idea?
&gt;
&gt; Paul Ingalls
&gt; Founder & CEO Fanzo
&gt; p...@fanzo.me
&gt; @paulingalls
&gt; http://www.linkedin.com/in/paulingalls
&gt;
&gt;
&gt;
&gt; On Aug 1, 2013, at 7:47 PM, Paul Ingalls  wrote:
&gt;
&gt; Thanks for the quick response Matthew!
&gt;
&gt; I gave that a shot, and if anything the performance was worse. When I
&gt; picked 128 I ran through the calculations on this page:
&gt;
&gt; http://docs.basho.com/riak/latest/ops/advanced/backends/leveldb/#Parameter-Planning
&gt;
&gt; and thought that would work, but it sounds like I was quite a bit off from
&gt; what you have below.
&gt;
&gt; Looking at risk control, the memory was staying pretty low, and watching top
&gt; the CPU was well in hand. iostat had very little of the CPU in iowait,
&gt; although it was writing a lot. I imagine, however, that this is missing a
&gt; lot of the details.
&gt;
&gt; Any other ideas? I can't imagine one get/update/put cycle per second is the
&gt; best I can doâ€¦
&gt;
&gt; Thanks!
&gt;
&gt; Paul Ingalls
&gt; Founder & CEO Fanzo
&gt; p...@fanzo.me
&gt; @paulingalls
&gt; http://www.linkedin.com/in/paulingalls
&gt;
&gt;
&gt;
&gt; On Aug 1, 2013, at 7:12 PM, Matthew Von-Maszewski 
&gt; wrote:
&gt;
&gt; Try cutting your max open files in half. I am working from my iPad not my
&gt; workstation so my numbers are rough. Will get better ones to you in the
&gt; morning.
&gt;
&gt; The math goes like this:
&gt;
&gt; - vnode/partition heap usage is (4Mbytes \\* (max\\_open\\_files -10)) + 8Mbyte
&gt; - you have 18 vnodes per server (multiply the above times 18)
&gt; - AAE (active anti-entropy is"on") so that adds (4Mbyte\\* 10 + 8Mbyte) times
&gt; 18 vnodes
&gt;
&gt; The three lines above give the total memory leveldb will attempt to use per
&gt; server if your dataset is large enough to fill it.
&gt;
&gt; Matthew
&gt;
&gt;
&gt; On Aug 1, 2013, at 21:33, Paul Ingalls  wrote:
&gt;
&gt; I should add more details about the nodes that crashed. I ran this for the
&gt; first time for all of 10 minutes.
&gt;
&gt; Here is the log from the first one:
&gt;
&gt; 2013-08-02 00:09:44 =ERROR REPORT====
&gt; \\*\\* State machine &lt;0.2368.0&gt; terminating
&gt; \\*\\* Last event in was unregistered
&gt; \\*\\* When State == active
&gt; \\*\\* Data ==
&gt; {state,114179815416476790484662877555959610910619729920,riak\\_kv\\_vnode,{deleted,{state,114179815416476790484662877555959610910619729920,riak\\_kv\\_eleveldb\\_backend,{state,&lt;&lt;&gt;&gt;,"/mnt/datadrive/riak/data/leveldb/114179815416476790484662877555959610910619729920",[{create\\_if\\_missing,true},{max\\_open\\_files,128},{use\\_bloomfilter,true},{write\\_buffer\\_size,58858594}],[{add\\_paths,[]},{allow\\_strfun,false},{anti\\_entropy,{on,[]}},{anti\\_entropy\\_build\\_limit,{1,3600000}},{anti\\_entropy\\_concurrency,2},{anti\\_entropy\\_data\\_dir,"/mnt/datadrive/riak/data/anti\\_entropy"},{anti\\_entropy\\_expire,604800000},{anti\\_entropy\\_leveldb\\_opts,[{write\\_buffer\\_size,4194304},{max\\_open\\_files,20}]},{anti\\_entropy\\_tick,15000},{create\\_if\\_missing,true},{data\\_root,"/mnt/datadrive/riak/data/leveldb"},{fsm\\_limit,50000},{hook\\_js\\_vm\\_count,2},{http\\_url\\_encoding,on},{included\\_applications,[]},{js\\_max\\_vm\\_mem,8},{js\\_thread\\_stack,16},{legacy\\_stats,true},{listkeys\\_backpressure,true},{map\\_js\\_vm\\_count,8},{mapred\\_2i\\_pipe,true},{mapred\\_name,"mapred"},{max\\_open\\_files,128},{object\\_format,v1},{reduce\\_js\\_vm\\_count,6},{stats\\_urlpath,"stats"},{storage\\_backend,riak\\_kv\\_eleveldb\\_backend},{use\\_bloomfilter,true},{vnode\\_vclocks,true},{write\\_buffer\\_size,58858594}],[],[],[{fill\\_cache,false}],true,false},{dict,0,16,16,8,80,48,{[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]},{{[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]}}},undefined,3000,1000,100,100,true,true,undefined}},riak@riak003,none,undefined,undefined,undefined,{pool,riak\\_kv\\_worker,10,[]},undefined,107615}
&gt; \\*\\* Reason for termination =
&gt; \\*\\*
&gt; {badarg,[{eleveldb,close,[&lt;&lt;&gt;&gt;],[]},{riak\\_kv\\_eleveldb\\_backend,stop,1,[{file,"src/riak\\_kv\\_eleveldb\\_backend.erl"},{line,149}]},{riak\\_kv\\_vnode,terminate,2,[{file,"src/riak\\_kv\\_vnode.erl"},{line,836}]},{riak\\_core\\_vnode,terminate,3,[{file,"src/riak\\_core\\_vnode.erl"},{line,847}]},{gen\\_fsm,terminate,7,[{file,"gen\\_fsm.erl"},{line,586}]},{proc\\_lib,init\\_p\\_do\\_apply,3,[{file,"proc\\_lib.erl"},{line,227}]}]}
&gt; 2013-08-02 00:09:44 =CRASH REPORT====
&gt; crasher:
&gt; initial call: riak\\_core\\_vnode:init/1
&gt; pid: &lt;0.2368.0&gt;
&gt; registered\\_name: []
&gt; exception exit:
&gt; {{badarg,[{eleveldb,close,[&lt;&lt;&gt;&gt;],[]},{riak\\_kv\\_eleveldb\\_backend,stop,1,[{file,"src/riak\\_kv\\_eleveldb\\_backend.erl"},{line,149}]},{riak\\_kv\\_vnode,terminate,2,[{file,"src/riak\\_kv\\_vnode.erl"},{line,836}]},{riak\\_core\\_vnode,terminate,3,[{file,"src/riak\\_core\\_vnode.erl"},{line,847}]},{gen\\_fsm,terminate,7,[{file,"gen\\_fsm.erl"},{line,586}]},{proc\\_lib,init\\_p\\_do\\_apply,3,[{file,"proc\\_lib.erl"},{line,227}]}]},[{gen\\_fsm,terminate,7,[{file,"gen\\_fsm.erl"},{line,589}]},{proc\\_lib,init\\_p\\_do\\_apply,3,[{file,"proc\\_lib.erl"},{line,227}]}]}
&gt; ancestors: [riak\\_core\\_vnode\\_sup,riak\\_core\\_sup,&lt;0.139.0&gt;]
&gt; messages: []
&gt; links: [&lt;0.142.0&gt;]
&gt; dictionary: [{random\\_seed,{8115,23258,22987}}]
&gt; trap\\_exit: true
&gt; status: running
&gt; heap\\_size: 196418
&gt; stack\\_size: 24
&gt; reductions: 12124
&gt; neighbours:
&gt; 2013-08-02 00:09:44 =SUPERVISOR REPORT====
&gt; Supervisor: {local,riak\\_core\\_vnode\\_sup}
&gt; Context: child\\_terminated
&gt; Reason:
&gt; {badarg,[{eleveldb,close,[&lt;&lt;&gt;&gt;],[]},{riak\\_kv\\_eleveldb\\_backend,stop,1,[{file,"src/riak\\_kv\\_eleveldb\\_backend.erl"},{line,149}]},{riak\\_kv\\_vnode,terminate,2,[{file,"src/riak\\_kv\\_vnode.erl"},{line,836}]},{riak\\_core\\_vnode,terminate,3,[{file,"src/riak\\_core\\_vnode.erl"},{line,847}]},{gen\\_fsm,terminate,7,[{file,"gen\\_fsm.erl"},{line,586}]},{proc\\_lib,init\\_p\\_do\\_apply,3,[{file,"proc\\_lib.erl"},{line,227}]}]}
&gt; Offender:
&gt; [{pid,&lt;0.2368.0&gt;},{name,undefined},{mfargs,{riak\\_core\\_vnode,start\\_link,undefined}},{restart\\_type,temporary},{shutdown,300000},{child\\_type,worker}]
&gt;
&gt; The second one looks like it ran out of heap, I assume I have something miss
&gt; configured here...
&gt;
&gt; ===== Fri Aug 2 00:51:28 UTC 2013
&gt; Erlang has closed
&gt; /home/fanzo/riak/rel/riak/bin/../lib/os\\_mon-2.2.9/priv/bin/memsup: Erlang
&gt; has closed.
&gt; ^M
&gt; Crash dump was written to: ./log/erl\\_crash.dump^M
&gt; eheap\\_alloc: Cannot allocate 5568010120 bytes of memory (of type "heap").^M
&gt;
&gt;
&gt; Paul Ingalls
&gt; Founder & CEO Fanzo
&gt; p...@fanzo.me
&gt; @paulingalls
&gt; http://www.linkedin.com/in/paulingalls
&gt;
&gt;
&gt;
&gt; On Aug 1, 2013, at 6:28 PM, Paul Ingalls  wrote:
&gt;
&gt; Couple of questions.
&gt;
&gt; I have migrated my system to use Riak on the back end. I have setup a 1.4
&gt; cluster with 128 partitions on 7 nodes with LevelDB as the store. Each node
&gt; looks like:
&gt;
&gt; Azure Large instance (4CPU 7GB RAM)
&gt; data directory is on a RAID 0
&gt; max files is set to 128
&gt; async thread on the VM is 16
&gt; everything else is defaults
&gt;
&gt; I'm using the 1.4.1 java client, connecting via the protocol buffer cluster.
&gt;
&gt; With this setup, I'm seeing poor throughput on my service load. I ran a
&gt; test for a bit and was seeing only a few gets/puts per second. And then
&gt; when I stopped the client two of the nodes crashed.
&gt;
&gt; I'm very new with Riak, so I figure I'm doing something wrong. I saw a note
&gt; on the list earlier of someone getting well over 1000 puts per second, so I
&gt; know it can move pretty fast.
&gt;
&gt; What is a good strategy for troubleshooting?
&gt;
&gt; How many fetch/update/store loops per second should I expect to see on a
&gt; cluster of this size?
&gt;
&gt; Thanks!
&gt;
&gt; Paul
&gt;
&gt; Paul Ingalls
&gt; Founder & CEO Fanzo
&gt; p...@fanzo.me
&gt; @paulingalls
&gt; http://www.linkedin.com/in/paulingalls
&gt;
&gt;
&gt;
&gt;
&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;
&gt;
&gt;
&gt;
&gt;
&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;

\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

