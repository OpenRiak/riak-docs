---
title: "Re: Partitions placement"
description: ""
project: community
lastmod: 2014-03-14T06:06:54-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg13807"
mailinglist_parent_id: "msg13801"
author_name: "Ciprian Manea"
project_section: "mailinglistitem"
sent_date: 2014-03-14T06:06:54-07:00
---


Hi Daniel,

"A Little Riak Book" covers the logic behind partition allocation in an
overly simplified way.

Riak will distribute partitions to vnodes in a pseudo-random fashion,
resulting in allocations like you described. These allocations are less
optimal when the number of riak nodes are small, hence we (strongly)
recommend 5+ nodes for production use.

Storing 3 data copies in 3 different servers sounds trivial to do, but not
that easy to scale up once the numbers of servers grows. To cope with
scalability, Riak introduces an "overlay". Data is first placed in
"partitions" (always a power of 2) which are then distributed to different
server nodes. As powers of 2 are not divisible by 3, this approach has a
problem at lower scale: some nodes will hold a few extra partitions (which
were not intended to be stored there).

If you know you are not going to need n\\_val greater then 3 in your buckets,
one way to hint this to Riak and get a better distribution of partitions to
nodes is to configure [0] target\\_n\\_val to 3.


[0]
http://docs.basho.com/riak/latest/ops/advanced/configs/configuration-files/


Regards,
Ciprian


On Fri, Mar 14, 2014 at 12:09 AM, Daniel Iwan  wrote:

&gt; Below is an output of my Riak cluster. 3 physical nodes. Ring size 128.
&gt; As far as I can tell when Riak installed fresh it is always place
&gt; partitions
&gt; in the same way on a ring as long as number of vnodes and servers is the
&gt; same.
&gt;
&gt; All presentations including "A Little Riak Book' show pretty picture of
&gt; ring
&gt; and nodes claiming partitions in a sequential fashion. That's clearly not
&gt; a
&gt; case.
&gt; Output below shows that node2 is picked as favourite, which means replicas
&gt; of certain keys will definitely be on the same hardware. Partitions are
&gt; split 44 + 42 + 42. Why not 43+43+42?
&gt;
&gt; Another thing, why the algorithm selects nodes in 'random' non-sequential
&gt; fashion? When the cluster gets created and nodes 2 & 3 are joined to node
&gt; 1,
&gt; it's a clear situation. Partitions are empty so vnodes could be assigned in
&gt; a way so there's no consecutive partitions on the same hw.
&gt; My issue is that in my case if node2 goes down and I'm storing some data
&gt; with N=2 I will definitely not be able access certain keys and more
&gt; surprisingly all 2i will no longer work for the buckets with N=2 due to
&gt; {error,insufficient\\_vnodes\\_available}. That is all 2i's for those buckets.
&gt;
&gt; I understand that when new nodes are attached Riak tries to avoid
&gt; reshuffling everything and just moves certain partitions, and at that point
&gt; you may end up with copies on the same physical nodes. But even then Riak
&gt; should make best effort and try not to put consecutive partitions on the
&gt; same server. If it has to move it anyway it could as well put it on any
&gt; other machine but the one that holds partition with preceding and following
&gt; index.
&gt; I also understand Riak does not guarantee that replicas are on distinct
&gt; servers (why? it should, at least for N=2 and N=3 if possible)
&gt;
&gt; I appreciate minimum recommended setup is 5 nodes and I should be storing
&gt; with N=3 minimum.
&gt; But I just find it confusing when presentations show something that is not
&gt; even remotely close to reality.
&gt;
&gt; Just to be clear I have nothing against Riak, I think it's great though bit
&gt; disappointing that there are no stronger conditions about replica placement
&gt; here.
&gt;
&gt; I'm probably missing something and simplifying too much. Any clarification
&gt; appreciated.
&gt;
&gt; Daniel
&gt;
&gt;
&gt; riak@10.173.240.1)2&gt;
&gt; (riak@10.173.240.1)2&gt; {ok, Ring} = riak\\_core\\_ring\\_manager:get\\_my\\_ring().
&gt; {ok,
&gt; {chstate\\_v2,'riak@10.173.240.1',
&gt; [{'riak@10.173.240.1',{303,63561952927}},
&gt; {'riak@10.173.240.2',{31,63561952907}},
&gt; {'riak@10.173.240.3',{25,63561952907}}],
&gt; {128,
&gt; [{0,'riak@10.173.240.1'},
&gt; {11417981541647679048466287755595961091061972992,
&gt; 'riak@10.173.240.2'},
&gt; {22835963083295358096932575511191922182123945984,
&gt; 'riak@10.173.240.2'},
&gt; {34253944624943037145398863266787883273185918976,
&gt; 'riak@10.173.240.3'},
&gt; {45671926166590716193865151022383844364247891968,
&gt; 'riak@10.173.240.1'},
&gt; {57089907708238395242331438777979805455309864960,
&gt; 'riak@10.173.240.2'},
&gt; {68507889249886074290797726533575766546371837952,
&gt; 'riak@10.173.240.2'},
&gt; {79925870791533753339264014289171727637433810944,
&gt; 'riak@10.173.240.3'},
&gt; {91343852333181432387730302044767688728495783936,
&gt; 'riak@10.173.240.1'},
&gt; {102761833874829111436196589800363649819557756928,
&gt; 'riak@10.173.240.2'},
&gt; {114179815416476790484662877555959610910619729920,
&gt; 'riak@10.173.240.2'},
&gt; {125597796958124469533129165311555572001681702912,
&gt; 'riak@10.173.240.3'},
&gt; {137015778499772148581595453067151533092743675904,
&gt; 'riak@10.173.240.1'},
&gt; {148433760041419827630061740822747494183805648896,
&gt; 'riak@10.173.240.2'},
&gt; {159851741583067506678528028578343455274867621888,
&gt; 'riak@10.173.240.2'},
&gt; {171269723124715185726994316333939416365929594880,
&gt; 'riak@10.173.240.3'},
&gt; {182687704666362864775460604089535377456991567872,
&gt; 'riak@10.173.240.1'},
&gt; {194105686208010543823926891845131338548053540864,
&gt; 'riak@10.173.240.2'},
&gt; {205523667749658222872393179600727299639115513856,
&gt; 'riak@10.173.240.2'},
&gt; {216941649291305901920859467356323260730177486848,
&gt;
&gt; and so on
&gt;
&gt;
&gt;
&gt; --
&gt; View this message in context:
&gt; http://riak-users.197444.n3.nabble.com/Partitions-placement-tp4030664.html
&gt; Sent from the Riak Users mailing list archive at Nabble.com.
&gt;
&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;
\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

