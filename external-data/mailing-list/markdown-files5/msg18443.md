---
title: "Re: Help with handling Riak disk failure"
description: ""
project: community
lastmod: 2017-09-19T10:54:33-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg18443"
mailinglist_parent_id: "msg18442"
author_name: "Leo"
project_section: "mailinglistitem"
sent_date: 2017-09-19T10:54:33-07:00
---


Dear Bryan,

Thank you very much for your answers. They are very helpful to me.
I will use more nodes (&gt;=5) in future.

From your experience with using Riak, what would your guess be for the
time taken to finish all the AAE transfers and be done with the
recovery for about 1 TB worth of data (assuming my cluster is
otherwise completely idle without any user accessing the cluster
during this process and that I am continuously watching the transfers
and re-enabling disabled AAE trees gradually )? I am just asking for
rough estimate from your past experience ( please quote from your
experience with a difference sized cluster / data size too ). My guess
is that it will take approx. 2 days or more. Do you concur?

Thanks,
Leo


On Tue, Sep 19, 2017 at 12:41 PM, Bryan Hunt
 wrote:
&gt; (0) Three nodes are insufficient, you should have 5 nodes
&gt; (1) You could iterate and read every object in the cluster - this would also
&gt; trigger read repair for every object
&gt; (2) - copied from Engel Sanchez response to a similar question April 10th
&gt; 2014 )
&gt;
&gt; \\* If AAE is disabled, you don't have to stop the node to delete the data in
&gt; the anti\\_entropy directories
&gt; \\* If AAE is enabled, deleting the AAE data in a rolling manner may trigger
&gt; an avalanche of read repairs between nodes with the bad trees and nodes
&gt; with good trees as the data seems to diverge.
&gt;
&gt; If your nodes are already up, with AAE enabled and with old incorrect trees
&gt; in the mix, there is a better way. You can dynamically disable AAE with
&gt; some console commands. At that point, without stopping the nodes, you can
&gt; delete all AAE data across the cluster. At a convenient time, re-enable
&gt; AAE. I say convenient because all trees will start to rebuild, and that
&gt; can be problematic in an overloaded cluster. Doing this over the weekend
&gt; might be a good idea unless your cluster can take the extra load.
&gt;
&gt; To dynamically disable AAE from the Riak console, you can run this command:
&gt;
&gt;&gt; riak\\_core\\_util:rpc\\_every\\_member\\_ann(riak\\_kv\\_entropy\\_manager, disable, [],
&gt; 60000).
&gt;
&gt; and enable with the similar:
&gt;
&gt;&gt; riak\\_core\\_util:rpc\\_every\\_member\\_ann(riak\\_kv\\_entropy\\_manager, enable, [],
&gt; 60000).
&gt;
&gt; That last number is just a timeout for the RPC operation. I hope this
&gt; saves you some extra load on your clusters.
&gt;
&gt; (3) Thatâ€™s going to be :
&gt; (3a) List all keys using the client of your choice
&gt; (3b) Fetch each object
&gt;
&gt; https://www.tiot.jp/riak-docs/riak/kv/2.2.3/developing/usage/reading-objects/
&gt;
&gt; https://www.tiot.jp/riak-docs/riak/kv/2.2.3/developing/usage/secondary-indexes/
&gt;
&gt;
&gt;
&gt;
&gt;
&gt;
&gt;
&gt; On 19 Sep 2017, at 18:31, Leo  wrote:
&gt;
&gt; Dear Riak users and experts,
&gt;
&gt; I really appreciate any help with my questions below.
&gt;
&gt; I have a 3 node Riak cluster with each having approx. 1 TB disk usage.
&gt; All of a sudden, one node's hard disk failed unrecoverably. So, I
&gt; added a new node using the following steps:
&gt;
&gt; 1) riak-admin cluster join 2) down the failed node 3) riak-admin
&gt; force-replace failed-node new-node 4) riak-admin cluster plan 5)
&gt; riak-admin cluster commit.
&gt;
&gt; This almost fixed the problem except that after lots of data transfers
&gt; and handoffs, now not all three nodes have 1 TB disk usage. Only two
&gt; of them have 1 TB disk usage. The other one is almost empty (few 10s
&gt; of GBs). This means there are no longer 3 copies on disk anymore. My
&gt; data is completely random (no two keys have same data associated with
&gt; them. So, compression of data cannot be the reason for less data on
&gt; disk),
&gt;
&gt; I also tried using the "riak-admin cluster replace failednode newnode"
&gt; command so that the leaving node handsoff data to the joining node.
&gt; This however is not helpful if the leaving node has a failed hard
&gt; disk. I want the remaining live vnodes to help the new node recreate
&gt; the lost data using their replica copies.
&gt;
&gt; I have three questions:
&gt;
&gt; 1) What commands should I run to forcefully make sure there are three
&gt; replicas on disk overall without waiting for read-repair or
&gt; anti-entropy to make three copies ? Bandwidth usage or CPU usage is
&gt; not a huge concern for me.
&gt;
&gt; 2) Also, I will be very grateful if someone lists the commands that I
&gt; can run using "riak attach" so that I can clear the AAE trees and
&gt; forcefully make sure all data has 3 copies.
&gt;
&gt; 3) I will be very thankful if someone helps me with the commands that
&gt; I should run to ensure that all data has 3 replicas on disk after the
&gt; disk failure (instead of just looking at the disk space usage in all
&gt; the nodes as hints)?
&gt;
&gt; Thanks,
&gt; Leo
&gt;
&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;
&gt;

\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

