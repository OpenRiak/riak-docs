---
title: "Re: Endless AAE keys repairing"
description: ""
project: community
lastmod: 2014-07-17T05:43:15-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg14502"
mailinglist_parent_id: "msg14501"
author_name: "Ryan Zezeski"
project_section: "mailinglistitem"
sent_date: 2014-07-17T05:43:15-07:00
---



On Jul 17, 2014, at 4:30 AM, Daniil Churikov  wrote:

&gt; Hello, In our test env we have 3 nodes riak 1.4.8-1 cluster on debians. 
&gt; According to logs: 2014-07-17 02:48:03.748 [info] 
&gt; &lt;0.10542.85&gt;@riak\\_kv\\_exchange\\_fsm:key\\_exchange:206 Repaired 1 keys during 
&gt; active anti-entropy exchange of 
&gt; {936274486415109681974235595958868809467081785344,3} between 
&gt; {936274486415109681974235595958868809467081785344,'riak@10.3.13.96'} and 
&gt; {981946412581700398168100746981252653831329677312,'riak@10.3.13.96'} Messages 
&gt; like this constantly appears, there is not so much load on this test cluster 
&gt; and I expected that eventually everything will be fixed, but this messages 
&gt; keep coming from day to day. In the past we had several issues with one of 
&gt; the cluster participants and as a result we did enabled AAE to fix it. What 
&gt; could be possble the reason of this? 

This is probably caused by regular puts. When AAE performs an exchange it 
takes snapshots of each tree in a concurrent manner. This means that a 
snapshot could occur while replicas for a given object are still in flight. 
For example:

1. User writes object O.
2. Coordinator sends O to 3 partitions A, B, and C.
3. Partition A accepts O and updates hash tree.
4. Entropy manager on node which own partition A decides to perform an exchange 
between A and B.
5. Snapshot is taken of hash tree for A.
6. Snapshot is taken of hash tree for B.
7. Partition B accepts O and updates hash tree (but the update is not reflected 
in the snapshot just taken)
8. Partition C accepts O and updates hash tree.
9. Exchange between A & B determines object is missing on B and performs a read 
repair.
10. Read repair notices that object O exists on all three partitions and there 
is nothing to be done.

The higher the load the more keys that could be included in one snapshot but 
not the other. I would say that any time your cluster is accepting writes it 
might be normal to see a handful of keys getting “repaired”. But if you see, 
say, more than 10 (especially if there are 0 outstanding writes) then that is 
probably a sign of real repair.

-Z
\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

