---
title: "Re: Minimal number of nodes for production"
description: ""
project: community
lastmod: 2013-04-10T15:03:40-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg10839"
mailinglist_parent_id: "msg10838"
author_name: "Tom Zeng"
project_section: "mailinglistitem"
sent_date: 2013-04-10T15:03:40-07:00
---


Thanks Eric for the info, that's very helpful. 7 was mentioned at the last
Riak DC meetup. not as the minimal but for better performance, when I was
chatting with a couple of Basho devs about performance benchmarking, and
about Riak is quite a bit slower on single node against Mongo.


On Wed, Apr 10, 2013 at 5:56 PM, Eric Redmond  wrote:

&gt;
&gt;
&gt; On Apr 10, 2013, at 2:26 PM, Tom Zeng  wrote:
&gt;
&gt; Hi list,
&gt;
&gt; We have a production installation with only 3 nodes and running on 1.2.1.
&gt; I'd appreciate to get some facts to convince IT to increase the number of
&gt; nodes to 7 and upgrade to 1.3. I heard people from Basho mentioned ideally
&gt; 7 nodes for production a couple of time, can someone explain why 7, is 4,
&gt; or 5 nodes good enough?
&gt;
&gt;
&gt; I'm not sure where you heard the number 7 as a minimum, unless if was for
&gt; a specific use-case. In general the minimum recommended number is 5 nodes.
&gt;
&gt; Running with only 3 nodes isn't a great idea. Since a core purpose of Riak
&gt; is to remain available in the face of outages, 3 will not support any
&gt; outage. Less than 3 is lower than the default replication value (N=3). This
&gt; is so important, in fact, that we recommend 5 solely to act as a buffer in
&gt; the case where 1 of the 5 is down, the remaining 4 is dangerously close to
&gt; the inflexible 3 node number. Even if you do not upgrade to 1.3, you really
&gt; need to have at least 5 nodes.
&gt;
&gt; There are many benefits to upgrading to 1.3, but one of the most
&gt; compelling from an operations point of view is active anti-entropy (AAE).
&gt; Rather than waiting on read-repair to fix inconsistent values (which is
&gt; passive), AAE routinely attempts to keep all node values in sync. This can
&gt; be a godsend if a node goes down, since you don't need to fore read-repair
&gt; when you bring the node back up by reading every key... you just let your
&gt; cluster actively self-heal.
&gt;
&gt;
&gt; Also on the 3 three nodes, the file size for the bitcask directory very
&gt; quite a bit: 21GB, 14GB, and 20GB. Could the node with only 14GB missing
&gt; something or it's expected to have such big difference?
&gt;
&gt;
&gt; There are several reasons sizes could be different. Values are not
&gt; yet/ever replicated (based on your N and W values). Files may have not been
&gt; compacted. Some keys have been deleted but not yet reaped...
&gt;
&gt; Thanks,
&gt; Tom
&gt;
&gt; --
&gt; Tom Zeng
&gt; Director of Engineering
&gt; Intridea, Inc. | www.intridea.com
&gt; t...@intridea.com
&gt; (o) 888.968.4332 x519
&gt; (c) 240-643-8728
&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;
&gt;
\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

