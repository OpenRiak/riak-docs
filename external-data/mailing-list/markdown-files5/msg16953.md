---
title: "Re: Riak-S2 javascript aws-sdk failing on multi-part uploads"
description: ""
project: community
lastmod: 2016-01-13T15:04:52-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg16953"
mailinglist_parent_id: "msg16952"
author_name: "John Fanjoy"
project_section: "mailinglistitem"
sent_date: 2016-01-13T15:04:52-08:00
---


Luke,

I may be able to do that. The only problem is without haproxy I have no way to 
inject CORS headers which the browser requires, but I may be able to write up 
small nodejs app to get past that and see if it is somehow related to haproxy. 
The fact that these errors are not present when using Cyberduck which is also 
talking to haproxy leads me to believe that’s not the cause, but it’s 
definitely worth testing.

--
John Fanjoy
Systems Engineer
jfan...@inetu.net





On 1/13/16, 5:55 PM, "Luke Bakken"  wrote:

&gt;John -
&gt;
&gt;The following error indicates that the connection was unexpectedly
&gt;closed by something outside of Riak while the chunk is uploading:
&gt;
&gt;{badmatch,{error,closed}}
&gt;
&gt;Is it possible to remove haproxy to test using the the aws-sdk?
&gt;
&gt;That is my first thought as to the cause of this issue, especially
&gt;since writing to S3 works with the same code.
&gt;
&gt;--
&gt;Luke Bakken
&gt;Engineer
&gt;lbak...@basho.com
&gt;
&gt;On Wed, Jan 13, 2016 at 2:46 PM, John Fanjoy  wrote:
&gt;&gt; Luke,
&gt;&gt;
&gt;&gt; Yes on both parts. To confirm cyberduck was using multi-part I actually 
&gt;&gt; tailed the console.log while it was uploading the file, and it uploaded the 
&gt;&gt; file in approx. 40 parts. Afterwards the parts were reassembled as you would 
&gt;&gt; expect. The AWS-SDK for javascript has an object called ManagedUpload which 
&gt;&gt; automatically switches to multi-part when the input is larger than the 
&gt;&gt; maxpartsize (default 5mb). I have confirmed that it is splitting the files 
&gt;&gt; up, but so far I’ve only ever seen one part get successfully uploaded before 
&gt;&gt; the others failed at which point it removes the upload (DELETE call) 
&gt;&gt; automatically. I also verified that the javascript I have in place does work 
&gt;&gt; with an actual AWS S3 bucket to rule out coding issues on my end and the 
&gt;&gt; same &gt;400mb file was successfully uploaded to the bucket I created there 
&gt;&gt; without issue.
&gt;&gt;
&gt;&gt; A few things worth mentioning that I missed before. I am running riak-s2 
&gt;&gt; behind haproxy. Haproxy is handling ssl and enabling CORS for browser based 
&gt;&gt; requests. I have tested smaller files (~4-5mb) and GET requests using the 
&gt;&gt; browser client and everything works with my current haproxy configuration, 
&gt;&gt; but the larger files are failing, usually after 1 part is successfully 
&gt;&gt; uploaded. I can also list bucket contents and delete existing contents. The 
&gt;&gt; only feature that is not working appears to be the multi-part uploads. We 
&gt;&gt; are running centOS 7 (kernel version 3.10.0-327.4.4.el7.x86\\_64). Please let 
&gt;&gt; me know if you have any further questions.
&gt;&gt;
&gt;&gt; --
&gt;&gt; John Fanjoy
&gt;&gt; Systems Engineer
&gt;&gt; jfan...@inetu.net
\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

