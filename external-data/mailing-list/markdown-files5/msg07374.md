---
title: "Re: Reip(ing) riak node created two copies in the cluster"
description: ""
project: community
lastmod: 2012-05-02T10:08:15-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg07374"
mailinglist_parent_id: "msg07373"
author_name: "Nitish Sharma"
project_section: "mailinglistitem"
sent_date: 2012-05-02T10:08:15-07:00
---



On May 2, 2012, at 6:12 PM, Jon Meredith wrote:

&gt; Hi Nitish, for this to work you'll have to stop all the nodes at the same 
&gt; time, clear the ring on all nodes, start up all nodes, then rejoin
&gt; 
&gt; If you clear the rings one node at a time, when you rejoin the nodes the ring 
&gt; with the old and new style names will be gossipped back to it and you'll 
&gt; still have both names.
Sorry for the confusion. I didn't clear the rings one node at a time while 
keeping other nodes live. Following are the steps I followed:
1. Stop Riak on all the nodes.
2. Remove ring directory from all nodes.
3. Start the nodes and rejoin.

&gt; I didn't realize you had a large amount of data - originally you said 
&gt; "Currently, we are hosting limited amount of data", but 200mil docs per node 
&gt; seems like a fair amount. Rebuilding that size cluster may take a long time.
&gt; 
Yeah, we are currently serving very limited amount because of Riak shortage. In 
total, we have almost 750 million documents served by Riak.
&gt; Your options as I see them are
&gt; 1) If you have backups of the ring files, you could revert the node name 
&gt; changes and get the cluster stable again on riak@IP. The ring files have a 
&gt; timestamp associated with them, but we only keep a few of the last ring 
&gt; files, so if enough gossip has happened then the pre-rename rings will have 
&gt; been destroyed. You will have to stop all nodes, put the ring files back as 
&gt; they were before the change and fix the names in vm.args and then restart the 
&gt; nodes.
&gt; 
&gt; 2) you can continue on the rebuild plan. stop all nodes, set the new names 
&gt; in vm.args, start the nodes again and rebuild the cluster, adding as many 
&gt; nodes as you can at once so they rebalance at the same time. When new nodes 
&gt; are added the claimant node works out ownership changes and will start a 
&gt; sequence of transfers. If new nodes are added once a sequence is under way 
&gt; the claimant will wait for that to complete, then check if there are any new 
&gt; nodes and repeat until all nodes are assigned. If you add all the nodes at 
&gt; once you will do less transfers over all.
&gt; 
&gt; 
&gt; If the cluster cannot be stopped, there are other things we might be able to 
&gt; do, but they're a bit more complex. What are your uptime requirements?
&gt; 
We have currently stopped the cluster and running on small amount of data. We 
can wait for the partition re-distribution to complete on Riak, but I don't 
have a strong feeling about it. "member\\_status" doesn't give us a correct 
picture: http://pastie.org/3849548. Is this expected behavior? I should also 
mention that all the nodes are still loading existing data and it will take few 
hours (2-3) until Riak KV is running on all of them.

Cheers
Nitish
&gt; Jon
&gt; 
&gt; 
&gt; 
&gt; On Wed, May 2, 2012 at 9:57 AM, Nitish Sharma  
&gt; wrote:
&gt; Hi Jon,
&gt; Thanks for your input. I've already started working on that lines. 
&gt; I stopped all the nodes, moved ring directory from one node, brought that one 
&gt; up, and issued join command to one other node (after moving the ring 
&gt; directory) - node2. While they were busy re-distributing the partitions, I 
&gt; started another node (node3) and issued join command (before risk\\_kv was 
&gt; running, since it takes some time to load existing data).
&gt; But after this, data handoffs are occurring only between node1 and node2. 
&gt; "member\\_status" says that node 3 owns 0% of the ring and 0% are pending.
&gt; We have a lot of data - each node serves around 200 million documents. Riak 
&gt; cluster is running 1.1.2.
&gt; Any suggestions?
&gt; 
&gt; Cheers
&gt; Nitish
&gt; On May 2, 2012, at 5:31 PM, Jon Meredith wrote:
&gt; 
&gt;&gt; Hi Nitish,
&gt;&gt; 
&gt;&gt; If you rebuild the cluster with the same ring size, the data will eventually 
&gt;&gt; get back to the right place. While the rebuild is taking place you may have 
&gt;&gt; notfounds for gets until the data has been handed off to the newly assigned 
&gt;&gt; owner (as it will be secondary handoff, not primary ownership handoff to get 
&gt;&gt; teh data back). If you don't have a lot of data stored in the cluster it 
&gt;&gt; shouldn't take too long.
&gt;&gt; 
&gt;&gt; The process would be to stop all nodes, move the files out of the ring 
&gt;&gt; directory to a safe place, start all nodes and rejoin. If you're using 
&gt;&gt; 1.1.x and you have capacity in your hardware you may want to increase 
&gt;&gt; handoff\\_concurrency to something like 4 to permit more transfers to happen 
&gt;&gt; across the cluster.
&gt;&gt; 
&gt;&gt; 
&gt;&gt; Jon.
&gt;&gt; 
&gt;&gt; 
&gt;&gt; 
&gt;&gt; On Wed, May 2, 2012 at 9:05 AM, Nitish Sharma  
&gt;&gt; wrote:
&gt;&gt; Hi,
&gt;&gt; We have a 12-node Riak cluster. Until now we were naming every new node as 
&gt;&gt; riak@. We then decided to rename the all the nodes to 
&gt;&gt; riak@, which makes troubleshooting easier.
&gt;&gt; After issuing reip command to two nodes, we noticed in the "status" that 
&gt;&gt; those 2 nodes were now appearing in the cluster with the old name as well as 
&gt;&gt; the new name. Other nodes were trying to handoff partitions to the "new" 
&gt;&gt; nodes, but apparently they were not able to. After this the whole cluster 
&gt;&gt; went down and completely stopped responding to any read/write requests.
&gt;&gt; member\\_status displayed old Riak name in "legacy" mode. Since this is our 
&gt;&gt; production cluster, we are desperately looking for some quick remedies. 
&gt;&gt; Issuing "force-remove" to the old names, restarting all the nodes, changing 
&gt;&gt; the riak names back to the old ones - none of it helped.
&gt;&gt; Currently, we are hosting limited amount of data. Whats an elegant way to 
&gt;&gt; recover from this mess? Would shutting off all the nodes, deleting the ring 
&gt;&gt; directory, and again forming the cluster work?
&gt;&gt; 
&gt;&gt; Cheers
&gt;&gt; Nitish
&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt; riak-users mailing list
&gt;&gt; riak-users@lists.basho.com
&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt; 
&gt;&gt; 
&gt;&gt; 
&gt;&gt; -- 
&gt;&gt; Jon Meredith
&gt;&gt; Platform Engineering Manager
&gt;&gt; Basho Technologies, Inc.
&gt;&gt; jmered...@basho.com
&gt;&gt; 
&gt; 
&gt; 
&gt; 
&gt; 
&gt; -- 
&gt; Jon Meredith
&gt; Platform Engineering Manager
&gt; Basho Technologies, Inc.
&gt; jmered...@basho.com
&gt; 

\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

