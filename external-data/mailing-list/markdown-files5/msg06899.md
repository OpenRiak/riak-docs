---
title: "Re: Problems writing objects to an half full bucket"
description: ""
project: community
lastmod: 2012-03-06T07:55:36-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg06899"
mailinglist_parent_id: "msg06895"
author_name: "Marco Monteiro"
project_section: "mailinglistitem"
sent_date: 2012-03-06T07:55:36-08:00
---


It makes sense, David. I'm going to give it a try.
Hopefully this will make it usable for the next month
until the issue is addressed.

I'll let you know how it goes.

Thanks,
Marco

On 6 March 2012 15:19, David Smith  wrote:

&gt; On Mon, Mar 5, 2012 at 9:55 PM, Marco Monteiro 
&gt; wrote:
&gt;
&gt; &gt; I'm using riak-js and the error I get is:
&gt; &gt;
&gt; &gt; { [Error: socket hang up] code: 'ECONNRESET' }
&gt;
&gt; That is a strange error -- are there any corresponding errors in
&gt; server logs? I would have expected a timeout or some such...
&gt;
&gt; &gt;
&gt; &gt; UUIDs. They are created by Riak. All my queries use 2i. The 2i are
&gt; integers
&gt; &gt; (representing seconds) and random strings (length 16) used as identifiers
&gt; &gt; for user sessions and similar.
&gt;
&gt; So, this explains why the problem goes away when you switch to an
&gt; empty bucket. A bit of background...
&gt;
&gt; If you're using the functionality in Riak that automatically generates
&gt; a UUID on PUT, you're going to get a uniformly distributed 160-bit
&gt; number (since the implementation SHA-1 hashes the input). This sort of
&gt; distribution is great for uniqueness, since there is a 1 in 2^160
&gt; chance (roughly) that you will encounter another similar ID. It can be
&gt; very bad from a caching perspective, however, if you have a cache that
&gt; uses pages of information for locality purposes. In a scheme such as
&gt; this (which is what LevelDB uses), the system will wind up churning
&gt; the cache constantly since the odds are quite low that the next UUID
&gt; to be accessed will be already in memory (remember, uniform
&gt; distribution of keys).
&gt;
&gt; LevelDB also makes this pathological case a bit worse by not having
&gt; bloom filters -- when inserting a new UUID, you will potentially have
&gt; to do 7 disk seeks just to determine if the UUID is not present. The
&gt; Google team is working to address this problem, but I'm guessing it'll
&gt; be a month or so before that's done and then we have to integrate with
&gt; Riak -- so we can't count on that just yet.
&gt;
&gt; Now, all is not lost. :)
&gt;
&gt; If you craft your keys so that there is some temporal locality \\_and\\_
&gt; the access pattern of your keys has some sort of exponential-ish
&gt; decay, you can still get very good performance out of LevelDB. One
&gt; simple way to do this is to prefix the current date-time on front of
&gt; the UUID, like so:
&gt;
&gt; 201203060806- (YMDhm-UUID)
&gt;
&gt; You could also use seconds since the epoch, etc. This has the effect
&gt; of keeping recently accessed/hot UUIDs on (close to) the same cache
&gt; page, and lets you avoid a lot of cache churn and typically
&gt; dramatically improves LevelDB performance.
&gt;
&gt; Does this help/make sense?
&gt;
&gt; D.
&gt; --
&gt; Dave Smith
&gt; VP, Engineering
&gt; Basho Technologies, Inc.
&gt; diz...@basho.com
&gt;
\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

