---
title: "Re: Yokozuna inconsistent search results"
description: ""
project: community
lastmod: 2016-03-11T08:20:37-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg17152"
mailinglist_parent_id: "msg17139"
author_name: "Oleksiy Krivoshey"
project_section: "mailinglistitem"
sent_date: 2016-03-11T08:20:37-08:00
---


Hi Fred,

This is production environment but I can delete the index. However this
index covers ~3500 buckets and there are probably 10,000,000 keys.

The index was created after the buckets. The schema for the index is just
the basic required fields (\\_yz\\_\\*) and nothing else.

Yes, I'm willing to resolve this. When you say to delete chunks\\_index, do
you mean the simple RpbYokozunaIndexDeleteReq or something else is required?

Thanks!




On 11 March 2016 at 17:08, Fred Dushin  wrote:

&gt; Hi Oleksiy,
&gt;
&gt; This is definitely pointing to an issue either in the coverage plan (which
&gt; determines the distributed query you are seeing) or in the data you have in
&gt; Solr. I am wondering if it is possible that you have some data in Solr
&gt; that is causing the rebuild of the YZ AAE tree to incorrectly represent
&gt; what is actually stored in Solr.
&gt;
&gt; What you did was to manually expire the YZ (Riak Search) AAE trees, which
&gt; caused them to rebuild from the entropy data stored in Solr. Another thing
&gt; we could try (if you are willing) would be to delete the 'chunks\\_index'
&gt; data in Solr (as well as the Yokozuna AAE data), and then let AAE repair
&gt; the missing data. What Riak will essentially do is compare the KV hash
&gt; trees with the YZ hash trees (which will be empty), too that it is missing
&gt; in Solr, and add it to Solr, as a result. This would effectively result in
&gt; re-indexing all of your data, but we are only talking about ~30k entries
&gt; (times 3, presumably, if your n\\_val is 3), so that shouldn't take too much
&gt; time, I wouldn't think. There is even some configuration you can use to
&gt; accelerate this process, if necessary.
&gt;
&gt; Is that something you would be willing to try? It would result in down
&gt; time on query. Is this production data or a test environment?
&gt;
&gt; -Fred
&gt;
&gt; On Mar 11, 2016, at 7:38 AM, Oleksiy Krivoshey  wrote:
&gt;
&gt; Here are two consequent requests, one returns 30118 keys, another 37134
&gt;
&gt; xml version="1.0" encoding="UTF-8"?
&gt; 
&gt; 
&gt; 0
&gt; 6
&gt; 
&gt; \\_yz\\_pn:92 OR \\_yz\\_pn:83 OR \\_yz\\_pn:71 OR
&gt; \\_yz\\_pn:59 OR \\_yz\\_pn:50 OR \\_yz\\_pn:38 OR \\_yz\\_pn:17 OR \\_yz\\_pn:5
&gt; \\_yz\\_pn:122 OR \\_yz\\_pn:110 OR \\_yz\\_pn:98 OR
&gt; \\_yz\\_pn:86 OR \\_yz\\_pn:74 OR \\_yz\\_pn:62 OR \\_yz\\_pn:26 OR \\_yz\\_pn:14 OR
&gt; \\_yz\\_pn:2
&gt; 
&gt; 10.0.1.1:8093/internal\\_solr/chunks\\_index,10.0.1.2:8093/internal\\_solr/chunks\\_index,10.0.1.3:8093/internal\\_solr/chunks\\_index,10.0.1.4:8093/internal\\_solr/chunks\\_index,10.0.1.5:8093/internal\\_solr/chunks\\_index
&gt; 
&gt; \\_yz\\_rb:0dmid2ilpyrfiuaqtvnc482f1esdchb5.chunks
&gt; (\\_yz\\_pn:124 AND (\\_yz\\_fpn:124 OR
&gt; \\_yz\\_fpn:123)) OR \\_yz\\_pn:116 OR \\_yz\\_pn:104 OR \\_yz\\_pn:80 OR \\_yz\\_pn:68 OR
&gt; \\_yz\\_pn:56 OR \\_yz\\_pn:44 OR \\_yz\\_pn:32 OR \\_yz\\_pn:20 OR \\_yz\\_pn:8
&gt; \\_yz\\_pn:113 OR \\_yz\\_pn:101 OR \\_yz\\_pn:89 OR
&gt; \\_yz\\_pn:77 OR \\_yz\\_pn:65 OR \\_yz\\_pn:53 OR \\_yz\\_pn:41 OR \\_yz\\_pn:29
&gt; \\_yz\\_pn:127 OR \\_yz\\_pn:119 OR \\_yz\\_pn:107 OR
&gt; \\_yz\\_pn:95 OR \\_yz\\_pn:47 OR \\_yz\\_pn:35 OR \\_yz\\_pn:23 OR \\_yz\\_pn:11
&gt; 0
&gt; 
&gt; 
&gt;  start="0"&gt;
&gt; 
&gt;
&gt; ------
&gt;
&gt;
&gt; xml version="1.0" encoding="UTF-8"?
&gt; 
&gt; 
&gt; 0
&gt; 10
&gt; 
&gt; \\_yz\\_pn:100 OR \\_yz\\_pn:88 OR \\_yz\\_pn:79 OR
&gt; \\_yz\\_pn:67 OR \\_yz\\_pn:46 OR \\_yz\\_pn:34 OR \\_yz\\_pn:25 OR \\_yz\\_pn:13 OR
&gt; \\_yz\\_pn:1
&gt; (\\_yz\\_pn:126 AND (\\_yz\\_fpn:126 OR
&gt; \\_yz\\_fpn:125)) OR \\_yz\\_pn:118 OR \\_yz\\_pn:106 OR \\_yz\\_pn:94 OR \\_yz\\_pn:82 OR
&gt; \\_yz\\_pn:70 OR \\_yz\\_pn:58 OR \\_yz\\_pn:22 OR \\_yz\\_pn:10
&gt; 
&gt; 10.0.1.1:8093/internal\\_solr/chunks\\_index,10.0.1.2:8093/internal\\_solr/chunks\\_index,10.0.1.3:8093/internal\\_solr/chunks\\_index,10.0.1.4:8093/internal\\_solr/chunks\\_index,10.0.1.5:8093/internal\\_solr/chunks\\_index
&gt; 
&gt; \\_yz\\_rb:0dmid2ilpyrfiuaqtvnc482f1esdchb5.chunks
&gt; \\_yz\\_pn:124 OR \\_yz\\_pn:112 OR \\_yz\\_pn:76 OR
&gt; \\_yz\\_pn:64 OR \\_yz\\_pn:52 OR \\_yz\\_pn:40 OR \\_yz\\_pn:28 OR \\_yz\\_pn:16 OR
&gt; \\_yz\\_pn:4
&gt; \\_yz\\_pn:121 OR \\_yz\\_pn:109 OR \\_yz\\_pn:97 OR
&gt; \\_yz\\_pn:85 OR \\_yz\\_pn:73 OR \\_yz\\_pn:61 OR \\_yz\\_pn:49 OR \\_yz\\_pn:37
&gt; \\_yz\\_pn:115 OR \\_yz\\_pn:103 OR \\_yz\\_pn:91 OR
&gt; \\_yz\\_pn:55 OR \\_yz\\_pn:43 OR \\_yz\\_pn:31 OR \\_yz\\_pn:19 OR \\_yz\\_pn:7
&gt; 0
&gt; 
&gt; 
&gt;  start="0"&gt;
&gt; 
&gt;
&gt; On 11 March 2016 at 12:05, Oleksiy Krivoshey  wrote:
&gt;
&gt;&gt; So event when I fixed 3 documents which caused AAE errors,
&gt;&gt; restarted AAE with riak\\_core\\_util:rpc\\_every\\_member\\_ann(yz\\_entropy\\_mgr,
&gt;&gt; expire\\_trees, [], 5000).
&gt;&gt; waited 5 days (now I see all AAE trees rebuilt in last 5 days and no AAE
&gt;&gt; or Solr errors), I still get inconsistent num\\_found.
&gt;&gt;
&gt;&gt; For a bucket with 30,000 keys each new search request can result in
&gt;&gt; difference in num\\_found for over 5,000.
&gt;&gt;
&gt;&gt; What else can I do to get consistent index, or at least not a 15%
&gt;&gt; difference.
&gt;&gt;
&gt;&gt; I even tried to walk through all the bucket keys and modifying them in a
&gt;&gt; hope that all Yokozuna instances in a cluster will pick them up, but no
&gt;&gt; luck.
&gt;&gt;
&gt;&gt; Thanks!
&gt;&gt; â€‹
&gt;&gt;
&gt;
&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;
&gt;
&gt;
&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;
&gt;
\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

