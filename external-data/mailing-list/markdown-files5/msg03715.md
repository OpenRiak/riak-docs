---
title: "Re: Problem with deleting keys"
description: ""
project: community
lastmod: 2011-06-16T09:25:26-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg03715"
author_name: "David Leimbach"
project_section: "mailinglistitem"
sent_date: 2011-06-16T09:25:26-07:00
---


On Thu, Jun 16, 2011 at 2:57 AM, Nico Meyer  wrote:

&gt; Hello David,
&gt;
&gt; this behaviour is quite expected if you think about how Riak works.
&gt; Assuming you use the default replication factor of n=3, each key is stored
&gt; on all of your three nodes. If you delete a key while one node (let's call
&gt; it A) is down, the key is deleted from the two nodes that are still up
&gt; (let's call them B and C), and remains on the downed node A.
&gt; Once node A is up again, the situation is indistinguishable from B and C
&gt; having a hard drive crash and loosing all their data, in that A has the key
&gt; and B and C know nothing about it.
&gt;

I think this is not the behavior anyone wants, regardless of "the way it
works". Cassandra does "tombstones", but even those can expire after 10
days (by default), so if it takes you 10 days to get node A back up again,
you're going to replicate it across the Cassandra ring as well.

Riak doesn't have tombstones (as far as I know) so, you have to make sure
all your nodes are up to do a delete. This, to me, seems like a misfeature.


&gt;
&gt; If you do a GET of the deleted key at this point, the result depends on the
&gt; r-value that you choose. For r&gt;1 you will get a not\\_found on the first get.
&gt; For r=1 you might get the data or a not\\_found, depending on which two nodes
&gt; answer first (see https://issues.basho.com/show\\_bug.cgi?id=992 about basic
&gt; quorum for an explanation). Also, at that point read repair will kick in and
&gt; re-replicate the key to all nodes, so subsequent GETs will always return the
&gt; original datum.
&gt;
&gt; listing keys on the other hand does not use quorum but just does a set
&gt; union of all keys of all the nodes in you cluster. Essentially it is
&gt; equivalent to r=1 without basic quorum. The same is true for map/reduce
&gt; queries to my knowledge
&gt;
&gt; The essential problem is that a real physical delete is indistinguishable
&gt; from data loss (or never having had the data in the first place), while
&gt; those two things are logically different.
&gt;

Right, there's no tombstone, or equivalent marker for deletion.


&gt; If you want to be sure that a key is deleted with all its replicas you must
&gt; delete it with a write quorum setting of w=n. Also you need to tell Riak not
&gt; to count fallback vnodes toward you write quorum. This feature is quite new
&gt; and I believe only available in the head revision. Also I forgot the name of
&gt; the parameter and don't know if it is even applicable for DELETEs.
&gt; Anyhow, if you do all this, your DELETEs will simply fail if any of the
&gt; nodes that has a copy of the key is down (so in your case, if any node is
&gt; down).
&gt;
&gt; This is a feature I didn't even know existed. (not using fallback vnodes)


&gt; If you only want to logically delete, and don't care about freeing the disk
&gt; space and RAM that is used by the key, you should use a special value, which
&gt; is interpreted by your application as a not found. That way you also get
&gt; proper conflict resolution between DELETEs and PUTs (say one client deletes
&gt; a key while another one updates it).
&gt;

Yes a tombstone marker could be implemented, possibly more safely, at the
application layer.


&gt;
&gt; Cheers,
&gt; Nico
&gt;
&gt; Am 16.06.2011 00:55, schrieb David Mitchell:
&gt;
&gt; Erlang: R13B04
&gt;
&gt; Riak: 0.14.2
&gt;
&gt;
&gt;
&gt; I have a three node cluster, and while one node was down, I deleted every
&gt; key in a certain bucket. Then, I started the node that was down, and it
&gt; joined the cluster.
&gt;
&gt;
&gt;
&gt; Now, when do a listing on these keys in this bucket, and I get the entire
&gt; list. I can also get the values of the bucket. However, when I try to
&gt; delete the keys, the keys are not deleted.
&gt;
&gt;
&gt;
&gt; Can anyone help me get the nodes back in a consistent state? I have tried
&gt; restarting the nodes.
&gt;
&gt;
&gt;
&gt; David
&gt;
&gt;
&gt;
&gt;
&gt;
&gt;
&gt;
&gt;
&gt;
&gt;
&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.comhttp://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;
&gt;
&gt;
&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;
&gt;
\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

