---
title: "Re: Ownership handoff timed out"
description: ""
project: community
lastmod: 2015-10-29T11:21:31-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg16723"
mailinglist_parent_id: "msg16718"
author_name: "Matthew Von-Maszewski"
project_section: "mailinglistitem"
sent_date: 2015-10-29T11:21:31-07:00
---


I queried Basho’s Client Services team. They tell me the upgrade / coexist 
should be no problem.

Matthew

&gt; On Oct 29, 2015, at 1:38 PM, Vladyslav Zakhozhai 
&gt;  wrote:
&gt; 
&gt; Matthew can you describe the bug more detail?
&gt; 
&gt; My plan was to migrate to eleveldb and only then to migrate to Riak 2.0. It 
&gt; seems that I need to change my plans to migrate to Riak 2.0 first. It is sad.
&gt; 
&gt; Is it safe to migrate Riak 1.4.12/Riak CS 1.5.0 to Riak 2.0 on production 
&gt; environment? According to official upgrade guides I can upgrade nodes one by 
&gt; one in the same cluster. So Riak 2.0 and Riak 1.4.12 nodes can coexist in one 
&gt; cluster. Am I right?
&gt; 
&gt; Thank you.
&gt; 
&gt; On Thu, Oct 29, 2015 at 7:04 PM Matthew Von-Maszewski  &gt; wrote:
&gt; Sad to say your LOG files suggest the same bug as seen elsewhere and fixed by 
&gt; recent changes in the leveldb code.
&gt; 
&gt; The tougher issue is that the fixes are currently only available for our 2.0 
&gt; product series. A backport would be non-trivial due to the number of places 
&gt; changed between 1.4 and 2.0 and the number of places the fix overlaps those 
&gt; changes. The corrected code is tagged “2.0.9” in eleveldb and leveldb.
&gt; 
&gt; The only path readily available to you is to have your receiving cluster 
&gt; upgraded to 2.0 Riak CS and manually build/patch eleveldb to the 2.0.9 
&gt; version. Then start your handoffs. (eleveldb version 2.0.9 is not present 
&gt; in any shipping version of Riak … yet). 
&gt; 
&gt; I will write again if I can think of an easier solution. But nothing is 
&gt; occurring to me or the team members I have queried.
&gt; 
&gt; Matthew
&gt; 
&gt; 
&gt;&gt; On Oct 29, 2015, at 12:14 PM, Vladyslav Zakhozhai 
&gt;&gt; &gt; wrote:
&gt;&gt; 
&gt; 
&gt;&gt; Hi,
&gt;&gt; 
&gt;&gt; Matthew thank for you answer. eleveldb LOGs are attached.
&gt;&gt; Here is LOGs from 2 eleveldb nodes (eggeater was not restarted; what about 
&gt;&gt; rattlesnake I'm not sure).
&gt;&gt; 
&gt;&gt; On Thu, Oct 29, 2015 at 5:24 PM Matthew Von-Maszewski &gt; &gt; wrote:
&gt;&gt; Hi,
&gt;&gt; 
&gt;&gt; There was a known eleveldb bug with handoff receiving that could cause a 
&gt;&gt; timeout. But it does not sound like bug fits your symptoms. However, I am 
&gt;&gt; willing to verify my diagnosis. I would need you to gather the LOG files 
&gt;&gt; from all vnodes on the RECEIVING side (or at least from the vnode that you 
&gt;&gt; are attempting and is failing).
&gt;&gt; 
&gt;&gt; I will check it for the symptoms of the known bug.
&gt;&gt; 
&gt;&gt; Note: the LOG files reset on each restart of Riak. So you must gather the 
&gt;&gt; LOG files right after the failure without restarting Riak.
&gt;&gt; 
&gt;&gt; Matthew
&gt;&gt; 
&gt;&gt; 
&gt;&gt;&gt; On Oct 29, 2015, at 11:11 AM, Vladyslav Zakhozhai 
&gt;&gt;&gt; &gt; wrote:
&gt;&gt;&gt; 
&gt;&gt;&gt; Hi,
&gt;&gt;&gt; 
&gt;&gt;&gt; I want to make small update. Jon your hint about problems on sender side is 
&gt;&gt;&gt; correct. As I've already told there problems with available resources on 
&gt;&gt;&gt; sender nodes. There are no enough available RAM which is a cause of 
&gt;&gt;&gt; swapiness and load on disks. Restarting of sender nodes helps me (at least 
&gt;&gt;&gt; temoprarily).
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; On Thu, Oct 29, 2015 at 12:19 PM Vladyslav Zakhozhai 
&gt;&gt;&gt; &gt; wrote:
&gt;&gt;&gt; Hi,
&gt;&gt;&gt; 
&gt;&gt;&gt; Average size of objects in Riak - 300 Kb. This objects are images. This 
&gt;&gt;&gt; data updates very very rearly (there almost no updates).
&gt;&gt;&gt; 
&gt;&gt;&gt; I have GC turned on and works:
&gt;&gt;&gt; root@python:~# riak-cs-gc status
&gt;&gt;&gt; There is no garbage collection in progress
&gt;&gt;&gt; The current garbage collection interval is: 900
&gt;&gt;&gt; The current garbage collection leeway time is: 86400
&gt;&gt;&gt; Last run started at: 20151029T100600Z
&gt;&gt;&gt; Next run scheduled for: 20151029T102100Z
&gt;&gt;&gt; 
&gt;&gt;&gt; Network misconfigurations were not detected. The result of your script 
&gt;&gt;&gt; shows correct info.
&gt;&gt;&gt; 
&gt;&gt;&gt; But I see that almost all nodes with bitcask suffers from low free memory 
&gt;&gt;&gt; and they swapped. I think that it can be an issue. But my question is, what 
&gt;&gt;&gt; workaround is for this problem.
&gt;&gt;&gt; 
&gt;&gt;&gt; I've wrote in my first post that I tuned handoff\\_timeout and 
&gt;&gt;&gt; handoff\\_receive\\_timeout (now this vaules are 300000 and 600000). But 
&gt;&gt;&gt; situation is the same.
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; On Tue, Oct 27, 2015 at 4:06 PM Jon Meredith &gt;&gt; &gt; wrote:
&gt;&gt;&gt; Hi,
&gt;&gt;&gt; 
&gt;&gt;&gt; Handoff problems without obvious disk issues can be due to the database 
&gt;&gt;&gt; containing large objects. Do you frequently update objects in CS, and if 
&gt;&gt;&gt; so have you had garbage collection running?
&gt;&gt;&gt; 
&gt;&gt;&gt; The timeout is happening on the receiver side after not receiving any tcp 
&gt;&gt;&gt; data for handoff\\_receive\\_timeout \\*milli\\*seconds. I know you said you 
&gt;&gt;&gt; increased it, but not how high. I would bump that up to 300000 to give the 
&gt;&gt;&gt; sender a chance to read larger objects off disk.
&gt;&gt;&gt; 
&gt;&gt;&gt; To check if the sender is transmitting, on the source node you could run
&gt;&gt;&gt; redbug:start("riak\\_core\\_handoff\\_sender:visit\\_item", [{arity, 
&gt;&gt;&gt; true},{print\\_file,"/tmp/visit\\_item.log"},{time, 3600000},{msgs, 1000000}]).
&gt;&gt;&gt; 
&gt;&gt;&gt; That file should fill fairly fast with an entry for every object the sender 
&gt;&gt;&gt; tries to transmit.
&gt;&gt;&gt; 
&gt;&gt;&gt; There's a long shot it could be network misconfiguration. Run this from the 
&gt;&gt;&gt; source node having problems 
&gt;&gt;&gt; 
&gt;&gt;&gt; rpc:multicall(erlang, apply, [fun() -&gt; TargetNode = node(), [\\_Name,Host] = 
&gt;&gt;&gt; string:tokens(atom\\_to\\_list(TargetNode), "@"), {ok, Port} = 
&gt;&gt;&gt; riak\\_core\\_gen\\_server:call({riak\\_core\\_handoff\\_listener, TargetNode}, 
&gt;&gt;&gt; handoff\\_port), HandoffIP = riak\\_core\\_handoff\\_listener:get\\_handoff\\_ip(), 
&gt;&gt;&gt; TNHandoffIP = case HandoffIP of error -&gt; Host; {ok, "0.0.0.0"} -&gt; Host; 
&gt;&gt;&gt; {ok, Other} -&gt; Other end, {node(), HandoffIP, TNHandoffIP, 
&gt;&gt;&gt; inet:gethostbyname(TNHandoffIP), Port} end, []]).
&gt;&gt;&gt; 
&gt;&gt;&gt; and it will print out a a list of remote nodes and IP addresses (and 
&gt;&gt;&gt; hopefully an empty list of failed nodes)
&gt;&gt;&gt; 
&gt;&gt;&gt; {[{'dev1@127.0.0.1 ', &lt;---- node name
&gt;&gt;&gt; {ok,"0.0.0.0"}, &lt;---- handoff ip address configured in 
&gt;&gt;&gt; app.config
&gt;&gt;&gt; "127.0.0.1", &lt;---- hostname passed to socket open
&gt;&gt;&gt; {ok,{hostent,"127.0.0.1",[],inet,4,[{127,0,0,1}]}}, &lt;--- DNS entry for 
&gt;&gt;&gt; hostname
&gt;&gt;&gt; 10019}], &lt;---- handoff port
&gt;&gt;&gt; []} &lt;--- empty list of errors
&gt;&gt;&gt; 
&gt;&gt;&gt; Good luck, Jon.
&gt;&gt;&gt; 
&gt;&gt;&gt; On Tue, Oct 27, 2015 at 3:55 AM Vladyslav Zakhozhai 
&gt;&gt;&gt; &gt; wrote:
&gt;&gt;&gt; Hi,
&gt;&gt;&gt; 
&gt;&gt;&gt; Jon thank you for the answer. During approval of my mail to this list I've 
&gt;&gt;&gt; troubleshoot my issue more deep. And yes, your are right. Neither {error, 
&gt;&gt;&gt; enotconn} nor max\\_concurrency is my problem.
&gt;&gt;&gt; 
&gt;&gt;&gt; I'm going to migrate my cluster entierly to eleveldb only, i.e. I need to 
&gt;&gt;&gt; refuse using bitcask. I have a talk with basho support and they said that 
&gt;&gt;&gt; it is tricky to tune bitcask on servers with 32 GB RAM (and I guess that it 
&gt;&gt;&gt; is not tricky, but it is impossible, because bitcask loads all keys in 
&gt;&gt;&gt; memory regardless of free available RAM). With LevelDB I have opportunity 
&gt;&gt;&gt; to tune using RAM on servers.
&gt;&gt;&gt; 
&gt;&gt;&gt; So I have 15 nodes with multibackend (bitcask for data and leveldb for 
&gt;&gt;&gt; metadata). 2 additional servers are without multibackend - only with 
&gt;&gt;&gt; leveldb. Now I'm not sure do I need still use mutibackend with levedb-only 
&gt;&gt;&gt; backend.
&gt;&gt;&gt; 
&gt;&gt;&gt; And my problem is (as I mentioned earlier) the following. On leveldb-only 
&gt;&gt;&gt; nodes I see handoffs timedout and no further progress.
&gt;&gt;&gt; 
&gt;&gt;&gt; On multibackend hosts I have configuration:
&gt;&gt;&gt; 
&gt;&gt;&gt; {riak\\_kv, [
&gt;&gt;&gt; {add\\_paths, ["/usr/lib/riak-cs/lib/riak\\_cs-1.5.0/ebin"]},
&gt;&gt;&gt; {storage\\_backend, riak\\_cs\\_kv\\_multi\\_backend},
&gt;&gt;&gt; {multi\\_backend\\_prefix\\_list, [{&lt;&lt;"0b:"&gt;&gt;, be\\_blocks}]},
&gt;&gt;&gt; {multi\\_backend\\_default, be\\_default},
&gt;&gt;&gt; {multi\\_backend, [
&gt;&gt;&gt; {be\\_default, riak\\_kv\\_eleveldb\\_backend, [
&gt;&gt;&gt; {max\\_open\\_files, 50},
&gt;&gt;&gt; {data\\_root, "/var/lib/riak/leveldb"}
&gt;&gt;&gt; ]},
&gt;&gt;&gt; {be\\_blocks, riak\\_kv\\_bitcask\\_backend, [
&gt;&gt;&gt; {data\\_root, "/var/lib/riak/bitcask"}
&gt;&gt;&gt; ]}
&gt;&gt;&gt; ]},
&gt;&gt;&gt; 
&gt;&gt;&gt; And for hosts with leveldb-only backend:
&gt;&gt;&gt; 
&gt;&gt;&gt; {riak\\_kv, [
&gt;&gt;&gt; {storage\\_backend, riak\\_kv\\_eleveldb\\_backend},
&gt;&gt;&gt; ...
&gt;&gt;&gt; {eleveldb, [ 
&gt;&gt;&gt; {data\\_root, "/var/lib/riak/leveldb"}
&gt;&gt;&gt; (default values for leveldb)
&gt;&gt;&gt; 
&gt;&gt;&gt; In leveldb logs I see nothing that could help me (no errors in logs).
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; On Mon, Oct 26, 2015 at 3:57 PM Jon Meredith &gt;&gt; &gt; wrote:
&gt;&gt;&gt; Hi,
&gt;&gt;&gt; 
&gt;&gt;&gt; I suspect your {error,enotconn} messages are unrelated - that's likely to 
&gt;&gt;&gt; be caused by an HTTP client closing the connection while Riak looks up 
&gt;&gt;&gt; some networking information about the requestor.
&gt;&gt;&gt; 
&gt;&gt;&gt; The max\\_concurrency message you are seeing is related to the handoff 
&gt;&gt;&gt; transfer limit - it should be labelled as informational. When a node has 
&gt;&gt;&gt; data to handoff it starts the handoff sender process and if there are 
&gt;&gt;&gt; either too many local handoff processes or too many on the remote side it 
&gt;&gt;&gt; exits with max\\_concurrency. You could increase with riak-admin 
&gt;&gt;&gt; transfer-limit but that probably won't help if you're timing out.
&gt;&gt;&gt; 
&gt;&gt;&gt; As you're using the multi-backend you're transferring data from bitcask and 
&gt;&gt;&gt; leveldb. The next place I would look is in the leveldb LOG files to see if 
&gt;&gt;&gt; there are any leveldb vnodes that are having problems that's preventing 
&gt;&gt;&gt; repair.
&gt;&gt;&gt; 
&gt;&gt;&gt; Jon
&gt;&gt;&gt; 
&gt;&gt;&gt; On Mon, Oct 26, 2015 at 7:15 AM Vladyslav Zakhozhai 
&gt;&gt;&gt; &gt; wrote:
&gt;&gt;&gt; Hello,
&gt;&gt;&gt; 
&gt;&gt;&gt; I have a problem with persistent timeouts during ownership handoffs. I've 
&gt;&gt;&gt; tried to surf over Internet and current mail list but no success.
&gt;&gt;&gt; 
&gt;&gt;&gt; I have Riak 1.4.12 cluster with 17 nodes. Almost all nodes use multibackend 
&gt;&gt;&gt; with bitcask and eleveldb as storage backends (we need multiple backend for 
&gt;&gt;&gt; Riak CS 1.5.0 integration).
&gt;&gt;&gt; 
&gt;&gt;&gt; Now I'm working to migrate Riak cluster to eleveldb as primary and only 
&gt;&gt;&gt; backend. For now I have 2 nodes with eleveldb backend in the same cluster.
&gt;&gt;&gt; 
&gt;&gt;&gt; During ownership handoff process I permanently see errors of timed out 
&gt;&gt;&gt; handoff receivers and sender.
&gt;&gt;&gt; 
&gt;&gt;&gt; Here is partial output of riak-admin transfers:
&gt;&gt;&gt; ...
&gt;&gt;&gt; transfer type: ownership\\_transfer
&gt;&gt;&gt; vnode type: riak\\_kv\\_vnode
&gt;&gt;&gt; partition: 331121464707782692405522344912282871640797216768
&gt;&gt;&gt; started: 2015-10-21 08:32:55 [46.66 min ago]
&gt;&gt;&gt; last update: no updates seen
&gt;&gt;&gt; total size: unknown
&gt;&gt;&gt; objects transferred: unknown
&gt;&gt;&gt; 
&gt;&gt;&gt; unknown 
&gt;&gt;&gt; riak@taipan.pleiad.uaprom  =======&gt; 
&gt;&gt;&gt; r...@eggeater.pleiad.uapr 
&gt;&gt;&gt; om 
&gt;&gt;&gt; | | 0% 
&gt;&gt;&gt; unknown 
&gt;&gt;&gt; 
&gt;&gt;&gt; transfer type: ownership\\_transfer
&gt;&gt;&gt; vnode type: riak\\_kv\\_vnode
&gt;&gt;&gt; partition: 336830455478606531929755488790080852186328203264
&gt;&gt;&gt; started: 2015-10-21 08:32:54 [46.68 min ago]
&gt;&gt;&gt; last update: no updates seen
&gt;&gt;&gt; total size: unknown
&gt;&gt;&gt; objects transferred: unknown
&gt;&gt;&gt; ...
&gt;&gt;&gt; 
&gt;&gt;&gt; Some of partition handoffs state never updates, some of them terminates 
&gt;&gt;&gt; after partial handoff objects and never starts again.
&gt;&gt;&gt; 
&gt;&gt;&gt; I see nothing in logs but following:
&gt;&gt;&gt; 
&gt;&gt;&gt; On receiver side:
&gt;&gt;&gt; 
&gt;&gt;&gt; 2015-10-21 11:33:55.131 [error] 
&gt;&gt;&gt; &lt;0.25390.1266&gt;@riak\\_core\\_handoff\\_receiver:handle\\_info:105 Handoff receiver 
&gt;&gt;&gt; for partition 331121464707782692405522344912282871640797216768 timed out 
&gt;&gt;&gt; after processing 0 objects.
&gt;&gt;&gt; 
&gt;&gt;&gt; On sender side:
&gt;&gt;&gt; 
&gt;&gt;&gt; 2015-10-21 11:01:58.879 [error] &lt;0.13177.1401&gt; CRASH REPORT Process 
&gt;&gt;&gt; &lt;0.13177.1401&gt; with 0 neighbours crashed with reason: no function clause 
&gt;&gt;&gt; matching webmachine\\_request:peer\\_from\\_peername({error,enotconn}, 
&gt;&gt;&gt; {webmachine\\_request,{wm\\_reqstate,#Port&lt;0.50978116&gt;,[],undefined,undefined,undefined,{wm\\_reqdata,...},...}})
&gt;&gt;&gt; line 150
&gt;&gt;&gt; 2015-10-21 11:32:50.055 [error] &lt;0.207.0&gt; Supervisor 
&gt;&gt;&gt; riak\\_core\\_handoff\\_sender\\_sup had child riak\\_core\\_handoff\\_sender started 
&gt;&gt;&gt; with {riak\\_core\\_handoff\\_sender,start\\_link,undefined} at &lt;0.22312.1090&gt; exit 
&gt;&gt;&gt; with reason max\\_concurrency in context child\\_terminated
&gt;&gt;&gt; 
&gt;&gt;&gt; {error, enotconn} - seems to be network issue. But I have no any problems 
&gt;&gt;&gt; with network. All hosts resolve their neighbors correctly and /etc/hosts on 
&gt;&gt;&gt; each node are correct.
&gt;&gt;&gt; 
&gt;&gt;&gt; I've tried to increase handoff\\_timeout and handoff\\_receive\\_timeout. But no 
&gt;&gt;&gt; success.
&gt;&gt;&gt; 
&gt;&gt;&gt; Forcing handoff helped me but for short period of time:
&gt;&gt;&gt; 
&gt;&gt;&gt; rpc:multicall([node() | nodes()], riak\\_core\\_vnode\\_manager, force\\_handoffs, 
&gt;&gt;&gt; []).
&gt;&gt;&gt; 
&gt;&gt;&gt; I see progress of handoffs (riak-admin transfers) but then I see handoff 
&gt;&gt;&gt; timed out again.
&gt;&gt;&gt; 
&gt;&gt;&gt; A week ago I've joined 4 nodes with bitcask. And there was no such problems.
&gt;&gt;&gt; 
&gt;&gt;&gt; I'm confused a little bit and need to understand my next steps in 
&gt;&gt;&gt; troubleshooting this issue.
&gt;&gt;&gt; 
&gt;&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt; riak-users@lists.basho.com 
&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com 
&gt;&gt;&gt; 
&gt;&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt; riak-users@lists.basho.com 
&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com 
&gt;&gt;&gt; 
&gt;&gt; 
&gt; 
&gt;&gt; 
&gt; 

\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

