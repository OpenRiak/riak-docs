---
title: "Re: Riak 2i http query much faster than python api?"
description: ""
project: community
lastmod: 2013-04-10T19:50:49-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg10845"
mailinglist_parent_id: "msg10844"
author_name: "Jeff Peck"
project_section: "mailinglistitem"
sent_date: 2013-04-10T19:50:49-07:00
---


&gt; Out of curiousity, how are you planning on segmenting the data?
&gt; 

My plan to segment the data would be to have a secondary index on a key called 
seg\\_id (or something similar).

When I add an object to Riak, I will set seg\\_id to be the first three 
characters of the md5 of the object's key, which should yield an even 
distribution.

Then, when querying the data, I will run map-reduce against each segment (so 
for 3 hexadecimal characters, it would be 4,096 map-reduce queries).

The inputs part of the query would look like this:

"inputs":{
 "bucket":"mybucket",
 "index":"seg\\_id\\_bin",
 "key":"aaa"
 }

I would run the map-reduce queries in parallel.

It sounds like a lot of work to just get the value of one field, which makes me 
think that there is a better way. Plus, I do not know that this will actually 
work as fast as I expect it to. That's why I'm asking here before I implement 
it.
&gt; Also, how are you setting up your servers? Single nodes? Multiple nodes?
&gt; 

I am using the default Riak installation (with leveldb as the backend and 
search turned on). I am on a 16 core 3Ghz node with 20Gb of memory, however it 
appears that Riak is not using all of the resources available to it. I suspect 
that this can be resolved by modifying the configuration

That said, if you, or anyone reading this, could suggest a configuration that 
is more suited for performing a relatively small batch operation across 900k 
(and soon to be about 5 million) or objects, that would be greatly appreciated.

Thanks!

- Jeff


On Apr 10, 2013, at 10:32 PM, Shuhao Wu  wrote:

&gt; Out of curiousity, how are you planning on segmenting the data? Map reduce 
&gt; will execute over the entire data set.
&gt; 
&gt; Also, how are you setting up your servers? Single nodes? Multiple nodes?
&gt; 
&gt; Shuhao
&gt; Sent from my phone.
&gt; 
&gt; On 2013-04-10 10:25 PM, "Jeff Peck"  wrote:
&gt; As a follow-up to this thread and my thread from earlier today, I am 
&gt; basically looking for a simple way to extract the value of a single field 
&gt; from approximately 900,000 documents (which happens to be indexed). I have 
&gt; been trying many options including a map-reduce function that executes 
&gt; entirely over http (taking out any python client bottlenecks). I let that run 
&gt; for over an hour before I stopped it. It did not return any output.
&gt; 
&gt; I also have tried grabbing a list of the 900k keys from a secondary index 
&gt; (very fast, about 11 seconds) and then trying to fetch each key in parallel 
&gt; (using curl and gnu parallel). That was also too slow to be feasible.
&gt; 
&gt; Is there something basic that I am missing?
&gt; 
&gt; One idea that I though of was to have a secondary index that is intended to 
&gt; split all of my data into segments. I would use the first three characters of 
&gt; the md5 of the document's key in hexadecimal format. So, the index would 
&gt; contain strings like "ae1", "2f4", "5ee", etc. Then, I can run my map-reduce 
&gt; query against \\*each\\* segment individually and possibly even in parallel.
&gt; 
&gt; I have observed that map-reduce is very fast with small sets of data (i.e. 
&gt; 5,000 objects), but with 900,000 objects it does not appear to run in a 
&gt; proportionately fast time. So, the idea is to divide the data into segments 
&gt; that can be better handled by map-reduce.
&gt; 
&gt; Before I implement this, I want to ask: Does this seem like the appropriate 
&gt; way to handle this type of operation? And, is there any better way to do this 
&gt; in the current version of Riak?
&gt; 
&gt; 
&gt; On Apr 10, 2013, at 6:10 PM, Shuhao Wu  wrote:
&gt; 
&gt;&gt; There are some inefficiencies in the python client... I've been profiling it 
&gt;&gt; recently and found that it occasionally takes the python client longer when 
&gt;&gt; you're on the same machine.
&gt;&gt; 
&gt;&gt; Perhaps Sean could comment?
&gt;&gt; 
&gt;&gt; Shuhao
&gt;&gt; Sent from my phone.
&gt;&gt; 
&gt;&gt; On 2013-04-10 4:04 PM, "Jeff Peck"  wrote:
&gt;&gt; Thanks Evan. I tried doing it in python like this (realizing that the 
&gt;&gt; previous way I did it uses MapReduce) and I had better results. It finished 
&gt;&gt; in 3.5 minutes, but nowhere close to the 15 seconds from the straight http 
&gt;&gt; query:
&gt;&gt; 
&gt;&gt; import riak
&gt;&gt; from pprint import pprint
&gt;&gt; 
&gt;&gt; bucket\\_name = "mybucket"
&gt;&gt; 
&gt;&gt; client = riak.RiakClient(port=8087,transport\\_class=riak.RiakPbcTransport)
&gt;&gt; bucket = client.bucket(bucket\\_name)
&gt;&gt; results = bucket.get\\_index('status\\_bin', 'PERSISTED')
&gt;&gt; 
&gt;&gt; print len(results)
&gt;&gt; 
&gt;&gt; 
&gt;&gt; On Apr 10, 2013, at 4:00 PM, Evan Vigil-McClanahan  
&gt;&gt; wrote:
&gt;&gt; 
&gt;&gt; &gt; get\\_index() is the right function there, I think.
&gt;&gt; &gt;
&gt;&gt; &gt; On Wed, Apr 10, 2013 at 2:53 PM, Jeff Peck  wrote:
&gt;&gt; &gt;&gt; I can grab over 900,000 keys from an indexs, using an http query in about 
&gt;&gt; &gt;&gt; 15 seconds, whereas the same operation in python times out after 5 
&gt;&gt; &gt;&gt; minutes. Does this indicate that I am using the python API incorrectly? 
&gt;&gt; &gt;&gt; Should I be relying on an http request initially when I need to grab this 
&gt;&gt; &gt;&gt; many keys?
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; (Note: This is tied to the question that I asked earlier, but is also a 
&gt;&gt; &gt;&gt; general question to help understand the proper usage of the python API.)
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; Thanks! Examples are below.
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; - Jeff
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; ---
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; HTTP:
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; $ time curl -s 
&gt;&gt; &gt;&gt; http://localhost:8098/buckets/mybucket/index/status\\_bin/PERSISTED | grep 
&gt;&gt; &gt;&gt; -o , | wc -l
&gt;&gt; &gt;&gt; 926047
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; real 0m14.583s
&gt;&gt; &gt;&gt; user 0m2.500s
&gt;&gt; &gt;&gt; sys 0m0.270s
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; ---
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; Python:
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; import riak
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; bucket = "my bucket"
&gt;&gt; &gt;&gt; client = riak.RiakClient(port=8098)
&gt;&gt; &gt;&gt; results = client.index(bucket, 'status\\_bin', 
&gt;&gt; &gt;&gt; 'PERSISTED').run(timeout=5\\*60\\*1000) # 5 minute timeout
&gt;&gt; &gt;&gt; print len(results)
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; (times out after 5 minutes)
&gt;&gt; &gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt; &gt;&gt; riak-users mailing list
&gt;&gt; &gt;&gt; riak-users@lists.basho.com
&gt;&gt; &gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt; 
&gt;&gt; 
&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt; riak-users mailing list
&gt;&gt; riak-users@lists.basho.com
&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt; 

\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

