---
title: "Re: Bucket planning"
description: ""
project: community
lastmod: 2010-04-19T23:17:41-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg00083"
mailinglist_parent_id: "msg00082"
author_name: "Brandon Smith"
project_section: "mailinglistitem"
sent_date: 2010-04-19T23:17:41-07:00
---


I think I understand your suggestion regarding an additional index in
other buckets.

To use your "keys per time slice" example, if I wanted to get all of
last month's phone call records, I will have already built up a
bucket-key pair that contains a list of keys...

GET /riak/callreport/2010\\_03

[["call","1234567890"],["call","1234567891"],["call","1234567892"]]


With this list, I could then use it as input for a Map/Reduce job. I
can see how these types of inverted indices can be built up through
the commit hooks (although at present, it seems providing and Erlang
function is the only option, right???).

Frankly, if I've got it right, it seems that this pattern is ripe for
being baked into Riak by way of some form of declarative means... or
through some pre-built commit functions.

Nevertheless, there are one conceptual leap that I am failing to grasp
about Riak...

1) How do I do the equivalent of a LIMIT/OFFSET in Riak?

I will be storing a very large number of bucket-key pairs of which I
need to retrieve a subset and always ordered chronologically. I
presume that if such an inverted index as shown above is maintained in
an array that chronological order can be preserved implicitly without
having to any type of forced ordering function.

However, how can I page through the results of a dataset? If even the
inverted index is sufficiently large (I estimate a constant of 57,600
events a day just for the initial event type Riak will be taking on...
other types of events not initially introduced will increase as the
number of customers in our multi-tenant system increase) is there a
pattern for paging through data in Riak?

Brandon

On Tue, Apr 20, 2010 at 12:04 AM, Alexander Sicular  wrote:
&gt; I would go with A. An advantage of your data is that it is immutable. Since
&gt; it never changes you can do extensive m/r pre computations and have them run
&gt; continuously on some frequency. I would also probably spend some time
&gt; thinking about the new pre/post hook features to potentially build some
&gt; additional index in other buckets. Like in particular keys per time slice.
&gt;
&gt; Do let us know how you proceed!
&gt;
&gt; -Alexander
&gt;
&gt;
&gt; @siculars on twitter
&gt; http://siculars.posterous.com
&gt;
&gt; Sent from my iPhone
&gt;
&gt; On Apr 19, 2010, at 21:36, Brandon Smith  wrote:
&gt;
&gt;&gt; I am looking to use Riak as a data store for events in our system.
&gt;&gt; Namely, I have a handful of event types now and anticipate much more
&gt;&gt; later.
&gt;&gt;
&gt;&gt; Consider the following "event":
&gt;&gt;
&gt;&gt; {
&gt;&gt;  "type" : "bws.stats",
&gt;&gt;  "host" : "10.1.55.101",
&gt;&gt;  "description" : "human friendly description",
&gt;&gt;  "details" : "string or Object",
&gt;&gt;  "timestamp" : 1234567890,
&gt;&gt;  "status" : "SUCCESS|FAIL"
&gt;&gt; }
&gt;&gt;
&gt;&gt; A JavaScript client will retrieve events and display them on a webpage
&gt;&gt; and filter the data based on:
&gt;&gt;
&gt;&gt; 1) show just one event type
&gt;&gt; 2) show all events for a given host
&gt;&gt; 3) show all events for last Wednesday
&gt;&gt; 4) show all events that had a status of SUCCESS
&gt;&gt; 5) show the 10 events before a specific FAIL that matches the same host
&gt;&gt; and type
&gt;&gt;
&gt;&gt; There are other ways that we plan to query the data, but I think you
&gt;&gt; get the idea.
&gt;&gt;
&gt;&gt; The event data payload is flexible if the above is not optimal for Riak.
&gt;&gt;
&gt;&gt; Thoughts on how to best implement in Riak?
&gt;&gt;
&gt;&gt; I've considered several approaches...
&gt;&gt;
&gt;&gt; A) Store all events in one bucket and build out extensive,
&gt;&gt; parameterized map/reduce functionality in order to return specialized
&gt;&gt; data sets as described above
&gt;&gt; B) Store all events in separate buckets (based on type?)... but where
&gt;&gt; does that leave me when needing to pivot on other fields
&gt;&gt; C) Have as many buckets as I want to build data sets and duplicate
&gt;&gt; data across the buckets... i.e. a "type" bucket, "host" bucket, and
&gt;&gt; "status" bucket... except this starts to get a bit painful in order to
&gt;&gt; populate buckets that I didn't anticipate
&gt;&gt; D) Store all events in one bucket and have categorical buckets whose
&gt;&gt; entries are NOT duplicate, but rather link to the main "event" bucket
&gt;&gt;
&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt; riak-users mailing list
&gt;&gt; riak-users@lists.basho.com
&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;

\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

