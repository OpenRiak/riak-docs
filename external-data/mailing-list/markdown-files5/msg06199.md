---
title: "Re: missing writes / inconsistent number of keys read after a bulk	write"
description: ""
project: community
lastmod: 2012-01-09T21:24:53-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg06199"
mailinglist_parent_id: "msg06189"
author_name: "Alexander Sicular"
project_section: "mailinglistitem"
sent_date: 2012-01-09T21:24:53-08:00
---


Not sure how your binding does it but MyBucket.keys() should be calling the 
list keys feature of riak via the streaming mode which chunks the key reply 
instead of buffering and sending one entire list of keys. 


@siculars on twitter
http://siculars.posterous.com

Sent from my iRotaryPhone

On Jan 9, 2012, at 17:39, Karthik K  wrote:

&gt; (Changing subject to reflect the problem better and reposting ). 
&gt; 
&gt; Any idea , based on the configuration inline as to what explains the 
&gt; inconsistency number of keys read after a bulk write ( say 1M of payload 1000 
&gt; bytes ). 
&gt; 
&gt; Any appropriate write flush setting , that is missed on the client / server 
&gt; side ? 
&gt; 
&gt; 
&gt; --
&gt; Karthik. 
&gt; 
&gt; 
&gt; On Fri, Jan 6, 2012 at 5:54 PM, Karthik K  wrote:
&gt; Further, 
&gt; ulimit -n is 10K on the box. 
&gt; 
&gt; 
&gt; # tail -100 /var//log/riak/erlang.log.1 
&gt; 
&gt; ....
&gt; 17:26:26.960 [info] alarm\\_handler: {set,{system\\_memory\\_high\\_watermark,[]}}
&gt; /usr/lib/riak/lib/os\\_mon-2.2.6/priv/bin/memsup: Erlang has closed. 
&gt; Erlang has closed
&gt; 17:26:34.443 [info] alarm\\_handler: {clear,system\\_memory\\_high\\_watermark}
&gt; 
&gt; 
&gt; So - is there a commit / flush setting that is missed when it comes to high 
&gt; volume writes ? 
&gt; 
&gt; 
&gt; On Fri, Jan 6, 2012 at 5:37 PM, Karthik K  wrote:
&gt; I am using Riak with LevelDB as the storage engine. 
&gt; 
&gt; app.config:
&gt; 
&gt; {storage\\_backend, riak\\_kv\\_eleveldb\\_backend},
&gt; 
&gt; 
&gt; {eleveldb, [
&gt; {data\\_root, "/var/lib/riak/leveldb"},
&gt; {write\\_buffer\\_size, 4194304}, %% 4MB in bytes
&gt; {max\\_open\\_files, 50}, %% Maximum number of files open at once per 
&gt; partition
&gt; {block\\_size, 65536}, %% 4K blocks
&gt; {cache\\_size, 33554432}, %% 32 MB default cache size per-partition
&gt; {verify\\_checksums, true} %% make sure data is what we expected it 
&gt; to be
&gt; ]},
&gt; 
&gt; 
&gt; 
&gt; 
&gt; I want to insert a million keys into the store ( into a given bucket ) . 
&gt; 
&gt; pseudo-code: 
&gt; riakClient = RiakFactory.pbcClient(); 
&gt; myBucket = riakClient.createBucket("myBucket").nVal(1).execute();
&gt; for (int i = 1; i &lt;= 1000000; ++i) {
&gt; final String key = String.valueOf(i);
&gt; myBucket.store(key, new String(payload)).returnBody(false);
&gt; }
&gt; 
&gt; 
&gt; after this operation, when I do: 
&gt; 
&gt; int count = 0;
&gt; for (String key : myBucket.keys() ) {
&gt; ++count; 
&gt; }
&gt; return count; 
&gt; 
&gt; This returns a total of 14K keys, while I was expecting close to 1 million or 
&gt; so. 
&gt; 
&gt; I am using riak-java-client (pbc). 
&gt; 
&gt; Which setting / missing client code can explain the discrepancy ? Thanks. 
&gt; 
&gt; 
&gt; 
&gt; 
&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

