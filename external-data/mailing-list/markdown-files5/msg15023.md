---
title: "Re: Memory-backend TTL"
description: ""
project: community
lastmod: 2014-10-13T17:04:08-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg15023"
mailinglist_parent_id: "msg15022"
author_name: "Lucas Grijander"
project_section: "mailinglistitem"
sent_date: 2014-10-13T17:04:08-07:00
---


Hi Luke.

I really appreciate your efforts to attempt to reproduce the problem. I
think that the configs are right. I have been doing also a lot of tests and
with 1 server/node, the memory bucket works flawlessly, as your test. The
Riak cluster where we have the problem has a multi\\_backend with 1 memory
backend, 2 bitcask backends and 2 leveldb backends. I have only changed the
parameter connection of the memory backend in our production code to
another new "cluster" with only 1 node, with the same config of Riak but
with only 1 memory backend under the multi configuration and, as I said,
all fine, the problem vanished. I deduce that the problem appears only with
more than 1 node and with a lot of requests.

In my tests with the production cluster with the problem ( 4 nodes),
finally I realized that the TTL is working but, randomly and suddenly, KEYS
already deleted appear, and KEYS with correct TTL disappear :-? (Maybe
something related with the some ETS internal table? ) This is the moment
when I can obtain KEYS already expired.

In summary:

- With cluster with 4 nodes (config below): All OK for a while and suddenly
we lost the last 20 seconds approx. of keys and OLD keys appear in the
list: curl -X GET http://localhost:8098/buckets/ttl\\_stg/keys?keys=true

buckets.default.last\\_write\\_wins = true
bitcask.io\\_mode = erlang
multi\\_backend.ttl\\_stg.storage\\_backend = memory
multi\\_backend.ttl\\_stg.memory\\_backend.ttl = 90s
multi\\_backend.ttl\\_stg.memory\\_backend.max\\_memory\\_per\\_vnode = 25MB
anti\\_entropy = passive
ring\\_size = 256

- With 1 node: All OK

buckets.default.n\\_val = 1
buckets.default.last\\_write\\_wins = true
buckets.default.r = 1
buckets.default.w = 1
multi\\_backend. ttl\\_stg.storage\\_backend = memory
multi\\_backend. ttl\\_stg.memory\\_backend.ttl = 90s
multi\\_backend. ttl\\_stg.memory\\_backend.max\\_memory\\_per\\_vnode = 250MB
ring\\_size = 16



Another note: With this 1 node (32GB RAM) and only activated the memory
backend I have realized than the memory consumption grows without control:


# riak-admin status|grep memory
memory\\_total : 17323130960
memory\\_processes : 235043016
memory\\_processes\\_used : 233078456
memory\\_system : 17088087944
memory\\_atom : 561761
memory\\_atom\\_used : 561127
memory\\_binary : 6737787976
memory\\_code : 14370908
memory\\_ets : 10295224544

# # riak-admin diag -d debug
[debug] Local RPC: os:getpid([]) [5000]
[debug] Running shell command: ps -o pmem,rss -p 17521
[debug] Shell command output:
%MEM RSS
60.5 19863800

Wow 18.9GB when the max\\_memory\\_per\\_vnode = 250MB. Is far away from the
value, 250\\*16vnodes = 4000MB. Is it that correct?

This is the riak-admin vnode-status of 1 vnode, the other 15 are with
similar data:

VNode: 1370157784997721485815954530671515330927436759040
Backend: riak\\_kv\\_multi\\_backend
Status:
[{&lt;&lt;"ttl\\_stg"&gt;&gt;,
 [{mod,riak\\_kv\\_memory\\_backend},
 {data\\_table\\_status,[{compressed,false},
 {memory,1156673},
 {owner,&lt;8343.9466.104&gt;},
 {heir,none},

 {name,riak\\_kv\\_1370157784997721485815954530671515330927436759040},
 {size,29656},
 {node,'riak@xxxxxxxx'},
 {named\\_table,false},
 {type,ordered\\_set},
 {keypos,1},
 {protection,protected}]},
 {index\\_table\\_status,[{compressed,false},
 {memory,89},
 {owner,&lt;8343.9466.104&gt;},
 {heir,none},

{name,riak\\_kv\\_1370157784997721485815954530671515330927436759040\\_i},
 {size,0},
 {node,'riak@xxxxxxxxx'},
 {named\\_table,false},
 {type,ordered\\_set},
 {keypos,1},
 {protection,protected}]},
 {time\\_table\\_status,[{compressed,false},
 {memory,75968936},
 {owner,&lt;8343.9466.104&gt;},
 {heir,none},

 {name,riak\\_kv\\_1370157784997721485815954530671515330927436759040\\_t},
 {size,2813661},
 {node,'riak@xxxxxxxxx'},
 {named\\_table,false},
 {type,ordered\\_set},
 {keypos,1},
 {protection,protected}]}]}]

Thanks!

2014-10-13 22:30 GMT+02:00 Luke Bakken :

&gt; Hi Lucas,
&gt;
&gt; I've tried reproducing this using a local Riak 2.0.1 node, however TTL
&gt; is working as expected.
&gt;
&gt; Here is the configuration I have in /etc/riak/riak.conf:
&gt;
&gt; storage\\_backend = multi
&gt; multi\\_backend.default = bc\\_default
&gt;
&gt; multi\\_backend.ttl\\_stg.storage\\_backend = memory
&gt; multi\\_backend.ttl\\_stg.memory\\_backend.ttl = 90s
&gt; multi\\_backend.ttl\\_stg.memory\\_backend.max\\_memory\\_per\\_vnode = 4MB
&gt;
&gt; multi\\_backend.bc\\_default.storage\\_backend = bitcask
&gt; multi\\_backend.bc\\_default.bitcask.data\\_root = /var/lib/riak/bc\\_default
&gt; multi\\_backend.bc\\_default.bitcask.io\\_mode = erlang
&gt;
&gt; This translates to the following in
&gt; /var/lib/riak/generated.configs/app.2014.10.13.13.13.29.config:
&gt;
&gt; {multi\\_backend\\_default,&lt;&lt;"bc\\_default"&gt;&gt;},
&gt; {multi\\_backend,
&gt; [{&lt;&lt;"ttl\\_stg"&gt;&gt;,riak\\_kv\\_memory\\_backend,[{ttl,90},{max\\_memory,4}]},
&gt; {&lt;&lt;"bc\\_default"&gt;&gt;,riak\\_kv\\_bitcask\\_backend,
&gt; [{io\\_mode,erlang},
&gt; {expiry\\_grace\\_time,0},
&gt; {small\\_file\\_threshold,10485760},
&gt; {dead\\_bytes\\_threshold,134217728},
&gt; {frag\\_threshold,40},
&gt; {dead\\_bytes\\_merge\\_trigger,536870912},
&gt; {frag\\_merge\\_trigger,60},
&gt; {max\\_file\\_size,2147483648},
&gt; {open\\_timeout,4},
&gt; {data\\_root,"/var/lib/riak/bc\\_default"},
&gt; {sync\\_strategy,none},
&gt; {merge\\_window,always},
&gt; {max\\_fold\\_age,-1},
&gt; {max\\_fold\\_puts,0},
&gt; {expiry\\_secs,-1},
&gt; {require\\_hint\\_crc,true}]}]}]},
&gt;
&gt; I set the bucket properties to use the ttl\\_stg backend:
&gt;
&gt; root@UBUNTU-12-1:~# cat ttl\\_stg-props.json
&gt; {"props":{"name":"ttl\\_stg","backend":"ttl\\_stg"}}
&gt;
&gt; root@UBUNTU-12-1:~# curl -XPUT -H'Content-type: application/json'
&gt; localhost:8098/buckets/ttl\\_stg/props --data-ascii @ttl\\_stg-props.json
&gt;
&gt; root@UBUNTU-12-1:~# curl -XGET localhost:8098/buckets/ttl\\_stg/props
&gt;
&gt; {"props":{"allow\\_mult":false,"backend":"ttl\\_stg","basic\\_quorum":false,"big\\_vclock":50,"chash\\_keyfun":{"mod":"riak\\_core\\_util","fun":"chash\\_std\\_keyfun"},"dvv\\_enabled":false,"dw":"quorum","last\\_write\\_wins":false,"linkfun":{"mod":"riak\\_kv\\_wm\\_link\\_walker","fun":"mapreduce\\_linkfun"},"n\\_val":3,"name":"ttl\\_stg","notfound\\_ok":true,"old\\_vclock":86400,"postcommit":[],"pr":0,"precommit":[],"pw":0,"r":"quorum","rw":"quorum","small\\_vclock":50,"w":"quorum","young\\_vclock":20}}
&gt;
&gt;
&gt; And used the following statement to PUT test data:
&gt;
&gt; curl -XPUT localhost:8098/buckets/ttl\\_stg/keys/1 -d "TEST $(date)"
&gt;
&gt; After 90 seconds, this is the response I get from Riak:
&gt;
&gt; root@UBUNTU-12-1:~# curl -XGET localhost:8098/buckets/ttl\\_stg/keys/1
&gt; not found
&gt;
&gt; I would carefully check all of the app.config / riak.conf files in
&gt; your cluster, the output of "riak config effective" and the bucket
&gt; properties for those buckets you expect to be using the memory backend
&gt; with TTL. I also recommend using the localhost:8098/buckets/ endpoint
&gt; instead of the deprecated riak/ endpoint.
&gt;
&gt; Please let me know if you have additional questions.
&gt; --
&gt; Luke Bakken
&gt; Engineer / CSE
&gt; lbak...@basho.com
&gt;
&gt;
&gt; On Fri, Oct 3, 2014 at 11:32 AM, Lucas Grijander
&gt;  wrote:
&gt; &gt; Hello,
&gt; &gt;
&gt; &gt; I have a memory backend in production with Riak 2.0.1, 4 servers and 256
&gt; &gt; vnodes. The servers have the same date and time.
&gt; &gt;
&gt; &gt; I have seen an odd performance with the ttl.
&gt; &gt;
&gt; &gt; This is the config:
&gt; &gt;
&gt; &gt; {&lt;&lt;"ttl\\_stg"&gt;&gt;,riak\\_kv\\_memory\\_backend,
&gt; &gt; [{ttl,90},{max\\_memory,25}]},
&gt; &gt;
&gt; &gt; For example, see this GET response in one of the riak servers:
&gt; &gt;
&gt; &gt; &lt; HTTP/1.1 200 OK
&gt; &gt; &lt; X-Riak-Vclock: a85hYGBgzGDKBVIc4otdfgR/7bfIYEpkzGNlKI1efJYvCwA=
&gt; &gt; &lt; Vary: Accept-Encoding
&gt; &gt; \\* Server MochiWeb/1.1 WebMachine/1.10.5 (jokes are better explained) is
&gt; not
&gt; &gt; blacklisted
&gt; &gt; &lt; Server: MochiWeb/1.1 WebMachine/1.10.5 (jokes are better explained)
&gt; &gt; &lt; Link: ; rel="up"
&gt; &gt; &lt; Last-Modified: Fri, 03 Oct 2014 17:40:05 GMT
&gt; &gt; &lt; ETag: "3c8bGoifWcOCSVn0otD5nI"
&gt; &gt; &lt; Date: Fri, 03 Oct 2014 17:47:50 GMT
&gt; &gt; &lt; Content-Type: application/json
&gt; &gt; &lt; Content-Length: 17
&gt; &gt;
&gt; &gt; If the TTL is 90 seconds, Why the GET doesn't return "not found" if the
&gt; &gt; difference between "Last-Modified" and "Date" (of the curl request) is
&gt; &gt; greater than the TTL?
&gt; &gt;
&gt; &gt; Thanks in advance!
&gt; &gt;
&gt; &gt;
&gt; &gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; &gt; riak-users mailing list
&gt; &gt; riak-users@lists.basho.com
&gt; &gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt; &gt;
&gt;
\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

