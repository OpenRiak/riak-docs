---
title: "Re: Absolute consistency"
description: ""
project: community
lastmod: 2012-01-05T12:56:14-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg06141"
mailinglist_parent_id: "msg06140"
author_name: "Jeremiah Peschka"
project_section: "mailinglistitem"
sent_date: 2012-01-05T12:56:14-08:00
---


I seem to recall reading somewhere, but can't find it now, that when n = 3
three physical nodes is the minimum and 5 nodes is the recommended
configuration.

Jeremiah Peschka
Founder, Brent Ozar PLF, LLC


On Thu, Jan 5, 2012 at 12:53 PM, Tim Robinson  wrote:

&gt; So with the original thread where with N=3 on 3 nodes. The developer
&gt; believed each node was getting a copy. When in fact 2 copies went to a
&gt; single node. So yes, there's redundancy and the "shock" value can go away
&gt; :) My apologies.
&gt;
&gt; That said, I have no ability to assess how much data space that is
&gt; wasting, but it seems like potentially 1/3 - correct?
&gt;
&gt; Another way to look at it, using the above noted case, is that I need to
&gt; double[1] the amount of hardware needed to achieve a single amount of
&gt; redundancy.
&gt;
&gt; [1] not specifically, but effectively.
&gt;
&gt;
&gt; -----Original Message-----
&gt; From: "Aphyr" 
&gt; Sent: Thursday, January 5, 2012 1:29pm
&gt; To: "Tim Robinson" 
&gt; Cc: "Runar Jordahl" , riak-users@lists.basho.com
&gt; Subject: Re: Absolute consistency
&gt;
&gt; On 01/05/2012 12:12 PM, Tim Robinson wrote:
&gt; &gt; Thank you for this info. I'm still somewhat confused.
&gt; &gt;
&gt; &gt; Why would anyone ever want 2 copies on one physical PC? Correct me if
&gt; &gt; I am wrong, but part of the sales pitch for Riak is that the cost of
&gt; &gt; hardware is lessened by distributing your data across a cluster of
&gt; &gt; less expensive machines as opposed to having it all one reside on an
&gt; &gt; enormous server with very little redundancy.
&gt; &gt;
&gt; &gt; The 2 copies of data on one physical PC provides no redundancy, but
&gt; &gt; increases hardware costs quite a bit.
&gt; &gt;
&gt; &gt; Right?
&gt;
&gt; Because in the case you expressed shock over, the pigeonhole
&gt; principle makes it \\*impossible\\* to store three copies of information in
&gt; two places without overlap. The alternative is lying to you about the
&gt; replica semantics. That would be bad.
&gt;
&gt; In the second case I described, it's an artifact of a simplistic but
&gt; correct vnode sharding algorithm which uses the partion ID modulo node
&gt; count to assign the node for each partition. When N is not a multiple of
&gt; n, the last and the first (or second, etc, you do the math) partitions
&gt; can wind up on the same node. If you don't use even multiples of n/N,
&gt; the proportion of data that does overlap on one node is on the order of
&gt; 1/64 to 1/1024 of the keyspace. This is not a significant operational cost.
&gt;
&gt; This \\*does\\* reduce fault tolerance: losing those two "special" nodes
&gt; (but not two arbitrary nodes) can destroy those special keys even though
&gt; they were stored with N=3. As the probability of losing two \\*particular\\*
&gt; nodes simultaneously compares favorably with the probability of losing
&gt; \\*any three\\* nodes simultaneously, I haven't been that concerned over it.
&gt; It takes roughly six hours for me to allocate a new machine and restore
&gt; the destroyed node's backup to it. Anecdotally, I think you're more
&gt; likely to see \\*cluster\\* failure than \\*dual node\\* failure in a small
&gt; distributed system, but that's a long story.
&gt;
&gt; The riak team has been aware of this since at least Jun 2010
&gt; (https://issues.basho.com/show\\_bug.cgi?id=228), and there are
&gt; operational workarounds involving target\\_n\\_val. As I understand it,
&gt; solving the key distribution problem is... nontrivial.
&gt;
&gt; --Kyle
&gt;
&gt;
&gt; Tim Robinson
&gt;
&gt;
&gt;
&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;
\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

