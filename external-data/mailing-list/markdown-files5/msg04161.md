---
title: "Re: scp data"
description: ""
project: community
lastmod: 2011-07-31T14:27:51-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg04161"
mailinglist_parent_id: "msg04151"
author_name: "francisco treacy"
project_section: "mailinglistitem"
sent_date: 2011-07-31T14:27:51-07:00
---


Thanks Dan.

It makes sense to mirror the existing cluster, and the procedure works fine.

2011/7/29 Daniel Reverri :
&gt; Hi Francisco
&gt; When tarring up the data directories on nodes it's important to remember
&gt; that the partition to node mapping as defined in the ring file will
&gt; determine where a cluster looks for data.
&gt; Here is a method for migrating data to a new cluster:
&gt; Using "riak-admin reip" to restore Bitcask tarballs to a new cluster
&gt; Backup process:
&gt; 1. Tarball the Bitcask and ring data directories for each node
&gt; \\* Package installed versions of Riak usually place this data in
&gt; "/var/lib/riak/bitcask" and "/var/lib/riak/ring"
&gt;
&gt; Restore process:
&gt; 1. Deploy a set of Riak nodes; same number of nodes as the backed up cluster
&gt; \\* Don't start or cluster the nodes
&gt; 2. Restore the Bitcask and ring data to the new set of nodes
&gt; 3. On each node run "riak-admin reip" for all nodes (explained in the
&gt; example below)
&gt; 4. Start the Riak nodes
&gt; \\* there is no need to cluster the nodes; the restored ring file will form
&gt; the cluster
&gt;
&gt; Example:
&gt; We have a cluster with 4 nodes (1, 2, 3, 4) which we want to backup and
&gt; restore to 4 new nodes (A, B, C, D)
&gt; Steps:
&gt; Backup the Bitcask and ring data directories for nodes 1, 2, 3, 4
&gt; Install Riak on nodes A, B, C, D
&gt; On node A:
&gt; Restore the Bitcask and ring data directories from node 1
&gt; Reip the ring file:
&gt; riak-admin reip node1 nodeA
&gt; riak-admin reip node2 nodeB
&gt; riak-admin reip node3 nodeC
&gt; riak-admin reip node4 nodeD
&gt; Repeat on nodes B, C, D using the back ups from 2, 3, 4 respectively
&gt; \\* The reip commands will be the same on all nodes
&gt; Start nodes A, B, C, D
&gt; Nodes A, B, C, D will form a cluster that is a clone of nodes 1, 2, 3, 4
&gt;
&gt; Limitations:
&gt; Backups must be restored to the same number of nodes from which they were
&gt; taken
&gt; Thanks
&gt; Dan
&gt; Sent from my iPhone
&gt; On Jul 29, 2011, at 3:22 PM, francisco treacy 
&gt; wrote:
&gt;
&gt; Hi all,
&gt;
&gt; I'm setting up a brand-new staging server, with Riak 0.14.2.
&gt;
&gt; As I want to get all production data, I thought I could scp the data
&gt; directory into the new one (besides the ring dir).
&gt;
&gt; So that's what I did, and then started the staging db for the first
&gt; time. It doesn't really work though. Many documents are present, but
&gt; for others I get 404s and possibly 500s as I see tons of crashes in
&gt; the logs (certainly many related to Luwak).
&gt;
&gt; I believed this kind of operation was legal... Is there something I'm
&gt; missing, or something that could help?
&gt;
&gt; Ideally, I want to transfer fresh production data to staging via a
&gt; cron job on a daily basis.
&gt;
&gt; Thanks,
&gt; Francisco
&gt;
&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;

\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

