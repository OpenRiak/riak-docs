---
title: "Re: Riak Search Map Reduce error"
description: ""
project: community
lastmod: 2013-11-20T08:46:49-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg13031"
mailinglist_parent_id: "msg13009"
author_name: "Roger Diller"
project_section: "mailinglistitem"
sent_date: 2013-11-20T08:46:49-08:00
---


I could dig up all our nitty gritty Riak details but I don't think that
will help really.

The point I think is this: Using search map reduce is not a viable way to
do real time search queries. Especially ones that may have 2000+ plus
results each. Couple that with search requests coming in every few seconds
from 300+ customer app instances and you literally bring Riak to it's
knees.

Not that Riak is the problem really, it's just we are using it in a way it
was not designed for. In essence, we are using Riak as a search engine for
our application data. Correct me if I'm wrong but Riak is more for storing
large amounts of KV data, but not really for finding that data in a search
sense.

Am I missing something here? Is there a viable way for doing real time
search queries on a bucket with 1 million keys?


On Mon, Nov 18, 2013 at 5:29 PM, Alexander Sicular wrote:

&gt; More info please...
&gt;
&gt; Version
&gt; Current config
&gt; Hardware
&gt; Data size
&gt; Search Schema
&gt; Etc.
&gt;
&gt; But I would probably say that your search is returning too many keys to
&gt; your mr. More inline.
&gt;
&gt; @siculars
&gt; http://siculars.posthaven.com
&gt;
&gt; Sent from my iRotaryPhone
&gt;
&gt; On Nov 18, 2013, at 13:59, Roger Diller 
&gt; wrote:
&gt;
&gt; Using the Riak Java client, I am executing a search map reduce like this:
&gt;
&gt; MapReduceResult result = riakClient.mapReduce(SEARCH\\_BUCKET,
&gt; search).execute();
&gt;
&gt;
&gt; ^is this part a typo. Cause otherwise it looks like you do a s&gt;mr, set the
&gt; search and then another s&gt;mr.
&gt;
&gt;
&gt; String search = "systemId:" + systemName + " AND indexId:" + indexId;
&gt;
&gt; MapReduceResult result = riakClient.mapReduce(SEARCH\\_BUCKET,
&gt; search).execute();
&gt;
&gt; This worked fine when the bucket contained a few thousand keys. Now that
&gt; we have far more data stored in the bucket (at least 250K keys), it's
&gt; throwing this generic error:
&gt;
&gt; com.basho.riak.client.RiakException: java.io.IOException:
&gt; {"error":"map\\_reduce\\_error"}
&gt;
&gt; We've also noticed that storing new key/values in the bucket has slowed
&gt; WAY down.
&gt;
&gt; Any idea what's going on?
&gt;
&gt;
&gt; Your data set is incorrectly sized to your production config.
&gt;
&gt; Are there limitations to Search Map Reduce?
&gt;
&gt;
&gt; Certainly
&gt;
&gt; Are there configuration options that need changed?
&gt;
&gt;
&gt; Possibly
&gt;
&gt; Any help would be greatly appreciated.
&gt;
&gt;
&gt; --
&gt; Roger Diller
&gt; Flex Rental Solutions, LLC
&gt; Email: ro...@flexrentalsolutions.com
&gt; Skype: rogerdiller
&gt; Time Zone: Eastern Time
&gt;
&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;
&gt;


-- 
Roger Diller
Flex Rental Solutions, LLC
Email: ro...@flexrentalsolutions.com
Skype: rogerdiller
Time Zone: Eastern Time
\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

