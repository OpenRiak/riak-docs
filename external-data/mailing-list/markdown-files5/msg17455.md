---
title: "Re: How to cold (re)boot a cluster with already existing node data"
description: ""
project: community
lastmod: 2016-06-06T07:35:45-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg17455"
mailinglist_parent_id: "msg17454"
author_name: "DeadZen"
project_section: "mailinglistitem"
sent_date: 2016-06-06T07:35:45-07:00
---


this might be helpful, an Omniti article.
https://omniti.com/seeds/migrating-riak-do-it-live

As to fixing this specific error. That iirc can be done doing a name change
in the ring to match your new node name. renaming the node will make that
orddict lookup succeed.
Theres a supplied admin utility for that.


On Sunday, June 5, 2016, Jan-Philip Loos  wrote:

&gt; Hi,
&gt;
&gt; we are using riak in a kuberentes cluster (on GKE). Sometimes it's
&gt; necessary to reboot the complete cluster to update the kubernetes-nodes.
&gt; This results in a complete shutdown of the riak cluster and the riak-nodes
&gt; are rescheduled with a new IP. So how can I handle this situation? How can
&gt; I form a new riak cluster out of the old nodes with new names?
&gt;
&gt; The /var/lib/riak directory is persisted. I had to delete the
&gt; /var/lib/riak/ring folder otherwise "riak start" crashed with this message
&gt; (but saved the old ring state in a tar):
&gt;
&gt; {"Kernel pid
&gt;&gt; terminated",application\\_controller,"{application\\_start\\_failure,riak\\_core,{{shutdown,{failed\\_to\\_start\\_child,riak\\_core\\_broadcast,{'EXIT',{function\\_clause,[{orddict,fetch,['
&gt;&gt; riak@10.44.2.8 
&gt;&gt; ',[]],[{file,\\"orddict.erl\\"},{line,72}]},{riak\\_core\\_broadcast,init\\_peers,1,[{file,\\"src/riak\\_core\\_broadcast.erl\\"},{line,616}]},{riak\\_core\\_broadcast,start\\_link,0,[{file,\\"src/riak\\_core\\_broadcast.erl\\"},{line,116}]},{supervisor,do\\_start\\_child,2,[{file,\\"supervisor.erl\\"},{line,310}]},{supervisor,start\\_children,3,[{file,\\"supervisor.erl\\"},{line,293}]},{supervisor,init\\_children,2,[{file,\\"supervisor.erl\\"},{line,259}]},{gen\\_server,init\\_it,6,[{file,\\"gen\\_server.erl\\"},{line,304}]},{proc\\_lib,init\\_p\\_do\\_apply,3,[{file,\\"proc\\_lib.erl\\"},{line,239}]}]}}}},{riak\\_core\\_app,start,[normal,[]]}}}"}
&gt;&gt; Crash dump was written to: /var/log/riak/erl\\_crash.dump
&gt;&gt; Kernel pid terminated (application\\_controller)
&gt;&gt; ({application\\_start\\_failure,riak\\_core,{{shutdown,{failed\\_to\\_start\\_child,riak\\_core\\_broadcast,{'EXIT',{function\\_clause,[{orddict,fetch,['
&gt;&gt; riak@10.44.2.8 ',
&gt;
&gt;
&gt; The I formed a new cluster via join & plan & commit.
&gt;
&gt; But now, I discovered a problems with incomplete and inconsistent
&gt; partitions:
&gt;
&gt; \\*$ \\*curl -Ss "
&gt; http://riak.default.svc.cluster.local:8098/buckets/users/keys?keys=true"
&gt; | jq '.[] | length'
&gt;
&gt; 3064
&gt;
&gt; \\*$\\* curl -Ss "
&gt; http://riak.default.svc.cluster.local:8098/buckets/users/keys?keys=true"
&gt; | jq '.[] | length'
&gt;
&gt; 2987
&gt;
&gt; \\*$\\* curl -Ss "
&gt; http://riak.default.svc.cluster.local:8098/buckets/users/keys?keys=true"
&gt; | jq '.[] | length'
&gt;
&gt; 705
&gt;
&gt; \\*$\\* curl -Ss "
&gt; http://riak.default.svc.cluster.local:8098/buckets/users/keys?keys=true"
&gt; | jq '.[] | length'
&gt; 3064
&gt;
&gt; Is there a way to fix this? I guess this is caused by the missing old
&gt; ring-state?
&gt;
&gt; Greetings
&gt;
&gt; Jan
&gt;
\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

