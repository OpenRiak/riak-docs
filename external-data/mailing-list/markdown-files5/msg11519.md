---
title: "mapreduce timeout"
description: ""
project: community
lastmod: 2013-07-14T04:17:57-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg11519"
author_name: "Deyan Dyankov"
project_section: "mailinglistitem"
sent_date: 2013-07-14T04:17:57-07:00
---


Hi everyone,

first time here. Thanks in advance.

I am experiencing issues with MapReduce and it seems to timeout after a certain 
volume data threshold is reached. The reducer is only one and here is the 
mapreduce initiation script:
#!/usr/bin/env ruby
[…]
@client = Riak::Client.new(
 :nodes =&gt; [
 {:host =&gt; 'db1', :pb\\_port =&gt; 8087, :http\\_port =&gt; 8098},
 {:host =&gt; 'db2', :pb\\_port =&gt; 8087, :http\\_port =&gt; 8098},
 {:host =&gt; 'db3', :pb\\_port =&gt; 8087, :http\\_port =&gt; 8098}
 ],
 :protocol =&gt; 'pbc'
)

start\\_key = "#{cust}:#{setup}:#{start\\_time}"
end\\_key = "#{cust}:#{setup}:#{end\\_time}"

result = Riak::MapReduce.new(@client).
 index(bucket\\_name, index\\_name, start\\_key..end\\_key).
 map('map95th').
 reduce('reduce95th', :arg =&gt; { 'reduce\\_phase\\_only\\_1' =&gt; true }, :keep =&gt; 
true).
 run()

puts result

The following is the code for the map95th and reduce95th javascript functions:
function map95th(v, keyData, arg) {
 var key\\_elements = v['key'].split(':');
 var cust = key\\_elements[0];
 var setup = key\\_elements[1];
 var sid = key\\_elements[2];
 var ts = key\\_elements[3];

 var result\\_key = cust + ':' + setup + ':' + ts;
 var obj = {}
 var obj\\_data = Riak.mapValuesJson(v)[0];

 obj\\_data['bps'] = (obj\\_data['rx\\_bytes'] + obj\\_data['tx\\_bytes']) / 60;
 return\\_val = obj\\_data['bps'];
 return [ return\\_val ];
}

// if used, this must be a single reducer! Call from Ruby like this:
// reduce('reduce95th', :arg =&gt; { 'reduce\\_phase\\_only\\_1' =&gt; true }, :keep =&gt; 
true).
function reduce95th(values) {
 var sorted = values.sort(function(a,b) { return a - b; });
 var pct = sorted.length / 100;
 var element\\_95th = pct \\* 95;
 element\\_95th = parseInt(element\\_95th, 10) + 1;

 return [ sorted[element\\_95th] ];
}



Now here is the interesting part. The MR goes through one record per minute. If 
I run it for a period of less than ~20 days, it executes. Otherwise, it times 
out:
[deyandyankov@azobook ~/repos/loshko/mapreduce/ruby (master)]$
[deyandyankov@azobook ~/repos/loshko/mapreduce/ruby (master)]$ ./95h.rb 
yellingtone default $((`date +%s` - 20 \\* 86400)) `date +%s`
125581.51666666666
[deyandyankov@azobook ~/repos/loshko/mapreduce/ruby (master)]$ ./95h.rb 
yellingtone default $((`date +%s` - 30 \\* 86400)) `date +%s`
/Users/deyandyankov/.rvm/gems/ruby-1.9.3-p392/gems/riak-client-1.1.1/lib/riak/client/beefcake\\_protobuffs\\_backend.rb:182:in
 `decode\\_response': Expected success from Riak but received 0. 
{"phase":1,"error":"timeout","input":null,"type":null,"stack":null} 
(Riak::ProtobuffsFailedRequest)
 from 
/Users/deyandyankov/.rvm/gems/ruby-1.9.3-p392/gems/riak-client-1.1.1/lib/riak/client/beefcake\\_protobuffs\\_backend.rb:116:in
 `mapred'
 from 
/Users/deyandyankov/.rvm/gems/ruby-1.9.3-p392/gems/riak-client-1.1.1/lib/riak/client.rb:325:in
 `block in mapred'
 from 
/Users/deyandyankov/.rvm/gems/ruby-1.9.3-p392/gems/riak-client-1.1.1/lib/riak/client.rb:435:in
 `block in recover\\_from'
 from 
/Users/deyandyankov/.rvm/gems/ruby-1.9.3-p392/gems/innertube-1.0.2/lib/innertube.rb:127:in
 `take'
 from 
/Users/deyandyankov/.rvm/gems/ruby-1.9.3-p392/gems/riak-client-1.1.1/lib/riak/client.rb:433:in
 `recover\\_from'
 from 
/Users/deyandyankov/.rvm/gems/ruby-1.9.3-p392/gems/riak-client-1.1.1/lib/riak/client.rb:379:in
 `protobuffs'
 from 
/Users/deyandyankov/.rvm/gems/ruby-1.9.3-p392/gems/riak-client-1.1.1/lib/riak/client.rb:133:in
 `backend'
 from 
/Users/deyandyankov/.rvm/gems/ruby-1.9.3-p392/gems/riak-client-1.1.1/lib/riak/client.rb:324:in
 `mapred'
 from 
/Users/deyandyankov/.rvm/gems/ruby-1.9.3-p392/gems/riak-client-1.1.1/lib/riak/map\\_reduce.rb:217:in
 `run'
 from ./95h.rb:29:in `'
[deyandyankov@azobook ~/repos/loshko/mapreduce/ruby (master)]$

The records being processed look lie this:
{"rx\\_bytes":3485395.0,"tx\\_bytes":1658479.0}

When running the script with more than 20 days worth of data (two records per 
minute are processed, which amounts to 2 \\* 60 \\* 24 \\* 20 = more than 57,600 
processed), the script times out and here are some things from the logs:
==&gt; /var/log/riak/erlang.log.1 &lt;==
Erlang has closed

==&gt; /var/log/riak/error.log &lt;==
2013-07-14 13:03:51.580 [error] &lt;0.709.0&gt;@riak\\_pipe\\_vnode:new\\_worker:768 Pipe 
worker startup failed:fitting was gone before startup

==&gt; /var/log/riak/console.log &lt;==
2013-07-14 13:03:51.584 [error] &lt;0.22049.4326&gt; gen\\_fsm &lt;0.22049.4326&gt; in state 
wait\\_for\\_input terminated with reason: timeout

==&gt; /var/log/riak/error.log &lt;==
2013-07-14 13:03:51.584 [error] &lt;0.22049.4326&gt; gen\\_fsm &lt;0.22049.4326&gt; in state 
wait\\_for\\_input terminated with reason: timeout

==&gt; /var/log/riak/console.log &lt;==
2013-07-14 13:03:51.940 [error] &lt;0.22049.4326&gt; CRASH REPORT Process 
&lt;0.22049.4326&gt; with 0 neighbours exited with reason: timeout in 
gen\\_fsm:terminate/7 line 611

==&gt; /var/log/riak/crash.log &lt;==
2013-07-14 13:03:51 =CRASH REPORT====
 crasher:
 initial call: riak\\_pipe\\_vnode\\_worker:init/1
 pid: &lt;0.22049.4326&gt;
 registered\\_name: []
 exception exit: 
{timeout,[{gen\\_fsm,terminate,7,[{file,"gen\\_fsm.erl"},{line,611}]},{proc\\_lib,init\\_p\\_do\\_apply,3,[{file,"proc\\_lib.erl"},{line,227}]}]}
 ancestors: [&lt;0.710.0&gt;,&lt;0.709.0&gt;,riak\\_core\\_vnode\\_sup,riak\\_core\\_sup,&lt;0.129.0&gt;]
 messages: []
 links: [&lt;0.710.0&gt;,&lt;0.709.0&gt;]
 dictionary: 
[{eunit,[{module,riak\\_pipe\\_vnode\\_worker},{partition,388211372416021087647853783690262677096107081728},{&lt;0.709.0&gt;,&lt;0.709.0&gt;},{details,{fitting\\_details,{fitting,&lt;18125.23420.4566&gt;,#Ref&lt;18125.0.5432.50467&gt;,&lt;&lt;"C�������������������"&gt;&gt;,1},1,riak\\_kv\\_w\\_reduce,{rct,#Fun,{struct,[{&lt;&lt;"reduce\\_phase\\_only\\_1"&gt;&gt;,true}]}},{fitting,&lt;18125.23418.4566&gt;,#Ref&lt;18125.0.5432.50467&gt;,sink,undefined},[{log,sink},{trace,[error]},{sink,{fitting,&lt;18125.23418.4566&gt;,#Ref&lt;18125.0.5432.50467&gt;,sink,undefined}},{sink\\_type,{fsm,10,infinity}}],64}}]}]
 trap\\_exit: false
 status: running
 heap\\_size: 832040
 stack\\_size: 24
 reductions: 1456611
 neighbours:

==&gt; /var/log/riak/error.log &lt;==
2013-07-14 13:03:51.940 [error] &lt;0.22049.4326&gt; CRASH REPORT Process 
&lt;0.22049.4326&gt; with 0 neighbours exited with reason: timeout in 
gen\\_fsm:terminate/7 line 611

==&gt; /var/log/riak/crash.log &lt;==
2013-07-14 13:03:52 =SUPERVISOR REPORT====
 Supervisor: {&lt;0.710.0&gt;,riak\\_pipe\\_vnode\\_worker\\_sup}
 Context: child\\_terminated
 Reason: timeout
 Offender: 
[{pid,&lt;0.22049.4326&gt;},{name,undefined},{mfargs,{riak\\_pipe\\_vnode\\_worker,start\\_link,undefined}},{restart\\_type,temporary},{shutdown,2000},{child\\_type,worker}]


==&gt; /var/log/riak/console.log &lt;==
2013-07-14 13:03:52.059 [error] &lt;0.710.0&gt; Supervisor riak\\_pipe\\_vnode\\_worker\\_sup 
had child undefined started with {riak\\_pipe\\_vnode\\_worker,start\\_link,undefined} 
at &lt;0.22049.4326&gt; exit with reason timeout in context child\\_terminated

==&gt; /var/log/riak/error.log &lt;==
2013-07-14 13:03:52.059 [error] &lt;0.710.0&gt; Supervisor riak\\_pipe\\_vnode\\_worker\\_sup 
had child undefined started with {riak\\_pipe\\_vnode\\_worker,start\\_link,undefined} 
at &lt;0.22049.4326&gt; exit with reason timeout in context child\\_terminated


The data is in leveldb and is accessed through secondary indexes. 
This is a 3 node cluster with 32GB ram, current usage is about 12G per node. 
n\\_val=3. The same issues occurs on a similar 2 node cluster with 8GB of ram 
(usage is ~6G).

The following is my app.config:
[
 {riak\\_api, [
 {pb\\_ip, "0.0.0.0" },
 {pb\\_port, 8087 },
 {pb\\_backlog, 100 }
 ]},
 {riak\\_core, [
 {default\\_bucket\\_props, [
 {n\\_val, 3},
 {last\\_write\\_wins, true}
 ]},
 {ring\\_state\\_dir, "/storage/riak/ring"},
 {ring\\_creation\\_size, 256},
 {http, [ {"0.0.0.0", 8098 } ]},
 {https, [{ "0.0.0.0", 8069 }]},
 {ssl, [
 {certfile, "/etc/ssl/riak/server.crt"},
 {cacertfile, "/etc/ssl/riak/root.crt"},
 {keyfile, "/etc/ssl/riak/server.key"}
 ]},
 {handoff\\_port, 8099 },
 {dtrace\\_support, false},
 {enable\\_health\\_checks, true},
 {platform\\_bin\\_dir, "/usr/sbin"},
 {platform\\_data\\_dir, "/storage/riak"},
 {platform\\_etc\\_dir, "/etc/riak"},
 {platform\\_lib\\_dir, "/usr/lib/riak/lib"},
 {platform\\_log\\_dir, "/var/log/riak"}
 ]},
 {riak\\_kv, [
 {storage\\_backend, riak\\_kv\\_eleveldb\\_backend},
 {anti\\_entropy, {on, []}},
 {anti\\_entropy\\_build\\_limit, {1, 3600000}},
 {anti\\_entropy\\_expire, 604800000},
 {anti\\_entropy\\_concurrency, 2},
 {anti\\_entropy\\_tick, 15000},
 {anti\\_entropy\\_data\\_dir, "/storage/riak/anti\\_entropy"},
 {anti\\_entropy\\_leveldb\\_opts, [{write\\_buffer\\_size, 4194304},
 {max\\_open\\_files, 20}]},

 {mapred\\_name, "mapred"},
 {mapred\\_2i\\_pipe, true},
 {map\\_js\\_vm\\_count, 16 },
 {reduce\\_js\\_vm\\_count, 12 },
 {hook\\_js\\_vm\\_count, 20 },
 {js\\_max\\_vm\\_mem, 8},
 {js\\_thread\\_stack, 16},
 {js\\_source\\_dir, "/etc/riak/mapreduce/js\\_source"},
 {http\\_url\\_encoding, on},
 {vnode\\_vclocks, true},
 {listkeys\\_backpressure, true},
 {vnode\\_mailbox\\_limit, {1, 5000}}
 ]},

 {riak\\_search, [
 {enabled, true}
 ]},

 {merge\\_index, [
 {data\\_root, "/storage/riak/merge\\_index"},
 {buffer\\_rollover\\_size, 1048576},
 {max\\_compact\\_segments, 20}
 ]},

 {bitcask, [
 {data\\_root, "/storage/riak/bitcask"}
 ]},

 {eleveldb, [
 {cache\\_size, 1024},
 {max\\_open\\_files, 64},
 {data\\_root, "/storage/riak/leveldb"}
 ]},

 {lager, [
 {handlers, [
 {lager\\_file\\_backend, [
 {"/var/log/riak/error.log", error, 10485760, 
"$D0", 5},
 {"/var/log/riak/console.log", info, 10485760, 
"$D0", 5}
 ]}
 ] },

 {crash\\_log, "/var/log/riak/crash.log"},
 {crash\\_log\\_msg\\_size, 65536},
 {crash\\_log\\_size, 10485760},
 {crash\\_log\\_date, "$D0"},
 {crash\\_log\\_count, 5},
 {error\\_logger\\_redirect, true}
 ]},

 {riak\\_sysmon, [
 {process\\_limit, 30},
 {port\\_limit, 2},
 {gc\\_ms\\_limit, 0},
 {heap\\_word\\_limit, 40111000},
 {busy\\_port, true},
 {busy\\_dist\\_port, true}
 ]},

 {sasl, [
 {sasl\\_error\\_logger, false}
 ]},

Sorry to bug you with such a long e-mail but I wanted to be as thorough as 
possible. I tried raising a few options but it didn't help: map\\_js\\_vm\\_count, 
reduce\\_js\\_vm\\_count, js\\_max\\_vm\\_mem
I also tried adding a timeout argument to the map reduce caller code but even 
if I set it to 60,000 or more (this is milliseconds), the script is terminating 
with timeout error after 10-12 secs. The same behaviour is observed if I use 
http instead of pbc.

What seems to be the problem? Is this a matter of configuration? I am surprised 
about the fact that the job runs with 20-25 days of data and not more.

thank you for your efforts,
Deyan\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

