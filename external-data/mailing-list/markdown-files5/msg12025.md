---
title: "Re: vm.args change for 15% to 80% improvement in leveldb"
description: ""
project: community
lastmod: 2013-08-14T07:43:21-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg12025"
mailinglist_parent_id: "msg12024"
author_name: "Matthew Von-Maszewski"
project_section: "mailinglistitem"
sent_date: 2013-08-14T07:43:21-07:00
---


"threads=8" is the key phrase … +S 4:4

On Aug 14, 2013, at 10:04 AM, Guido Medina  wrote:

&gt; For the following information should it be +S 4:4 or +S 4:8?
&gt; 
&gt; root@somehost# lshw -C processor
&gt; \\*-cpu
&gt; description: CPU
&gt; product: Intel(R) Core(TM) i7 CPU 930 @ 2.80GHz
&gt; vendor: Intel Corp.
&gt; physical id: 4
&gt; bus info: cpu@0
&gt; version: Intel(R) Core(TM) i7 CPU 930 @ 2.80GHz
&gt; serial: To Be Filled By O.E.M.
&gt; slot: CPU 1
&gt; size: 1600MHz
&gt; capacity: 1600MHz
&gt; width: 64 bits
&gt; clock: 133MHz
&gt; capabilities: x86-64 fpu fpu\\_exception wp vme de pse tsc msr pae mce 
&gt; cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 
&gt; ss ht tm pbe syscall nx rdtscp constant\\_tsc arch\\_perfmon pebs bts rep\\_good 
&gt; nopl xtopology nonstop\\_tsc aperfmperf pni dtes64 monitor ds\\_cpl vmx est tm2 
&gt; ssse3 cx16 xtpr pdcm sse4\\_1 sse4\\_2 popcnt lahf\\_lm ida dtherm tpr\\_shadow vnmi 
&gt; flexpriority ept vpid cpufreq
&gt; configuration: cores=4 enabledcores=4 threads=8
&gt; 
&gt; Thanks,
&gt; 
&gt; Guido.
&gt; 
&gt; On 14/08/13 01:38, Matthew Von-Maszewski wrote:
&gt;&gt; \\*\\* The following is copied from Basho's leveldb wiki page:
&gt;&gt; 
&gt;&gt; https://github.com/basho/leveldb/wiki/Riak-tuning-1
&gt;&gt; 
&gt;&gt; 
&gt;&gt; 
&gt;&gt; Summary:
&gt;&gt; 
&gt;&gt; leveldb has a higher read and write throughput in Riak if the Erlang 
&gt;&gt; scheduler count is limited to half the number of CPU cores. Tests have 
&gt;&gt; demonstrated improvements of 15% to 80% greater throughput.
&gt;&gt; 
&gt;&gt; The scheduler limit is set in the vm.args file:
&gt;&gt; 
&gt;&gt; +S x:x
&gt;&gt; 
&gt;&gt; where "x" is the number of schedulers Erlang may use. Erlang's default value 
&gt;&gt; of "x" is the total number of CPUs in the system. For Riak installations 
&gt;&gt; using leveldb, the recommendation is to set "x" to half the number of CPUs. 
&gt;&gt; Virtual environments are not yet tested.
&gt;&gt; 
&gt;&gt; Example: for 24 CPU system
&gt;&gt; 
&gt;&gt; +S 12:12
&gt;&gt; 
&gt;&gt; Discussion:
&gt;&gt; 
&gt;&gt; We have tested a limited number of CPU configurations and customer loads. In 
&gt;&gt; all cases, there is a performance increase when the +S option is added to 
&gt;&gt; the vm.args file to reduce the number of Erlang schedulers. The working 
&gt;&gt; hypothesis is that the Erlang schedulers perform enough "busy wait" work 
&gt;&gt; that they always create context switch away from leveldb when leveldb is 
&gt;&gt; actually the only system task with real work.
&gt;&gt; 
&gt;&gt; The tests included 8 CPU (no hyper threading, physical cores only) and 24 
&gt;&gt; CPU (12 physical cores with hyper threading) systems. All were 64bit Intel 
&gt;&gt; platforms. Generalized findings:
&gt;&gt; 
&gt;&gt; • servers running higher number of vnodes (64) had larger performance 
&gt;&gt; gains than those with fewer (8)
&gt;&gt; • servers running SSD arrays had larger performance gains than those 
&gt;&gt; running SATA arrays
&gt;&gt; • Get and Write operations showed performance gains, 2i query 
&gt;&gt; operations (leveldb iterators) were unchanged
&gt;&gt; • Not recommended for servers with less than 8 CPUs (go no lower than 
&gt;&gt; +S 4:4)
&gt;&gt; 
&gt;&gt; Performance improvements were as high as 80% over extended, heavily loaded 
&gt;&gt; intervals on servers with SSD arrays and 64 vnodes. No test resulted in 
&gt;&gt; worse performance due to the addition of +S x:x.
&gt;&gt; 
&gt;&gt; The +S x:x configuration change does not have to be implemented 
&gt;&gt; simultaneously to an entire Riak cluster. The change may be applied to a 
&gt;&gt; single server for verification. Steps: update the vm.args file, then restart 
&gt;&gt; the Riak node. Erlang command line changes to schedules were ineffective.
&gt;&gt; 
&gt;&gt; This configuration change has been running in at least one large, 
&gt;&gt; multi-datacenter production environment for several months.
&gt;&gt; 
&gt;&gt; 
&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt; riak-users mailing list
&gt;&gt; riak-users@lists.basho.com
&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt; 
&gt; 
&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com


\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

