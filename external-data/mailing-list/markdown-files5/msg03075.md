---
title: "Re: Random questions"
description: ""
project: community
lastmod: 2011-04-18T15:58:40-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg03075"
mailinglist_parent_id: "msg03071"
author_name: "Ben Tilly"
project_section: "mailinglistitem"
sent_date: 2011-04-18T15:58:40-07:00
---


On Mon, Apr 18, 2011 at 3:12 PM, Jon Brisbin  wrote:
&gt;
&gt; On Apr 18, 2011, at 2:47 PM, Ben Tilly wrote:
&gt;
&gt;&gt; The first is that I've heard that there is a limit to the size of the
&gt;&gt; headers that can be sent using the http interface and therefore the
&gt;&gt; number of links that you can use.  Is this true, and if so does anyone
&gt;&gt; know what the limit is?
&gt;
&gt; I suppose it depends on the length of your key. I've heard in the
&gt; thousands. If you're using command-line clients or curl or something,
&gt; you might find it gets untenable really quickly.
&gt;
&gt;&gt;
&gt;&gt; My second question is this.  It seems that if accesses are configured
&gt;&gt; so that writes go to a quorum, and reads also comes from a quorum,
&gt;&gt; then once a write is reported as complete, a read is close to
&gt;&gt; guaranteed to come back with that write.  How reliable is this
&gt;&gt; behavior?  I'm sure that there are boundary cases around a machine
&gt;&gt; joining in the same window that the write/read is happening, but I'd
&gt;&gt; like to know what they are.
&gt;
&gt; This is completely unscientific, but in a large test suite with
&gt; many hundreds of write and immediate read operations, a random number
&gt; of them will fail for no apparent reason. I traced this down to the
&gt; quorum thing. If you have a whole lot of writes and immediate reads
&gt; that need to happen (e.g. in a test suite), you have to specify a
&gt; full write (i.e. to every single vnode with no exceptions).

In your test suite what are N, R, W and DW in your unit test? (N =
number of partitions data is stored on, R is the number that need to
be read, W is the number to respond to a write before a write is
complete, and DW is the number that have to say that they have written
data to disk.)

My understanding is that the default values say that N = 3, R = 1, W =
2, and DW = 0. With that configuration race conditions are easy to
create. They should be much harder to create with N = 3, R = 2, W =
2, and DW = 2. With that configuration my understanding is that every
read has to see at least one node that got written to unless we are in
the process of failing over a node in which case I don't know what
guarantees exist.
&gt;&gt;
&gt;&gt; My third question is how bad an idea people think it is to try to get
&gt;&gt; some form of atomic behavior from Riak by locking important read/write
&gt;&gt; pairs.
&gt;
&gt; There's some very interesting work being done lately to implement the
&gt; Zookeeper protocol for doing atomic checkpointing (which would be
&gt; necessary to achieve atomic operations in a distributed system) but it
&gt; seems way too complicated for most of what people need this kind of
&gt; thing for.
&gt;
&gt; If true atomicity is really a concern, then use Redis and write a
&gt; pub/sub handler to update your Riak documents whenever things change.
&gt;
&gt; You know we could probably take the Riak RabbitMQ postcommit hook and
&gt; adapt it to use Redis for something along these lines... :)
&gt;
&gt; Thanks!
&gt;
&gt; Jon Brisbin
&gt;
&gt; http://jbrisbin.com
&gt; Twitter: @j\\_brisbin
&gt;
&gt;
&gt;

\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

