---
title: "How to cold (re)boot a cluster with already existing node data"
description: ""
project: community
lastmod: 2016-06-06T02:25:28-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg17454"
author_name: "Jan-Philip Loos"
project_section: "mailinglistitem"
sent_date: 2016-06-06T02:25:28-07:00
---


Hi,

we are using riak in a kuberentes cluster (on GKE). Sometimes it's
necessary to reboot the complete cluster to update the kubernetes-nodes.
This results in a complete shutdown of the riak cluster and the riak-nodes
are rescheduled with a new IP. So how can I handle this situation? How can
I form a new riak cluster out of the old nodes with new names?

The /var/lib/riak directory is persisted. I had to delete the
/var/lib/riak/ring folder otherwise "riak start" crashed with this message
(but saved the old ring state in a tar):

{"Kernel pid
&gt; terminated",application\\_controller,"{application\\_start\\_failure,riak\\_core,{{shutdown,{failed\\_to\\_start\\_child,riak\\_core\\_broadcast,{'EXIT',{function\\_clause,[{orddict,fetch,['
&gt; riak@10.44.2.8
&gt; ',[]],[{file,\\"orddict.erl\\"},{line,72}]},{riak\\_core\\_broadcast,init\\_peers,1,[{file,\\"src/riak\\_core\\_broadcast.erl\\"},{line,616}]},{riak\\_core\\_broadcast,start\\_link,0,[{file,\\"src/riak\\_core\\_broadcast.erl\\"},{line,116}]},{supervisor,do\\_start\\_child,2,[{file,\\"supervisor.erl\\"},{line,310}]},{supervisor,start\\_children,3,[{file,\\"supervisor.erl\\"},{line,293}]},{supervisor,init\\_children,2,[{file,\\"supervisor.erl\\"},{line,259}]},{gen\\_server,init\\_it,6,[{file,\\"gen\\_server.erl\\"},{line,304}]},{proc\\_lib,init\\_p\\_do\\_apply,3,[{file,\\"proc\\_lib.erl\\"},{line,239}]}]}}}},{riak\\_core\\_app,start,[normal,[]]}}}"}
&gt; Crash dump was written to: /var/log/riak/erl\\_crash.dump
&gt; Kernel pid terminated (application\\_controller)
&gt; ({application\\_start\\_failure,riak\\_core,{{shutdown,{failed\\_to\\_start\\_child,riak\\_core\\_broadcast,{'EXIT',{function\\_clause,[{orddict,fetch,['
&gt; riak@10.44.2.8',


The I formed a new cluster via join & plan & commit.

But now, I discovered a problems with incomplete and inconsistent
partitions:

\\*$ \\*curl -Ss "
http://riak.default.svc.cluster.local:8098/buckets/users/keys?keys=true" |
jq '.[] | length'

3064

\\*$\\* curl -Ss "
http://riak.default.svc.cluster.local:8098/buckets/users/keys?keys=true" |
jq '.[] | length'

2987

\\*$\\* curl -Ss "
http://riak.default.svc.cluster.local:8098/buckets/users/keys?keys=true" |
jq '.[] | length'

705

\\*$\\* curl -Ss "
http://riak.default.svc.cluster.local:8098/buckets/users/keys?keys=true" |
jq '.[] | length'
3064

Is there a way to fix this? I guess this is caused by the missing old
ring-state?

Greetings

Jan
\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

