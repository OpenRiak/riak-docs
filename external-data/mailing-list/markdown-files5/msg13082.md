---
title: "Re: Runaway \"Failed to compact\" errors"
description: ""
project: community
lastmod: 2013-11-24T14:58:23-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg13082"
mailinglist_parent_id: "msg13081"
author_name: "Joe Caswell"
project_section: "mailinglistitem"
sent_date: 2013-11-24T14:58:23-08:00
---


Justin,

 The binary in the log entry below equates to:
{&lt;&lt;"collector-collect-twitter"&gt;&gt;,&lt;&lt;"data\\_followers"&gt;&gt;,&lt;&lt;32897-byte string&gt;&gt;}

 Hope this helps.

Joe
From: Justin Long 
Date: Sunday, November 24, 2013 5:17 PM
To: Joe Caswell 
Cc: Richard Shaw , riak-users

Subject: Re: Runaway "Failed to compact" errors

Thanks Joe. I would agree that would probably be the problem. I am concerned
since none of the fields of objects I am storing in Riak would produce a key
larger than 32kb. Here’s a sample Scala (Java-based) POJO that represents an
object in the problem bucket using the Riak-Java-Client:

case class InstagramCache(
 @(JsonProperty@field)("identityId")
 @(RiakKey@field)
 val identityId: String, // ID of user on social network
 
 @(JsonProperty@field)("userId")
 @(RiakIndex@field)(name = "userId")
 val userId: String, // associated user ID on platform
 
 @(JsonProperty@field)("data")
 val data: Map[String, Option[String]],
 
 @(JsonProperty@field)("updated")
 var updated: Date
 
)

The fields identityId and userId would rarely exceed 30 characters. Is Riak
trying to index the whole object?

Thanks



On Nov 24, 2013, at 2:11 PM, Joe Caswell  wrote:

&gt; Justin,
&gt; 
&gt; The terms being stored in merge index are too large. The maximum size for an
&gt; {Index, Field, Term} key is 32k bytes.
&gt; The binary blob in your log entry represents a tuple that was 32952 bytes.
&gt; Since merge index uses a 15-bit integer to store term size, if the
&gt; term\\_to\\_binary of the given key is larger than 32767, high bits are lost,
&gt; effectively storing ( mod 32767) bytes.
&gt; When this data is read back, binary\\_to\\_term is unable to reconstruct the key
&gt; due the missing bytes, and throws a badarg exception.
&gt; 
&gt; Search index repair is document here:
&gt; http://docs.basho.com/riak/1.4.0/cookbooks/Repairing-Search-Indexes/
&gt; However, you would need to first modify your extractor to not produce search
&gt; keys larger than 32k or the corruption issues will recur.
&gt; 
&gt; Joe Caswell
&gt; 
&gt; 
&gt; From: Richard Shaw 
&gt; Date: Sunday, November 24, 2013 4:25 PM
&gt; To: Justin Long 
&gt; Cc: riak-users 
&gt; Subject: Re: Runaway "Failed to compact" errors
&gt; 
&gt; Ok thanks Justin, please can you change the vm.args on each node with the
&gt; following and then restart each node
&gt; 
&gt; -env ERL\\_MAX\\_ETS\\_TABLES 256000
&gt; 
&gt; I'd also like you to please confirm the ulimit on each server
&gt; 
&gt; $ riak attach
&gt; os:cmd("ulimit -n").
&gt; 
&gt; If you're running Riak &gt;=1.4 then exit with Ctrl+c then a and if you're
&gt; running 1.3 or older then Ctrl+d to exit
&gt; 
&gt; I would recommend upping the ulimit to 65536 if its not already there [0]
&gt; 
&gt; [0]http://docs.basho.com/riak/latest/ops/tuning/open-files-limit/
&gt; 
&gt; I'm going to need to sign off at this point Justin, I'll see if a colleague
&gt; can take over.
&gt; 
&gt; 
&gt; Kind regards,
&gt; 
&gt; Richard
&gt; 
&gt; 
&gt; 
&gt; On 24 November 2013 21:00, Justin Long  wrote:
&gt;&gt; Hi Richard,
&gt;&gt; 
&gt;&gt; Result turned up empty on the failed node. Here¹s what is in vm.args:
&gt;&gt; 
&gt;&gt; ------------------------------------------------------
&gt;&gt; 
&gt;&gt; # Name of the riak node
&gt;&gt; -name riak@192.168.3.3
&gt;&gt; 
&gt;&gt; ## Cookie for distributed erlang. All nodes in the same cluster
&gt;&gt; ## should use the same cookie or they will not be able to communicate.
&gt;&gt; -setcookie riak
&gt;&gt; 
&gt;&gt; ## Heartbeat management; auto-restarts VM if it dies or becomes unresponsive
&gt;&gt; ## (Disabled by default..use with caution!)
&gt;&gt; ##-heart
&gt;&gt; 
&gt;&gt; ## Enable kernel poll and a few async threads
&gt;&gt; +K true
&gt;&gt; +A 64
&gt;&gt; 
&gt;&gt; ## Treat error\\_logger warnings as warnings
&gt;&gt; +W w
&gt;&gt; 
&gt;&gt; ## Increase number of concurrent ports/sockets
&gt;&gt; -env ERL\\_MAX\\_PORTS 4096
&gt;&gt; 
&gt;&gt; ## Tweak GC to run more often
&gt;&gt; -env ERL\\_FULLSWEEP\\_AFTER 0
&gt;&gt; 
&gt;&gt; ## Set the location of crash dumps
&gt;&gt; -env ERL\\_CRASH\\_DUMP /var/log/riak/erl\\_crash.dump
&gt;&gt; 
&gt;&gt; ## Raise the ETS table limit
&gt;&gt; -env ERL\\_MAX\\_ETS\\_TABLES 22000
&gt;&gt; 
&gt;&gt; ------------------------------------------------------
&gt;&gt; 
&gt;&gt; 
&gt;&gt; Before I received your email, I have since isolated the node and
&gt;&gt; force-removed it from the cluster. In the meantime, I brought up a new fresh
&gt;&gt; node and joined it to the cluster. When Riak went to handoff some of the
&gt;&gt; RiakSearch indexes here is what was popping up in console.log:
&gt;&gt; 
&gt;&gt; ------------------------------------------------------
&gt;&gt; 
&gt;&gt; &lt;0.4262.0&gt;@merge\\_index\\_backend:async\\_fold\\_fun:116 failed to iterate the index
&gt;&gt; with reason 
&gt;&gt; {badarg,[{erlang,binary\\_to\\_term,[&lt;&lt;131,104,3,109,0,0,0,25,99,111,108,108,101,
&gt;&gt; 99,116,111,114,45,99,111,108,108,101,99,116,45,116,119,105,116,116,101,114,10
&gt;&gt; 9,0,0,0,14,100,97,116,97,95,102,111,108,108,111,119,101,114,115,109,0,0,128,1
&gt;&gt; 99,123,34,105,100,115,34,58,91,49,52,52,55,51,54,54,57,53,48,44,53,48,48,55,5
&gt;&gt; 3,57,48,55,44,52,51,56,49,55,53,52,56,53,44,49,51,54,53,49,50,49,52,50,44,52,
&gt;&gt; 54,50,52,52,54,56,51,44,49,48,55,57,56,55,49,50,48,48,44,55,55,48,56,51,54,55
&gt;&gt; ,57,44,50,56,51,56,51,57,55,56,44,49,57,50,48,55,50,55,51,48,44,51,57,54,57,5
&gt;&gt; 6,56,57,56,55,44,50,56,48,50,54,51,56,48,52,44,53,57,50,56,56,53,50,51,48,44,
&gt;&gt; 49,50,52,55,53,56,57,53,55,56,44,49,55,51,56,56,51,53,52,50,44,49,53,56,57,54
&gt;&gt; ,51,50,50,50,48,44,53,53,49,51,57,57,51,48,49,44,50,50,48,53,52,55,52,55,55,5
&gt;&gt; 4,44,49,51,51,52,57,56,57,56,50,53,44,51,49,50,51,57,53,55,54,50&gt;&gt;],[]},{mi\\_s
&gt;&gt; egment,iterate\\_all\\_bytes,2,[{file,"src/mi\\_segment.erl"},{line,167}]},{mi\\_serv
&gt;&gt; er,'-group\\_iterator/2-fun-1-',2,[{file,"src/mi\\_server.erl"},{line,725}]},{mi\\_
&gt;&gt; server,'-group\\_iterator/2-fun-0-',2,[{file,"src/mi\\_server.erl"},{line,722}]},
&gt;&gt; {mi\\_server,iterate2,5,[{file,"src/mi\\_server.erl"},{line,693}]}]} and partial
&gt;&gt; acc 
&gt;&gt; {{ho\\_acc,226,ok,#Fun,riak\\_search\\_vnode,&lt;
&gt;&gt; 0.4042.0&gt;,#Port&lt;0.754041&gt;,{274031556999544297163190906134303066185487351808,2
&gt;&gt; 74031556999544297163190906134303066185487351808},{ho\\_stats,{1385,326398,49843
&gt;&gt; 4},undefined,14225,2426123},gen\\_tcp,50226},{{&lt;&lt;"collector-collect-instagram-c
&gt;&gt; ache"&gt;&gt;,{&lt;&lt;"data\\_follows"&gt;&gt;,&lt;&lt;"who"&gt;&gt;}},[{&lt;&lt;"3700758"&gt;&gt;,[{p,[561]}],138375942
&gt;&gt; 9413536},{&lt;&lt;"368835984"&gt;&gt;,[{p,[297,303]}],1383611963556763},{&lt;&lt;"368835984"&gt;&gt;,
&gt;&gt; [{p,[298,304]}],1383756713657753},{&lt;&lt;"31715058"&gt;&gt;,[{p,[325]}],138361199635219
&gt;&gt; 3}]},4}
&gt;&gt; 2013-11-24 20:53:17.468 [error]
&gt;&gt; &lt;0.16054.14&gt;@riak\\_core\\_handoff\\_sender:start\\_fold:215 ownership\\_handoff
&gt;&gt; transfer of riak\\_search\\_vnode from 'riak@192.168.3.2'
&gt;&gt; 274031556999544297163190906134303066185487351808 to 'riak@192.168.3.13'
&gt;&gt; 274031556999544297163190906134303066185487351808 failed because of
&gt;&gt; error:{badmatch,{error,{badarg,[{erlang,binary\\_to\\_term,[&lt;&lt;131,104,3,109,0,0,0
&gt;&gt; ,25,99,111,108,108,101,99,116,111,114,45,99,111,108,108,101,99,116,45,116,119
&gt;&gt; ,105,116,116,101,114,109,0,0,0,14,100,97,116,97,95,102,111,108,108,111,119,10
&gt;&gt; 1,114,115,109,0,0,128,199,123,34,105,100,115,34,58,91,49,52,52,55,51,54,54,57
&gt;&gt; ,53,48,44,53,48,48,55,53,57,48,55,44,52,51,56,49,55,53,52,56,53,44,49,51,54,5
&gt;&gt; 3,49,50,49,52,50,44,52,54,50,52,52,54,56,51,44,49,48,55,57,56,55,49,50,48,48,
&gt;&gt; 44,55,55,48,56,51,54,55,57,44,50,56,51,56,51,57,55,56,44,49,57,50,48,55,50,55
&gt;&gt; ,51,48,44,51,57,54,57,56,56,57,56,55,44,50,56,48,50,54,51,56,48,52,44,53,57,5
&gt;&gt; 0,56,56,53,50,51,48,44,49,50,52,55,53,56,57,53,55,56,44,49,55,51,56,56,51,53,
&gt;&gt; 52,50,44,49,53,56,57,54,51,50,50,50,48,44,53,53,49,51,57,57,51,48,49,44,50,50
&gt;&gt; ,48,53,52,55,52,55,55,54,44,49,51,51,52,57,56,57,56,50,53,44,51,49,50,51,57,5
&gt;&gt; 3,55,54,50&gt;&gt;],[]},{mi\\_segment,iterate\\_all\\_bytes,2,[{file,"src/mi\\_segment.erl"
&gt;&gt; },{line,167}]},{mi\\_server,'-group\\_iterator/2-fun-1-',2,[{file,"src/mi\\_server.
&gt;&gt; erl"},{line,725}]},{mi\\_server,'-group\\_iterator/2-fun-0-',2,[{file,"src/mi\\_ser
&gt;&gt; ver.erl"},{line,722}]},{mi\\_server,iterate2,5,[{file,"src/mi\\_server.erl"},{lin
&gt;&gt; e,693}]}]},{{ho\\_acc,226,ok,#Fun,riak\\_sea
&gt;&gt; rch\\_vnode,&lt;0.4042.0&gt;,#Port&lt;0.754041&gt;,{274031556999544297163190906134303066185
&gt;&gt; 487351808,274031556999544297163190906134303066185487351808},{ho\\_stats,{1385,3
&gt;&gt; 26398,498434},undefined,14225,2426123},gen\\_tcp,50226},{{&lt;&lt;"collector-collect-
&gt;&gt; instagram-cache"&gt;&gt;,{&lt;&lt;"data\\_follows"&gt;&gt;,&lt;&lt;"who"&gt;&gt;}},[{&lt;&lt;"3700758"&gt;&gt;,[{p,[561]}
&gt;&gt; ],1383759429413536},{&lt;&lt;"368835984"&gt;&gt;,[{p,[297,303]}],1383611963556763},{&lt;&lt;"36
&gt;&gt; 8835984"&gt;&gt;,[{p,[298,304]}],1383756713657753},{&lt;&lt;"31715058"&gt;&gt;,[{p,[325]}],1383
&gt;&gt; 611996352193}]},4}}}
&gt;&gt; [{riak\\_core\\_handoff\\_sender,start\\_fold,5,[{file,"src/riak\\_core\\_handoff\\_sender.
&gt;&gt; erl"},{line,161}]}]
&gt;&gt; 
&gt;&gt; ------------------------------------------------------
&gt;&gt; 
&gt;&gt; I am aware that values that bucket might be larger than most of our other
&gt;&gt; objects. Not sure if that would cause the issues, though. Thanks for your
&gt;&gt; help!
&gt;&gt; 
&gt;&gt; J
&gt;&gt; 
&gt;&gt; 
&gt;&gt; 
&gt;&gt; On Nov 24, 2013, at 12:51 PM, Richard Shaw  wrote:
&gt;&gt; 
&gt;&gt;&gt; Hi Justin,
&gt;&gt;&gt; 
&gt;&gt;&gt; Please can you run this command to look for compaction errors in the leveldb
&gt;&gt;&gt; logs on the node with the crash log entries
&gt;&gt;&gt; 
&gt;&gt;&gt; grep -R "Compaction error" /var/lib/riak/leveldb/\\*/LOG
&gt;&gt;&gt; 
&gt;&gt;&gt; Where the path matches your path to the leveldb dir
&gt;&gt;&gt; 
&gt;&gt;&gt; Thanks
&gt;&gt;&gt; 
&gt;&gt;&gt; Richard
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; On 24 November 2013 10:45, Justin Long  wrote:
&gt;&gt;&gt;&gt; Hello everyone,
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Our Riak cluster has failed after what seems to be an issue in LevelDB.
&gt;&gt;&gt;&gt; Noticed that a process running a segment compact has started to throw
&gt;&gt;&gt;&gt; errors non-stop. I opened a Stack Overflow question here where you will
&gt;&gt;&gt;&gt; find a lot of log data:
&gt;&gt;&gt;&gt; http://stackoverflow.com/questions/20172878/riak-is-throwing-failed-to-comp
&gt;&gt;&gt;&gt; act-like-crazy
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Here is exactly what we're getting in console.log:
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 2013-11-24 10:38:46.803 [info]
&gt;&gt;&gt;&gt; &lt;0.19760.0&gt;@riak\\_core\\_handoff\\_receiver:process\\_message:99 Receiving handoff
&gt;&gt;&gt;&gt; data for partition
&gt;&gt;&gt;&gt; riak\\_search\\_vnode:1050454301831586472458898473514828420377701515264
&gt;&gt;&gt;&gt; 2013-11-24 10:38:47.239 [info]
&gt;&gt;&gt;&gt; &lt;0.19760.0&gt;@riak\\_core\\_handoff\\_receiver:handle\\_info:69 Handoff receiver for
&gt;&gt;&gt;&gt; partition 1050454301831586472458898473514828420377701515264 exited after
&gt;&gt;&gt;&gt; processing 5409 objects
&gt;&gt;&gt;&gt; 2013-11-24 10:38:49.743 [error] emulator Error in process &lt;0.19767.0&gt; on
&gt;&gt;&gt;&gt; node 'riak@192.168.3.3' with exit value:
&gt;&gt;&gt;&gt; {badarg,[{erlang,binary\\_to\\_term,[&lt;&lt;260
&gt;&gt;&gt;&gt; bytes&gt;&gt;],[]},{mi\\_segment,iterate\\_all\\_bytes,2,[{file,"src/mi\\_segment.erl"},{
&gt;&gt;&gt;&gt; line,167}]},{mi\\_server,'-group\\_iterator/2-fun-0-',2,[{file,"src/mi\\_server.e
&gt;&gt;&gt;&gt; rl"},{line,722}]},{mi\\_server,'-group\\_iterator/2-fun-1-'...
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 2013-11-24 10:38:49.743 [error] &lt;0.580.0&gt;@mi\\_scheduler:worker\\_loop:141
&gt;&gt;&gt;&gt; Failed to compact &lt;0.11868.0&gt;:
&gt;&gt;&gt;&gt; {badarg,[{erlang,binary\\_to\\_term,[&lt;&lt;131,104,3,109,0,0,0,25,99,111,108,108,10
&gt;&gt;&gt;&gt; 1,99,116,111,114,45,99,111,108,108,101,99,116,45,116,119,105,116,116,101,11
&gt;&gt;&gt;&gt; 4,109,0,0,0,14,100,97,116,97,95,102,111,108,108,111,119,101,114,115,109,0,0
&gt;&gt;&gt;&gt; ,128,203,123,34,105,100,115,34,58,91,49,54,50,51,53,50,50,50,50,51,44,49,55
&gt;&gt;&gt;&gt; ,51,55,51,52,52,50,44,49,50,56,51,52,52,56,55,51,57,44,51,57,56,56,57,56,50
&gt;&gt;&gt;&gt; ,51,52,44,49,52,52,55,51,54,54,57,53,48,44,53,48,48,55,53,57,48,55,44,52,51
&gt;&gt;&gt;&gt; ,56,49,55,53,52,56,53,44,49,51,54,53,49,50,49,52,50,44,52,54,50,52,52,54,56
&gt;&gt;&gt;&gt; ,51,44,49,48,55,57,56,55,49,50,48,48,44,55,55,48,56,51,54,55,57,44,50,56,51
&gt;&gt;&gt;&gt; ,56,51,57,55,56,44,49,57,50,48,55,50,55,51,48,44,51,57,54,57,56,56,57,56,55
&gt;&gt;&gt;&gt; ,44,50,56,48,50,54,51,56,48,52,44,53,57,50,56,56,53,50,51,48,44,49,50,52,55
&gt;&gt;&gt;&gt; ,53,56,57,53,55,56,44,49,55,51,56,56,51,53,52,50,44,49,53,56,57,54,51,50,50
&gt;&gt;&gt;&gt; ,50,48,44,53,53,49,51&gt;&gt;],[]},{mi\\_segment,iterate\\_all\\_bytes,2,[{file,"src/mi
&gt;&gt;&gt;&gt; \\_segment.erl"},{line,167}]},{mi\\_server,'-group\\_iterator/2-fun-0-',2,[{file,
&gt;&gt;&gt;&gt; "src/mi\\_server.erl"},{line,722}]},{mi\\_server,'-group\\_iterator/2-fun-1-',2,[
&gt;&gt;&gt;&gt; {file,"src/mi\\_server.erl"},{line,725}]},{mi\\_server,'-group\\_iterator/2-fun-0
&gt;&gt;&gt;&gt; -',2,[{file,"src/mi\\_server.erl"},{line,722}]},{mi\\_server,'-group\\_iterator/2
&gt;&gt;&gt;&gt; -fun-1-',2,[{file,"src/mi\\_server.erl"},{line,725}]},{mi\\_server,'-group\\_iter
&gt;&gt;&gt;&gt; ator/2-fun-0-',2,[{file,"src/mi\\_server.erl"},{line,722}]},{mi\\_segment\\_write
&gt;&gt;&gt;&gt; r,from\\_iterator,4,[{file,"src/mi\\_segment\\_writer.erl"},{line,110}]}]}
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; The log is just full of them. Thanks for your help! We need to get this
&gt;&gt;&gt;&gt; cluster back up ASAP, appreciated!
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; - Justin
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt; 
&gt; 
&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_ riak-users mailing list
&gt; riak-users@lists.basho.comhttp://lists.basho.com/mailman/listinfo/riak-users\\_l
&gt; ists.basho.com



\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

