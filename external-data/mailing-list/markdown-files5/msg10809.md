---
title: "Re: Map phase timeout"
description: ""
project: community
lastmod: 2013-04-09T17:26:03-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg10809"
mailinglist_parent_id: "msg10794"
author_name: "Matt Black"
project_section: "mailinglistitem"
sent_date: 2013-04-09T17:26:03-07:00
---


Hi Dmitri,

Thanks for your clarification. I was pretty sure that was how it would work
- and so I had planned a different way of migrating to a new backend. I
intended to introduce new nodes which have the eleveldb backend configured,
and presumed that Riak would move data into this backend as the node joined
the cluster. Then I would migrate out the bitcask nodes one-by-one.

Would this approach work? Or will I need to look at a migration tool?

Matt


On 10 April 2013 00:06, Dmitri Zagidulin  wrote:

&gt; Matt,
&gt;
&gt; Just for clarity - you mention that you plan to move the backend to
&gt; LevelDB before backing up old data.
&gt; I just want to caution and say - if you switch the config setting from
&gt; Bitcask to LevelDB and restart the cluster, Riak does not automatically
&gt; migrate the data for you, to the new back end.
&gt;
&gt; Meaning, if you just switch to LevelDB (without backing up data), you'll
&gt; have an empty cluster running on leveldb, and you'd have no way to access
&gt; the old data in Bitcask. Backing up and restoring data is helpful precisely
&gt; in the areas of migrating to a different back end (or to a different ring
&gt; size).
&gt;
&gt; (You probably knew this, and have a migration plan in mind already, but I
&gt; just wanted to make sure).
&gt;
&gt; If you need a good "logical backup" tool, take a look at
&gt; https://github.com/dankerrigan/riak-data-migrator (it's java-based, but
&gt; is pretty good at backing up the contents of one or more buckets to disk,
&gt; and then restoring afterwards). (As opposed to "file based backup" as
&gt; described in http://docs.basho.com/riak/latest/cookbooks/Backups/ , which
&gt; is the recommended approach for backups for a production cluster, but won't
&gt; help you in migrating to a different backend).
&gt;
&gt; Dmitri
&gt;
&gt;
&gt; On Mon, Apr 8, 2013 at 7:20 PM, Matt Black wrote:
&gt;
&gt;&gt; All,
&gt;&gt;
&gt;&gt; Huge thanks for your replies. It seems to me that our approach with
&gt;&gt; MapReduce queries has been fundamentally wrong, and that I should rewrite
&gt;&gt; my backup script to use sequential GETs. Currently we're on the bitcask
&gt;&gt; backend, and on our roadmap is a move over to eleveldb and the application
&gt;&gt; of appropriate 2i across the whole dataset. Looks like that will be the
&gt;&gt; next step - before doing any backup of old data.
&gt;&gt;
&gt;&gt; Matt
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt; On 9 April 2013 01:01, Dmitri Zagidulin  wrote:
&gt;&gt;
&gt;&gt;&gt; Matt,
&gt;&gt;&gt;
&gt;&gt;&gt; My recommendation to you is - don't use MapReduce for this use case.
&gt;&gt;&gt; Fetch the objects via regular Riak GETs (using connection pooling and
&gt;&gt;&gt; multithreading, preferably).
&gt;&gt;&gt;
&gt;&gt;&gt; I'm assuming that you have a list of keys (either by keeping track of
&gt;&gt;&gt; them externally to Riak, or via a Secondary Index query or a Search query),
&gt;&gt;&gt; and you want to back up those objects.
&gt;&gt;&gt;
&gt;&gt;&gt; The natural inclination, once you know the keys, is to want to fetch all
&gt;&gt;&gt; of those objects via a single query, and MapReduce immediately comes to
&gt;&gt;&gt; mind. (And to most developers, writing the MR function in Javascript is
&gt;&gt;&gt; easier and more familiar than in Erlang). Unfortunately, as Christian
&gt;&gt;&gt; mentioned, it's very easy for the JS VMs to run out of resources and crash
&gt;&gt;&gt; or time out. In addition, I've found that rewriting the MapReduce in Erlang
&gt;&gt;&gt; affords only a bit more resources -- once you hit a certain number of keys
&gt;&gt;&gt; that you want to fetch, or a certain object size threshold, even Erlang MR
&gt;&gt;&gt; jobs can time out (keep in mind, while the Map phase can happen in parallel
&gt;&gt;&gt; on all of the nodes in a cluster, all the object values have to be
&gt;&gt;&gt; serialized on the single coordinating node, which becomes the bottleneck).
&gt;&gt;&gt;
&gt;&gt;&gt; The workaround for this, even though it might seem counter-intuitive, is
&gt;&gt;&gt; -- if you know the list of keys, fetch them using GETs. Even a naive
&gt;&gt;&gt; single-threaded "while loop" way of fetching the objects can often be
&gt;&gt;&gt; faster than a MapReduce job (for this use case), and it doesn't time out.
&gt;&gt;&gt; Add to that connection-pooling and multiple worker threads, and this method
&gt;&gt;&gt; is invariably faster.
&gt;&gt;&gt;
&gt;&gt;&gt; Dmitri
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; On Mon, Apr 8, 2013 at 4:27 AM, Christian Dahlqvist &gt;&gt; &gt; wrote:
&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Hi Matt,
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; If you have a complicated mapreduce job containing multiple phases
&gt;&gt;&gt;&gt; implemented in JavaScript, you will most likely see a lot of contention for
&gt;&gt;&gt;&gt; the JavaScript VMs which will cause problems. While you can tune the
&gt;&gt;&gt;&gt; configuration [1], you may find that you will need a very large pool size
&gt;&gt;&gt;&gt; in order to properly support your job, especially for map phases as these
&gt;&gt;&gt;&gt; run in parallel.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; The best way to speed up the mapreduce job and get around the VM pool
&gt;&gt;&gt;&gt; contention is to implement the mapreduce functions in Erlang.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Best regards,
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Christian
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; [1]
&gt;&gt;&gt;&gt; http://docs.basho.com/riak/1.2.0/references/appendices/MapReduce-Implementation/#Configuration-Tuning-for-Javascript
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; --------------------
&gt;&gt;&gt;&gt; Christian Dahlqvist
&gt;&gt;&gt;&gt; Client Services Engineer
&gt;&gt;&gt;&gt; Basho Technologies
&gt;&gt;&gt;&gt; EMEA Office
&gt;&gt;&gt;&gt; E-mail: christ...@basho.com
&gt;&gt;&gt;&gt; Skype: c.dahlqvist
&gt;&gt;&gt;&gt; Mobile: +44 7890 590 910
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; On 8 Apr 2013, at 08:20, Matt Black  wrote:
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Thanks for the reply, Christian.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; I didn't explain well enough in my first post - the map reduce
&gt;&gt;&gt;&gt; operation is merely loading a bunch of objects, and a Python script which
&gt;&gt;&gt;&gt; makes the connection to Riak then will write these objects to disk. (It's
&gt;&gt;&gt;&gt; probably obvious, but I'm using javascript and riak python client.)
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; The query itself has many map phases where a composite object is built
&gt;&gt;&gt;&gt; up from related objects spread across many buckets.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; I was hoping there may be some kind of timeout I could adjust on a
&gt;&gt;&gt;&gt; per-map phase basis - clutching at straws really.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Cheers
&gt;&gt;&gt;&gt; Matt
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; On 8 April 2013 17:14, Christian Dahlqvist  wrote:
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; Hi,
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; Without having access to the mapreduce functions you are running, I
&gt;&gt;&gt;&gt;&gt; would assume that a mapreduce job both writing data to disk as well as
&gt;&gt;&gt;&gt;&gt; deleting the written record from Riak might be quite slow. This is not
&gt;&gt;&gt;&gt;&gt; really a use case mapreduce was designed for, and when a mapreduce job
&gt;&gt;&gt;&gt;&gt; crashes or times out it is difficult to know how far along the processing
&gt;&gt;&gt;&gt;&gt; of different records it got.
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; I would therefore recommend considering running this type of archiving
&gt;&gt;&gt;&gt;&gt; and delete job as an external batch process instead as it will give you
&gt;&gt;&gt;&gt;&gt; better control over the execution and avoid timeout problems.
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; Best regards,
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; Christian
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; On 8 Apr 2013, at 00:49, Matt Black  wrote:
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; &gt; Dear list,
&gt;&gt;&gt;&gt;&gt; &gt;
&gt;&gt;&gt;&gt;&gt; &gt; I'm currently getting a timeout during a single phase of a
&gt;&gt;&gt;&gt;&gt; multi-phase map reduce query. Is there anything I can do to assist this in
&gt;&gt;&gt;&gt;&gt; running?
&gt;&gt;&gt;&gt;&gt; &gt;
&gt;&gt;&gt;&gt;&gt; &gt; It's purpose is to backup and remove objects from Riak, so it will
&gt;&gt;&gt;&gt;&gt; run periodically during quiet times moving old data out of Riak into file
&gt;&gt;&gt;&gt;&gt; storage.
&gt;&gt;&gt;&gt;&gt; &gt;
&gt;&gt;&gt;&gt;&gt; &gt; Traceback (most recent call last):
&gt;&gt;&gt;&gt;&gt; &gt; File "./tools/rolling\\_backup.py", line 185, in 
&gt;&gt;&gt;&gt;&gt; &gt; main()
&gt;&gt;&gt;&gt;&gt; &gt; File "./tools/rolling\\_backup.py", line 181, in main
&gt;&gt;&gt;&gt;&gt; &gt; args.func(\\*\\*kwargs)
&gt;&gt;&gt;&gt;&gt; &gt; File "/srv/backup/tools/mapreduce.py", line 295, in do\\_map\\_reduce
&gt;&gt;&gt;&gt;&gt; &gt; raise e
&gt;&gt;&gt;&gt;&gt; &gt; Exception:
&gt;&gt;&gt;&gt;&gt; {"phase":2,"error":"timeout","input":"[&lt;&lt;\\"cart-products\\"&gt;&gt;,&lt;&lt;\\"cd67d7f6e2688bc2089e6fa79506ac05-2\\"&gt;&gt;,{struct,[{&lt;&lt;\\"uid\\"&gt;&gt;,&lt;&lt;\\"cd67d7f6e2688bc2089e6fa79506ac05\\"&gt;&gt;},{&lt;&lt;\\"cart\\"&gt;&gt;,{struct,[{&lt;&lt;\\"expired\\_ts\\"&gt;&gt;,&lt;&lt;\\"2013-03-05T19:12:23.906228\\"&gt;&gt;},{&lt;&lt;\\"last\\_updated\\"&gt;&gt;,&lt;&lt;\\"2013-03-05T19:12:23.906242\\"&gt;&gt;},{&lt;&lt;\\"tags\\"&gt;&gt;,{struct,[{&lt;&lt;\\"type\\"&gt;&gt;,&lt;&lt;\\"AB\\"&gt;&gt;}]}},{&lt;&lt;\\"completed\\"&gt;&gt;,false},{&lt;&lt;\\"created\\"&gt;&gt;,&lt;&lt;\\"2013-03-04T02:10:18.638413\\"&gt;&gt;},{&lt;&lt;\\"products\\"&gt;&gt;,[{struct,[{&lt;&lt;\\"cost\\"&gt;&gt;,0},{&lt;&lt;\\"bundleName\\"&gt;&gt;,&lt;&lt;\\"Product\\"&gt;&gt;},...]},...]},...]}},...]}]","type":"exit","stack":"[{riak\\_kv\\_w\\_reduce,'-js\\_runner/1-fun-0-',3,[{file,\\"src/riak\\_kv\\_w\\_reduce.erl\\"},{line,283}]},{riak\\_kv\\_w\\_reduce,reduce,3,[{file,\\"src/riak\\_kv\\_w\\_reduce.erl\\"},{line,206}]},{riak\\_kv\\_w\\_reduce,maybe\\_reduce,2,[{file,\\"src/riak\\_kv\\_w\\_reduce.erl\\"},{line,157}]},{riak\\_pipe\\_vnode\\_worker,process\\_input,3,[{file,\\"src/riak\\_pipe\\_vnode\\_worker.erl\\"},{line,444}]},{riak\\_pipe\\_vnode\\_worker,wait\\_for\\_input,2,[{file,\\"src/riak\\_pipe\\_vnode\\_worker.erl\\"},{line,376}]},{gen\\_fsm,handle\\_msg,7,[{file,\\"gen\\_fsm.erl\\"},{line,494}]},{proc\\_lib,...}]"}
&gt;&gt;&gt;&gt;&gt; &gt;
&gt;&gt;&gt;&gt;&gt; &gt;
&gt;&gt;&gt;&gt;&gt; &gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt;&gt;&gt;&gt; &gt; riak-users mailing list
&gt;&gt;&gt;&gt;&gt; &gt; riak-users@lists.basho.com
&gt;&gt;&gt;&gt;&gt; &gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;
&gt;
&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;
&gt;
\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

