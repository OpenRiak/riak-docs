---
title: "Re: RiakNode class in Python client 2.0"
description: ""
project: community
lastmod: 2013-08-06T19:08:03-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg11923"
mailinglist_parent_id: "msg11922"
author_name: "Sean Cribbs"
project_section: "mailinglistitem"
sent_date: 2013-08-06T19:08:03-07:00
---


Oh, sorry I misunderstood. I thought celery workers were threads within the
same Python interpreter. By all means have one client per Python process,
but see if you can keep it around between tasks. If the throughput on your
task queue is high enough, you should be able to make use of the benefit of
persistent connections and load-balancing.


On Tue, Aug 6, 2013 at 8:10 PM, Matt Black wrote:

&gt; Hi Sean,
&gt;
&gt; I would indeed like to take advantage of the pooling features of the new
&gt; client. Shared objects across Celery workers isn't something I'd ever
&gt; really looked at before - but it seems the only real way to share across
&gt; workers (ie, processes) is to use memcache or similar. Which makes sense.
&gt;
&gt; Cheers
&gt; Matt
&gt;
&gt;
&gt;
&gt; On 6 August 2013 23:04, Sean Cribbs  wrote:
&gt;
&gt;&gt; Hi Matt,
&gt;&gt;
&gt;&gt; You are correct about the Decaying class, it is a wrapper for an
&gt;&gt; exponentially-decaying error rate. The idea is, you configure your client
&gt;&gt; to connect to multiple Riak nodes, and if one goes down or is restarted,
&gt;&gt; the possibility of its selection for new connections can be automatically
&gt;&gt; reduced by detecting network errors. Of course, this is moot when you are
&gt;&gt; connecting to the cluster via a load-balancer; at that point the
&gt;&gt; error-tracking logic can't help you.
&gt;&gt;
&gt;&gt; If you are creating and throwing away RiakClient objects frequently,
&gt;&gt; neither the error-tracking nor the connection pool will help you much.
&gt;&gt; However, I urge you to consider keeping a single client around, in some
&gt;&gt; globally accessible place (module constant? config object?). Your
&gt;&gt; concurrent workers will get to take advantage of existing connections, and
&gt;&gt; your socket/file-handle usage will be less.
&gt;&gt;
&gt;&gt;
&gt;&gt; On Tue, Aug 6, 2013 at 1:07 AM, Matt Black wrote:
&gt;&gt;
&gt;&gt;&gt; I've been looking through the changes to the RiakClient class - and have
&gt;&gt;&gt; a question about the nodes list and RiakNode/Decaying class.
&gt;&gt;&gt;
&gt;&gt;&gt; It seems that the Decaying class is used internally to track error\\_rates
&gt;&gt;&gt; (request failure\\_rate?) of nodes in the available pool.
&gt;&gt;&gt;
&gt;&gt;&gt; In the application I maintain, we're accessing Riak in many concurrent
&gt;&gt;&gt; celery workers - so my assumption would be that this internal error\\_rate
&gt;&gt;&gt; tracking wouldn't do much good. Have I got this right? Is the client's
&gt;&gt;&gt; internal "nodes" list only really useful in a long running application?
&gt;&gt;&gt; (ie, when I'm not instantiating RiakClient fresh for each query).
&gt;&gt;&gt;
&gt;&gt;&gt; We currently have a "pool" of Riak nodes in some configuration, from
&gt;&gt;&gt; which a random IP is chosen for each request.
&gt;&gt;&gt;
&gt;&gt;&gt; Thanks
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt; --
&gt;&gt; Sean Cribbs 
&gt;&gt; Software Engineer
&gt;&gt; Basho Technologies, Inc.
&gt;&gt; http://basho.com/
&gt;&gt;
&gt;
&gt;


-- 
Sean Cribbs 
Software Engineer
Basho Technologies, Inc.
http://basho.com/
\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

