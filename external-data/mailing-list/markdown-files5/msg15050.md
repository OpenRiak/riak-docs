---
title: "Re: Memory-backend TTL"
description: ""
project: community
lastmod: 2014-10-20T06:44:35-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg15050"
mailinglist_parent_id: "msg15049"
author_name: "Luke Bakken"
project_section: "mailinglistitem"
sent_date: 2014-10-20T06:44:35-07:00
---


Lucas,

Thanks for all the detailed information. This is not expected
behavior. What MIME type are you using for storing the long integer
data (64 binary bits, I assume)?

I'd like to try and reproduce this. There have been issues with TTL
and max\\_memory but they should have been fixed for Riak 2.0.
--
Luke Bakken
Engineer / CSE
lbak...@basho.com


On Mon, Oct 20, 2014 at 1:56 AM, Lucas Grijander
 wrote:
&gt; Hi Luke,
&gt;
&gt; Indeed, when removed the thousands of requests, the memory is stabilized.
&gt; However the memory consumption is still very high:
&gt;
&gt; riak-admin status |grep memory
&gt; memory\\_total : 18494760128
&gt; memory\\_processes : 145363184
&gt; memory\\_processes\\_used : 142886424
&gt; memory\\_system : 18349396944
&gt; memory\\_atom : 561761
&gt; memory\\_atom\\_used : 554496
&gt; memory\\_binary : 7108243240
&gt; memory\\_code : 13917820
&gt; memory\\_ets : 11200328880
&gt;
&gt; I have test also with Riak 1.4.10 and the performance is the same.
&gt;
&gt; Is it normal that the "memory\\_ets" has more than 10GB when we have a
&gt; "ring\\_size" of 16 and a max\\_memory\\_per\\_vnode = 250MB?
&gt;
&gt; 2014-10-15 20:50 GMT+02:00 Lucas Grijander :
&gt;&gt;
&gt;&gt; Hi Luke.
&gt;&gt;
&gt;&gt; About the first issue:
&gt;&gt;
&gt;&gt; - From the beginning, the servers are all running ntpd. They are Ubuntu
&gt;&gt; 14.04 and the ntpd service is installed and running by default.
&gt;&gt; - Anti-entropy was also disabled from the beginning:
&gt;&gt;
&gt;&gt; {anti\\_entropy,{off,[]}},
&gt;&gt;
&gt;&gt;
&gt;&gt; About the second issue, I am perplex because, after 2 restarts of the Riak
&gt;&gt; server, just now there is a big memory consumption but is not growing like
&gt;&gt; the previous days. The only change was to remove this code (it was used
&gt;&gt; thousands of times/s). It was a possible workaround about the previous
&gt;&gt; problem with the TTL but this code now is useless because the TTL is working
&gt;&gt; fine with this node alone:
&gt;&gt;
&gt;&gt; self.db.delete((key)
&gt;&gt; self.db.get(key, r=1)
&gt;&gt;
&gt;&gt;
&gt;&gt; # riak-admin status|grep memory
&gt;&gt; memory\\_total : 18617871264
&gt;&gt; memory\\_processes : 224480232
&gt;&gt; memory\\_processes\\_used : 222700176
&gt;&gt; memory\\_system : 18393391032
&gt;&gt; memory\\_atom : 561761
&gt;&gt; memory\\_atom\\_used : 552862
&gt;&gt; memory\\_binary : 7135206080
&gt;&gt; memory\\_code : 13779729
&gt;&gt; memory\\_ets : 11209256232
&gt;&gt;
&gt;&gt; The problem is that I don't remember if the code change was after or
&gt;&gt; before the second restart. I am going to restart the riak server again and I
&gt;&gt; will report you about if the "possible memory leak" is reproduced.
&gt;&gt;
&gt;&gt; This is the props of the bucket:
&gt;&gt;
&gt;&gt; {"props":{"allow\\_mult":false,"backend":"ttl\\_stg","basic\\_quorum":false,"big\\_vclock":50,"chash\\_keyfun":{"mod":"riak\\_core\\_util","fun":"chash\\_std\\_keyfun"},"dvv\\_enabled":false,"dw":"quorum","last\\_write\\_wins":true,"linkfun":{"mod":"riak\\_kv\\_wm\\_link\\_walker","fun":"mapreduce\\_linkfun"},"n\\_val":1,"name":"ttl\\_stg","notfound\\_ok":true,"old\\_vclock":86400,"postcommit":[],"pr":0,"precommit":[],"pw":0,"r":1,"rw":"quorum","small\\_vclock":50,"w":1,"young\\_vclock":20}}
&gt;&gt;
&gt;&gt; About the data that we put into the bucket are all with this schema:
&gt;&gt;
&gt;&gt; KEY: Alphanumeric with a length of 47
&gt;&gt; DATA: Long integer.
&gt;&gt;
&gt;&gt; # riak-admin status|grep puts
&gt;&gt; vnode\\_puts : 84708
&gt;&gt; vnode\\_puts\\_total : 123127430
&gt;&gt; node\\_puts : 83169
&gt;&gt; node\\_puts\\_total : 123128062
&gt;&gt;
&gt;&gt; # riak-admin status|grep gets
&gt;&gt; vnode\\_gets : 162314
&gt;&gt; vnode\\_gets\\_total : 240433213
&gt;&gt; node\\_gets : 162317
&gt;&gt; node\\_gets\\_total : 240433216
&gt;&gt;
&gt;&gt; 2014-10-14 16:26 GMT+02:00 Luke Bakken :
&gt;&gt;&gt;
&gt;&gt;&gt; Hi Lucas,
&gt;&gt;&gt;
&gt;&gt;&gt; With regard to the mysterious key deletion / resurrection, please do
&gt;&gt;&gt; the following:
&gt;&gt;&gt;
&gt;&gt;&gt; \\* Ensure your servers are all running ntpd and have their time
&gt;&gt;&gt; synchronized as closely as possible.
&gt;&gt;&gt; \\* Disable anti-entropy. I suspect this is causing the strange behavior
&gt;&gt;&gt; you're seeing with keys.
&gt;&gt;&gt;
&gt;&gt;&gt; Your single node cluster memory consumption issue is a bit of a
&gt;&gt;&gt; puzzler. I'm assuming you're using default bucket settings and not
&gt;&gt;&gt; using bucket types based on your previous emails, and that allow\\_mult
&gt;&gt;&gt; is still false for your ttl\\_stg bucket. Can you tell me more about the
&gt;&gt;&gt; data you're putting into that bucket for testing? I'll try and
&gt;&gt;&gt; reproduce it with my single node cluster.
&gt;&gt;&gt;
&gt;&gt;&gt; --
&gt;&gt;&gt; Luke Bakken
&gt;&gt;&gt; Engineer / CSE
&gt;&gt;&gt; lbak...@basho.com
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; On Mon, Oct 13, 2014 at 5:02 PM, Lucas Grijander
&gt;&gt;&gt;  wrote:
&gt;&gt;&gt; &gt; Hi Luke.
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; I really appreciate your efforts to attempt to reproduce the problem. I
&gt;&gt;&gt; &gt; think that the configs are right. I have been doing also a lot of tests
&gt;&gt;&gt; &gt; and
&gt;&gt;&gt; &gt; with 1 server/node, the memory bucket works flawlessly, as your test.
&gt;&gt;&gt; &gt; The
&gt;&gt;&gt; &gt; Riak cluster where we have the problem has a multi\\_backend with 1
&gt;&gt;&gt; &gt; memory
&gt;&gt;&gt; &gt; backend, 2 bitcask backends and 2 leveldb backends. I have only changed
&gt;&gt;&gt; &gt; the
&gt;&gt;&gt; &gt; parameter connection of the memory backend in our production code to
&gt;&gt;&gt; &gt; another
&gt;&gt;&gt; &gt; new "cluster" with only 1 node, with the same config of Riak but with
&gt;&gt;&gt; &gt; only 1
&gt;&gt;&gt; &gt; memory backend under the multi configuration and, as I said, all fine,
&gt;&gt;&gt; &gt; the
&gt;&gt;&gt; &gt; problem vanished. I deduce that the problem appears only with more than
&gt;&gt;&gt; &gt; 1
&gt;&gt;&gt; &gt; node and with a lot of requests.
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; In my tests with the production cluster with the problem ( 4 nodes),
&gt;&gt;&gt; &gt; finally
&gt;&gt;&gt; &gt; I realized that the TTL is working but, randomly and suddenly, KEYS
&gt;&gt;&gt; &gt; already
&gt;&gt;&gt; &gt; deleted appear, and KEYS with correct TTL disappear :-? (Maybe
&gt;&gt;&gt; &gt; something
&gt;&gt;&gt; &gt; related with the some ETS internal table? ) This is the moment when I
&gt;&gt;&gt; &gt; can
&gt;&gt;&gt; &gt; obtain KEYS already expired.
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; In summary:
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; - With cluster with 4 nodes (config below): All OK for a while and
&gt;&gt;&gt; &gt; suddenly
&gt;&gt;&gt; &gt; we lost the last 20 seconds approx. of keys and OLD keys appear in the
&gt;&gt;&gt; &gt; list:
&gt;&gt;&gt; &gt; curl -X GET http://localhost:8098/buckets/ttl\\_stg/keys?keys=true
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; buckets.default.last\\_write\\_wins = true
&gt;&gt;&gt; &gt; bitcask.io\\_mode = erlang
&gt;&gt;&gt; &gt; multi\\_backend.ttl\\_stg.storage\\_backend = memory
&gt;&gt;&gt; &gt; multi\\_backend.ttl\\_stg.memory\\_backend.ttl = 90s
&gt;&gt;&gt; &gt; multi\\_backend.ttl\\_stg.memory\\_backend.max\\_memory\\_per\\_vnode = 25MB
&gt;&gt;&gt; &gt; anti\\_entropy = passive
&gt;&gt;&gt; &gt; ring\\_size = 256
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; - With 1 node: All OK
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; buckets.default.n\\_val = 1
&gt;&gt;&gt; &gt; buckets.default.last\\_write\\_wins = true
&gt;&gt;&gt; &gt; buckets.default.r = 1
&gt;&gt;&gt; &gt; buckets.default.w = 1
&gt;&gt;&gt; &gt; multi\\_backend. ttl\\_stg.storage\\_backend = memory
&gt;&gt;&gt; &gt; multi\\_backend. ttl\\_stg.memory\\_backend.ttl = 90s
&gt;&gt;&gt; &gt; multi\\_backend. ttl\\_stg.memory\\_backend.max\\_memory\\_per\\_vnode = 250MB
&gt;&gt;&gt; &gt; ring\\_size = 16
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; Another note: With this 1 node (32GB RAM) and only activated the memory
&gt;&gt;&gt; &gt; backend I have realized than the memory consumption grows without
&gt;&gt;&gt; &gt; control:
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; # riak-admin status|grep memory
&gt;&gt;&gt; &gt; memory\\_total : 17323130960
&gt;&gt;&gt; &gt; memory\\_processes : 235043016
&gt;&gt;&gt; &gt; memory\\_processes\\_used : 233078456
&gt;&gt;&gt; &gt; memory\\_system : 17088087944
&gt;&gt;&gt; &gt; memory\\_atom : 561761
&gt;&gt;&gt; &gt; memory\\_atom\\_used : 561127
&gt;&gt;&gt; &gt; memory\\_binary : 6737787976
&gt;&gt;&gt; &gt; memory\\_code : 14370908
&gt;&gt;&gt; &gt; memory\\_ets : 10295224544
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; # # riak-admin diag -d debug
&gt;&gt;&gt; &gt; [debug] Local RPC: os:getpid([]) [5000]
&gt;&gt;&gt; &gt; [debug] Running shell command: ps -o pmem,rss -p 17521
&gt;&gt;&gt; &gt; [debug] Shell command output:
&gt;&gt;&gt; &gt; %MEM RSS
&gt;&gt;&gt; &gt; 60.5 19863800
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; Wow 18.9GB when the max\\_memory\\_per\\_vnode = 250MB. Is far away from the
&gt;&gt;&gt; &gt; value, 250\\*16vnodes = 4000MB. Is it that correct?
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; This is the riak-admin vnode-status of 1 vnode, the other 15 are with
&gt;&gt;&gt; &gt; similar data:
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; VNode: 1370157784997721485815954530671515330927436759040
&gt;&gt;&gt; &gt; Backend: riak\\_kv\\_multi\\_backend
&gt;&gt;&gt; &gt; Status:
&gt;&gt;&gt; &gt; [{&lt;&lt;"ttl\\_stg"&gt;&gt;,
&gt;&gt;&gt; &gt; [{mod,riak\\_kv\\_memory\\_backend},
&gt;&gt;&gt; &gt; {data\\_table\\_status,[{compressed,false},
&gt;&gt;&gt; &gt; {memory,1156673},
&gt;&gt;&gt; &gt; {owner,&lt;8343.9466.104&gt;},
&gt;&gt;&gt; &gt; {heir,none},
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; {name,riak\\_kv\\_1370157784997721485815954530671515330927436759040},
&gt;&gt;&gt; &gt; {size,29656},
&gt;&gt;&gt; &gt; {node,'riak@xxxxxxxx'},
&gt;&gt;&gt; &gt; {named\\_table,false},
&gt;&gt;&gt; &gt; {type,ordered\\_set},
&gt;&gt;&gt; &gt; {keypos,1},
&gt;&gt;&gt; &gt; {protection,protected}]},
&gt;&gt;&gt; &gt; {index\\_table\\_status,[{compressed,false},
&gt;&gt;&gt; &gt; {memory,89},
&gt;&gt;&gt; &gt; {owner,&lt;8343.9466.104&gt;},
&gt;&gt;&gt; &gt; {heir,none},
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; {name,riak\\_kv\\_1370157784997721485815954530671515330927436759040\\_i},
&gt;&gt;&gt; &gt; {size,0},
&gt;&gt;&gt; &gt; {node,'riak@xxxxxxxxx'},
&gt;&gt;&gt; &gt; {named\\_table,false},
&gt;&gt;&gt; &gt; {type,ordered\\_set},
&gt;&gt;&gt; &gt; {keypos,1},
&gt;&gt;&gt; &gt; {protection,protected}]},
&gt;&gt;&gt; &gt; {time\\_table\\_status,[{compressed,false},
&gt;&gt;&gt; &gt; {memory,75968936},
&gt;&gt;&gt; &gt; {owner,&lt;8343.9466.104&gt;},
&gt;&gt;&gt; &gt; {heir,none},
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; {name,riak\\_kv\\_1370157784997721485815954530671515330927436759040\\_t},
&gt;&gt;&gt; &gt; {size,2813661},
&gt;&gt;&gt; &gt; {node,'riak@xxxxxxxxx'},
&gt;&gt;&gt; &gt; {named\\_table,false},
&gt;&gt;&gt; &gt; {type,ordered\\_set},
&gt;&gt;&gt; &gt; {keypos,1},
&gt;&gt;&gt; &gt; {protection,protected}]}]}]
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; Thanks!
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; 2014-10-13 22:30 GMT+02:00 Luke Bakken :
&gt;&gt;&gt; &gt;&gt;
&gt;&gt;&gt; &gt;&gt; Hi Lucas,
&gt;&gt;&gt; &gt;&gt;
&gt;&gt;&gt; &gt;&gt; I've tried reproducing this using a local Riak 2.0.1 node, however TTL
&gt;&gt;&gt; &gt;&gt; is working as expected.
&gt;&gt;&gt; &gt;&gt;
&gt;&gt;&gt; &gt;&gt; Here is the configuration I have in /etc/riak/riak.conf:
&gt;&gt;&gt; &gt;&gt;
&gt;&gt;&gt; &gt;&gt; storage\\_backend = multi
&gt;&gt;&gt; &gt;&gt; multi\\_backend.default = bc\\_default
&gt;&gt;&gt; &gt;&gt;
&gt;&gt;&gt; &gt;&gt; multi\\_backend.ttl\\_stg.storage\\_backend = memory
&gt;&gt;&gt; &gt;&gt; multi\\_backend.ttl\\_stg.memory\\_backend.ttl = 90s
&gt;&gt;&gt; &gt;&gt; multi\\_backend.ttl\\_stg.memory\\_backend.max\\_memory\\_per\\_vnode = 4MB
&gt;&gt;&gt; &gt;&gt;
&gt;&gt;&gt; &gt;&gt; multi\\_backend.bc\\_default.storage\\_backend = bitcask
&gt;&gt;&gt; &gt;&gt; multi\\_backend.bc\\_default.bitcask.data\\_root = /var/lib/riak/bc\\_default
&gt;&gt;&gt; &gt;&gt; multi\\_backend.bc\\_default.bitcask.io\\_mode = erlang
&gt;&gt;&gt; &gt;&gt;
&gt;&gt;&gt; &gt;&gt; This translates to the following in
&gt;&gt;&gt; &gt;&gt; /var/lib/riak/generated.configs/app.2014.10.13.13.13.29.config:
&gt;&gt;&gt; &gt;&gt;
&gt;&gt;&gt; &gt;&gt; {multi\\_backend\\_default,&lt;&lt;"bc\\_default"&gt;&gt;},
&gt;&gt;&gt; &gt;&gt; {multi\\_backend,
&gt;&gt;&gt; &gt;&gt; [{&lt;&lt;"ttl\\_stg"&gt;&gt;,riak\\_kv\\_memory\\_backend,[{ttl,90},{max\\_memory,4}]},
&gt;&gt;&gt; &gt;&gt; {&lt;&lt;"bc\\_default"&gt;&gt;,riak\\_kv\\_bitcask\\_backend,
&gt;&gt;&gt; &gt;&gt; [{io\\_mode,erlang},
&gt;&gt;&gt; &gt;&gt; {expiry\\_grace\\_time,0},
&gt;&gt;&gt; &gt;&gt; {small\\_file\\_threshold,10485760},
&gt;&gt;&gt; &gt;&gt; {dead\\_bytes\\_threshold,134217728},
&gt;&gt;&gt; &gt;&gt; {frag\\_threshold,40},
&gt;&gt;&gt; &gt;&gt; {dead\\_bytes\\_merge\\_trigger,536870912},
&gt;&gt;&gt; &gt;&gt; {frag\\_merge\\_trigger,60},
&gt;&gt;&gt; &gt;&gt; {max\\_file\\_size,2147483648},
&gt;&gt;&gt; &gt;&gt; {open\\_timeout,4},
&gt;&gt;&gt; &gt;&gt; {data\\_root,"/var/lib/riak/bc\\_default"},
&gt;&gt;&gt; &gt;&gt; {sync\\_strategy,none},
&gt;&gt;&gt; &gt;&gt; {merge\\_window,always},
&gt;&gt;&gt; &gt;&gt; {max\\_fold\\_age,-1},
&gt;&gt;&gt; &gt;&gt; {max\\_fold\\_puts,0},
&gt;&gt;&gt; &gt;&gt; {expiry\\_secs,-1},
&gt;&gt;&gt; &gt;&gt; {require\\_hint\\_crc,true}]}]}]},
&gt;&gt;&gt; &gt;&gt;
&gt;&gt;&gt; &gt;&gt; I set the bucket properties to use the ttl\\_stg backend:
&gt;&gt;&gt; &gt;&gt;
&gt;&gt;&gt; &gt;&gt; root@UBUNTU-12-1:~# cat ttl\\_stg-props.json
&gt;&gt;&gt; &gt;&gt; {"props":{"name":"ttl\\_stg","backend":"ttl\\_stg"}}
&gt;&gt;&gt; &gt;&gt;
&gt;&gt;&gt; &gt;&gt; root@UBUNTU-12-1:~# curl -XPUT -H'Content-type: application/json'
&gt;&gt;&gt; &gt;&gt; localhost:8098/buckets/ttl\\_stg/props --data-ascii @ttl\\_stg-props.json
&gt;&gt;&gt; &gt;&gt;
&gt;&gt;&gt; &gt;&gt; root@UBUNTU-12-1:~# curl -XGET localhost:8098/buckets/ttl\\_stg/props
&gt;&gt;&gt; &gt;&gt;
&gt;&gt;&gt; &gt;&gt;
&gt;&gt;&gt; &gt;&gt; {"props":{"allow\\_mult":false,"backend":"ttl\\_stg","basic\\_quorum":false,"big\\_vclock":50,"chash\\_keyfun":{"mod":"riak\\_core\\_util","fun":"chash\\_std\\_keyfun"},"dvv\\_enabled":false,"dw":"quorum","last\\_write\\_wins":false,"linkfun":{"mod":"riak\\_kv\\_wm\\_link\\_walker","fun":"mapreduce\\_linkfun"},"n\\_val":3,"name":"ttl\\_stg","notfound\\_ok":true,"old\\_vclock":86400,"postcommit":[],"pr":0,"precommit":[],"pw":0,"r":"quorum","rw":"quorum","small\\_vclock":50,"w":"quorum","young\\_vclock":20}}
&gt;&gt;&gt; &gt;&gt;
&gt;&gt;&gt; &gt;&gt;
&gt;&gt;&gt; &gt;&gt; And used the following statement to PUT test data:
&gt;&gt;&gt; &gt;&gt;
&gt;&gt;&gt; &gt;&gt; curl -XPUT localhost:8098/buckets/ttl\\_stg/keys/1 -d "TEST $(date)"
&gt;&gt;&gt; &gt;&gt;
&gt;&gt;&gt; &gt;&gt; After 90 seconds, this is the response I get from Riak:
&gt;&gt;&gt; &gt;&gt;
&gt;&gt;&gt; &gt;&gt; root@UBUNTU-12-1:~# curl -XGET localhost:8098/buckets/ttl\\_stg/keys/1
&gt;&gt;&gt; &gt;&gt; not found
&gt;&gt;&gt; &gt;&gt;
&gt;&gt;&gt; &gt;&gt; I would carefully check all of the app.config / riak.conf files in
&gt;&gt;&gt; &gt;&gt; your cluster, the output of "riak config effective" and the bucket
&gt;&gt;&gt; &gt;&gt; properties for those buckets you expect to be using the memory backend
&gt;&gt;&gt; &gt;&gt; with TTL. I also recommend using the localhost:8098/buckets/ endpoint
&gt;&gt;&gt; &gt;&gt; instead of the deprecated riak/ endpoint.
&gt;&gt;&gt; &gt;&gt;
&gt;&gt;&gt; &gt;&gt; Please let me know if you have additional questions.
&gt;&gt;&gt; &gt;&gt; --
&gt;&gt;&gt; &gt;&gt; Luke Bakken
&gt;&gt;&gt; &gt;&gt; Engineer / CSE
&gt;&gt;&gt; &gt;&gt; lbak...@basho.com
&gt;&gt;&gt; &gt;&gt;
&gt;&gt;&gt; &gt;&gt;
&gt;&gt;&gt; &gt;&gt; On Fri, Oct 3, 2014 at 11:32 AM, Lucas Grijander
&gt;&gt;&gt; &gt;&gt;  wrote:
&gt;&gt;&gt; &gt;&gt; &gt; Hello,
&gt;&gt;&gt; &gt;&gt; &gt;
&gt;&gt;&gt; &gt;&gt; &gt; I have a memory backend in production with Riak 2.0.1, 4 servers and
&gt;&gt;&gt; &gt;&gt; &gt; 256
&gt;&gt;&gt; &gt;&gt; &gt; vnodes. The servers have the same date and time.
&gt;&gt;&gt; &gt;&gt; &gt;
&gt;&gt;&gt; &gt;&gt; &gt; I have seen an odd performance with the ttl.
&gt;&gt;&gt; &gt;&gt; &gt;
&gt;&gt;&gt; &gt;&gt; &gt; This is the config:
&gt;&gt;&gt; &gt;&gt; &gt;
&gt;&gt;&gt; &gt;&gt; &gt; {&lt;&lt;"ttl\\_stg"&gt;&gt;,riak\\_kv\\_memory\\_backend,
&gt;&gt;&gt; &gt;&gt; &gt; [{ttl,90},{max\\_memory,25}]},
&gt;&gt;&gt; &gt;&gt; &gt;
&gt;&gt;&gt; &gt;&gt; &gt; For example, see this GET response in one of the riak servers:
&gt;&gt;&gt; &gt;&gt; &gt;
&gt;&gt;&gt; &gt;&gt; &gt; &lt; HTTP/1.1 200 OK
&gt;&gt;&gt; &gt;&gt; &gt; &lt; X-Riak-Vclock: a85hYGBgzGDKBVIc4otdfgR/7bfIYEpkzGNlKI1efJYvCwA=
&gt;&gt;&gt; &gt;&gt; &gt; &lt; Vary: Accept-Encoding
&gt;&gt;&gt; &gt;&gt; &gt; \\* Server MochiWeb/1.1 WebMachine/1.10.5 (jokes are better explained)
&gt;&gt;&gt; &gt;&gt; &gt; is
&gt;&gt;&gt; &gt;&gt; &gt; not
&gt;&gt;&gt; &gt;&gt; &gt; blacklisted
&gt;&gt;&gt; &gt;&gt; &gt; &lt; Server: MochiWeb/1.1 WebMachine/1.10.5 (jokes are better
&gt;&gt;&gt; &gt;&gt; &gt; explained)
&gt;&gt;&gt; &gt;&gt; &gt; &lt; Link: ; rel="up"
&gt;&gt;&gt; &gt;&gt; &gt; &lt; Last-Modified: Fri, 03 Oct 2014 17:40:05 GMT
&gt;&gt;&gt; &gt;&gt; &gt; &lt; ETag: "3c8bGoifWcOCSVn0otD5nI"
&gt;&gt;&gt; &gt;&gt; &gt; &lt; Date: Fri, 03 Oct 2014 17:47:50 GMT
&gt;&gt;&gt; &gt;&gt; &gt; &lt; Content-Type: application/json
&gt;&gt;&gt; &gt;&gt; &gt; &lt; Content-Length: 17
&gt;&gt;&gt; &gt;&gt; &gt;
&gt;&gt;&gt; &gt;&gt; &gt; If the TTL is 90 seconds, Why the GET doesn't return "not found" if
&gt;&gt;&gt; &gt;&gt; &gt; the
&gt;&gt;&gt; &gt;&gt; &gt; difference between "Last-Modified" and "Date" (of the curl request)
&gt;&gt;&gt; &gt;&gt; &gt; is
&gt;&gt;&gt; &gt;&gt; &gt; greater than the TTL?
&gt;&gt;&gt; &gt;&gt; &gt;
&gt;&gt;&gt; &gt;&gt; &gt; Thanks in advance!
&gt;&gt;&gt; &gt;&gt; &gt;
&gt;&gt;&gt; &gt;&gt; &gt;
&gt;&gt;&gt; &gt;&gt; &gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt;&gt; &gt;&gt; &gt; riak-users mailing list
&gt;&gt;&gt; &gt;&gt; &gt; riak-users@lists.basho.com
&gt;&gt;&gt; &gt;&gt; &gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt;&gt; &gt;&gt; &gt;
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt;&gt; &gt; riak-users mailing list
&gt;&gt;&gt; &gt; riak-users@lists.basho.com
&gt;&gt;&gt; &gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt;&gt; &gt;
&gt;&gt;
&gt;&gt;
&gt;

\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

