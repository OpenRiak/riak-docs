---
title: "Re: Fwd: RiakCS 504 Timeout on s3cmd for certain keys"
description: ""
project: community
lastmod: 2014-08-18T19:04:40-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg14726"
mailinglist_parent_id: "msg14711"
author_name: "Kota Uenishi"
project_section: "mailinglistitem"
sent_date: 2014-08-18T19:04:40-07:00
---


Alex,

Riak CS 1.4.5 and 1.5.0 had a lot of improvement after those articles you
put the URL, not it is not using Riak's bucket listing but using Riak's
internal API for more efficient listing. What version of Riak CS are you
using? I want you to make sure you're using those versions and a line
`{fold\\_objects\\_for\\_list\\_keys, true},` at riak\\_cs section of app.config
(assuming all other Riak part correctly configured).

&gt; Based on this I’m thinking that cost of this type of query is only going
to get worse over time as we add more keys to this bucket (unless secondary
indexes can be added). Or am I totally out to lunch here and there’s some
other underlying problem?

The strange part is s3cmd. Riak CS has incremental bucket listing API that
requires clients to iterate on every 1000 objects (common prefixes), but
s3cmd iterates all the specified bucket before printing them all. You can
observe how s3cmd and Riak CS interacts if you specify '-d' option like
this:

```
s3cmd -d -c yours.s3cfg ls -r s3://yourbucket/yourdir/
```

I would expect Riak CS's listing API is not much slow as to need 5 seconds
(or, say, &gt;10 seconds) because, on each request it just returns 1000
objects.

There might be another possibility on slow query - if you had many (say,
more than 10 thousands) deleted objects on the same bucket it might affect
each 1000 listing. This will eventually be solved as Riak CS's garbage
collection removes deleted manifests, which is just marked as deleted (and
to be ignored correctly).

[1]
http://www.quora.com/Riak/Is-it-really-expensive-for-Riak-to-list-all-buckets-Why

On Thu, Aug 14, 2014 at 6:05 AM, Alex Millar  wrote:

&gt; Good afternoon Charlie,
&gt;
&gt; So the issue we’re having is only with bucket listing.
&gt;
&gt; alxndrmlr@alxndrmlr-mbp $ time s3cmd -c .s3cfg-riakcs-admin ls
&gt; s3://bonfirehub-resources-can-east-doc-conversion
&gt; DIR
&gt; s3://bonfirehub-resources-can-east-doc-conversion/organizations/
&gt;
&gt; real 2m0.747s
&gt; user 0m0.076s
&gt; sys 0m0.030s
&gt;
&gt; where as…
&gt;
&gt; alxndrmlr@alxndrmlr-mbp $ time s3cmd -c .s3cfg-riakcs-admin ls
&gt; s3://bonfirehub-resources-can-east-doc-conversion/organizations/OrganizationID-1/documents/proposals
&gt; DIR
&gt; s3://bonfirehub-resources-can-east-doc-conversion/organizations/OrganizationID-1/documents/proposals/
&gt;
&gt; real 0m10.262s
&gt; user 0m0.075s
&gt; sys 0m0.028s
&gt;
&gt; The contents of this bucket contains a lot of very small files (basically
&gt; for each PDF we receive I split it to .JPG foreach page and store them
&gt; here. Based on the my latest counts it looks like we have around \\*170,000\\* 
&gt; .JPG
&gt; files in that bucket.
&gt;
&gt; Here’s a snippet from the HAProxy log for the 504 timeouts…
&gt;
&gt; Aug 12 16:01:34  
&gt; localhost.localdomain
&gt; haproxy[4718]: 192.0.223.236:48457 [12/Aug/2014:16:01:24.454] riak\\_cs~
&gt; riak\\_cs\\_backend/riak3 161/0/0/-1/10162 504 194 - - sH-- 0/0/0/0/0 0/0 {
&gt; bonfirehub-resources-can-east-doc-conversion.bf-riakcs.com} "GET
&gt; /?delimiter=/ HTTP/1.1"
&gt;
&gt; I’ve put together a video showing off the top results of each of the 5
&gt; riak nodes while performing $ time s3cmd -c .s3cfg-riakcs-admin ls
&gt; s3://bonfirehub-resources-can-east-doc-conversion
&gt;
&gt;
&gt; https://dl.dropboxusercontent.com/u/5723659/RiakCS%20ls%20monitoring%20results.mov
&gt;
&gt; Now I’ve had a hunch this is just a fundamentally expensive operation
&gt; which exceeds the 5000ms response time threshold set in our HAProxy config
&gt; (which I raised during the video to illustrate what’s going on). After
&gt; reading
&gt; http://www.quora.com/Riak/Is-it-really-expensive-for-Riak-to-list-all-buckets-Why
&gt; and http://www.paperplanes.de/2011/12/13/list-all-of-the-riak-keys.html I’m
&gt; feeling like this is just a fundamental issue with the data structure in
&gt; Riak.
&gt;
&gt; Based on this I’m thinking that cost of this type of query is only going
&gt; to get worse over time as we add more keys to this bucket (unless secondary
&gt; indexes can be added). Or am I totally out to lunch here and there’s some
&gt; other underlying problem?
&gt;
&gt; I’ve cc’d the mailing list on this as suggested.
&gt;
&gt; [image: Bonfire Logo] \\*Alex Millar\\*, CTO
&gt; Office: 1-800-354-8010 ext. 704 &lt;+18003548010&gt;
&gt; Mobile: 519-729-2539 &lt;+15197292539&gt;
&gt; \\*GoBonfire\\*.com 
&gt;
&gt; From: Charlie Voiselle  
&gt; Reply: Charlie Voiselle &gt; 
&gt; Date: August 13, 2014 at 10:36:51 AM
&gt; To: Alex Millar &gt; 
&gt; Cc: Tad Bickford &gt; 
&gt; Subject: Fwd: RiakCS 504 Timeout on s3cmd for certain keys
&gt;
&gt;
&gt;
&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;
&gt;


-- 
Kota UENISHI / @kuenishi
Basho Japan KK
\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

