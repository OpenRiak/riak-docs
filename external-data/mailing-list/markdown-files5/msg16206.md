---
title: "Re: Recommended way to delete keys"
description: ""
project: community
lastmod: 2015-06-04T13:00:59-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg16206"
mailinglist_parent_id: "msg16198"
author_name: "Alexander Sicular"
project_section: "mailinglistitem"
sent_date: 2015-06-04T13:00:59-07:00
---


But then you need to use bitcask... And you have one ttl per backend I believe. 
If that works for you...

-Alexander 

@siculars
http://siculars.posthaven.com

Sent from my iRotaryPhone

&gt; On Jun 4, 2015, at 12:24, Bryce Verdier  wrote:
&gt; 
&gt; I realize I'm kind of late to this party, but what about
&gt; using the auto-expire feature and letting riak do the deletion of data
&gt; for you?
&gt; 
&gt; The link is for an older version, but I know the
&gt; functionality still exists in riak2.
&gt; http://docs.basho.com/riak/latest/community/faqs/developing/#how-can-i-automatically-expire-a-key-from-riak
&gt; 
&gt; Warm regards,
&gt; Bryce
&gt; 
&gt; 
&gt; On Thu, 4 Jun 2015 09:28:04 +0200
&gt; Daniel Abrahamsson  wrote:
&gt; 
&gt;&gt; Hi Peter,
&gt;&gt; 
&gt;&gt; What is "large-scale" in your case? How many keys do you need to
&gt;&gt; delete, and how often?
&gt;&gt; 
&gt;&gt; //Daniel
&gt;&gt; 
&gt;&gt; On Wed, Jun 3, 2015 at 9:54 PM, Peter Herndon 
&gt;&gt; wrote:
&gt;&gt; 
&gt;&gt;&gt; Interesting thought. It might work for us, it might not, I’ll have
&gt;&gt;&gt; to check with our CTO to see whether the expense makes sense under
&gt;&gt;&gt; our circumstances.
&gt;&gt;&gt; 
&gt;&gt;&gt; Thanks!
&gt;&gt;&gt; 
&gt;&gt;&gt; —Peter
&gt;&gt;&gt;&gt; On Jun 3, 2015, at 2:21 PM, Drew Kerrigan 
&gt;&gt;&gt;&gt; wrote:
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Another idea for a large-scale one-time removal of data, as well
&gt;&gt;&gt;&gt; as an
&gt;&gt;&gt; opportunity for a fresh start, would be to:
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 1. set up multi-data center replication between 2 clusters
&gt;&gt;&gt;&gt; 2. implement a recv/2 hook on the sink which refuses data from the
&gt;&gt;&gt; buckets / keys you would like to ignore / delete
&gt;&gt;&gt;&gt; 3. trigger a full sync replication
&gt;&gt;&gt;&gt; 4. start using the sync as your new source of data sans the
&gt;&gt;&gt;&gt; ignored data
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Obviously this is costly, but it should have a fairly minimal
&gt;&gt;&gt;&gt; impact to
&gt;&gt;&gt; existing production users other than the moment that you switch
&gt;&gt;&gt; traffic from the old cluster to the new one.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Caveats: Not all Riak features are supported with MDC (search
&gt;&gt;&gt;&gt; indexes
&gt;&gt;&gt; and strong consistency in particular).
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; On Wed, Jun 3, 2015 at 2:11 PM Peter Herndon 
&gt;&gt;&gt;&gt; wrote:
&gt;&gt;&gt;&gt; Sadly, this is a production cluster already using leveldb as the
&gt;&gt;&gt; backend. With that constraint in mind, and rebuilding the cluster
&gt;&gt;&gt; not really being an option to enable multi-backends or bitcask,
&gt;&gt;&gt; what would our best approach be?
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Thanks!
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; —Peter
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; On Jun 3, 2015, at 12:09 PM, Alexander Sicular
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt; wrote:
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; We are actively investigating better options for deletion of
&gt;&gt;&gt;&gt;&gt; large
&gt;&gt;&gt; amounts of keys. As Sargun mentioned, deleting the data dir for an
&gt;&gt;&gt; entire backend via an operationalized rolling restart is probably
&gt;&gt;&gt; the best approach right now for killing large amounts of keys.
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; But if your key space can fit in memory the best way to kill
&gt;&gt;&gt;&gt;&gt; keys is
&gt;&gt;&gt; to use bitcask ttl if that's an option. 1. If you can even use
&gt;&gt;&gt; bitcask in your environment due to the memory overhead and 2. If
&gt;&gt;&gt; your use case allows for ttls which it may considering you may
&gt;&gt;&gt; already be using time bound buckets....
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; -Alexander
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; @siculars
&gt;&gt;&gt;&gt;&gt; http://siculars.posthaven.com
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; Sent from my iRotaryPhone
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; On Jun 3, 2015, at 09:54, Sargun Dhillon 
&gt;&gt;&gt;&gt;&gt; wrote:
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; You could map your keys to a given bucket, and that bucket to
&gt;&gt;&gt;&gt;&gt;&gt; a given
&gt;&gt;&gt; backend using multi\\_backend. There is some cost to having lots of
&gt;&gt;&gt; backends (memory overhead, FDs, etc...). When you want to do a mass
&gt;&gt;&gt; drop, you could down the node, and delete that given backend, and
&gt;&gt;&gt; bring it up. Caveat: AAE, MDC, nor mutable data play well with this
&gt;&gt;&gt; scenario.
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; On Wed, Jun 3, 2015 at 10:43 AM, Peter Herndon
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt; wrote:
&gt;&gt;&gt;&gt;&gt;&gt; Hi list,
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; We’re looking for the best way to handle large scale
&gt;&gt;&gt;&gt;&gt;&gt; expiration of
&gt;&gt;&gt; no-longer-useful data stored in Riak. We asked a while back, and the
&gt;&gt;&gt; recommendation was to store the data in time-segmented buckets
&gt;&gt;&gt; (bucket per day or per month), query on the current buckets, and
&gt;&gt;&gt; use the streaming list keys API to handle slowly deleting the
&gt;&gt;&gt; buckets that have aged out.
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; Is that still the best approach for doing this kind of task?
&gt;&gt;&gt;&gt;&gt;&gt; Or is
&gt;&gt;&gt; there a better approach?
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; Thanks!
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; —Peter Herndon
&gt;&gt;&gt;&gt;&gt;&gt; Sr. Application Engineer
&gt;&gt;&gt;&gt;&gt;&gt; @Bitly
&gt;&gt;&gt;&gt;&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt;&gt;&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt;&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt;&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt;&gt;&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt;&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt;&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt; 
&gt; 
&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

