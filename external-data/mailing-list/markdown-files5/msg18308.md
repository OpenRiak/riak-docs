---
title: "Re: Solr search response time spikes"
description: ""
project: community
lastmod: 2017-07-03T03:03:02-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg18308"
mailinglist_parent_id: "msg18284"
author_name: "sean mcevoy"
project_section: "mailinglistitem"
sent_date: 2017-07-03T03:03:02-07:00
---


Hi List, Fred,

After a week of going cross-eyed looking at stats & trying to engineer a
test case to make this happen in the test env I think I've made a
breakthrough.

We have a low but steady level of riak traffic but our application level
actions that result in solr reads are actually fairly infrequent. And when
one of these actions occur it results in multiple parallel reads to our
solr indexes.

What I've observed is that our timeouts are most easily reproduced after a
period of inactivity. And once I see a timeout after 2 seconds I kick off
multiple other reads to random keys and observe that some return instantly
while others can take several seconds, but then all return at the same time.

It's almost as if some shards in the java VM have gone to sleep due to
inactivity and we see a cluster of timeouts when we try to read from it.

I'm setting up a "pinger" script in our prod env to keep these awake and
see if our observed timeout rate reduces.

If this is actually our problem are there any JVM config options we can use
to keep the index active all the time?

//Sean.

On Fri, Jun 23, 2017 at 1:48 PM, sean mcevoy  wrote:

&gt; Hi Fred,
&gt;
&gt; Thanks for taking the time!
&gt; Yes, I noticed that unbalance yesterday when writing, looked into it after
&gt; sending and found our config is corrupt with one node ommitted and another
&gt; in there twice.
&gt; But, with such low traffic levels and the spikes being on the non-favoured
&gt; node I'm not currently ranking that as a likely factor.
&gt;
&gt;
&gt; Another interesting case from last night, this sample was taken at
&gt; 2017-6-23 06:04:09
&gt;
&gt; Riak node 1
&gt; "search\\_query\\_throughput\\_one": 27
&gt; "search\\_query\\_latency\\_max": 10417
&gt;
&gt; Riak node 2
&gt; "search\\_query\\_throughput\\_one": 49
&gt; "search\\_query\\_latency\\_max": 8952
&gt;
&gt; Riak node 3
&gt; "search\\_query\\_throughput\\_one": 18
&gt; "search\\_query\\_throughput\\_count": 2507
&gt; "search\\_query\\_latency\\_min": 1757
&gt; "search\\_query\\_latency\\_median": 14775
&gt; "search\\_query\\_latency\\_mean": 5628361
&gt; "search\\_query\\_latency\\_max": 18298854
&gt; "search\\_query\\_latency\\_999": 18298854
&gt; "search\\_query\\_latency\\_99": 18298854
&gt; "search\\_query\\_latency\\_95": 16539782
&gt;
&gt; Riak node 4
&gt; "search\\_query\\_throughput\\_one": 25
&gt; "search\\_query\\_latency\\_max": 10217
&gt;
&gt;
&gt; Brushing up my maths and focussing on node 3, from the 99 & 95% figures we
&gt; can tell the 2 slowest response times were 18,298 & 16,539ms, 34,837 ms in
&gt; total.
&gt; And from the request count for the minute & the mean we can tell that in
&gt; total these 18 requests spent a total of 101,310 ms being processed.
&gt; From the median & min we know the 9 quickest took between 18 & 265 ms in
&gt; total.
&gt; This leaves in the region of 66 sec for the other 7 requests, enough for
&gt; all 7 to have timed out.
&gt;
&gt;
&gt; Cross referencing with our application logs I can see:
&gt;
&gt; On application node 1 at 2017-06-23 06:03:17 we had 3 search request
&gt; timeouts to index A with 3 different filters, one field of which, lets call
&gt; it field X, had the same value.
&gt; We immediately retried these and at 2017-06-23 06:03:19 2 of those timed
&gt; out again and were retried again.
&gt; They all succeeded on this retry, so this suggests that the same requests
&gt; sent to other riak nodes was fine, but to this riak node at this time was a
&gt; problem.
&gt;
&gt; On application node 2 at:
&gt; 2017-06-23 06:03:27
&gt; 2017-06-23 06:03:29
&gt; 2017-06-23 06:03:31
&gt; 2017-06-23 06:03:33
&gt;
&gt; we had 4 more timeouts on search requests to index A, these requests had 2
&gt; different filters but in both cases field X had the same value as in the
&gt; previous example.
&gt;
&gt;
&gt; So these application logs show 9 riak timeouts, that must correlate with
&gt; the riak stats.
&gt; I can't definitively say that no other search requests went to this riak
&gt; node between 06:03:15 & 06:03:33 but the circumstantial evidence is that it
&gt; had a problem for 18 seconds, which is quiet a big window.
&gt;
&gt;
&gt; The index that all these requests were directed at currently has 490K
&gt; entries with 8 different fields defined in each. The corresponding riak
&gt; bucket has allow\\_mult = false, if that's relevant.
&gt;
&gt; We see a similar pattern on our test system, I'm going to setup a test to
&gt; repeatedly do searches and see if I can trigger this consistently. Will let
&gt; ye know if anything interesting comes out of it.
&gt;
&gt; I know it's relatively new to the product, do we know is riak solr used
&gt; much in production systems?
&gt; I assume no one else has seen these spikes?
&gt;
&gt; //Sean.
&gt;
&gt;
&gt; On Thu, Jun 22, 2017 at 9:40 PM, Fred Dushin  wrote:
&gt;
&gt;&gt; It's pretty strange that you are seeing no search latency measurements on
&gt;&gt; node 5. Are you sure your round robining is working? Are you favoring
&gt;&gt; node 1?
&gt;&gt;
&gt;&gt; In general, I don't think which node you hit for query should make a
&gt;&gt; difference, but I'd have to stare at the code some to be sure. In essence,
&gt;&gt; all the node that services the query does is convert the query into a
&gt;&gt; sharded Solr query based on a coverage plan, which changes every minute or
&gt;&gt; so, and then runs the sharded query on the local Solr node. The Solr node
&gt;&gt; then distributes the query to the rest of the nodes in the cluster, but
&gt;&gt; that's all Solr comms -- Riak is out of the picture, by then.
&gt;&gt;
&gt;&gt; Now, if you have a lot of sharded queries accumulating on one node, that
&gt;&gt; might make a difference to Solr. I am not a Solr expert, and I don't even
&gt;&gt; play one on TV. But maybe the fact that you are not hitting node 5 is
&gt;&gt; relevant for that reason?
&gt;&gt;
&gt;&gt; Can you do more analysis on your client, to make sure you are not
&gt;&gt; favoring node 1?
&gt;&gt;
&gt;&gt; -Fred
&gt;&gt;
&gt;&gt; &gt; On Jun 22, 2017, at 10:20 AM, sean mcevoy 
&gt;&gt; wrote:
&gt;&gt; &gt;
&gt;&gt; &gt; Hi List,
&gt;&gt; &gt;
&gt;&gt; &gt; We have a standard riak cluster with 5 nodes and at the minute the
&gt;&gt; traffic levels are fairly low. Each of our application nodes has 25 client
&gt;&gt; connections, 5 to each riak node which get selected in a round robin.
&gt;&gt; &gt;
&gt;&gt; &gt; Our application level requests involve multiple riak requests so our
&gt;&gt; traffic tends to make requests in small bursts. Everything works fine for
&gt;&gt; KV gets, puts & deletes but we're seeing timeouts & weird response time
&gt;&gt; spikes on solr search operations.
&gt;&gt; &gt;
&gt;&gt; &gt; In the past 36 hours (the only period I have riak stats for) I see one
&gt;&gt; response time of 38.8 seconds, 3 hours earlier a response time of 20.8
&gt;&gt; seconds, and the third biggest spike is an acceptable 3.5 seconds.
&gt;&gt; &gt;
&gt;&gt; &gt; See below all search\\_query stats for the minute of the 38 sec sample.
&gt;&gt; In the application request we made 5 riak search requests to the same index
&gt;&gt; in parallel, which happens for each request of this type and normally
&gt;&gt; doesn't have an issue. But in this case all 5 timed out, and one timed out
&gt;&gt; again on retry with the other 4 succeeding.
&gt;&gt; &gt;
&gt;&gt; &gt; Anyone ever seen anything like this before? Is there any known deadlock
&gt;&gt; in solr that I might hit if I make the same request on another connection
&gt;&gt; before the first has completed? This is what we do when our riak client
&gt;&gt; times out after 2 seconds and immediately retries.
&gt;&gt; &gt;
&gt;&gt; &gt; Any advice or pointers welcomed.
&gt;&gt; &gt; Thanks,
&gt;&gt; &gt; //Sean.
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt; &gt; Riak node 1
&gt;&gt; &gt; search\\_query\\_throughput\\_one: 14
&gt;&gt; &gt; search\\_query\\_throughput\\_count: 259
&gt;&gt; &gt; search\\_query\\_latency\\_min: 2776
&gt;&gt; &gt; search\\_query\\_latency\\_median: 69411
&gt;&gt; &gt; search\\_query\\_latency\\_mean: 4900973
&gt;&gt; &gt; search\\_query\\_latency\\_max: 38887902
&gt;&gt; &gt; search\\_query\\_latency\\_999: 38887902
&gt;&gt; &gt; search\\_query\\_latency\\_99: 38887902
&gt;&gt; &gt; search\\_query\\_latency\\_95: 2046215
&gt;&gt; &gt; search\\_query\\_fail\\_one: 0
&gt;&gt; &gt; search\\_query\\_fail\\_count: 0
&gt;&gt; &gt;
&gt;&gt; &gt; Riak node 2
&gt;&gt; &gt; search\\_query\\_throughput\\_one: 22
&gt;&gt; &gt; search\\_query\\_throughput\\_count: 564
&gt;&gt; &gt; search\\_query\\_latency\\_min: 4006
&gt;&gt; &gt; search\\_query\\_latency\\_median: 8800
&gt;&gt; &gt; search\\_query\\_latency\\_mean: 11834
&gt;&gt; &gt; search\\_query\\_latency\\_max: 25509
&gt;&gt; &gt; search\\_query\\_latency\\_999: 25509
&gt;&gt; &gt; search\\_query\\_latency\\_99: 25509
&gt;&gt; &gt; search\\_query\\_latency\\_95: 24035
&gt;&gt; &gt; search\\_query\\_fail\\_one: 0
&gt;&gt; &gt; search\\_query\\_fail\\_count: 0
&gt;&gt; &gt;
&gt;&gt; &gt; Riak node 3
&gt;&gt; &gt; search\\_query\\_throughput\\_one: 6
&gt;&gt; &gt; search\\_query\\_throughput\\_count: 298
&gt;&gt; &gt; search\\_query\\_latency\\_min: 3200
&gt;&gt; &gt; search\\_query\\_latency\\_median: 15391
&gt;&gt; &gt; search\\_query\\_latency\\_mean: 18062
&gt;&gt; &gt; search\\_query\\_latency\\_max: 31759
&gt;&gt; &gt; search\\_query\\_latency\\_999: 31759
&gt;&gt; &gt; search\\_query\\_latency\\_99: 31759
&gt;&gt; &gt; search\\_query\\_latency\\_95: 31759
&gt;&gt; &gt; search\\_query\\_fail\\_one: 0
&gt;&gt; &gt; search\\_query\\_fail\\_count: 0
&gt;&gt; &gt;
&gt;&gt; &gt; Riak node 4
&gt;&gt; &gt; search\\_query\\_throughput\\_one: 8
&gt;&gt; &gt; search\\_query\\_throughput\\_count: 334
&gt;&gt; &gt; search\\_query\\_latency\\_min: 2404
&gt;&gt; &gt; search\\_query\\_latency\\_median: 7230
&gt;&gt; &gt; search\\_query\\_latency\\_mean: 10211
&gt;&gt; &gt; search\\_query\\_latency\\_max: 22502
&gt;&gt; &gt; search\\_query\\_latency\\_999: 22502
&gt;&gt; &gt; search\\_query\\_latency\\_99: 22502
&gt;&gt; &gt; search\\_query\\_latency\\_95: 22502
&gt;&gt; &gt; search\\_query\\_fail\\_one: 0
&gt;&gt; &gt; search\\_query\\_fail\\_count: 0
&gt;&gt; &gt;
&gt;&gt; &gt; Riak node 5
&gt;&gt; &gt; search\\_query\\_throughput\\_one: 0
&gt;&gt; &gt; search\\_query\\_throughput\\_count: 0
&gt;&gt; &gt; search\\_query\\_latency\\_min: 0
&gt;&gt; &gt; search\\_query\\_latency\\_median: 0
&gt;&gt; &gt; search\\_query\\_latency\\_mean: 0
&gt;&gt; &gt; search\\_query\\_latency\\_max: 0
&gt;&gt; &gt; search\\_query\\_latency\\_999: 0
&gt;&gt; &gt; search\\_query\\_latency\\_99: 0
&gt;&gt; &gt; search\\_query\\_latency\\_95: 0
&gt;&gt; &gt; search\\_query\\_fail\\_one: 0
&gt;&gt; &gt; search\\_query\\_fail\\_count: 0
&gt;&gt; &gt;
&gt;&gt; &gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt; &gt; riak-users mailing list
&gt;&gt; &gt; riak-users@lists.basho.com
&gt;&gt; &gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt;
&gt;&gt;
&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt; riak-users mailing list
&gt;&gt; riak-users@lists.basho.com
&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt;
&gt;
&gt;
\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

