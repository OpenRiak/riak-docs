---
title: "Re: riak handoffs stalled"
description: ""
project: community
lastmod: 2014-07-14T05:35:29-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg14491"
mailinglist_parent_id: "msg14490"
author_name: "Леонид Рябоштан"
project_section: "mailinglistitem"
sent_date: 2014-07-14T05:35:29-07:00
---


Hello,

riak version is 1.1.4-1. We set transfer limit in config made it equal to 4.

I don't think we have riak-admin transfer-limit or riak-admin cluster plan.

The problem is that damn nodes can't pass partition between each other,
probably because they're too big. Each 5k files(leveldb backend) and
weights 10GB each. There're no problems with smaller partitions. We can't
find anything usefull on handoff fail in riak or system logs. Seems like
ulimit and erlang ports are way higher, we increased it 4 times today.

It begins like:
2014-07-14 12:22:45.518 UTC [info]
&lt;0.10544.0&gt;@riak\\_core\\_handoff\\_sender:start\\_fold:83 Starting handoff of
partition riak\\_kv\\_vnode 68507889249886074290797726533575766546371837952
from 'riak@192.168.153.182' to 'riak@192.168.164.133'

And ends like:
2014-07-14 08:43:28.829 UTC [error]
&lt;0.2264.0&gt;@riak\\_core\\_handoff\\_sender:start\\_fold:152 Handoff of partition
riak\\_kv\\_vnode 68507889249886074290797726533575766546371837952 from '
riak@192.168.153.182' to 'riak@192.168.164.133' FAILED after sending
1318000 objects in 1455.15 seconds: closed
2014-07-14 10:40:18.294 UTC [error]
&lt;0.11555.0&gt;@riak\\_core\\_handoff\\_sender:start\\_fold:152 Handoff of partition
riak\\_kv\\_vnode 68507889249886074290797726533575766546371837952 from '
riak@192.168.153.182' to 'riak@192.168.164.133' FAILED after sending 911000
objects in 2734.48 seconds: closed
2014-07-14 09:43:43.197 UTC [error]
&lt;0.26922.2&gt;@riak\\_core\\_handoff\\_sender:start\\_fold:152 Handoff of partition
riak\\_kv\\_vnode 68507889249886074290797726533575766546371837952 from '
riak@192.168.153.182' to 'riak@192.168.164.133' FAILED after sending 32000
objects in 963.06 seconds: timeout

Maybe we need to check something else on target node? Actually it always
runs in GC problems:
2014-07-14 12:30:03.579 UTC [info]
&lt;0.99.0&gt;@riak\\_core\\_sysmon\\_handler:handle\\_event:85 monitor long\\_gc &lt;0.468.0&gt;
[{initial\\_call,{riak\\_kv\\_js\\_vm,init,1}},{almost\\_current\\_function,{xmerl\\_ucs,expand\\_utf8\\_1,3}},{message\\_queue\\_len,0}]
[{timeout,118},{old\\_heap\\_block\\_size,0},{heap\\_block\\_size,196418},{mbuf\\_size,0},{stack\\_size,45},{old\\_heap\\_size,0},{heap\\_size,136165}]
2014-07-14 12:30:44.386 UTC [info]
&lt;0.99.0&gt;@riak\\_core\\_sysmon\\_handler:handle\\_event:85 monitor long\\_gc &lt;0.713.0&gt;
[{initial\\_call,{riak\\_core\\_vnode,init,1}},{almost\\_current\\_function,{gen\\_fsm,loop,7}},{message\\_queue\\_len,0}]
[{timeout,126},{old\\_heap\\_block\\_size,0},{heap\\_block\\_size,1597},{mbuf\\_size,0},{stack\\_size,38},{old\\_heap\\_size,0},{heap\\_size,658}]

Probably we have some CPU issues here, but node is not under load currently.

Thank you,
Leonid


2014-07-14 16:11 GMT+04:00 Ciprian Manea :

&gt; Hi Leonid,
&gt;
&gt; Which Riak version are you running?
&gt;
&gt; Have you committed\\* the cluster plan after issuing the cluster
&gt; force-remove  commands?
&gt;
&gt; What is the output of $ riak-admin transfer-limit, ran from one of your
&gt; riak nodes?
&gt;
&gt;
&gt; \\*Do not run this command yet if you have not done it already.
&gt; Please run a riak-admin cluster plan and attach its output here.
&gt;
&gt;
&gt; Thanks,
&gt; Ciprian
&gt;
&gt;
&gt; On Mon, Jul 14, 2014 at 2:41 PM, Леонид Рябоштан &lt;
&gt; leonid.riabosh...@twiket.com&gt; wrote:
&gt;
&gt;&gt; Hello, guys,
&gt;&gt;
&gt;&gt; It seems like we ran into emergency. I wonder if there can be any help on
&gt;&gt; that.
&gt;&gt;
&gt;&gt; Everything that happened below was because we were trying to rebalace
&gt;&gt; space used by nodes that we running out of space.
&gt;&gt;
&gt;&gt; Cluster is 7 machines now, member\\_status looks like:
&gt;&gt; Attempting to restart script through sudo -u riak
&gt;&gt; ================================= Membership
&gt;&gt; ==================================
&gt;&gt; Status Ring Pending Node
&gt;&gt;
&gt;&gt; -------------------------------------------------------------------------------
&gt;&gt; valid 15.6% 20.3% 'riak@192.168.135.180'
&gt;&gt; valid 0.0% 0.0% 'riak@192.168.152.90'
&gt;&gt; valid 0.0% 0.0% 'riak@192.168.153.182'
&gt;&gt; valid 26.6% 23.4% 'riak@192.168.164.133'
&gt;&gt; valid 27.3% 21.1% 'riak@192.168.177.36'
&gt;&gt; valid 8.6% 15.6% 'riak@192.168.194.138'
&gt;&gt; valid 21.9% 19.5% 'riak@192.168.194.149'
&gt;&gt;
&gt;&gt; -------------------------------------------------------------------------------
&gt;&gt; Valid:7 / Leaving:0 / Exiting:0 / Joining:0 / Down:0
&gt;&gt;
&gt;&gt; 2 nodes with 0 Ring was made to force leave the cluster, they have plenty
&gt;&gt; of data on them which is now seems to be not accessible. Handoffs are stuck
&gt;&gt; it seems. Node 'riak@192.168.152.90'(is in same situation as '
&gt;&gt; riak@192.168.153.182') tries to handoff partitions to '
&gt;&gt; riak@192.168.164.133' but fails for unknown reason after huge
&gt;&gt; timeouts(from 5 to 40 minutes). Partition it's trying to move is about 10Gb
&gt;&gt; in size. It grows slowly on target node, but probably it's just usual
&gt;&gt; writes from normal operation. It doesn't get any smaller on source node.
&gt;&gt;
&gt;&gt; I wonder is there any way to let cluster know that we want those nodes to
&gt;&gt; be actually members of source node and there's no actual need to transfer
&gt;&gt; them? How to redo cluster ownership balance? Revert this force-leave stuff.
&gt;&gt;
&gt;&gt; Thank you,
&gt;&gt; Leonid
&gt;&gt;
&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt; riak-users mailing list
&gt;&gt; riak-users@lists.basho.com
&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt;
&gt;&gt;
&gt;
\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

