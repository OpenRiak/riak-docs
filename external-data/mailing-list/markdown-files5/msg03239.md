---
title: "Re: 'not found' after join"
description: ""
project: community
lastmod: 2011-05-05T13:28:32-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg03239"
mailinglist_parent_id: "msg03233"
author_name: "Mike Oxford"
project_section: "mailinglistitem"
sent_date: 2011-05-05T13:28:32-07:00
---


As someone not familiar with Riak's internals...

You have xN replication.
Make the transition-nodes be part of an x(N+1) and write-only (eg, they
don't count in the read quorum.)

If you're set up for x3 replication, then the transition bucket ends up as
part of an x4 replication.
As more queries come in, you will get up to 4 responses, but the write-only
response gets tossed.
A split would give you up to a 2x2 where it's really 2x1+1 and you can
rebuild normally.
Once your x4 replication is consistant you remove one node from the
replication set, taking you back to 3.

This, while not trivial, seems to leverage many of the pieces you already
have in place and avoids the
problem of "slow replication fubars requests for indeterminate amounts of
time."

Just a suggestion from the peanut gallery... :)

-mox



On Thu, May 5, 2011 at 1:06 PM, Andy Gross  wrote:

&gt;
&gt; Alex's description roughly matches up with some of our plans to address
&gt; this issue.
&gt;
&gt; As with almost anything, this comes down to a tradeoff between consistency
&gt; and availability. In the case of joining nodes, making the
&gt; join/handoff/ownership claim process more "atomic" requires a higher degree
&gt; of consensus from the machines in the cluster. The current process (which
&gt; is clearly non-optimal) allows nodes to join the ring as long as they can
&gt; contact one current ring member. A more atomic process would introduce
&gt; consensus issues that might prevent nodes from joining in partitioned
&gt; scenarios.
&gt;
&gt; A good solution would probably involve some consistency knobs around the
&gt; join process to deal with a spectrum of failure/partition scenarios.
&gt;
&gt; This is something of which we are acutely aware and are actively pursuing
&gt; solutions for a near-term release.
&gt;
&gt; - Andy
&gt;
&gt;
&gt; On Thu, May 5, 2011 at 12:22 PM, Alexander Sicular wrote:
&gt;
&gt;&gt; I'm really loving this thread. Generating great ideas for the way
&gt;&gt; things should be... in the future. It seems to me that "the ring
&gt;&gt; changes immediately" is actually the problem as Ryan astutely
&gt;&gt; mentions. One way the future could look is :
&gt;&gt;
&gt;&gt; - a new node comes online
&gt;&gt; - introductions are made
&gt;&gt; - candidate vnodes are selected for migration (&lt;- insert pixie dust magic
&gt;&gt; here)
&gt;&gt; - the number of simultaneous migrations are configurable, fewer for
&gt;&gt; limited interruption or more for quicker completion
&gt;&gt; - vnodes are migrated
&gt;&gt; - once migration is completed, ownership is claimed
&gt;&gt;
&gt;&gt; Selecting vnodes for migration is where the unicorn cavalry attack the
&gt;&gt; dragons den. If done right(er) the algorithm could be swappable to
&gt;&gt; optimize for different strategies. Don't ask me how to implement it,
&gt;&gt; I'm only a yellow belt in erlang-fu.
&gt;&gt;
&gt;&gt; Cheers,
&gt;&gt; Alexander
&gt;&gt;
&gt;&gt; On Thu, May 5, 2011 at 13:33, Ryan Zezeski  wrote:
&gt;&gt; &gt; John,
&gt;&gt; &gt; All great points. The problem is that the ring changes immediately when
&gt;&gt; a
&gt;&gt; &gt; node is added. So now, all the sudden, the preflist is potentially
&gt;&gt; pointing
&gt;&gt; &gt; to nodes that don't have the data and they won't have that data until
&gt;&gt; &gt; handoff occurs. The faster that data gets transferred, the less time
&gt;&gt; your
&gt;&gt; &gt; clients have to hit 'notfound'.
&gt;&gt; &gt; However, I agree completely with what you're saying. This is just a
&gt;&gt; side
&gt;&gt; &gt; effect of how the system currently works. In a perfect world we
&gt;&gt; wouldn't
&gt;&gt; &gt; care how long handoff takes and we would also do some sort of automatic
&gt;&gt; &gt; congestion control akin to TCP Reno or something. The preflist would
&gt;&gt; still
&gt;&gt; &gt; point to the "old" partitions until all data has been successfully
&gt;&gt; handed
&gt;&gt; &gt; off, and then and only then would we flip the switch for that vnode.
&gt;&gt; I'm
&gt;&gt; &gt; pretty sure that's where we are heading (I say "pretty sure" b/c I just
&gt;&gt; &gt; joined the team and haven't been heavily involved in these specific
&gt;&gt; talks
&gt;&gt; &gt; yet).
&gt;&gt; &gt; It's all coming down the pipe...
&gt;&gt; &gt; As for your specific I/O question re handoff\\_concurrecy, you might be
&gt;&gt; right.
&gt;&gt; &gt; I would think it depends on hardware/platform/etc. I was offering it
&gt;&gt; as a
&gt;&gt; &gt; possible stopgap to minimize Greg's pain. It's certainly a cure to a
&gt;&gt; &gt; symptom, not the problem itself.
&gt;&gt; &gt; -Ryan
&gt;&gt; &gt;
&gt;&gt; &gt; On Thu, May 5, 2011 at 1:10 PM, John D. Rowell  wrote:
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; Hi Ryan, Greg,
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; 2011/5/5 Ryan Zezeski 
&gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt;&gt; 1. For example, riak\\_core has a `handoff\\_concurrency` setting that
&gt;&gt; &gt;&gt;&gt; determines how many vnodes can concurrently handoff on a given node.
&gt;&gt; By
&gt;&gt; &gt;&gt;&gt; default this is set to 4. That's going to take a while with your 2048
&gt;&gt; &gt;&gt;&gt; vnodes and all :)
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; Won't that make the handoff situation potentially worse? From the
&gt;&gt; thread I
&gt;&gt; &gt;&gt; understood that the main problem was that the cluster was shuffling too
&gt;&gt; much
&gt;&gt; &gt;&gt; data around and thus becoming unresponsive and/or returning unexpected
&gt;&gt; &gt;&gt; results (like "not founds"). I'm attributing the concerns more to an
&gt;&gt; &gt;&gt; excessive I/O situation than to how long the handoff takes. If the
&gt;&gt; handoff
&gt;&gt; &gt;&gt; can be made transparent (no or little side effects) I don't think most
&gt;&gt; &gt;&gt; people will really care (e.g. the "fix the cluster tomorrow" anecdote).
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; How about using a percentage of available I/O to throttle the vnode
&gt;&gt; &gt;&gt; handoff concurrency? Start with 1, and monitor the node's I/O (kinda
&gt;&gt; like
&gt;&gt; &gt;&gt; 'atop' does, collection CPU, disk and network metrics), if it is below
&gt;&gt; the
&gt;&gt; &gt;&gt; expected usage, then increase the vnode handoff concurrency, and
&gt;&gt; vice-versa.
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; I for one would be perfectly happy if the handoff took several hours
&gt;&gt; (even
&gt;&gt; &gt;&gt; days) if we could maintain the core riak\\_kv characteristics intact
&gt;&gt; during
&gt;&gt; &gt;&gt; those events. We've all seen looooong RAID rebuild times, and it's
&gt;&gt; usually
&gt;&gt; &gt;&gt; better to just sit tight and keep the rebuild speed low (slower I/O)
&gt;&gt; while
&gt;&gt; &gt;&gt; keeping all of the dependent systems running smoothly.
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; cheers
&gt;&gt; &gt;&gt; -jd
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt; &gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt; &gt; riak-users mailing list
&gt;&gt; &gt; riak-users@lists.basho.com
&gt;&gt; &gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt;
&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt; riak-users mailing list
&gt;&gt; riak-users@lists.basho.com
&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt;
&gt;
&gt;
&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;
&gt;
\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

