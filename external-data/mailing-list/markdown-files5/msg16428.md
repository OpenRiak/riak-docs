---
title: "Re: why leaving riak cluster so slowly and how to accelerate the speed"
description: ""
project: community
lastmod: 2015-08-17T08:28:42-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg16428"
mailinglist_parent_id: "msg16414"
author_name: "Amao"
project_section: "mailinglistitem"
sent_date: 2015-08-17T08:28:42-07:00
---


There are so many pending transfer on production server . That is different 
between production and developing, that is my concern.

Sent from my iPhone

&gt; On 2015年8月15日, at 上午3:00, Dmitri Zagidulin  wrote:
&gt; 
&gt; Pending 0% just means no pending transfers, the cluster state is stable. 
&gt; 
&gt; If you've successfully tested the process on a test cluster, there's no 
&gt; reason why it'd be different in production. 
&gt; 
&gt;&gt; On Friday, August 14, 2015, changmao wang  wrote:
&gt;&gt; During last three days, I setup a developing riak cluster with five nodes, 
&gt;&gt; and used "s3cmd" to upload 18GB testing data(maybe 20 thousands of files).
&gt;&gt; After that, I tried to let one node leaving the cluster, and then shutdown 
&gt;&gt; and mark down it. Replacing the IP address and joining the cluster again.
&gt;&gt; The above whole processes were successful. However, I'm not sure whether no 
&gt;&gt; not it can be done on production environment. 
&gt;&gt; 
&gt;&gt; I followed below the docs to do above steps:
&gt;&gt; 
&gt;&gt; http://docs.basho.com/riak/latest/ops/running/nodes/renaming/
&gt;&gt; 
&gt;&gt; After I run "riak-admin cluster leave riak@'x.x.x.x'" ,"riak-admin cluster 
&gt;&gt; plan", "riak-admin cluster commit", then checked the member-status, the main 
&gt;&gt; difference of leaving cluster on production and developing environment are 
&gt;&gt; as below:
&gt;&gt; 
&gt;&gt; root@cluster-s3-dev-hd1:~# riak-admin member-status
&gt;&gt; ================================= Membership 
&gt;&gt; ==================================
&gt;&gt; Status Ring Pending Node
&gt;&gt; -------------------------------------------------------------------------------
&gt;&gt; leaving 18.8% 0.0% 'riak@10.21.236.185'
&gt;&gt; valid 21.9% 25.0% 'riak@10.21.236.181'
&gt;&gt; valid 21.9% 25.0% 'riak@10.21.236.182'
&gt;&gt; valid 18.8% 25.0% 'riak@10.21.236.183'
&gt;&gt; valid 18.8% 25.0% 'riak@10.21.236.184'
&gt;&gt; -------------------------------------------------------------------------------
&gt;&gt; 
&gt;&gt; several minutes elapsed, the then checking the status as below:
&gt;&gt; 
&gt;&gt; 
&gt;&gt; root@cluster-s3-dev-hd1:~# riak-admin member-status
&gt;&gt; ================================= Membership 
&gt;&gt; ==================================
&gt;&gt; Status Ring Pending Node
&gt;&gt; -------------------------------------------------------------------------------
&gt;&gt; leaving 12.5% 0.0% 'riak@10.21.236.185'
&gt;&gt; valid 21.9% 25.0% 'riak@10.21.236.181'
&gt;&gt; valid 28.1% 25.0% 'riak@10.21.236.182'
&gt;&gt; valid 18.8% 25.0% 'riak@10.21.236.183'
&gt;&gt; valid 18.8% 25.0% 'riak@10.21.236.184'
&gt;&gt; -------------------------------------------------------------------------------
&gt;&gt; Valid:4 / Leaving:1 / Exiting:0 / Joining:0 / Down:0
&gt;&gt; 
&gt;&gt; After that, I shutdown riak with "riak stop", and mark down it on active 
&gt;&gt; nodes.
&gt;&gt; My question is what's the meaning ot "Pending 0.0%"?
&gt;&gt; 
&gt;&gt; On production cluster, the status are as below:
&gt;&gt; root@cluster1-hd12:/root/scripts# riak-admin transfers
&gt;&gt; 'riak@10.21.136.94' waiting to handoff 5 partitions
&gt;&gt; 'riak@10.21.136.93' waiting to handoff 5 partitions
&gt;&gt; 'riak@10.21.136.92' waiting to handoff 5 partitions
&gt;&gt; 'riak@10.21.136.91' waiting to handoff 5 partitions
&gt;&gt; 'riak@10.21.136.86' waiting to handoff 5 partitions
&gt;&gt; 'riak@10.21.136.81' waiting to handoff 2 partitions
&gt;&gt; 'riak@10.21.136.76' waiting to handoff 3 partitions
&gt;&gt; 'riak@10.21.136.71' waiting to handoff 5 partitions
&gt;&gt; 'riak@10.21.136.66' waiting to handoff 5 partitions
&gt;&gt; 
&gt;&gt; And there're active transfers. On developing environment, there're no 
&gt;&gt; active transfers after my running of "riak-admin cluster commit".
&gt;&gt; Can I follow the same steps as developing environment to run it on 
&gt;&gt; production cluster?
&gt;&gt; 
&gt;&gt; 
&gt;&gt; 
&gt;&gt;&gt; On Wed, Aug 12, 2015 at 10:39 PM, Dmitri Zagidulin  
&gt;&gt;&gt; wrote:
&gt;&gt;&gt; Responses inline.
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; On Tue, Aug 11, 2015 at 12:53 PM, changmao wang  
&gt;&gt;&gt;&gt; wrote:
&gt;&gt;&gt;&gt; 1. About backuping new nodes of four and then using 'riak-admin 
&gt;&gt;&gt;&gt; force-replace'. what's the status of new added nodes?
&gt;&gt;&gt;&gt; as you know, we want to replace one of leaving nodes.
&gt;&gt;&gt; 
&gt;&gt;&gt; I don't understand the question. Doing 'riak-admin force-replace' on one of 
&gt;&gt;&gt; the nodes that's leaving should overwrite the leave request and tell it to 
&gt;&gt;&gt; change its node id / ip address. (If that doesn't work, stop the leaving 
&gt;&gt;&gt; node, and do a 'riak-admin reip' command instead).
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 2. what's the risk of 'riak-admin force-remove' 'riak@10.21.136.91' 
&gt;&gt;&gt;&gt; without backup?
&gt;&gt;&gt;&gt; As you know, now the node(riak@10.21.136.91) is a member of the cluster, 
&gt;&gt;&gt;&gt; and keeping almost 2.5TB data, maybe 10 percent of the whole cluster.
&gt;&gt;&gt; 
&gt;&gt;&gt; The only reason I asked about backup is because it sounded like you cleared 
&gt;&gt;&gt; the disk on it. If it currently has the data, then it'll be fine. 
&gt;&gt;&gt; Force-remove just changes the IP address, and doesn't delete the data or 
&gt;&gt;&gt; anything.
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; On Tue, Aug 11, 2015 at 7:32 PM, Dmitri Zagidulin  
&gt;&gt;&gt;&gt; wrote:
&gt;&gt;&gt;&gt; 1. How to force leave "leaving"'s nodes without data loss?
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; This depends on - did you back up the data directory of the 4 new nodes, 
&gt;&gt;&gt;&gt; before you reformatted them? 
&gt;&gt;&gt;&gt; If you backed them up (and then restored the data directory once you 
&gt;&gt;&gt;&gt; reformatted them), you can try:
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; riak-admin force-replace 'riak@10.21.136.91' 'riak@&gt;&gt;&gt; address is for that node&gt;'
&gt;&gt;&gt;&gt; (same for the other 3)
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; If you did not back up those nodes, the only thing you can do is force 
&gt;&gt;&gt;&gt; them to leave, and then join the new ones. So, for each of the 4:
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; riak-admin force-remove 'riak@10.21.136.91' 'riak@10.21.136.66'
&gt;&gt;&gt;&gt; (same for the other 3)
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; In either case, after force-replacing or force-removing, you have to join 
&gt;&gt;&gt;&gt; the new nodes to the cluster, before you commit.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; riak-admin join 'riak@new node' 'riak@10.21.136.66'
&gt;&gt;&gt;&gt; (same for the other 3)
&gt;&gt;&gt;&gt; and finally:
&gt;&gt;&gt;&gt; riak-cluster plan
&gt;&gt;&gt;&gt; riak-cluster commit
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; As for the error, the reason you're seeing it, is because the other nodes 
&gt;&gt;&gt;&gt; can't contact the 4 that are supposed to be leaving. (Since you wiped 
&gt;&gt;&gt;&gt; them).
&gt;&gt;&gt;&gt; The amount of time that passed doesn't matter, the cluster will be waiting 
&gt;&gt;&gt;&gt; for those nodes to leave indefinitely, unless you force-remove or 
&gt;&gt;&gt;&gt; force-replace.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; On Tue, Aug 11, 2015 at 1:32 AM, changmao wang  
&gt;&gt;&gt;&gt;&gt; wrote:
&gt;&gt;&gt;&gt;&gt; HI Dmitri,
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; For your question,
&gt;&gt;&gt;&gt;&gt; 3) Re-formatted those four nodes and re-installed Riak. Here is where it 
&gt;&gt;&gt;&gt;&gt; gets tricky though. Several questions for you:
&gt;&gt;&gt;&gt;&gt; - Did you attempt to re-join those 4 reinstalled nodes into the cluster? 
&gt;&gt;&gt;&gt;&gt; What was the output of the cluster join and cluster plan commands?
&gt;&gt;&gt;&gt;&gt; - Did the IP address change, after they were reformatted? If so, you 
&gt;&gt;&gt;&gt;&gt; probably need to use something like 'reip' at this point: 
&gt;&gt;&gt;&gt;&gt; http://docs.basho.com/riak/latest/ops/running/tools/riak-admin/#reip
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; I did NOT try to re-join those 4 re-join those 4 reinstalled nodes into 
&gt;&gt;&gt;&gt;&gt; the cluster. As you know, member-status shows 'they're leaving" as below:
&gt;&gt;&gt;&gt;&gt; riak-admin member-status
&gt;&gt;&gt;&gt;&gt; ================================= Membership 
&gt;&gt;&gt;&gt;&gt; ==================================
&gt;&gt;&gt;&gt;&gt; Status Ring Pending Node
&gt;&gt;&gt;&gt;&gt; -------------------------------------------------------------------------------
&gt;&gt;&gt;&gt;&gt; leaving 10.9% 10.9% 'riak@10.21.136.91'
&gt;&gt;&gt;&gt;&gt; leaving 9.4% 10.9% 'riak@10.21.136.92'
&gt;&gt;&gt;&gt;&gt; leaving 7.8% 10.9% 'riak@10.21.136.93'
&gt;&gt;&gt;&gt;&gt; leaving 7.8% 10.9% 'riak@10.21.136.94'
&gt;&gt;&gt;&gt;&gt; valid 10.9% 10.9% 'riak@10.21.136.66'
&gt;&gt;&gt;&gt;&gt; valid 10.9% 10.9% 'riak@10.21.136.71'
&gt;&gt;&gt;&gt;&gt; valid 14.1% 10.9% 'riak@10.21.136.76'
&gt;&gt;&gt;&gt;&gt; valid 17.2% 12.5% 'riak@10.21.136.81'
&gt;&gt;&gt;&gt;&gt; valid 10.9% 10.9% 'riak@10.21.136.86'
&gt;&gt;&gt;&gt;&gt; -------------------------------------------------------------------------------
&gt;&gt;&gt;&gt;&gt; Valid:5 / Leaving:4 / Exiting:0 / Joining:0 / Down:0
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; two weeks elapsed, 'riak-admin member-status' shows same result. I don't 
&gt;&gt;&gt;&gt;&gt; know which step ring hand off?
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; I did not changed the IP address of four newly adding nodes. 
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; My questions:
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; 1. How to force leave "leaving"'s nodes without data loss?
&gt;&gt;&gt;&gt;&gt; 2. I have found some errors related to handoff of partition in 
&gt;&gt;&gt;&gt;&gt; /etc/riak/log/errors.
&gt;&gt;&gt;&gt;&gt; Details are as below:
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; 2015-07-30 16:04:33.643 [error] 
&gt;&gt;&gt;&gt;&gt; &lt;0.12872.15&gt;@riak\\_core\\_handoff\\_sender:start\\_fold:262 ownership\\_transfer 
&gt;&gt;&gt;&gt;&gt; transfer of riak\\_kv\\_vnode from 'riak@10.21.136.76' 
&gt;&gt;&gt;&gt;&gt; 45671926166590716193865151022383844364247891968 to 'riak@10.21.136.93' 
&gt;&gt;&gt;&gt;&gt; 45671926166590716193865151022383844364247891968 failed because of enotconn
&gt;&gt;&gt;&gt;&gt; 2015-07-30 16:04:33.643 [error] 
&gt;&gt;&gt;&gt;&gt; &lt;0.197.0&gt;@riak\\_core\\_handoff\\_manager:handle\\_info:289 An outbound handoff 
&gt;&gt;&gt;&gt;&gt; of partition riak\\_kv\\_vnode 
&gt;&gt;&gt;&gt;&gt; 45671926166590716193865151022383844364247891968 was terminated for 
&gt;&gt;&gt;&gt;&gt; reason: {shutdown,{error,enotconn}}
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; I have searched it with google and found related articles. However, 
&gt;&gt;&gt;&gt;&gt; there's no solution.
&gt;&gt;&gt;&gt;&gt; http://lists.basho.com/pipermail/riak-users\\_lists.basho.com/2014-October/016052.html
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; On Mon, Aug 10, 2015 at 10:09 PM, Dmitri Zagidulin 
&gt;&gt;&gt;&gt;&gt;&gt;  wrote:
&gt;&gt;&gt;&gt;&gt;&gt; Hi Changmao,
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; The state of the cluster can be determined from running 'riak-admin 
&gt;&gt;&gt;&gt;&gt;&gt; member-status' and 'riak-admin ring-status'.
&gt;&gt;&gt;&gt;&gt;&gt; If I understand the sequence of events, you:
&gt;&gt;&gt;&gt;&gt;&gt; 1) Joined four new nodes to the cluster. (Which crashed due to not 
&gt;&gt;&gt;&gt;&gt;&gt; enough disk space)
&gt;&gt;&gt;&gt;&gt;&gt; 2) Removed them from the cluster via 'riak-admin cluster leave'. This 
&gt;&gt;&gt;&gt;&gt;&gt; is a "planned remove" command, and expects for the nodes to gradually 
&gt;&gt;&gt;&gt;&gt;&gt; hand off their partitions (to transfer ownership) before actually 
&gt;&gt;&gt;&gt;&gt;&gt; leaving. So this is probably the main problem - the ring is stuck 
&gt;&gt;&gt;&gt;&gt;&gt; waiting for those nodes to properly hand off.
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; 3) Re-formatted those four nodes and re-installed Riak. Here is where it 
&gt;&gt;&gt;&gt;&gt;&gt; gets tricky though. Several questions for you:
&gt;&gt;&gt;&gt;&gt;&gt; - Did you attempt to re-join those 4 reinstalled nodes into the cluster? 
&gt;&gt;&gt;&gt;&gt;&gt; What was the output of the cluster join and cluster plan commands?
&gt;&gt;&gt;&gt;&gt;&gt; - Did the IP address change, after they were reformatted? If so, you 
&gt;&gt;&gt;&gt;&gt;&gt; probably need to use something like 'reip' at this point: 
&gt;&gt;&gt;&gt;&gt;&gt; http://docs.basho.com/riak/latest/ops/running/tools/riak-admin/#reip
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; The 'failed because of enotconn' error message is happening because the 
&gt;&gt;&gt;&gt;&gt;&gt; cluster is waiting to hand off partitions to .94, but cannot connect to 
&gt;&gt;&gt;&gt;&gt;&gt; it.
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; Anyways, here's what I recommend. If you can lose the data, it's 
&gt;&gt;&gt;&gt;&gt;&gt; probably easier to format and reinstall the whole cluster.
&gt;&gt;&gt;&gt;&gt;&gt; If not, you can 'force-remove' those four nodes, one by one (see 
&gt;&gt;&gt;&gt;&gt;&gt; http://docs.basho.com/riak/latest/ops/running/cluster-admin/#force-remove
&gt;&gt;&gt;&gt;&gt;&gt; )
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; On Thu, Aug 6, 2015 at 11:55 PM, changmao wang 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;  wrote:
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Dmitri,
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Thanks for your quick reply.
&gt;&gt;&gt;&gt;&gt;&gt;&gt; my question are as below:
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 1. what's the current status of the whole cluster? Is't doing data 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; balance?
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 2. there's so many errors during one of the node error log. how to 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; handle it?
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 2015-08-05 01:38:59.717 [error] 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; &lt;0.23000.298&gt;@riak\\_core\\_handoff\\_sender:start\\_fold:262 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; ownership\\_transfer transfer of riak\\_kv\\_vnode from 'riak@10.21.136.81' 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 525227150915793236229449236757414210188850757632 to 'riak@10.21.136.94' 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 525227150915793236229449236757414210188850757632 failed because of 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; enotconn
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 2015-08-05 01:38:59.718 [error] 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; &lt;0.195.0&gt;@riak\\_core\\_handoff\\_manager:handle\\_info:289 An outbound handoff 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; of partition riak\\_kv\\_vnode 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 525227150915793236229449236757414210188850757632 was terminated for 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; reason: {shutdown,{error,enotconn}}
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; During the last 5 days, there's no changes of the "riak-admin member 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; status" output.
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 3. how to accelerate the data balance? 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; On Fri, Aug 7, 2015 at 6:41 AM, Dmitri Zagidulin 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  wrote:
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Ok, I think I understand so far. So what's the question?
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; On Thursday, August 6, 2015, Changmao.Wang 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  wrote:
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Hi Riak users,
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Before adding new nodes, the cluster only have five nodes. The member 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; list are as below:
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 10.21.136.66,10.21.136.71,10.21.136.76,10.21.136.81,10.21.136.86.
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; We did not setup http proxy for the cluster, only one node of the 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; cluster provide the http service. so the CPU load is always high on 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; this node.
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; After that, I added four nodes (10.21.136.[91-94]) to those cluster. 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; During the ring/data balance progress, each node failed(riak stopped) 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; because of disk 100% full.
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; I used multi-disk path to "data\\_root" parameter in 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; '/etc/riak/app.config'. Each disk is only 580MB size. 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; As you know, bitcask storage engine did not support multi-disk path. 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; After one of the disks is 100% full, it can not switch next idle 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; disk. So the "riak" service is down.
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; After that, I removed the new add four nodes at active nodes with 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; "riak-admin cluster leave riak@'10.21.136.91'".
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; and then stop "riak" service on other active new nodes, reformat the 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; above new nodes with LVM disk management (bind 6 disk with virtual 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; disk group).
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Replace the "data-root" parameter with one folder, and then start 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; "riak" service again. After that, the cluster began the data balance 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; again. 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; That's the whole story.
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Amao
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; From: "Dmitri Zagidulin" 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; To: "Changmao.Wang" 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Sent: Thursday, August 6, 2015 10:46:59 PM
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Subject: Re: why leaving riak cluster so slowly and how to accelerate 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; the speed
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Hi Amao,
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Can you explain a bit more which steps you've taken, and what the 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; problem is?
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Which nodes have been added, and which nodes are leaving the cluster?
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; On Tue, Jul 28, 2015 at 11:03 PM, Changmao.Wang 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  wrote:
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Hi Raik user group,
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; I'm using riak and riak-cs 1.4.2. Last weekend, I added four nodes 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; to cluster with 5 nodes. However, it's failed with one of disks 100% 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; full.
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; As you know bitcask storage engine can not support multifolders.
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; After that, I restarted the "riak" and leave the cluster with the 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; command "riak-admin cluster leave" and "riak-admin cluster plan", 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; and the commit.
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; However, riak is always doing KV balance after my submit leaving 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; command. I guess that it's doing join cluster progress.
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Could you show us how to accelerate the leaving progress? I have 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; tuned the "transfer-limit" parameters on 9 nodes.
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; below is some commands output:
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; riak-admin member-status
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; ================================= Membership 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; ==================================
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Status Ring Pending Node
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; -------------------------------------------------------------------------------
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; leaving 6.3% 10.9% 'riak@10.21.136.91'
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; leaving 9.4% 10.9% 'riak@10.21.136.92'
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; leaving 6.3% 10.9% 'riak@10.21.136.93'
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; leaving 6.3% 10.9% 'riak@10.21.136.94'
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; valid 10.9% 10.9% 'riak@10.21.136.66'
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; valid 12.5% 10.9% 'riak@10.21.136.71'
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; valid 18.8% 10.9% 'riak@10.21.136.76'
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; valid 18.8% 12.5% 'riak@10.21.136.81'
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; valid 10.9% 10.9% 'riak@10.21.136.86'
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; riak-admin transfer\\_limit
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; =============================== Transfer Limit 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; ================================
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Limit Node
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; -------------------------------------------------------------------------------
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 200 'riak@10.21.136.66'
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 200 'riak@10.21.136.71'
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 100 'riak@10.21.136.76'
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 100 'riak@10.21.136.81'
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 200 'riak@10.21.136.86'
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 500 'riak@10.21.136.91'
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 500 'riak@10.21.136.92'
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 500 'riak@10.21.136.93'
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 500 'riak@10.21.136.94'
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Any more details for your diagnosing the problem?
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Amao
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; -- 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Amao Wang
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Best & Regards
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt;&gt;&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt;&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt;&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; -- 
&gt;&gt;&gt;&gt;&gt; Amao Wang
&gt;&gt;&gt;&gt;&gt; Best & Regards
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; -- 
&gt;&gt;&gt; Amao Wang
&gt;&gt;&gt; Best & Regards
&gt;&gt; 
&gt;&gt; 
&gt;&gt; 
&gt;&gt; -- 
&gt;&gt; Amao Wang
&gt;&gt; Best & Regards
\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

