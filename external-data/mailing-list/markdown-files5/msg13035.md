---
title: "Re: Riak Search Map Reduce error"
description: ""
project: community
lastmod: 2013-11-20T10:04:08-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg13035"
mailinglist_parent_id: "msg13031"
author_name: "Ryan Zezeski"
project_section: "mailinglistitem"
sent_date: 2013-11-20T10:04:08-08:00
---


Roger,

Riak Search has a hardcoded max result set size of 100K items. It enforces
this to prevent blowing out memory and causing other issues. Riak Search
definitely has some issues when it comes to handling a use case like yours.

That said, our new Search solution in 2.0 (code named Yokozuna) should do a
lot better. Not only does it not have the hardcoded 100K limit but it
should also execute the queries faster. In some cases by 1-3 orders of
magnitude (10-1000x). At that point you're more likely to be slowed down by
the map-reduce. You might even be able to remove that stage by using stored
fields, but I'd need to know more about your use case.

I agree that current Riak (pre 2.0) is not a general search solution. Riak
Search can work very well but it requires some hand holding and careful
vigilance of how you index and query the data. I feel that the new Search
(Yokozuna) fixes this in many ways. In general, it has more robust search
support and lower, more consistent latency. Yokozuna would also have no
issues dealing with 1 million objects. My micro benchmark that I run is
1-10 million objects. Granted, they are small plain-text objects, but I'm
fairly confident it would work with your 1 million objects.

I realize that Riak 2.0, and thus the new search functionality, is not out
yet. We have an early release, Riak 2.0.0pre5 [1], that you can try. I also
do monthly releases of the new search functionality [2]. So if you want to
kick the tires I can point you in the right direction.

-Z

[1]: http://docs.basho.com/riak/2.0.0pre5/downloads/

[2]: https://github.com/basho/yokozuna/blob/develop/docs/INSTALL.md


On Wed, Nov 20, 2013 at 11:45 AM, Roger Diller &lt;
ro...@flexrentalsolutions.com&gt; wrote:

&gt; I could dig up all our nitty gritty Riak details but I don't think that
&gt; will help really.
&gt;
&gt; The point I think is this: Using search map reduce is not a viable way to
&gt; do real time search queries. Especially ones that may have 2000+ plus
&gt; results each. Couple that with search requests coming in every few seconds
&gt; from 300+ customer app instances and you literally bring Riak to it's
&gt; knees.
&gt;
&gt; Not that Riak is the problem really, it's just we are using it in a way it
&gt; was not designed for. In essence, we are using Riak as a search engine for
&gt; our application data. Correct me if I'm wrong but Riak is more for storing
&gt; large amounts of KV data, but not really for finding that data in a search
&gt; sense.
&gt;
&gt; Am I missing something here? Is there a viable way for doing real time
&gt; search queries on a bucket with 1 million keys?
&gt;
&gt;
&gt; On Mon, Nov 18, 2013 at 5:29 PM, Alexander Sicular wrote:
&gt;
&gt;&gt; More info please...
&gt;&gt;
&gt;&gt; Version
&gt;&gt; Current config
&gt;&gt; Hardware
&gt;&gt; Data size
&gt;&gt; Search Schema
&gt;&gt; Etc.
&gt;&gt;
&gt;&gt; But I would probably say that your search is returning too many keys to
&gt;&gt; your mr. More inline.
&gt;&gt;
&gt;&gt; @siculars
&gt;&gt; http://siculars.posthaven.com
&gt;&gt;
&gt;&gt; Sent from my iRotaryPhone
&gt;&gt;
&gt;&gt; On Nov 18, 2013, at 13:59, Roger Diller 
&gt;&gt; wrote:
&gt;&gt;
&gt;&gt; Using the Riak Java client, I am executing a search map reduce like this:
&gt;&gt;
&gt;&gt; MapReduceResult result = riakClient.mapReduce(SEARCH\\_BUCKET,
&gt;&gt; search).execute();
&gt;&gt;
&gt;&gt;
&gt;&gt; ^is this part a typo. Cause otherwise it looks like you do a s&gt;mr, set
&gt;&gt; the search and then another s&gt;mr.
&gt;&gt;
&gt;&gt;
&gt;&gt; String search = "systemId:" + systemName + " AND indexId:" + indexId;
&gt;&gt;
&gt;&gt; MapReduceResult result = riakClient.mapReduce(SEARCH\\_BUCKET,
&gt;&gt; search).execute();
&gt;&gt;
&gt;&gt; This worked fine when the bucket contained a few thousand keys. Now that
&gt;&gt; we have far more data stored in the bucket (at least 250K keys), it's
&gt;&gt; throwing this generic error:
&gt;&gt;
&gt;&gt; com.basho.riak.client.RiakException: java.io.IOException:
&gt;&gt; {"error":"map\\_reduce\\_error"}
&gt;&gt;
&gt;&gt; We've also noticed that storing new key/values in the bucket has slowed
&gt;&gt; WAY down.
&gt;&gt;
&gt;&gt; Any idea what's going on?
&gt;&gt;
&gt;&gt;
&gt;&gt; Your data set is incorrectly sized to your production config.
&gt;&gt;
&gt;&gt; Are there limitations to Search Map Reduce?
&gt;&gt;
&gt;&gt;
&gt;&gt; Certainly
&gt;&gt;
&gt;&gt; Are there configuration options that need changed?
&gt;&gt;
&gt;&gt;
&gt;&gt; Possibly
&gt;&gt;
&gt;&gt; Any help would be greatly appreciated.
&gt;&gt;
&gt;&gt;
&gt;&gt; --
&gt;&gt; Roger Diller
&gt;&gt; Flex Rental Solutions, LLC
&gt;&gt; Email: ro...@flexrentalsolutions.com
&gt;&gt; Skype: rogerdiller
&gt;&gt; Time Zone: Eastern Time
&gt;&gt;
&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt; riak-users mailing list
&gt;&gt; riak-users@lists.basho.com
&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt;
&gt;&gt;
&gt;
&gt;
&gt; --
&gt; Roger Diller
&gt; Flex Rental Solutions, LLC
&gt; Email: ro...@flexrentalsolutions.com
&gt; Skype: rogerdiller
&gt; Time Zone: Eastern Time
&gt;
&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;
&gt;
\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

