---
title: "Re: leveldb Hot Threads in 1.4.9?"
description: ""
project: community
lastmod: 2014-07-05T10:36:06-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg14460"
mailinglist_parent_id: "msg14344"
author_name: "Matthew Von-Maszewski"
project_section: "mailinglistitem"
sent_date: 2014-07-05T10:36:06-07:00
---


Tom,

Basho prides itself on quickly responding to all user queries. I have failed 
that tradition in this case. Please accept my apologies.


The LOG data suggests leveldb is not stalling, especially not for 4 hours. 
Therefore the problem is related to disk utilization.

You appear to have large values. I see .sst files where the average value is 
100K to 1Mbyte in size. Is this intentional, or might you have a sibling 
problem?


My assessment is that your lower levels are full and therefore cascading 
regularly. "cascading" is like the typical champagne glass pyramid you see at 
weddings. Once all the glasses are full, new champagne at the top causes each 
subsequent layer to overflow into the one below that. You have the same 
problem, but with data. 

Your large values have filled each of the lower levels and regularly cause 
cascading data between multiple levels. The cascading is causing each 100K 
value write to become the equivalent of a 300K or 500K value as levels 
overflow. This cascading is chewing up your hard disk performance (by reducing 
the amount of time the hard drive has available for read requests).

The leveldb code for Riak 2.0 has increased the size of all the levels. The 
table of sizes is found at the top of leveldb's db/version\\_set.cc. You could 
patch your current code if desired with this table from 2.0:

{ 
 
 {10485760, 262144000, 57671680, 209715200, 0, 
420000000, true}, 
 {10485760, 82914560, 57671680, 419430400, 0, 
209715200, true}, 
 {10485760, 314572800, 57671680, 3082813440, 200000000, 
314572800, false}, 
 {10485760, 419430400, 57671680, 6442450944ULL, 4294967296ULL, 
419430400, false}, 
 {10485760, 524288000, 57671680, 128849018880ULL, 85899345920ULL, 
524288000, false}, 
 {10485760, 629145600, 57671680, 2576980377600ULL, 1717986918400ULL, 
629145600, false}, 
 {10485760, 734003200, 57671680, 51539607552000ULL, 34359738368000ULL, 
734003200, false} 
}; 
 

You cannot take the entire 2.0 leveldb into your 1.4 code base due to various 
option changes.


Let me know if this helps. I have previously hypothesized that "grooming" 
compactions should be limited to one thread total. However my test datasets 
never demonstrated a benefit. Your dataset might be the case that proves the 
benefit. I will go find the grooming patch to hot\\_threads for you if the above 
table proves insufficient.

Matthew




On Jul 2, 2014, at 9:20 PM, Tom Lanyon  wrote:

&gt; Hi Matthew, 
&gt; 
&gt; Just thought I'd see whether you were back from your travels and had had a 
&gt; chance to take a look at the log file provided?
&gt; 
&gt; There's no rush if you haven't had a chance!
&gt; 
&gt; Regards,
&gt; Tom
&gt; 
&gt; 
&gt; On Tuesday, 24 June 2014 at 10:45, Tom Lanyon wrote:
&gt; 
&gt;&gt; No problem, Matthew. 
&gt;&gt; 
&gt;&gt; Appreciate you taking a look when you have time.
&gt;&gt; 
&gt;&gt; Regards,
&gt;&gt; Tom
&gt;&gt; 
&gt;&gt; 
&gt;&gt; On Tuesday, 24 June 2014 at 9:45, Matthew Von-Maszewski wrote:
&gt;&gt; 
&gt;&gt;&gt; Tom,
&gt;&gt;&gt; 
&gt;&gt;&gt; I have been distracted today and on a plane tomorrow. I apologize for the 
&gt;&gt;&gt; delayed response. It may be late tomorrow before I can share further 
&gt;&gt;&gt; thoughts. 
&gt;&gt;&gt; 
&gt;&gt;&gt; Again my apologies.
&gt;&gt;&gt; 
&gt;&gt;&gt; Matthew Von-Maszewski
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; On Jun 23, 2014, at 8:58, Tom Lanyon &gt;&gt; (mailto:tom+r...@oneshoeco.com)&gt; wrote:
&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Thanks; the combined\\_log for our Riak node 3 is here:
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; https://www.dropbox.com/s/krhhwnplpeyhl0c/riak3-combined\\_log-20140623.log.gz
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Let me know if you can't retrieve/view it.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; With timestamps relative to this log file, at 2014/06/23-05:35 our 
&gt;&gt;&gt;&gt; monitoring detected node3's Riak as "down"; it wasn't serving any client 
&gt;&gt;&gt;&gt; protobuf requests, "riak ping" didn't respond and all of the other nodes 
&gt;&gt;&gt;&gt; marked node 3 as unreachable. We watched the process and it was busy doing 
&gt;&gt;&gt;&gt; leveldb compactions so we left it alone and it eventually recovered at 
&gt;&gt;&gt;&gt; 2014/06/23-09:32 (so ~4 hours unresponsive).
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Yes - this cluster started at 1.2.1 and then I believe it went to 1.3.1, 
&gt;&gt;&gt;&gt; 1.4.2 and now 1.4.8. However, we went from 1.3.1--&gt;1.4.2 in September 2013 
&gt;&gt;&gt;&gt; and 1.4.2--&gt;1.4.8 in May, so we've been running 1.4.x for many months - 
&gt;&gt;&gt;&gt; does this fit with the 'one time cost of upgrading' you mentioned?
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Regards,
&gt;&gt;&gt;&gt; Tom
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; On Monday, 23 June 2014 at 19:29, Matthew Von-Maszewski wrote:
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; Yes, off list is fine for the data files. I may or may not respond via 
&gt;&gt;&gt;&gt;&gt; the list depending upon what I find.
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; I did recall a case where leveldb seems unresponsive for hours. This case 
&gt;&gt;&gt;&gt;&gt; was a one time cost of upgrading some 1.2 or 1.3 systems to 1.4. Would 
&gt;&gt;&gt;&gt;&gt; that happen to describe your scenario?
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; Matthew Von-Maszewski
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; On Jun 23, 2014, at 0:28, Tom Lanyon &gt;&gt;&gt;&gt; (mailto:tom+r...@oneshoeco.com)&gt; wrote:
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; Hi Matthew, 
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; Thanks for the response and apologies for my off-list reply.
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; I can send a combined\\_log example directly to you if that helps? It's 
&gt;&gt;&gt;&gt;&gt;&gt; 13MB gzip'ed.
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; Regards,
&gt;&gt;&gt;&gt;&gt;&gt; Tom
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; On Monday, 23 June 2014 at 12:30, Matthew Von-Maszewski wrote:
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Hot threads is included with 1.4.9. The leveldb source file 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; leveldb/util//hot\\_threads.cc (http://hot\\_threads.cc 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; (http://\\_threads.cc) (http://\\_threads.cc) (http://\\_threads.cc)) is the 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; key file.
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; The code helps throughput, but is not magical. "unresponsive for hours" 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; is not a known problem in the 1.4.x code base. Would you mind posting 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; an aggregate LOG file from a period when this happens?
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; sort /var/lib/riak/\\*/LOG &gt;combined\\_log
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Substitute your actual data path for /var/lib/riak.
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Matthew Von-Maszewski
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; On Jun 22, 2014, at 22:07, Tom Lanyon &gt;&gt;&gt;&gt;&gt;&gt; (mailto:tom+r...@oneshoeco.com)&gt; wrote:
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Could someone please confirm whether 1.4.9 includes "Hot Threads" in 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; leveldb? 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; The release notes have a link to it, but I couldn't find my way 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; through the rebar & git maze to be absolutely sure it is in 1.4.9 but 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; not 1.4.8.
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; We're seeing nodes unresponsive for hours during large compactions and 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; wondered if this leveldb improvement would help.
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Thanks,
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Tom
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; riak-users@lists.basho.com (mailto:riak-users@lists.basho.com)
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt; 
&gt; 
&gt; 
&gt; 

\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

