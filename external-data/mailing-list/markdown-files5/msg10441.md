---
title: "Re: Bigger data than disk space?"
description: ""
project: community
lastmod: 2013-03-14T14:17:52-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg10441"
mailinglist_parent_id: "msg10439"
author_name: "Alexander Sicular"
project_section: "mailinglistitem"
sent_date: 2013-03-14T14:17:52-07:00
---


You have to think at the cluster level. If you have 10 GB of data and if your 
replication factor is three then your total data across the cluster will be 

10 GB x 3 replicas = 30 GB across the cluster

Now, if you have four physical machines in your cluster each will be 
responsible for 1/4 that data. 

0.25 x 30 GB = 7.5 GB

That is because the vnodes are evenly divided amongst physical machines in the 
cluster. 

-Alexander Sicular

@siculars

On Mar 14, 2013, at 5:08 PM, "Kevin Burton"  wrote:

&gt; Then that is not quite as bad but still if I have 10 GB of data and to
&gt; support replication that requires 30 GB of disk space, what if I only have
&gt; 20 GB of disk space per physical node?
&gt; 
&gt; -----Original Message-----
&gt; From: Mark Phillips [mailto:m...@basho.com] 
&gt; Sent: Thursday, March 14, 2013 4:05 PM
&gt; To: Kevin Burton
&gt; Cc: Alexander Sicular; riak-users@lists.basho.com
&gt; Subject: Re: Bigger data than disk space?
&gt; 
&gt; Kevin,
&gt; 
&gt; On Thu, Mar 14, 2013 at 1:56 PM, Kevin Burton 
&gt; wrote:
&gt;&gt; So that is what I am missing. If each vnode keeps an entire copy of my 
&gt;&gt; data and I have 4 physical node then there are 16 vnodes per physical 
&gt;&gt; node. That would mean I have the data replicated 16 times per physical 
&gt;&gt; node. 10 GB turns into 160GB etc. Right? So won’t I run out of disk space?
&gt;&gt; 
&gt; 
&gt; Your raw data set is replicated 3 times by default. Three different vnodes
&gt; of your total (by default 64) will be responsible for each replica. So, 10GB
&gt; raw = 30GB replicated.
&gt; 
&gt; Mark
&gt; 
&gt;&gt; 
&gt;&gt; 
&gt;&gt; From: Alexander Sicular [mailto:sicul...@gmail.com]
&gt;&gt; Sent: Thursday, March 14, 2013 3:51 PM
&gt;&gt; 
&gt;&gt; 
&gt;&gt; To: Kevin Burton
&gt;&gt; Cc: riak-users@lists.basho.com
&gt;&gt; Subject: Re: Bigger data than disk space?
&gt;&gt; 
&gt;&gt; 
&gt;&gt; 
&gt;&gt; Each vnode keeps \\_an entire copy\\_ of your data. There is no striping, 
&gt;&gt; which I think you are conflating with RAID. Default replication (also 
&gt;&gt; configured in etc/app.config) is set to three. In which case, three 
&gt;&gt; entire copies of your data are kept on three different vnodes and if 
&gt;&gt; you indeed have five physical nodes in your cluster you are guaranteed 
&gt;&gt; to have each of those three vnodes on different physical machines.
&gt;&gt; 
&gt;&gt; 
&gt;&gt; -Alexander Sicular
&gt;&gt; 
&gt;&gt; 
&gt;&gt; 
&gt;&gt; @siculars
&gt;&gt; 
&gt;&gt; 
&gt;&gt; 
&gt;&gt; On Mar 14, 2013, at 4:42 PM, "Kevin Burton" 
&gt;&gt; wrote:
&gt;&gt; 
&gt;&gt; 
&gt;&gt; 
&gt;&gt; Thank you. Let me get it straight. I have a 4 node cluster (4 physical 
&gt;&gt; machines). If I have not made any changes to the ring size then I have 
&gt;&gt; 16
&gt;&gt; (64/4) vnodes. Each physical node stores the actual data (the value) 
&gt;&gt; of about ¼ of the data size. So when querying the data with a key 
&gt;&gt; given the number of vnodes it can be determined which physical machine the
&gt; data is on.
&gt;&gt; There must be enough redundancy built in so that if one or more of the 
&gt;&gt; physical machines go down the remaining physical machines can 
&gt;&gt; reconstruct the values lost by the lost vnodes. Correct so far? Now 
&gt;&gt; where does replication some in? The documentation indicates that there 
&gt;&gt; are 3 copies of the data (default) made. How is this changed and how 
&gt;&gt; can this replication of the data be taken advantage of?
&gt;&gt; 
&gt;&gt; 
&gt;&gt; 
&gt;&gt; From: Alexander Sicular [mailto:sicul...@gmail.com]
&gt;&gt; Sent: Thursday, March 14, 2013 3:28 PM
&gt;&gt; To: Kevin Burton
&gt;&gt; Cc: riak-users@lists.basho.com
&gt;&gt; Subject: Re: Bigger data than disk space?
&gt;&gt; 
&gt;&gt; 
&gt;&gt; 
&gt;&gt; Hi Kevin,
&gt;&gt; 
&gt;&gt; 
&gt;&gt; 
&gt;&gt; The Riak distribution model is not based on "buckets" but rather the 
&gt;&gt; hash of the bucket/key combination. That hash (and associated data) is 
&gt;&gt; then allocated against a "vnode". A vnode, in turn, is one of n where 
&gt;&gt; n is the ring\\_creation\\_size (default is 64, modify in etc/app.config). 
&gt;&gt; Each physical machine in a Riak cluster claims an equal share of the 
&gt;&gt; ring. For example, a cluster with five machines (the recommended 
&gt;&gt; minimum for a production
&gt;&gt; cluster) and the default ring\\_creation\\_size will have 64/5 vnodes per 
&gt;&gt; physical machine (not sure if they round down or up but all machines 
&gt;&gt; will have about the same number of vnodes). What you would do to make 
&gt;&gt; more data available is either add a machine to the cluster whose 
&gt;&gt; available disk space is equal or greater than the cluster member with 
&gt;&gt; the least amount of total space or increase the space on all machines
&gt; already in the cluster.
&gt;&gt; 
&gt;&gt; 
&gt;&gt; 
&gt;&gt; tl;dr add a machine to your cluster.
&gt;&gt; 
&gt;&gt; 
&gt;&gt; 
&gt;&gt; 
&gt;&gt; -Alexander Sicular
&gt;&gt; 
&gt;&gt; 
&gt;&gt; 
&gt;&gt; @siculars
&gt;&gt; 
&gt;&gt; 
&gt;&gt; 
&gt;&gt; On Mar 14, 2013, at 3:41 PM, Kevin Burton 
&gt; wrote:
&gt;&gt; 
&gt;&gt; 
&gt;&gt; 
&gt;&gt; 
&gt;&gt; I am relatively new to Riak so forgive me if this has been asked 
&gt;&gt; before. I have a very thin understanding of a Riak cluster and 
&gt;&gt; understand somewhat about replication. In planning I foresee a time 
&gt;&gt; when the amount of data exceeds the disk space that is available to a 
&gt;&gt; single node. What facilities are there to essentially “split” a bucket 
&gt;&gt; across several servers? How is this handled?
&gt;&gt; 
&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt; riak-users mailing list
&gt;&gt; riak-users@lists.basho.com
&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt; 
&gt;&gt; 
&gt;&gt; 
&gt;&gt; 
&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt; riak-users mailing list
&gt;&gt; riak-users@lists.basho.com
&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt; 
&gt; 


\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

