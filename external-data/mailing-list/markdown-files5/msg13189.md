---
title: "Re: Upgrade from 1.3.1 to 1.4.2 => high IO"
description: ""
project: community
lastmod: 2013-12-10T08:10:02-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg13189"
mailinglist_parent_id: "msg13186"
author_name: "Matthew Von-Maszewski"
project_section: "mailinglistitem"
sent_date: 2013-12-10T08:10:02-08:00
---


2i is not my expertise, so I had to discuss you concerns with another Basho 
developer. He says:

Between 1.3 and 1.4, the 2i query did change but not the 2i on-disk format. 
You must wait for all nodes to update if you desire to use the new 2i query. 
The 2i data will properly write/update on both 1.3 and 1.4 machines during the 
migration.

Does that answer your question?


And yes, you might see available disk space increase during the upgrade 
compactions if your dataset contains numerous delete "tombstones". The Riak 
2.0 code includes a new feature called "aggressive delete" for leveldb. This 
feature is more proactive in pushing delete tombstones through the levels to 
free up disk space much more quickly (especially if you perform block deletes 
every now and then).

Matthew


On Dec 10, 2013, at 10:44 AM, Simon Effenberg  wrote:

&gt; Hi Matthew,
&gt; 
&gt; see inline..
&gt; 
&gt; On Tue, 10 Dec 2013 10:38:03 -0500
&gt; Matthew Von-Maszewski  wrote:
&gt; 
&gt;&gt; The sad truth is that you are not the first to see this problem. And yes, 
&gt;&gt; it has to do with your 950GB per node dataset. And no, nothing to do but 
&gt;&gt; sit through it at this time.
&gt;&gt; 
&gt;&gt; While I did extensive testing around upgrade times before shipping 1.4, 
&gt;&gt; apparently there are data configurations I did not anticipate. You are 
&gt;&gt; likely seeing a cascade where a shift of one file from level-1 to level-2 is 
&gt;&gt; causing a shift of another file from level-2 to level-3, which causes a 
&gt;&gt; level-3 file to shift to level-4, etc … then the next file shifts from 
&gt;&gt; level-1.
&gt;&gt; 
&gt;&gt; The bright side of this pain is that you will end up with better write 
&gt;&gt; throughput once all the compaction ends.
&gt; 
&gt; I have to deal with that.. but my problem is now, if I'm doing this
&gt; node by node it looks like 2i searches aren't possible while 1.3 and
&gt; 1.4 nodes exists in the cluster. Is there any problem which leads me to
&gt; an 2i repair marathon or could I easily wait for some hours for each
&gt; node until all merges are done before I upgrade the next one? (2i
&gt; searches can fail for some time.. the APP isn't having problems with
&gt; that but are new inserts with 2i indices processed successfully or do
&gt; I have to do the 2i repair?)
&gt; 
&gt; /s
&gt; 
&gt; one other good think: saving disk space is one advantage ;)..
&gt; 
&gt; 
&gt;&gt; 
&gt;&gt; Riak 2.0's leveldb has code to prevent/reduce compaction cascades, but that 
&gt;&gt; is not going to help you today.
&gt;&gt; 
&gt;&gt; Matthew
&gt;&gt; 
&gt;&gt; On Dec 10, 2013, at 10:26 AM, Simon Effenberg  
&gt;&gt; wrote:
&gt;&gt; 
&gt;&gt;&gt; Hi @list,
&gt;&gt;&gt; 
&gt;&gt;&gt; I'm trying to upgrade our Riak cluster from 1.3.1 to 1.4.2 .. after
&gt;&gt;&gt; upgrading the first node (out of 12) this node seems to do many merges.
&gt;&gt;&gt; the sst\\_\\* directories changes in size "rapidly" and the node is having
&gt;&gt;&gt; a disk utilization of 100% all the time.
&gt;&gt;&gt; 
&gt;&gt;&gt; I know that there is something like that:
&gt;&gt;&gt; 
&gt;&gt;&gt; "The first execution of 1.4.0 leveldb using a 1.3.x or 1.2.x dataset
&gt;&gt;&gt; will initiate an automatic conversion that could pause the startup of
&gt;&gt;&gt; each node by 3 to 7 minutes. The leveldb data in "level #1" is being
&gt;&gt;&gt; adjusted such that "level #1" can operate as an overlapped data level
&gt;&gt;&gt; instead of as a sorted data level. The conversion is simply the
&gt;&gt;&gt; reduction of the number of files in "level #1" to being less than eight
&gt;&gt;&gt; via normal compaction of data from "level #1" into "level #2". This is
&gt;&gt;&gt; a one time conversion."
&gt;&gt;&gt; 
&gt;&gt;&gt; but it looks much more invasive than explained here or doesn't have to
&gt;&gt;&gt; do anything with the (probably seen) merges.
&gt;&gt;&gt; 
&gt;&gt;&gt; Is this "normal" behavior or could I do anything about it?
&gt;&gt;&gt; 
&gt;&gt;&gt; At the moment I'm stucked with the upgrade procedure because this high
&gt;&gt;&gt; IO load would probably lead to high response times.
&gt;&gt;&gt; 
&gt;&gt;&gt; Also we have a lot of data (per node ~950 GB).
&gt;&gt;&gt; 
&gt;&gt;&gt; Cheers
&gt;&gt;&gt; Simon
&gt;&gt;&gt; 
&gt;&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt; 
&gt; 
&gt; 
&gt; -- 
&gt; Simon Effenberg | Site Ops Engineer | mobile.international GmbH
&gt; Fon: + 49-(0)30-8109 - 7173
&gt; Fax: + 49-(0)30-8109 - 7131
&gt; 
&gt; Mail: seffenb...@team.mobile.de
&gt; Web: www.mobile.de
&gt; 
&gt; Marktplatz 1 | 14532 Europarc Dreilinden | Germany
&gt; 
&gt; 
&gt; Geschäftsführer: Malte Krüger
&gt; HRB Nr.: 18517 P, Amtsgericht Potsdam
&gt; Sitz der Gesellschaft: Kleinmachnow 


\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

