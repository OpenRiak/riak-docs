---
title: "Re: Core Claim and Property-Based Tests"
description: ""
project: community
lastmod: 2017-05-17T10:11:13-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg18220"
mailinglist_parent_id: "msg18218"
author_name: "Martin Sumner"
project_section: "mailinglistitem"
sent_date: 2017-05-17T10:11:13-07:00
---


Jon,

Many thanks for taking the time to look at this. You've given me lots to
think about, so I will take some time before updating my write-up to take
account of your feedback.

I need to go back and look at the safe transfers issues. I spent some time
trying to work out how the claimaint transitions from having a plan to
committing the plan, and just kept getting lost in the code ... and put it
to one side. I will be brave and dive back in, and have a look at the
simulator as well.

The time it takes to expand a cluster, and the cost of that expansion on
the existing nodes in the cluster (both during the fold, and the legacy of
the page-cache impact after the fold) is something we're worried about.
One of the motivations behind the leveled backend was perhaps to have an
alternative to handoff/fold\\_objects when moving vnodes, whereby the backend
could just ship the WAL files instead (and the receiving vnode on the
joining node would rebuild the KeyStore from the shipped WAL files).
Perhaps a vnode proxy solution might be better.

With the physical run-time promises I wasn't thinking of anything clever
with fallback that necessarily guaranteed in more cases that it would be
written to two physical nodes, but just that the writing client could be
aware when it has been written to two physical nodes even when two primary
vnodes are unavailable in the preflist. Currently the NHS uses pw=2 as a
proxy for guaranteeing something has been written to two physical nodes,
but this kills availability on two nodes failing, even though as
target\\_n\\_val is 4 the PUT coordinator may be able to wait and confirm that
it was written to two physical nodes in all cases.

I'm going to read the jump consistent hash paper - it looks interesting.
Thoughts about radical changes that will support things like AZ-awareness
are shoved to the back of my mind at present, as even if they are possible,
it feels like transitioning old clusters to a radically changed ring (or
even ringless) algorithm would just be too hard. One thing related to
this, is that we're assuming any future open-source full-sync type
replication will need to be de-coupled from the need to have consistent
ring-sizes (or perhaps even consistent ways of calculating the ring), as
the in-cluster ring-resizing is now dropped as a documented feature having
only ever been an experimental one - the only real option will be to be to
migrate to a new cluster with a different ring-size through replication.


Thanks again

Martin

On 17 May 2017 at 16:34, Jon Meredith  wrote:

&gt;
&gt; Thanks for the excellent writeup.
&gt;
&gt; I have a few notes on your writeup and then a little history to help
&gt; explain the motivation for the v3 work.
&gt;
&gt; The Claiming Problem
&gt;
&gt; One other property of the broader claim algorithm + claimant + handoff
&gt; manager group of processes that's worth mentioning is safety during
&gt; transition. The cluster should ensure that target N-val copies
&gt; are always available even during transitions. Much earlier in Riak's
&gt; life the claim would just execute and ownership transfer immediately,
&gt; without putting the data in place (fine, it's eventually consistent,
&gt; right?)
&gt; but that meant if more than two vnodes in a preference list changed
&gt; ownership then clients would read not found until at least one of the
&gt; objects it was receiving had transferred. The claimant now shepherds
&gt; those
&gt; transitions so it should be safe. The solution of transferring the
&gt; data before ownership has fixed the notfound problem, but Riak lost
&gt; agility in adding capacity to the cluster - existing data has to transfer
&gt; to new nodes before they are freed up, and they continue to grow
&gt; while waiting. In hindsight, Ryan Zezeski's plan of just adding new
&gt; capacity and proxying back to the original vnode is probably a better
&gt; option.
&gt;
&gt; Predicting load on the cluster is also difficult with the single
&gt; ring with a target n-val set at creation time being used for all
&gt; buckets despite their n-value. To compute the operations sent to
&gt; each vnode you need to know the proportion of access to each N-value.
&gt;
&gt; There's also the problem that if a bucket is created with an N-value
&gt; larger than target N all bets are off about the number of physical nodes
&gt; values are written to (\\*cough\\* strong consistency N-5)
&gt;
&gt; Having a partitioning-scheme-per-N-value is one way of sidestepping the
&gt; load prediction and max-N problems.
&gt;
&gt; Promixity of Vnodes
&gt;
&gt; An alternate solution to the target\\_n\\_val problem is to change the way
&gt; fallback partitions are added and apply an additional uniqueness
&gt; constraint
&gt; as target nodes are added. That provides safety against multiple node
&gt; failures (although can potentially cause loading problems). I think
&gt; you imply this a couple of points when you talk about 'at runtime'.
&gt;
&gt; Proximity of vnodes as the partition list wraps
&gt;
&gt; One kludge I considered solving the wraparound problem is to go from
&gt; a ring to a 'spiral' where you add extra target\\_n\\_val-1 additional
&gt; vnodes that alias the few vnodes in the ring.
&gt;
&gt; Using the pathalogically bad (vnodes) Q=4, (nodes) S=3, (nval) N=3
&gt; ```
&gt; v0 | v1 | v2 | v3
&gt; nA | nB | nC | nA
&gt;
&gt; p0 = [ {v1, nB} {v2, Nc} {v3, nA} ]
&gt; p1 = [ {v2, Nc} {v3, nA} {v0, nA} ] &lt;&lt;&lt; Bad
&gt; p2 = [ {v3, nA} {v0, nA} {v1, nB} ] &lt;&lt;&lt; Bad
&gt; p3 = [ {v0, nA} {v1, nB} {v2, nC} ]
&gt; ```
&gt; You get 2/4 preflists violating target\\_n\\_val=3.
&gt;
&gt; If you extend the ring to allow aliasing (i.e. go beyond 2^160) but
&gt; only use it for assignment
&gt;
&gt; ```
&gt; v0 | v1 | v2 | v3 | v0' | v1'
&gt; nA | nB | nC | nA | nB | nC
&gt;
&gt; p0 = [ {v1, nB} {v2, Nc} {v3, nA} ]
&gt; p1 = [ {v2, Nc} {v3, nA} {v0', nB} ]
&gt; p2 = [ {v3, nA} {v0', nB} {v1', nB} ]
&gt; p3 = [ {v0, nA} {v1, nB} {v2, nC} ]
&gt; ```
&gt; The additional vnodes can never be hashed directly, just during
&gt; wraparound.
&gt;
&gt;
&gt; As you say, the v3 algorithm was written (by me) a long time ago and
&gt; never made it to production. It was due to a few factors, partly
&gt; the non-determinism, partly because I didn't like the (very stupid)
&gt; optimization system tying up the claimant node for multiple seconds,
&gt; but more troublingly when we did some commissioning tests for a large
&gt; customer that ran with a ring size of 256 with 60 nodes we experienced
&gt; a performance drop of around 5% when the cluster was maxed out for
&gt; reads. The diversity measurements were much 'better' in that the
&gt; v3 claimed cluster was far more diverse and performed better during
&gt; node failures, but the (unproven) fear that having a greater number
&gt; of saturated disterl connections between nodes dropped performance
&gt; without explanation stopped me from promoting it to default.
&gt;
&gt; The reason the v3 algorithm was created was to resolve problems with
&gt; longer lived clusters created with the v2 claim that had had nodes
&gt; added and removed over time. I don't remember all the details now,
&gt; but I think the cluster had a ring size of 1024 (to future proof,
&gt; as no 2I/listkey on that cluster) and somewhere between 15-30 nodes.
&gt;
&gt; In that particular configuration, the v2 algorithm had left the original
&gt; sequential node assignment (n1, n2, ..., n15, n1, n2, ...) and assigned
&gt; new nodes in place, but that left many places were the original sequential
&gt; assignments still existed.
&gt;
&gt; What we hadn't realized at the time is that sequential node assignment
&gt; is the \\*worst\\* possible plan for handling fallback load.
&gt;
&gt; If with N=3 if a node goes down, all of the responsibility for that
&gt; node is shift to another single node in the cluster.
&gt;
&gt; n1 | n2 | n3 | n4 | n1 | n2 | n3 | n4 (Q=8 S=4,TargetN4)
&gt;
&gt; Partition All Up n4 down
&gt; (position)
&gt; 0 n2 n3 n4 n2 n3 n1
&gt; 1 n3 n4 n1 n3 n1 n1
&gt; 2 n4 n1 n2 n1 n1 n2
&gt; 3 n1 n2 n3 n1 n2 n3
&gt; 4 n2 n3 n4 n2 n3 n1
&gt; 5 n3 n4 n1 n3 n1 n1
&gt; 6 n4 n1 n2 n1 n1 n2
&gt; 7 n1 n2 n3 n1 n2 n3
&gt;
&gt; With all nodes up, the number of times each node appears in a preflist
&gt; is equal. 6 \\* n1, 6 \\* n2, 6 \\* n3, 6 \\* n4 each appears (TN\\*Q/S)
&gt;
&gt; But during single node failure
&gt; 12 \\* n1, 6 \\* n2, 6 \\* n3, n4 down.
&gt;
&gt; The load on n1 is doubled.
&gt;
&gt; In the real scenario, although it was no longer sequentially assigned
&gt; there were still a large number of very similar preference lists to
&gt; the original assignment (as growing a few nodes on that ring size
&gt; only reassigns preference lists in proportion to the new nodes claiming
&gt; partitions).
&gt;
&gt; The production cluster was running fairly close to capacity, so the
&gt; increased loading during failure, even though it wasn't as bad as doubled
&gt; was enough to push it over the performance 'step' lowering tail latencies
&gt; and slowed it down enough to overload the vnodes and exhaust memory
&gt; crashing the next node causing a cascade. This was before vnodes had
&gt; overload protection so would present differently now.
&gt;
&gt;
&gt; Another pre-claimant problem that shaped some of the earlier claim
&gt; code vnode 'want' threshods was that when the nodes were individually
&gt; allowed to say if they wanted to claim more vnodes (with the
&gt; wants\\_claim function, before calling choose\\_claim), there were some states
&gt; the cluster would get into where two nodes both decided they were under
&gt; capacity and continually tried to claim, causing the vnode to flip/flop
&gt; back and forth between them (that was a reason for writing one of the early
&gt; QuickCheck tests).
&gt;
&gt;
&gt; I'm not sure if you've encountered it or not, but the riak\\_core\\_claim\\_sim
&gt; is also a good tool for testing the behavior of the claim functions and
&gt; the claimant. You don't mention it in your write up, but one of the
&gt; important functions of the claimant is to make sure it only performs
&gt; safe transitions between rings. It makes sure that the n val is not
&gt; violated during handoff.
&gt;
&gt;
&gt;
&gt; What to do?
&gt;
&gt; Fixing the claim algorithm is one way of doing things, but I worry
&gt; it has a number of problems that are hard to solve (multi-AZ, multi-Nval
&gt; etc).
&gt;
&gt; One more radical option is to dump the ring and just publish a table
&gt; per-vnode of the nodes and vnode hash you'd like to service them.
&gt; Riak doesn't really need consistent hashing - it doesn't \\*really\\* use
&gt; it's original form (the Dynamo A scheme), and is more of a hybrid
&gt; of the B/C schemes.
&gt;
&gt; Use cluster metadata and publish out the tables, update riak\\_core\\_apl
&gt; to take the new data and serve up the preference lists. Obviously
&gt; it trickles into things like the vnode and handoff managers, but it
&gt; may be possible.
&gt;
&gt; That gives you the advantage of no longer being constrained in how
&gt; you assign the nodes - a separation of policy and execution. You
&gt; could keep the existing ring based algorithms, or you could do something
&gt; better.
&gt;
&gt; It may be interesting to change the number of vnodes/hashing algorithm
&gt; too. Jordan West was a big fan of Consistent Jump Hashing at one point.
&gt;
&gt; The thing you give up if you lose the power-of-2 partitioning scheme
&gt; is the ability to split and combine partitions. Each partition in
&gt; a 64 vnode ring maps to exactly two (non-consecutive) partitions in a 128
&gt; vnode ring. Which is a very nice for replicating between clusters
&gt; with different ring sizes and localizing where to look for data.
&gt;
&gt; Good luck!
&gt;
&gt;
&gt;
&gt;
&gt; On Wed, May 17, 2017 at 6:37 AM Daniel Abrahamsson 
&gt; wrote:
&gt;
&gt;&gt; Thanks for the writeup and detailed investigation, Martin.
&gt;&gt;
&gt;&gt; We ran into these issues a few months when we expanded a 5 node cluster
&gt;&gt; into a 8 node cluster. We ended up rebuilding the cluster and writing a
&gt;&gt; small escript to verify that the generated riak ring lived up to our
&gt;&gt; requirements (which were 1: to survive an AZ outage, and 2: to survive any
&gt;&gt; 2 nodes going down at the same time).
&gt;&gt;
&gt;&gt; This will be a great document to refer to when explaining the subtleties
&gt;&gt; of setting up a Riak cluster.
&gt;&gt;
&gt;&gt; //Daniel
&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt; riak-users mailing list
&gt;&gt; riak-users@lists.basho.com
&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt;
&gt;
\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

