---
title: "Re: Riak 1.4 test on Azure - Webmachine error at path ..."
description: ""
project: community
lastmod: 2013-07-28T13:09:50-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg11800"
mailinglist_parent_id: "msg11799"
author_name: "Matthew Von-Maszewski"
project_section: "mailinglistitem"
sent_date: 2013-07-28T13:09:50-07:00
---


Christian,

leveldb has two independent caches: file cache and data block cache. You have 
raised the data block cache from its default 8M to 256M per your earlier note. 
I would recommend the follow:

{max\\_open\\_files, 50}, %% 50 \\* 4Mbytes allocation for file cache
{cache\\_size, 104857600}, %% 100Mbytes for data block cache

The max\\_open\\_files default is 20 (which is internally reduced by 10). You are 
likely thrashing file opens. The file cache is far more important to 
performance than the data block cache. 

Find the LOG file within one of your database "vnode" directories. Look for a 
line like this ' compacted to: files[ 0 9 25 14 2 0 0 ]'. You would like to be 
covering that total count of files (plus 10) with your max\\_open\\_files setting. 
Take the cache\\_size down to as low as 8Mbytes to achieve the coverage. Once 
you are down to 8Mbytes of cache\\_size, you should go no lower and give up on 
full max\\_open\\_files coverage.

Summary: total memory per vnode in 1.4 is (max\\_open\\_files - 10) \\* 4Mbytes + 
cache\\_size;



Matthew

On Jul 28, 2013, at 3:53 PM, Christian Rosnes  
wrote:

&gt; 
&gt; 
&gt; 
&gt; On Thu, Jul 25, 2013 at 2:16 PM, Christian Rosnes 
&gt;  wrote:
&gt; 
&gt; During a test I just performed on a small Riak 1.4 cluster setup on Azure,
&gt; I started seeing the Riak errors messages listed below after about 10 
&gt; minutes. 
&gt; 
&gt; The simple test was performed using lastest Jmeter running on two Azure 
&gt; instances, 
&gt; which also each runs haproxy and loadbalances the http/rest requests between 
&gt; the 4 Riak nodes.
&gt; 
&gt; 
&gt; An Update:
&gt; 
&gt; Increased some sysctl.conf network parameters on the Riak instances.
&gt; I have now been running 3 consecutive 1-hour JMeter tests with no errors:
&gt; 
&gt; &gt;&gt; 600 JMeter threads - 3600 seconds
&gt; [FINAL RESULTS] 
&gt; total count: 5526844, overall avg: 386 (ms), 
&gt; overall tps: 1536.6 (p/sec), recent tps: 1932.1 (p/sec), errors: 0
&gt; 
&gt; &gt;&gt; 400 JMeter threads - 3600 seconds
&gt; [FINAL RESULTS] 
&gt; total count: 6350600, overall avg: 224 (ms), 
&gt; overall tps: 1765.7 (p/sec), recent tps: 1849.1 (p/sec), errors: 0
&gt; 
&gt; &gt;&gt; 300 JMeter threads - 3600 seconds
&gt; [FINAL RESULTS]
&gt; total count: 5997689, overall avg: 178 (ms), 
&gt; overall tps: 1666.5 (p/sec), recent tps: 1744.7 (p/sec), errors: 0
&gt; 
&gt; A drop in performance compared to previous Azure results 
&gt; (from around 2000 req/s for 1-hour tests), but it may be caused
&gt; by the move of Riak data directory from ephemeral 
&gt; /mnt/resource partitions to persistent XFS partitions; 
&gt; 'iostat' reports that the new partitions are near 100% 
&gt; io utilization during the tests.
&gt; 
&gt; Will run a few more tests, then on to testing on some larger 
&gt; (and a few more) Azure instances and compare result with 
&gt; similar instances on AWS.
&gt; 
&gt; Christian
&gt; @NorSoulx
&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

