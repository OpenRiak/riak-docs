---
title: "Re: Questions about ring size"
description: ""
project: community
lastmod: 2013-03-11T04:23:11-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg10372"
mailinglist_parent_id: "msg10367"
author_name: "Chris Read"
project_section: "mailinglistitem"
sent_date: 2013-03-11T04:23:11-07:00
---


Thanks Mark...


On Sat, Mar 9, 2013 at 1:29 AM, Mark Phillips  wrote:

&gt; Hi Chris,
&gt;
&gt; Thanks for the detailed write up. These are some great data points.
&gt;
&gt; We're doing some work right now to make large rings (where "large" =
&gt; more than 512 partitions) more efficient in terms of start and
&gt; convergence time, and handoff.
&gt;
&gt;
Good to hear.


&gt; First things first: since your test cluster has no data in it, adding
&gt; "forced\\_ownership\\_handoff" to your riak\\_core section of your
&gt; app.config and up'ing it to something higher than your ring size
&gt; should help hasten convergence. \\*This is only useful for the purposes
&gt; of testing and should not be done in production.\\* That would look like
&gt; this:
&gt;
&gt; {forced\\_ownership\\_handoff, 512}
&gt;
&gt;
I'm doing this to understand current production problems. If it should not
be done in production then I'm not interested :D


&gt; You could also increase the "max\\_concurrency" setting (which also has
&gt; to be added to the riak\\_core section in your app.config). This
&gt; defaults to "2". You could also look at lowering the
&gt; "vnode\\_management\\_timer" from "10000" (10 seconds by default).
&gt;
&gt;
Can you point me at more documentation for "max\\_concurrency"? I've looked
at the code, and it's clear what "vnode\\_management\\_timer" does, but my
Elrang-foo is not good enough to be certain on the impacts of
"max\\_concurrency". How does it interact with "handoff\\_concurrency" (if at
all) which we currently have at 8?


&gt; Back to the current limitations of Riak..
&gt;
&gt; A few members of the Basho eng team - primarily Joe Blomstedt - have
&gt; been hacking on the ring-relate code for the last week or so are
&gt; making some great progress. The improvements will be in the 1.4
&gt; release (though it is a few months out from being official). To quote
&gt; Joe from an internal email: "in my current work-in-progress branch, I
&gt; successfully joined 4-nodes together using a 16384 ring yesterday.
&gt; Still took about 20 min, but working on bringing that down even
&gt; further today. Also, impact to cluster performance is worlds
&gt; different."
&gt;
&gt; So, we're well aware of the improvements that need to be made in the
&gt; arena and are working quickly to improve. I think Joe has plans to
&gt; share his working code with the list in the near future (via a GitHub
&gt; PR/Issue I suspect), so look out for that.
&gt;
&gt; In the interim, I would stick with a ring size of 512 or less for
&gt; productions clusters if you're not already live, and lean on some
&gt; beefier hardware to mitigate the current inefficiencies with large
&gt; rings until the code is purified.
&gt;
&gt;
We're already live with a ring size of 512, and we're pretty close to 100TB
of data in there and we're seeing handoff problems already which is why I'm
investigating this further. As for beefiness of hardware, we're currently
running dual 6 core Intel X5675 machines with 96G RAM with 10G Ethernet
links between nodes. We're not going to get beefier any time soon.

We're currently growing the cluster from the 10 nodes we started with to 20
to cope with the load, but it takes almost 3 days for handoff to a new node
to happen. We're doing it one by one instead of as a bulk add operation
because with almost 200G per partition the window of "not found" errors
between a new node taking the partition and completing transfer to serve
the data is already uncomfortably high.



&gt; Let us know if you have any other questions. Thanks for your testing
&gt; and patience.
&gt;

One more question I have is around changing the ring size on a running
cluster. Is it something that you're working on? While we're at less than
about 150TB of data in our cluster we can probably find a way to build a
second cluster and transfer the data over, but we expect to reach the stage
where we'll have over 500TB of data in riak, and at that stage we won't be
able to build a second cluster and don't want to be stuck with almost 1TB
of data per partition...

Thanks,

Chris


&gt;
&gt; Mark
&gt;
&gt;
&gt; On Fri, Mar 8, 2013 at 6:37 AM, Chris Read  wrote:
&gt; &gt; Greetings all...
&gt; &gt;
&gt; &gt; While I can find lots of documentation about what a ring is and how it's
&gt; &gt; using in Riak, I've found very little that's actually useful about
&gt; &gt; determining the right size for your system. The most useful formula I've
&gt; &gt; found so far has been the simple:
&gt; &gt;
&gt; &gt; ring size = 2 ^ (ceiling(log(max nodes \\* min partitions per node, 2)))
&gt; &gt;
&gt; &gt; Where the minimum recommended number of partitions per node is 10 (as per
&gt; &gt;
&gt; http://docs.basho.com/riak/latest/cookbooks/faqs/operations-faq/#is-it-possible-to-change-the-number-of-partitions
&gt; ).
&gt; &gt;
&gt; &gt; Nothing tells me though what sane upper bound is for the amount of data
&gt; in a
&gt; &gt; partition, or the overhead inside the cluster of managing larger ring
&gt; sizes.
&gt; &gt; My gut feel though is that more than a couple of hundred gigabytes per
&gt; &gt; partition is getting a bit much.
&gt; &gt;
&gt; &gt; I've done some initial testing of ring sizes across a cluster of 9
&gt; physical
&gt; &gt; machines and have seen some concerning results. All the numbers below are
&gt; &gt; done on the same hardware running Ubuntu 12.04 with Riak 1.3.0 (official
&gt; &gt; .deb release):
&gt; &gt;
&gt; &gt; Ring Size | 512 | 1024 | 2048 |
&gt; &gt; Create Cluster | 01:53 | 05:41 | 0:12:58 |
&gt; &gt; Remove Node | 04:01 | 10:31 | 0:31:13 |
&gt; &gt; Add Node | 01:05 | 05:22 | 1:04:49 |
&gt; &gt;
&gt; &gt; All this is done with NO DATA in the cluster at all - so why does it take
&gt; &gt; over an hour to add a new node when ring=2048?
&gt; &gt;
&gt; &gt; Does it have anything to do with the concerns raised on this thread:
&gt; &gt;
&gt; https://groups.google.com/forum/?fromgroups=#!topic/nosql-databases/DZkgkgd9YnA
&gt; &gt;
&gt; &gt; Thanks,
&gt; &gt;
&gt; &gt; Chris
&gt; &gt;
&gt; &gt;
&gt; &gt;
&gt; &gt;
&gt; &gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; &gt; riak-users mailing list
&gt; &gt; riak-users@lists.basho.com
&gt; &gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt; &gt;
&gt;
\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

