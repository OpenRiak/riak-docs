---
title: "Re: max_files_limit and AAE"
description: ""
project: community
lastmod: 2013-11-12T08:19:59-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg12954"
mailinglist_parent_id: "msg12953"
author_name: "Luke Bakken"
project_section: "mailinglistitem"
sent_date: 2013-11-12T08:19:59-08:00
---


Hi Dave,

You can use the lsof command to find files opened by the riak user.

--
Luke Bakken
CSE
lbak...@basho.com


On Tue, Nov 12, 2013 at 8:08 AM, Dave Brady  wrote:

&gt; I looked at the spreadsheet, Matthew, thanks! It's much more
&gt; comprehensive than what's on the website.
&gt;
&gt; We never had any RAM issues, howver, during the incidents. All of the
&gt; machines had 35 GB or more free RAM, with no pages swapped out.
&gt;
&gt; I still just don't understand what could have caused these errors.
&gt;
&gt; One of the bad nodes (from yesterday's outage) has 42,000 total .sst files
&gt; across its 25 VNodes (about 1,700 .sst files/VNode).
&gt;
&gt; How could 65,536 filehandles got exhausted? Would not Riak have had to
&gt; open every single .sst file, \\*and\\* do , to hit that
&gt; limit?
&gt;
&gt; Is there command I can use in the CLI that gives the number of open files?
&gt;
&gt; And now for something only slightly different...
&gt;
&gt; We have surmised that part of how the out-of-files problem appears to
&gt; manifest itself caused our three recent cluster-wide outages.
&gt;
&gt; We are using haproxy with the recommended config, so haproxy is set for
&gt; leastconn. What seems to have happened is that Riak continued to respond
&gt; positively (at least a good part of the time) to haproxy's default
&gt; aliveness check. This caused haproxy to send all new connection requests
&gt; to the bad nodes, once existing connections on the bad nodes completed.
&gt; Our cluster, in very short order, was for most intents and purposes dead.
&gt;
&gt; We saw in all of our apps' logs multitudes of connection (re)attempts,
&gt; which we didn't at first attribute to Riak/HAProxy. I had to get the
&gt; cluster back up very quickly, so I simply did a rolling restart.
&gt;
&gt; This last time (yesterday), I had a little more time to investigate. All
&gt; our apps returned to normal immediately after the second of the two
&gt; identified bad nodes was restarted.
&gt;
&gt; --
&gt; Dave Brady
\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

