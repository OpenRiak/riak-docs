---
title: "Re: Riak Nodes Crashing"
description: ""
project: community
lastmod: 2014-12-08T09:07:42-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg15331"
mailinglist_parent_id: "msg15328"
author_name: "Matthew Von-Maszewski"
project_section: "mailinglistitem"
sent_date: 2014-12-08T09:07:42-08:00
---


Satish,

This additional information continues to support my suspicion that the memory 
management is not fully accounting for your number of open files. A large 
query can cause many files that were previously unused to open. An open table 
file in leveldb uses memory heavily (for the file's block index and bloom 
filter). Also, leveldb will allow the memory limit to be knowingly exceeded in 
the case of queries that cover large segments of the key space.

There are fixes for both of those scenarios in Riak 2.0, but not in the 1.x 
series.

Matthew 


On Dec 8, 2014, at 11:22 AM, ender  wrote:

&gt; Hello Matthew,
&gt; 
&gt; I was going through my cluster setup again, checking up on stuff, when I 
&gt; noticed something. So just for some background, when I originally started 
&gt; using Riak it was as a replacement for MongoDB. To get things up and running 
&gt; quickly I "cheated" and just wrote some code that took a MongoDB query and 
&gt; recast it as a Riak search query. Then I enabled the search hook on all my 
&gt; buckets, and it just worked! Of course, Riak search wasn't the fastest thing 
&gt; on 2 legs, so I started to redo my data model to make it more key-value store 
&gt; friendly. Where I had to, I used secondary indexes. Once I'd converted the 
&gt; data model for a bucket I would remove the search hook on that bucket. On 
&gt; Friday evening I discovered that on 2 of my buckets I'd forgotten to remove 
&gt; the search hook. One of the buckets only has a couple of thousand records in 
&gt; it, so no big deal. But the other one! - that bucket has the most reads, the 
&gt; most writes, and has over 200M records stored in it. I removed the search 
&gt; hook on both of those buckets and the cluster has been stable over the 
&gt; weekend and is still up as of now. I did not disable active anti-entropy, 
&gt; since I did not want to change too many variables at the same time. I will 
&gt; do that today. Question is, was this a coincidence or do you think it's 
&gt; possible the search indexing was causing the OOM errors?
&gt; 
&gt; Satish
&gt; 
&gt; On Sat, Dec 6, 2014 at 6:59 AM, Matthew Von-Maszewski  
&gt; wrote:
&gt; Satish,
&gt; 
&gt; I do NOT recommend adding a sixth node before the other five are stable 
&gt; again. There was another customer that did that recently and things just got 
&gt; worse due to the vnode handoff actions to the sixth node.
&gt; 
&gt; I do recommend one or both of the following:
&gt; 
&gt; - disable active anti-entropy in app.config, {anti\\_entropy, {off, []}}. Then 
&gt; restart all nodes. We quickly replaced 1.4.7 due to an bug in the active 
&gt; anti-entropy. I do not know the details of the bug. But no one had seen a 
&gt; crash from it. However, you may be seeing a long term problem due to that 
&gt; same bug. The anti-entropy feature in 1.4.7 is not really protecting your 
&gt; data anyway. It might as well be disabled until you are ready to upgrade.
&gt; 
&gt; - further reduce the max\\_open\\_files parameter simply to get memory stable: 
&gt; use 75 instead of the recent 150. You must restart all nodes after making 
&gt; the change in app.config.
&gt; 
&gt; 
&gt; I will need to solicit support from others at Basho if the two workarounds 
&gt; above do not stabilize the cluster. 
&gt; 
&gt; Matthew
&gt; 
&gt; On Dec 5, 2014, at 5:54 PM, ender  wrote:
&gt; 
&gt;&gt; Would adding a 6th node mean each node would use less memory as a stopgap 
&gt;&gt; measure?
&gt;&gt; 
&gt;&gt; On Fri, Dec 5, 2014 at 2:20 PM, ender  wrote:
&gt;&gt; Hey Matthew it just crashed again. This time I got the syslog and leveldb 
&gt;&gt; logs right away.
&gt;&gt; 
&gt;&gt; 
&gt;&gt; 
&gt;&gt; On Fri, Dec 5, 2014 at 11:43 AM, Matthew Von-Maszewski  
&gt;&gt; wrote:
&gt;&gt; Satish,
&gt;&gt; 
&gt;&gt; Here is a key line from /var/log/messages:
&gt;&gt; 
&gt;&gt; Dec 5 06:52:43 ip-10-196-72-106 kernel: [26881589.804401] beam.smp invoked 
&gt;&gt; oom-killer: gfp\\_mask=0x201da, order=0, oom\\_adj=0, oom\\_score\\_adj=0
&gt;&gt; 
&gt;&gt; The log entry does NOT match the timestamps of the crash.log and error.log 
&gt;&gt; below. But that is ok. The operating system killed off Riak. There would 
&gt;&gt; have be no notification in the Riak log's of the operating system's actions.
&gt;&gt; 
&gt;&gt; The fact that the out of memory monitor, oom-killer, killed Riak further 
&gt;&gt; supports the change to max\\_open\\_files. I recommend we now wait to see if 
&gt;&gt; the problem occurs again.
&gt;&gt; 
&gt;&gt; 
&gt;&gt; Matthew
&gt;&gt; 
&gt;&gt; 
&gt;&gt; On Dec 5, 2014, at 2:35 PM, ender  wrote:
&gt;&gt; 
&gt;&gt;&gt; Hey Matthew,
&gt;&gt;&gt; 
&gt;&gt;&gt; The crash occurred around 3:00am:
&gt;&gt;&gt; 
&gt;&gt;&gt; -rw-rw-r-- 1 riak riak 920 Dec 5 03:01 crash.log
&gt;&gt;&gt; -rw-rw-r-- 1 riak riak 617 Dec 5 03:01 error.log
&gt;&gt;&gt; 
&gt;&gt;&gt; I have attached the syslog that covers that time. I also went ahead and 
&gt;&gt;&gt; changed max\\_open\\_files in app.config to to 150 from 315.
&gt;&gt;&gt; 
&gt;&gt;&gt; Satish
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; On Fri, Dec 5, 2014 at 11:29 AM, Matthew Von-Maszewski  
&gt;&gt;&gt; wrote:
&gt;&gt;&gt; Satish,
&gt;&gt;&gt; 
&gt;&gt;&gt; The "key" system log varies by Linux platform. Yes, /var/log/messages may 
&gt;&gt;&gt; hold some key clues. Again, be sure the file covers the time of a crash.
&gt;&gt;&gt; 
&gt;&gt;&gt; Matthew
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; On Dec 5, 2014, at 1:29 PM, ender  wrote:
&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Hey Matthew,
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; I see a /var/log/messages file, but no syslog or system.log etc. Is it 
&gt;&gt;&gt;&gt; the messages file you want?
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Satish
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; On Fri, Dec 5, 2014 at 10:06 AM, Matthew Von-Maszewski 
&gt;&gt;&gt;&gt;  wrote:
&gt;&gt;&gt;&gt; Satish,
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; I find nothing compelling in the log or the app.config. Therefore I have 
&gt;&gt;&gt;&gt; two additional suggestions/requests:
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; - lower max\\_open\\_files in app.config to to 150 from 315. There was one 
&gt;&gt;&gt;&gt; other customer report regarding the limit not properly stopping out of 
&gt;&gt;&gt;&gt; memory (OOM) conditions.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; - try to locate a /var/log/syslog\\* file from a node that contains the time 
&gt;&gt;&gt;&gt; of the crash. There may be helpful information there. Please send that 
&gt;&gt;&gt;&gt; along.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Unrelated to this crash … 1.4.7 has a known bug in its active anti-entropy 
&gt;&gt;&gt;&gt; (AAE) logic. This bug is NOT known to cause a crash. The bug does cause 
&gt;&gt;&gt;&gt; AAE to be unreliable for data restoration. The proper steps for upgrading 
&gt;&gt;&gt;&gt; to the current release (1.4.12) are:
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; -- across the entire cluster
&gt;&gt;&gt;&gt; - disable anti\\_entropy in app.config on all nodes: {anti\\_entropy, {off, 
&gt;&gt;&gt;&gt; []}}
&gt;&gt;&gt;&gt; - perform a rolling restart of all nodes … AAE is now disabled in the 
&gt;&gt;&gt;&gt; cluster 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; -- on each node
&gt;&gt;&gt;&gt; - stop the node
&gt;&gt;&gt;&gt; - remove (erase all files and directories) /vol/lib/riak/anti\\_entropy
&gt;&gt;&gt;&gt; - update Riak to the new software revision
&gt;&gt;&gt;&gt; - start the node again
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; -- across the entire cluster
&gt;&gt;&gt;&gt; - enable anti\\_entropy in app.config on all nodes: {anti\\_entropy, {on, []}}
&gt;&gt;&gt;&gt; - perform a rolling restart of all nodes … AAE is now enabled in the 
&gt;&gt;&gt;&gt; cluster 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; The nodes will start rebuilding the AAE hash data. Suggest you perform 
&gt;&gt;&gt;&gt; the last rolling restart during a low utilization time of your cluster.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Matthew
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; On Dec 5, 2014, at 11:02 AM, ender  wrote:
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; Hi Matthew,
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; Riak version: 1.4.7
&gt;&gt;&gt;&gt;&gt; 5 Nodes in cluster
&gt;&gt;&gt;&gt;&gt; RAM: 30GB
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; The leveldb logs are attached.
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; On Thu, Dec 4, 2014 at 1:34 PM, Matthew Von-Maszewski 
&gt;&gt;&gt;&gt;&gt;  wrote:
&gt;&gt;&gt;&gt;&gt; Satish,
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; Some questions:
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; - what version of Riak are you running? logs suggest 1.4.7
&gt;&gt;&gt;&gt;&gt; - how many nodes in your cluster?
&gt;&gt;&gt;&gt;&gt; - what is the physical memory (RAM size) of each node?
&gt;&gt;&gt;&gt;&gt; - would you send the leveldb LOG files from one of the crashed servers:
&gt;&gt;&gt;&gt;&gt; tar -czf satish\\_LOG.tgz /vol/lib/riak/leveldb/\\*/LOG\\*
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; Matthew
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; On Dec 4, 2014, at 4:02 PM, ender  wrote:
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; &gt; My RIak installation has been running successfully for about a year. 
&gt;&gt;&gt;&gt;&gt; &gt; This week nodes suddenly started randomly crashing. The machines have 
&gt;&gt;&gt;&gt;&gt; &gt; plenty of memory and free disk space, and looking in the ring directory 
&gt;&gt;&gt;&gt;&gt; &gt; nothing appears to amiss:
&gt;&gt;&gt;&gt;&gt; &gt;
&gt;&gt;&gt;&gt;&gt; &gt; [ec2-user@ip-10-196-72-247 ~]$ ls -l /vol/lib/riak/ring
&gt;&gt;&gt;&gt;&gt; &gt; total 80
&gt;&gt;&gt;&gt;&gt; &gt; -rw-rw-r-- 1 riak riak 17829 Nov 29 19:42 
&gt;&gt;&gt;&gt;&gt; &gt; riak\\_core\\_ring.default.20141129194225
&gt;&gt;&gt;&gt;&gt; &gt; -rw-rw-r-- 1 riak riak 17829 Dec 3 19:07 
&gt;&gt;&gt;&gt;&gt; &gt; riak\\_core\\_ring.default.20141203190748
&gt;&gt;&gt;&gt;&gt; &gt; -rw-rw-r-- 1 riak riak 17829 Dec 4 16:29 
&gt;&gt;&gt;&gt;&gt; &gt; riak\\_core\\_ring.default.20141204162956
&gt;&gt;&gt;&gt;&gt; &gt; -rw-rw-r-- 1 riak riak 17847 Dec 4 20:45 
&gt;&gt;&gt;&gt;&gt; &gt; riak\\_core\\_ring.default.20141204204548
&gt;&gt;&gt;&gt;&gt; &gt;
&gt;&gt;&gt;&gt;&gt; &gt; [ec2-user@ip-10-196-72-247 ~]$ du -h /vol/lib/riak/ring
&gt;&gt;&gt;&gt;&gt; &gt; 84K /vol/lib/riak/ring
&gt;&gt;&gt;&gt;&gt; &gt;
&gt;&gt;&gt;&gt;&gt; &gt; I have attached a tarball with the app.config file plus all the logs 
&gt;&gt;&gt;&gt;&gt; &gt; from the node at the time of the crash. Any help much appreciated!
&gt;&gt;&gt;&gt;&gt; &gt;
&gt;&gt;&gt;&gt;&gt; &gt; Satish
&gt;&gt;&gt;&gt;&gt; &gt;
&gt;&gt;&gt;&gt;&gt; &gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt;&gt;&gt;&gt; &gt; riak-users mailing list
&gt;&gt;&gt;&gt;&gt; &gt; riak-users@lists.basho.com
&gt;&gt;&gt;&gt;&gt; &gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt; 
&gt;&gt; 
&gt;&gt; 
&gt; 
&gt; 

\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

