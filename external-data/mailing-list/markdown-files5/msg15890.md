---
title: "Re: Different numFound request to riak search"
description: ""
project: community
lastmod: 2015-03-13T00:24:56-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg15890"
mailinglist_parent_id: "msg15885"
author_name: "Roma Lakotko"
project_section: "mailinglistitem"
sent_date: 2015-03-13T00:24:56-07:00
---


Hello.

Good news. After i remove folder, and start yz\\_entropy\\_mgr:init([]) and
then re-save objects it's start to return correct numFound (test on 2
buckets, about 6000 requests). This is tested on dev instance.

So, as i understand, i need to make same operation on production cluster on
each node?

And, is this bug already patched? Or in future i can have same problem
after add/remove nodes?

Thanks,
Roman Lakotko.


2015-03-12 18:56 GMT+03:00 Roma Lakotko :

&gt; No, its simple riak search http request.
&gt; 12 марта 2015 г. 18:54 пользователь "Zeeshan Lakhani" 
&gt; написал:
&gt;
&gt; Are you running mapreduce with Solr queries?
&gt;&gt;
&gt;&gt;
&gt;&gt; On Mar 12, 2015, at 11:50 AM, Roma Lakotko  wrote:
&gt;&gt;
&gt;&gt; I don't see any solr errors. But each 10-20 minutes on prod and once a
&gt;&gt; day on dev i see strange errors:
&gt;&gt;
&gt;&gt; 2015-03-11 09:18:10.668 [error] &lt;0.234.0&gt; Supervisor
&gt;&gt; riak\\_pipe\\_fitting\\_sup had child undefined started with
&gt;&gt; riak\\_pipe\\_fitting:start\\_link() at &lt;0.12060.2&gt; exit with reason noproc in
&gt;&gt; context shutdown\\_error
&gt;&gt; 2015-03-12 13:12:05.200 [error] &lt;0.379.0&gt; Supervisor riak\\_kv\\_mrc\\_sink\\_sup
&gt;&gt; had child undefined started with riak\\_kv\\_mrc\\_sink:start\\_link() at
&gt;&gt; &lt;0.6601.1&gt; exit with reason noproc in context shutdown\\_error
&gt;&gt;
&gt;&gt; For both prod and dev instance values are:
&gt;&gt;
&gt;&gt; anti\\_entropy\\_build\\_limit -&gt; {ok,{1,3600000}}
&gt;&gt; anti\\_entropy\\_concurrency -&gt; {ok,2}
&gt;&gt; anti\\_entropy\\_tick - &gt; undefined
&gt;&gt;
&gt;&gt; I delete data folder and run init method, i'll results after it rebuild
&gt;&gt; trees.
&gt;&gt;
&gt;&gt; 2015-03-12 18:22 GMT+03:00 Zeeshan Lakhani :
&gt;&gt;
&gt;&gt;&gt; Are you noticing any Solr errors in the logs?
&gt;&gt;&gt;
&gt;&gt;&gt; For your container instance, you can attempt to clear the AAE trees and
&gt;&gt;&gt; force a rebuild by removing the entropy directories in 
&gt;&gt;&gt; `./data/yz\\_anti\\_entropy`
&gt;&gt;&gt; and running `yz\\_entropy\\_mgr:init([])` via `riak attach`. Or, you can
&gt;&gt;&gt; let AAE occur naturally (after removing the entropy data) and up the
&gt;&gt;&gt; concurrency/build\\_limit/tick (using set\\_env). You can see what you’re
&gt;&gt;&gt; current settings are by calling...
&gt;&gt;&gt;
&gt;&gt;&gt; ```
&gt;&gt;&gt; riak\\_core\\_util:rpc\\_every\\_member\\_ann(application, get\\_env, [riak\\_kv,
&gt;&gt;&gt; anti\\_entropy\\_build\\_limit],infinity).
&gt;&gt;&gt; riak\\_core\\_util:rpc\\_every\\_member\\_ann(application, get\\_env, [riak\\_kv,
&gt;&gt;&gt; anti\\_entropy\\_concurrency],infinity).
&gt;&gt;&gt; riak\\_core\\_util:rpc\\_every\\_member\\_ann(application, get\\_env, [yokozuna,
&gt;&gt;&gt; anti\\_entropy\\_tick],infinity).
&gt;&gt;&gt; ```
&gt;&gt;&gt;
&gt;&gt;&gt; … on any of the nodes. Query coverage is R=1, but the values should be
&gt;&gt;&gt; replicated across.
&gt;&gt;&gt;
&gt;&gt;&gt; Thanks.
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; On Mar 12, 2015, at 9:51 AM, Roma Lakotko  wrote:
&gt;&gt;&gt;
&gt;&gt;&gt; Hello Zeeshan.
&gt;&gt;&gt;
&gt;&gt;&gt; While i run queries no delete object is occurs.
&gt;&gt;&gt;
&gt;&gt;&gt; Stats on production and developer nodes output something like this:
&gt;&gt;&gt; https://gist.github.com/romulka/d0254aa193a9dbb52b67
&gt;&gt;&gt;
&gt;&gt;&gt; On dev container:
&gt;&gt;&gt;
&gt;&gt;&gt; /etc/riak# grep anti\\_entropy \\*
&gt;&gt;&gt; riak.conf:anti\\_entropy = active
&gt;&gt;&gt; riak.conf.dpkg-dist:anti\\_entropy = active
&gt;&gt;&gt;
&gt;&gt;&gt; ll -h /var/lib/riak/yz\\_anti\\_entropy/
&gt;&gt;&gt; total 264K
&gt;&gt;&gt; drwxrwxr-x 66 riak riak 4.0K Sep 25 12:08 ./
&gt;&gt;&gt; drwxr-xr-x 12 riak riak 4.0K Dec 9 12:19 ../
&gt;&gt;&gt; drwxr-xr-x 9 riak riak 4.0K Mar 12 12:01 0/
&gt;&gt;&gt; drwxr-xr-x 9 riak riak 4.0K Mar 12 12:01
&gt;&gt;&gt; 1004782375664995756265033322492444576013453623296/
&gt;&gt;&gt; drwxr-xr-x 9 riak riak 4.0K Mar 12 12:01
&gt;&gt;&gt; 1027618338748291114361965898003636498195577569280/
&gt;&gt;&gt; ....
&gt;&gt;&gt;
&gt;&gt;&gt; On prod:
&gt;&gt;&gt;
&gt;&gt;&gt; grep anti\\_entropy \\* /etc/riak/ -&gt; empty
&gt;&gt;&gt;
&gt;&gt;&gt; root@riak-21:/var/lib/riak/yz\\_anti\\_entropy# ll -h
&gt;&gt;&gt; total 64K
&gt;&gt;&gt; drwxrwxr-x 16 riak riak 4.0K Dec 4 03:44 ./
&gt;&gt;&gt; drwxr-xr-x 14 riak riak 4.0K Dec 9 12:10 ../
&gt;&gt;&gt; drwxr-xr-x 9 riak riak 4.0K Dec 4 03:44 0/
&gt;&gt;&gt; drwxr-xr-x 9 riak riak 4.0K Mar 12 12:57
&gt;&gt;&gt; 1027618338748291114361965898003636498195577569280/
&gt;&gt;&gt; ....
&gt;&gt;&gt;
&gt;&gt;&gt; I'm already try re-save all keys, it doesn't helps.
&gt;&gt;&gt;
&gt;&gt;&gt; Production cluster have 7 node, start from 3. So yes, nodes was
&gt;&gt;&gt; added/delete sometimes.
&gt;&gt;&gt;
&gt;&gt;&gt; On dev, i have 1 instance in docker container, never added to cluster.
&gt;&gt;&gt; But data in that riak is imported from production cluster a while ago.
&gt;&gt;&gt;
&gt;&gt;&gt; I can give you a copy of container, if you need to.
&gt;&gt;&gt;
&gt;&gt;&gt; Thanks,
&gt;&gt;&gt; Ronan Lakotko
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; 2015-03-12 16:36 GMT+03:00 Zeeshan Lakhani :
&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Hello Roma,
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Have you deleted this object at some point in your runs? Please make
&gt;&gt;&gt;&gt; sure AAE is running by checking search’s AAE status, `riak-admin search
&gt;&gt;&gt;&gt; aae-status`, and that data exists in the correct directory,
&gt;&gt;&gt;&gt; `./data/yz\\_anti\\_entropy` (
&gt;&gt;&gt;&gt; http://docs.basho.com/riak/latest/ops/advanced/configs/search/). You
&gt;&gt;&gt;&gt; may just need to perform a read-repair by performing a fetch of the object
&gt;&gt;&gt;&gt; itself first, before performing search queries again.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Also, have you left or added nodes? I’m guessing that even your 1 node
&gt;&gt;&gt;&gt; instance is still running a cluster on that one node, right?
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Thanks.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Zeeshan Lakhani
&gt;&gt;&gt;&gt; programmer |
&gt;&gt;&gt;&gt; software engineer at @basho |
&gt;&gt;&gt;&gt; org. member/founder of @papers\\_we\\_love | paperswelove.org
&gt;&gt;&gt;&gt; twitter =&gt; @zeeshanlakhani
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; On Mar 12, 2015, at 5:59 AM, Roma Lakotko  wrote:
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Each request to riak search return different results. It's return
&gt;&gt;&gt;&gt; different numFound.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; I use request like this:
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; http://localhost:8098/search/query/assets?wt=json&q=type:\\*&sort=\\_yz\\_rk%20asc
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; If add start offset it can return:
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; http://localhost:8098/search/query/assets?wt=json&q=type:\\*&sort=\\_yz\\_rk%20asc&start=1247
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; "response": {
&gt;&gt;&gt;&gt; "numFound": 1248,
&gt;&gt;&gt;&gt; "start": 1247,
&gt;&gt;&gt;&gt; "docs": [
&gt;&gt;&gt;&gt; {
&gt;&gt;&gt;&gt; "\\_yz\\_id": 
&gt;&gt;&gt;&gt; "1\\*default\\*assets\\*fff63ecf-a0c4-4ecf-b24d-c493ca3a302f\\*44",
&gt;&gt;&gt;&gt; "\\_yz\\_rk": "fff63ecf-a0c4-4ecf-b24d-c493ca3a302f",
&gt;&gt;&gt;&gt; "\\_yz\\_rt": "default",
&gt;&gt;&gt;&gt; "\\_yz\\_rb": "assets"
&gt;&gt;&gt;&gt; }
&gt;&gt;&gt;&gt; ]
&gt;&gt;&gt;&gt; }
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; On next request it return something like this
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; "numFound": 1224,
&gt;&gt;&gt;&gt; "start": 1247,
&gt;&gt;&gt;&gt; "docs": []
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; I have 1 node installation, and no process write to Riak.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; I have same problem this production cluster with 7 nodes.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Scheme for document
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; xml version="1.0" encoding="UTF-8" ?
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; &gt;&gt;&gt; stored="false" /&gt;
&gt;&gt;&gt;&gt; &gt;&gt;&gt; /&gt;
&gt;&gt;&gt;&gt; &gt;&gt;&gt; stored="false" /&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; &gt;&gt;&gt; stored="false" /&gt;
&gt;&gt;&gt;&gt; &gt;&gt;&gt; stored="false" multiValued="true" /&gt;
&gt;&gt;&gt;&gt; &gt;&gt;&gt; /&gt;
&gt;&gt;&gt;&gt; &gt;&gt;&gt; stored="false" /&gt;
&gt;&gt;&gt;&gt; &gt;&gt;&gt; stored="false" /&gt;
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; &gt;&gt;&gt; multiValued="false" required="true"/&gt;
&gt;&gt;&gt;&gt; &gt;&gt;&gt; multiValued="false"/&gt;
&gt;&gt;&gt;&gt; &gt;&gt;&gt; multiValued="false"/&gt;
&gt;&gt;&gt;&gt; &gt;&gt;&gt; multiValued="false"/&gt;
&gt;&gt;&gt;&gt; &gt;&gt;&gt; multiValued="false"/&gt;
&gt;&gt;&gt;&gt; &gt;&gt;&gt; multiValued="false"/&gt;
&gt;&gt;&gt;&gt; &gt;&gt;&gt; multiValued="false"/&gt;
&gt;&gt;&gt;&gt; &gt;&gt;&gt; multiValued="false"/&gt;
&gt;&gt;&gt;&gt; &gt;&gt;&gt; multiValued="false"/&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; \\_yz\\_id
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; &gt;&gt;&gt; omitNorms="true"/&gt;
&gt;&gt;&gt;&gt; &gt;&gt;&gt; sortMissingLast="true" omitNorms="true"/&gt;
&gt;&gt;&gt;&gt; &gt;&gt;&gt; omitNorms="true"/&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; &gt;&gt;&gt; omitNorms="true"/&gt;
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; &gt;&gt;&gt; multiValued="true" class="solr.StrField" /&gt;
&gt;&gt;&gt;&gt; &gt;&gt;&gt; sortMissingLast="true" omitNorms="true"&gt;
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; &gt;&gt;&gt; replacement='е' replace='all'/&gt;
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Best regards,
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Roman
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;
&gt;&gt;
\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

