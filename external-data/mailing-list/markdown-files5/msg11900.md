---
title: "Re: erlang go boom"
description: ""
project: community
lastmod: 2013-08-05T15:04:50-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg11900"
mailinglist_parent_id: "msg11898"
author_name: "Paul Ingalls"
project_section: "mailinglistitem"
sent_date: 2013-08-05T15:04:50-07:00
---


Hey Kresten,

Thanks for the response!

I learned my lesson on setting bucket properties. So all buckets currently use 
the defaults.

here is the output from one of our nodes:

total 40
drwxr-xr-x 2 root root 4096 Aug 5 21:10 ./
drwxr-xr-x 6 root root 4096 Aug 4 17:26 ../
-rw-r--r-- 1 root root 14187 Aug 4 17:37 riak\\_core\\_ring.default.20130804173753
-rw-r--r-- 1 root root 14586 Aug 5 21:10 riak\\_core\\_ring.default.20130805211043

another node:

total 72
drwxr-xr-x 2 root root 4096 Aug 5 21:10 ./
drwxr-xr-x 6 root root 4096 Aug 4 17:26 ../
-rw-r--r-- 1 root root 14187 Aug 4 17:37 riak\\_core\\_ring.default.20130804173753
-rw-r--r-- 1 root root 14358 Aug 5 20:56 riak\\_core\\_ring.default.20130805205650
-rw-r--r-- 1 root root 14529 Aug 5 21:09 riak\\_core\\_ring.default.20130805210924
-rw-r--r-- 1 root root 14586 Aug 5 21:10 riak\\_core\\_ring.default.20130805211043

looks like the largest number of files in that directory on any node is 5

Paul

Paul Ingalls
Founder & CEO Fanzo
p...@fanzo.me
@paulingalls
http://www.linkedin.com/in/paulingalls



On Aug 5, 2013, at 2:52 PM, Kresten Krab Thorup  wrote:

&gt; I'd think the large #buckets could be the issue; especially if there is any 
&gt; bucket properties being set, because that would cause the ring data structure 
&gt; to be enormous.
&gt; 
&gt; Could you provide an "ls -l" output of the riak data/ring directory?
&gt; 
&gt; Sent from my iPhone
&gt; 
&gt; On 05/08/2013, at 21.52, "Paul Ingalls" &gt; 
&gt; wrote:
&gt; 
&gt; As promised in previous email, I hit a fairly big problem over the weekend 
&gt; and then reproduced it this morning and I was wondering if I could get some 
&gt; help.
&gt; 
&gt; Basically, I was running my code against our risk cluster and everything was 
&gt; moving along just fine. However, at some point Riak just seems to hit a 
&gt; wall. I get to a certain scale of content, about 9-10 GB per node, and about 
&gt; half the cluster gives up the ghost.
&gt; 
&gt; I've done this twice from scratch now, the first time I thought maybe I was 
&gt; trying to push too many transactions per second. But the second time I 
&gt; reduced the speed and changed some settings in the app.config that I thought 
&gt; would reduce memory usage. Ran it again, and 24 hrs later I still hit the 
&gt; wall (almost the same place).
&gt; 
&gt; So, I figure I'm still doing something wrong, I'm just not sure what. I'm 
&gt; obviously hitting some kind of heap issue, but I'm not sure what else I can 
&gt; do about it. I'm hoping there is something obvious I'm missing in my 
&gt; ignorance, cuz at the moment I'm stuck...
&gt; 
&gt; Some details:
&gt; 
&gt; 7 node cluster running 1.4
&gt; each VM has 7GB RAM and 4 CPUs
&gt; the data directory is on a RAID0 with 750GB of space
&gt; 128 partitions, levelDB backend
&gt; using links and secondary indexes
&gt; lots of buckets (over 10 million), a couple buckets have lots of keys (one 
&gt; was around 6.6 million keys when it crashed, the other around 3.7 million).
&gt; Values are pretty small, almost all are just a few bytes. There is one 
&gt; bucket, the largest (6.6 million keys), with value sizes between 1-2k.
&gt; 
&gt; primary custom app config settings:
&gt; {kernel,
&gt; [
&gt; {inet\\_dist\\_listen\\_min, 6000},
&gt; {inet\\_dist\\_listen\\_max, 7999}
&gt; ]},
&gt; 
&gt; {riak\\_core, [
&gt; {default\\_bucket\\_props, [
&gt; {allow\\_mult, true},
&gt; {r, 1},
&gt; {w, 1},
&gt; {dw, 1},
&gt; {dw, 1}
&gt; ]},
&gt; {ring\\_creation\\_size, 128},
&gt; ]},
&gt; 
&gt; {riak\\_kv, [
&gt; {storage\\_backend, riak\\_kv\\_eleveldb\\_backend},
&gt; ]}
&gt; 
&gt; {eleveldb, [
&gt; {max\\_open\\_files, 32}
&gt; ]},
&gt; 
&gt; custom vm.args settings
&gt; +A 16
&gt; 
&gt; Here is some of the error information I am seeing when it all goes boom:
&gt; 
&gt; \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*
&gt; \\* On one of the nodes that crashes:
&gt; \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*
&gt; last lines of console.log
&gt; -------------------
&gt; 2013-08-05 17:59:08.071 [info] 
&gt; &lt;0.29251.556&gt;@riak\\_kv\\_exchange\\_fsm:key\\_exchange:206 Repaired 1 keys during 
&gt; active anti-entropy exchange of 
&gt; {468137243207554840987117797979434404733540892672,3} between 
&gt; {479555224749202520035584085735030365824602865664,riak@riak001} and 
&gt; {490973206290850199084050373490626326915664838656,riak@riak002}
&gt; 2013-08-05 18:01:10.234 [info] 
&gt; &lt;0.83.0&gt;@riak\\_core\\_sysmon\\_handler:handle\\_event:92 monitor large\\_heap 
&gt; &lt;0.14832.557&gt; 
&gt; [{initial\\_call,{riak\\_kv\\_get\\_fsm,init,1}},{almost\\_current\\_function,{riak\\_object,encode\\_maybe\\_binary,1}},{message\\_queue\\_len,1}]
&gt; 
&gt; [{old\\_heap\\_block\\_size,0},{heap\\_block\\_size,47828850},{mbuf\\_size,0},{stack\\_size,10},{old\\_heap\\_size,0},{heap\\_size,40978448}]
&gt; 2013-08-05 18:01:10.672 [info] 
&gt; &lt;0.83.0&gt;@riak\\_core\\_sysmon\\_handler:handle\\_event:92 monitor large\\_heap 
&gt; &lt;0.12133.557&gt; 
&gt; [{initial\\_call,{riak\\_api\\_pb\\_server,init,1}},{almost\\_current\\_function,{riak\\_pb\\_kv\\_codec,'-encode\\_content\\_meta/3-lc$^0/1-1-',1}},{message\\_queue\\_len,0}]
&gt; 
&gt; [{old\\_heap\\_block\\_size,0},{heap\\_block\\_size,47828850},{mbuf\\_size,0},{stack\\_size,45},{old\\_heap\\_size,0},{heap\\_size,40978360}]
&gt; 2013-08-05 18:01:12.993 [info] 
&gt; &lt;0.83.0&gt;@riak\\_core\\_sysmon\\_handler:handle\\_event:92 monitor large\\_heap 
&gt; &lt;0.14832.557&gt; 
&gt; [{initial\\_call,{riak\\_kv\\_get\\_fsm,init,1}},{almost\\_current\\_function,{dict,fold\\_bucket,3}},{message\\_queue\\_len,1}]
&gt; 
&gt; [{old\\_heap\\_block\\_size,0},{heap\\_block\\_size,59786060},{mbuf\\_size,0},{stack\\_size,50},{old\\_heap\\_size,0},{heap\\_size,40978626}]
&gt; 2013-08-05 18:01:13.816 [info] 
&gt; &lt;0.83.0&gt;@riak\\_core\\_sysmon\\_handler:handle\\_event:92 monitor large\\_heap 
&gt; &lt;0.14832.557&gt; 
&gt; [{initial\\_call,{riak\\_kv\\_get\\_fsm,init,1}},{almost\\_current\\_function,{dict,fold\\_bucket,3}},{message\\_queue\\_len,1}]
&gt; 
&gt; [{old\\_heap\\_block\\_size,0},{heap\\_block\\_size,59786060},{mbuf\\_size,0},{stack\\_size,50},{old\\_heap\\_size,0},{heap\\_size,40978626}]
&gt; 2013-08-05 18:01:13.819 [info] 
&gt; &lt;0.83.0&gt;@riak\\_core\\_sysmon\\_handler:handle\\_event:92 monitor large\\_heap 
&gt; &lt;0.12133.557&gt; 
&gt; [{initial\\_call,{riak\\_api\\_pb\\_server,init,1}},{almost\\_current\\_function,{riak\\_kv\\_pb,pack,5}},{message\\_queue\\_len,0}]
&gt; 
&gt; [{old\\_heap\\_block\\_size,0},{heap\\_block\\_size,59786060},{mbuf\\_size,0},{stack\\_size,200971},{old\\_heap\\_size,0},{heap\\_size,47627275}]
&gt; 2013-08-05 18:01:14.594 [info] 
&gt; &lt;0.83.0&gt;@riak\\_core\\_sysmon\\_handler:handle\\_event:92 monitor large\\_heap 
&gt; &lt;0.14832.557&gt; 
&gt; [{initial\\_call,{riak\\_kv\\_get\\_fsm,init,1}},{almost\\_current\\_function,{riak\\_object,encode\\_maybe\\_binary,1}},{message\\_queue\\_len,1}]
&gt; 
&gt; [{old\\_heap\\_block\\_size,0},{heap\\_block\\_size,116769640},{mbuf\\_size,0},{stack\\_size,49},{old\\_heap\\_size,0},{heap\\_size,81956796}]
&gt; 2013-08-05 18:01:15.729 [info] 
&gt; &lt;0.83.0&gt;@riak\\_core\\_sysmon\\_handler:handle\\_event:92 monitor large\\_heap 
&gt; &lt;0.12133.557&gt; 
&gt; [{initial\\_call,{riak\\_api\\_pb\\_server,init,1}},{almost\\_current\\_function,{riak\\_kv\\_pb,encode,2}},{message\\_queue\\_len,0}]
&gt; 
&gt; [{old\\_heap\\_block\\_size,0},{heap\\_block\\_size,74732575},{mbuf\\_size,0},{stack\\_size,81},{old\\_heap\\_size,0},{heap\\_size,42778355}]
&gt; 2013-08-05 18:01:15.878 [info] 
&gt; &lt;0.83.0&gt;@riak\\_core\\_sysmon\\_handler:handle\\_event:92 monitor large\\_heap 
&gt; &lt;0.14832.557&gt; 
&gt; [{initial\\_call,{riak\\_kv\\_get\\_fsm,init,1}},{almost\\_current\\_function,{riak\\_object,encode\\_maybe\\_binary,1}},{message\\_queue\\_len,1}]
&gt; 
&gt; [{old\\_heap\\_block\\_size,0},{heap\\_block\\_size,116769640},{mbuf\\_size,0},{stack\\_size,49},{old\\_heap\\_size,0},{heap\\_size,81956794}]
&gt; 2013-08-05 18:01:15.878 [info] 
&gt; &lt;0.83.0&gt;@riak\\_core\\_sysmon\\_handler:handle\\_event:92 monitor large\\_heap 
&gt; &lt;0.14832.557&gt; 
&gt; [{initial\\_call,{riak\\_kv\\_get\\_fsm,init,1}},{almost\\_current\\_function,{riak\\_object,encode\\_maybe\\_binary,1}},{message\\_queue\\_len,1}]
&gt; 
&gt; [{old\\_heap\\_block\\_size,0},{heap\\_block\\_size,116769640},{mbuf\\_size,0},{stack\\_size,41},{old\\_heap\\_size,0},{heap\\_size,81956788}]
&gt; 2013-08-05 18:01:15.878 [info] 
&gt; &lt;0.83.0&gt;@riak\\_core\\_sysmon\\_handler:handle\\_event:92 monitor large\\_heap 
&gt; &lt;0.14832.557&gt; 
&gt; [{initial\\_call,{riak\\_kv\\_get\\_fsm,init,1}},{almost\\_current\\_function,{riak\\_object,encode\\_maybe\\_binary,1}},{message\\_queue\\_len,1}]
&gt; 
&gt; [{old\\_heap\\_block\\_size,0},{heap\\_block\\_size,116769640},{mbuf\\_size,0},{stack\\_size,52},{old\\_heap\\_size,0},{heap\\_size,81956774}]
&gt; 2013-08-05 18:01:15.878 [info] 
&gt; &lt;0.83.0&gt;@riak\\_core\\_sysmon\\_handler:handle\\_event:92 monitor large\\_heap 
&gt; &lt;0.14832.557&gt; 
&gt; [{initial\\_call,{riak\\_kv\\_get\\_fsm,init,1}},{almost\\_current\\_function,{riak\\_object,encode\\_maybe\\_binary,1}},{message\\_queue\\_len,1}]
&gt; 
&gt; [{old\\_heap\\_block\\_size,0},{heap\\_block\\_size,116769640},{mbuf\\_size,0},{stack\\_size,52},{old\\_heap\\_size,0},{heap\\_size,81956791}]
&gt; 2013-08-05 18:01:15.880 [info] 
&gt; &lt;0.83.0&gt;@riak\\_core\\_sysmon\\_handler:handle\\_event:92 monitor large\\_heap 
&gt; &lt;0.12133.557&gt; 
&gt; [{initial\\_call,{riak\\_api\\_pb\\_server,init,1}},{almost\\_current\\_function,{lists,reverse,1}},{message\\_queue\\_len,0}]
&gt; 
&gt; [{old\\_heap\\_block\\_size,0},{heap\\_block\\_size,74732575},{mbuf\\_size,0},{stack\\_size,71},{old\\_heap\\_size,0},{heap\\_size,69315695}]
&gt; 
&gt; 
&gt; in erlang.log:
&gt; ----------------------
&gt; ===== ALIVE Mon Aug 5 17:56:26 UTC 2013
&gt; Erlang has closed
&gt; /home/fanzo/riak/rel/riak/bin/../lib/os\\_mon-2.2.9/priv/bin/memsup: Erlang has 
&gt; closed.
&gt; 
&gt; ===== Mon Aug 5 18:13:22 UTC 2013
&gt; ^M
&gt; Crash dump was written to: ./log/erl\\_crash.dump^M
&gt; eheap\\_alloc: Cannot allocate 934157120 bytes of memory (of type "heap").^M
&gt; 
&gt; with this initial crash, nothing is written to crash.log or error.log. There 
&gt; is a big honking erl\\_crash.dump file though...
&gt; 
&gt; \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*
&gt; \\* on the next node:
&gt; \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*
&gt; last lines of console.log
&gt; ----------------------
&gt; 2013-08-05 17:49:02.789 [info] 
&gt; &lt;0.3014.550&gt;@riak\\_kv\\_exchange\\_fsm:key\\_exchange:206 Repaired 1 keys during 
&gt; active anti-entropy exchange of 
&gt; {91343852333181432387730302044767688728495783936,3} between 
&gt; {91343852333181432387730302044767688728495783936,riak@riak002} and 
&gt; {114179815416476790484662877555959610910619729920,riak@riak004}
&gt; 2013-08-05 18:00:19.830 [info] &lt;0.61.0&gt; alarm\\_handler: 
&gt; {set,{system\\_memory\\_high\\_watermark,[]}}
&gt; 2013-08-05 18:00:56.272 [info] 
&gt; &lt;0.85.0&gt;@riak\\_core\\_sysmon\\_handler:handle\\_event:92 monitor large\\_heap 
&gt; &lt;0.7460.0&gt; 
&gt; [{initial\\_call,{riak\\_core\\_vnode,init,1}},{almost\\_current\\_function,{eleveldb,get,3}},{message\\_queue\\_len,6}]
&gt; 
&gt; [{old\\_heap\\_block\\_size,0},{heap\\_block\\_size,59786060},{mbuf\\_size,0},{stack\\_size,43},{old\\_heap\\_size,0},{heap\\_size,40978885}]
&gt; 
&gt; in erlang.log:
&gt; ----------------------
&gt; ===== ALIVE Mon Aug 5 17:56:55 UTC 2013
&gt; /home/fanzo/riak/rel/riak/bin/../lib/os\\_mon-2.2.9/priv/bin/memsup: Erlang has 
&gt; closed.Erlang has closed
&gt; 
&gt; 
&gt; ===== Mon Aug 5 18:13:33 UTC 2013
&gt; ^M
&gt; Crash dump was written to: ./log/erl\\_crash.dump^M
&gt; eheap\\_alloc: Cannot allocate 934157120 bytes of memory (of type "heap").^M
&gt; 
&gt; \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*
&gt; \\* on the next node:
&gt; \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*
&gt; console.log looks similar to the first
&gt; 
&gt; nothing was logged in erlang.log and there was no crash dump. Looks like the 
&gt; beam proc just crashed hard.
&gt; 
&gt; 
&gt; \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*
&gt; \\* on the next node:
&gt; \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*
&gt; last lines of console.log
&gt; ----------------------
&gt; console.log looks similar to the first
&gt; 
&gt; in erlang.log:
&gt; ----------------------
&gt; ===== Mon Aug 5 18:01:43 UTC 2013
&gt; Erlang has closed
&gt; /home/fanzo/riak/rel/riak/bin/../lib/os\\_mon-2.2.9/priv/bin/memsup: Erlang has 
&gt; closed.
&gt; ^M
&gt; Crash dump was written to: ./log/erl\\_crash.dump^M
&gt; eheap\\_alloc: Cannot allocate 2280657000 bytes of memory (of type "heap").^M
&gt; 
&gt; 
&gt; If I try to start things back up, the nodes keep periodically crashing, 
&gt; usually within a few minutes. And I can't put any more data in without it 
&gt; blowing up...
&gt; 
&gt; Any help would be appreciated.
&gt; 
&gt; Paul Ingalls
&gt; Founder & CEO Fanzo
&gt; p...@fanzo.me
&gt; @paulingalls
&gt; http://www.linkedin.com/in/paulingalls
&gt; 
&gt; 
&gt; 
&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

