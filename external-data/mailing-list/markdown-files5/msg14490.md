---
title: "Re: riak handoffs stalled"
description: ""
project: community
lastmod: 2014-07-14T05:13:27-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg14490"
mailinglist_parent_id: "msg14489"
author_name: "Ciprian Manea"
project_section: "mailinglistitem"
sent_date: 2014-07-14T05:13:27-07:00
---


Hi Leonid,

Which Riak version are you running?

Have you committed\\* the cluster plan after issuing the cluster force-remove
 commands?

What is the output of $ riak-admin transfer-limit, ran from one of your
riak nodes?


\\*Do not run this command yet if you have not done it already.
Please run a riak-admin cluster plan and attach its output here.


Thanks,
Ciprian


On Mon, Jul 14, 2014 at 2:41 PM, Леонид Рябоштан &lt;
leonid.riabosh...@twiket.com&gt; wrote:

&gt; Hello, guys,
&gt;
&gt; It seems like we ran into emergency. I wonder if there can be any help on
&gt; that.
&gt;
&gt; Everything that happened below was because we were trying to rebalace
&gt; space used by nodes that we running out of space.
&gt;
&gt; Cluster is 7 machines now, member\\_status looks like:
&gt; Attempting to restart script through sudo -u riak
&gt; ================================= Membership
&gt; ==================================
&gt; Status Ring Pending Node
&gt;
&gt; -------------------------------------------------------------------------------
&gt; valid 15.6% 20.3% 'riak@192.168.135.180'
&gt; valid 0.0% 0.0% 'riak@192.168.152.90'
&gt; valid 0.0% 0.0% 'riak@192.168.153.182'
&gt; valid 26.6% 23.4% 'riak@192.168.164.133'
&gt; valid 27.3% 21.1% 'riak@192.168.177.36'
&gt; valid 8.6% 15.6% 'riak@192.168.194.138'
&gt; valid 21.9% 19.5% 'riak@192.168.194.149'
&gt;
&gt; -------------------------------------------------------------------------------
&gt; Valid:7 / Leaving:0 / Exiting:0 / Joining:0 / Down:0
&gt;
&gt; 2 nodes with 0 Ring was made to force leave the cluster, they have plenty
&gt; of data on them which is now seems to be not accessible. Handoffs are stuck
&gt; it seems. Node 'riak@192.168.152.90'(is in same situation as '
&gt; riak@192.168.153.182') tries to handoff partitions to '
&gt; riak@192.168.164.133' but fails for unknown reason after huge
&gt; timeouts(from 5 to 40 minutes). Partition it's trying to move is about 10Gb
&gt; in size. It grows slowly on target node, but probably it's just usual
&gt; writes from normal operation. It doesn't get any smaller on source node.
&gt;
&gt; I wonder is there any way to let cluster know that we want those nodes to
&gt; be actually members of source node and there's no actual need to transfer
&gt; them? How to redo cluster ownership balance? Revert this force-leave stuff.
&gt;
&gt; Thank you,
&gt; Leonid
&gt;
&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;
&gt;
\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

