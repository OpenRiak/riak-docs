---
title: "Re: Delete keys still in Riak db"
description: ""
project: community
lastmod: 2013-03-07T12:52:03-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg10355"
mailinglist_parent_id: "msg10353"
author_name: "John Daily"
project_section: "mailinglistitem"
sent_date: 2013-03-07T12:52:03-08:00
---


Thanks for the detailed explanation of what you're seeing. As it turns out, 
your results are typical.

Usually the deletion happens automatically across all nodes, but as you've 
seen, there's no guarantee of that. Reading a key will trigger read repair to 
clean up lingering objects, and is effectively the only way to do so.

Active anti-entropy in Riak 1.3.0 has most of what's necessary to automatically 
clean up all deleted objects, but that is not a current feature. I'll make sure 
a ticket is filed with that feature request.

-John Daily
Technical Evangelist
jda...@basho.com


On Mar 7, 2013, at 3:10 PM, Daniel Iwan  wrote:

&gt; In our tests we are adding 3000 keys into 3-node Riak db right after nodes
&gt; have joined. 
&gt; For each key one node reads it and modifies it and another node does the same 
&gt; but also deletes the key when it sees other change (key is no longer needed). 
&gt; After all keys are processed our test framework checks if all keys have been 
&gt; processed and deleted (bucket listing)
&gt; 
&gt; Now, since deletion is not instant in Riak I believe here is 3 second delay, 
&gt; we are doing that check in a loop listing and counting number of keys in a 
&gt; bucket to check the progress. So far we've seen number of keys going down, 
&gt; eventually reaching zero. Sometimes the number of keys remained greater than 
&gt; zero, but after attempt to get a value we got null (Java client) which 
&gt; indicates key has been deleted.
&gt; It's not perfect but it worked so far.
&gt; 
&gt; When we changed to ring size 512 things got bit hairy. 
&gt; Our test fails and we have 24, 37 or 44 keys hanging around never going down 
&gt; to zero, but rather going up and down between those numbers.
&gt; 
&gt; Test fails eventually. After that when I checked riak-admin transfer I could 
&gt; see 96 partitions to be transfer on every node. BTW Active Transfer section 
&gt; was showing nothing (I'm pretty sure there should be MB/s now) 
&gt; In the end partitions pending for transfer hit zero few minuter after test 
&gt; failed but that did not change anything.
&gt; Listing keys in the bucket via http client showed between 24-44 keys.
&gt; Manual reading via http client shows keys does not exist, and it looks like 
&gt; read-repair removes it, but somehow number of keys cannot reach 0
&gt; 
&gt; When those keys will be removed? Do I need to get every key to trigger 
&gt; read-repair?
&gt; Any suggestions what to change to improve that behaviour?
&gt; 
&gt; Bucket has allow\\_multi set to true, which I believe is necessary for that 
&gt; kind of usage.
&gt; Riak db is in version 1.2.1 and backend LevelDB
&gt; 
&gt; Regards
&gt; Daniel
&gt; 
&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com


\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

