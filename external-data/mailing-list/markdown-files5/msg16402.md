---
title: "Re: why leaving riak cluster so slowly and how to accelerate the speed"
description: ""
project: community
lastmod: 2015-08-11T04:55:03-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg16402"
mailinglist_parent_id: "msg16401"
author_name: "changmao wang"
project_section: "mailinglistitem"
sent_date: 2015-08-11T04:55:03-07:00
---


1. About backuping new nodes of four and then using 'riak-admin
force-replace'. what's the status of new added nodes?
as you know, we want to replace one of leaving nodes.

2. what's the risk of 'riak-admin force-remove' 'riak@10.21.136.91' without
backup?
As you know, now the node(riak@10.21.136.91) is a member of the cluster,
and keeping almost 2.5TB data, maybe 10 percent of the whole cluster.



On Tue, Aug 11, 2015 at 7:32 PM, Dmitri Zagidulin 
wrote:

&gt; 1. How to force leave "leaving"'s nodes without data loss?
&gt;
&gt; This depends on - did you back up the data directory of the 4 new nodes,
&gt; before you reformatted them?
&gt; If you backed them up (and then restored the data directory once you
&gt; reformatted them), you can try:
&gt;
&gt; riak-admin force-replace 'riak@10.21.136.91' 'riak@ address is for that node&gt;'
&gt; (same for the other 3)
&gt;
&gt; If you did not back up those nodes, the only thing you can do is force
&gt; them to leave, and then join the new ones. So, for each of the 4:
&gt;
&gt; riak-admin force-remove 'riak@10.21.136.91' 'riak@10.21.136.66'
&gt; (same for the other 3)
&gt;
&gt; In either case, after force-replacing or force-removing, you have to join
&gt; the new nodes to the cluster, before you commit.
&gt;
&gt; riak-admin join 'riak@new node' 'riak@10.21.136.66'
&gt; (same for the other 3)
&gt; and finally:
&gt; riak-cluster plan
&gt; riak-cluster commit
&gt;
&gt; As for the error, the reason you're seeing it, is because the other nodes
&gt; can't contact the 4 that are supposed to be leaving. (Since you wiped them).
&gt; The amount of time that passed doesn't matter, the cluster will be waiting
&gt; for those nodes to leave indefinitely, unless you force-remove or
&gt; force-replace.
&gt;
&gt;
&gt;
&gt; On Tue, Aug 11, 2015 at 1:32 AM, changmao wang 
&gt; wrote:
&gt;
&gt;&gt; HI Dmitri,
&gt;&gt;
&gt;&gt; For your question,
&gt;&gt; 3) Re-formatted those four nodes and re-installed Riak. Here is where it
&gt;&gt; gets tricky though. Several questions for you:
&gt;&gt; - Did you attempt to re-join those 4 reinstalled nodes into the cluster?
&gt;&gt; What was the output of the cluster join and cluster plan commands?
&gt;&gt; - Did the IP address change, after they were reformatted? If so, you
&gt;&gt; probably need to use something like 'reip' at this point:
&gt;&gt; http://docs.basho.com/riak/latest/ops/running/tools/riak-admin/#reip
&gt;&gt;
&gt;&gt; I did NOT try to re-join those 4 re-join those 4 reinstalled nodes into
&gt;&gt; the cluster. As you know, member-status shows 'they're leaving" as below:
&gt;&gt; riak-admin member-status
&gt;&gt; ================================= Membership
&gt;&gt; ==================================
&gt;&gt; Status Ring Pending Node
&gt;&gt;
&gt;&gt; -------------------------------------------------------------------------------
&gt;&gt; leaving 10.9% 10.9% 'riak@10.21.136.91'
&gt;&gt; leaving 9.4% 10.9% 'riak@10.21.136.92'
&gt;&gt; leaving 7.8% 10.9% 'riak@10.21.136.93'
&gt;&gt; leaving 7.8% 10.9% 'riak@10.21.136.94'
&gt;&gt; valid 10.9% 10.9% 'riak@10.21.136.66'
&gt;&gt; valid 10.9% 10.9% 'riak@10.21.136.71'
&gt;&gt; valid 14.1% 10.9% 'riak@10.21.136.76'
&gt;&gt; valid 17.2% 12.5% 'riak@10.21.136.81'
&gt;&gt; valid 10.9% 10.9% 'riak@10.21.136.86'
&gt;&gt;
&gt;&gt; -------------------------------------------------------------------------------
&gt;&gt; Valid:5 / Leaving:4 / Exiting:0 / Joining:0 / Down:0
&gt;&gt;
&gt;&gt; two weeks elapsed, 'riak-admin member-status' shows same result. I don't
&gt;&gt; know which step ring hand off?
&gt;&gt;
&gt;&gt; I did not changed the IP address of four newly adding nodes.
&gt;&gt;
&gt;&gt; My questions:
&gt;&gt;
&gt;&gt; 1. How to force leave "leaving"'s nodes without data loss?
&gt;&gt; 2. I have found some errors related to handoff of partition in
&gt;&gt; /etc/riak/log/errors.
&gt;&gt; Details are as below:
&gt;&gt;
&gt;&gt; 2015-07-30 16:04:33.643 [error]
&gt;&gt; &lt;0.12872.15&gt;@riak\\_core\\_handoff\\_sender:start\\_fold:262 ownership\\_transfer
&gt;&gt; transfer of riak\\_kv\\_vnode from 'riak@10.21.136.76'
&gt;&gt; 45671926166590716193865151022383844364247891968 to 'riak@10.21.136.93'
&gt;&gt; 45671926166590716193865151022383844364247891968 failed because of enotconn
&gt;&gt; 2015-07-30 16:04:33.643 [error]
&gt;&gt; &lt;0.197.0&gt;@riak\\_core\\_handoff\\_manager:handle\\_info:289 An outbound handoff of
&gt;&gt; partition riak\\_kv\\_vnode 45671926166590716193865151022383844364247891968 was
&gt;&gt; terminated for reason: {shutdown,{error,enotconn}}
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt; I have searched it with google and found related articles. However,
&gt;&gt; there's no solution.
&gt;&gt;
&gt;&gt; http://lists.basho.com/pipermail/riak-users\\_lists.basho.com/2014-October/016052.html
&gt;&gt;
&gt;&gt;
&gt;&gt; On Mon, Aug 10, 2015 at 10:09 PM, Dmitri Zagidulin 
&gt;&gt; wrote:
&gt;&gt;
&gt;&gt;&gt; Hi Changmao,
&gt;&gt;&gt;
&gt;&gt;&gt; The state of the cluster can be determined from running 'riak-admin
&gt;&gt;&gt; member-status' and 'riak-admin ring-status'.
&gt;&gt;&gt; If I understand the sequence of events, you:
&gt;&gt;&gt; 1) Joined four new nodes to the cluster. (Which crashed due to not
&gt;&gt;&gt; enough disk space)
&gt;&gt;&gt; 2) Removed them from the cluster via 'riak-admin cluster leave'. This
&gt;&gt;&gt; is a "planned remove" command, and expects for the nodes to gradually hand
&gt;&gt;&gt; off their partitions (to transfer ownership) before actually leaving. So
&gt;&gt;&gt; this is probably the main problem - the ring is stuck waiting for those
&gt;&gt;&gt; nodes to properly hand off.
&gt;&gt;&gt;
&gt;&gt;&gt; 3) Re-formatted those four nodes and re-installed Riak. Here is where it
&gt;&gt;&gt; gets tricky though. Several questions for you:
&gt;&gt;&gt; - Did you attempt to re-join those 4 reinstalled nodes into the cluster?
&gt;&gt;&gt; What was the output of the cluster join and cluster plan commands?
&gt;&gt;&gt; - Did the IP address change, after they were reformatted? If so, you
&gt;&gt;&gt; probably need to use something like 'reip' at this point:
&gt;&gt;&gt; http://docs.basho.com/riak/latest/ops/running/tools/riak-admin/#reip
&gt;&gt;&gt;
&gt;&gt;&gt; The 'failed because of enotconn' error message is happening because the
&gt;&gt;&gt; cluster is waiting to hand off partitions to .94, but cannot connect to it.
&gt;&gt;&gt;
&gt;&gt;&gt; Anyways, here's what I recommend. If you can lose the data, it's
&gt;&gt;&gt; probably easier to format and reinstall the whole cluster.
&gt;&gt;&gt; If not, you can 'force-remove' those four nodes, one by one (see
&gt;&gt;&gt; http://docs.basho.com/riak/latest/ops/running/cluster-admin/#force-remove
&gt;&gt;&gt; )
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; On Thu, Aug 6, 2015 at 11:55 PM, changmao wang 
&gt;&gt;&gt; wrote:
&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Dmitri,
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Thanks for your quick reply.
&gt;&gt;&gt;&gt; my question are as below:
&gt;&gt;&gt;&gt; 1. what's the current status of the whole cluster? Is't doing data
&gt;&gt;&gt;&gt; balance?
&gt;&gt;&gt;&gt; 2. there's so many errors during one of the node error log. how to
&gt;&gt;&gt;&gt; handle it?
&gt;&gt;&gt;&gt; 2015-08-05 01:38:59.717 [error]
&gt;&gt;&gt;&gt; &lt;0.23000.298&gt;@riak\\_core\\_handoff\\_sender:start\\_fold:262 ownership\\_transfer
&gt;&gt;&gt;&gt; transfer of riak\\_kv\\_vnode from 'riak@10.21.136.81'
&gt;&gt;&gt;&gt; 525227150915793236229449236757414210188850757632 to 'riak@10.21.136.94'
&gt;&gt;&gt;&gt; 525227150915793236229449236757414210188850757632 failed because of enotconn
&gt;&gt;&gt;&gt; 2015-08-05 01:38:59.718 [error]
&gt;&gt;&gt;&gt; &lt;0.195.0&gt;@riak\\_core\\_handoff\\_manager:handle\\_info:289 An outbound handoff of
&gt;&gt;&gt;&gt; partition riak\\_kv\\_vnode 525227150915793236229449236757414210188850757632
&gt;&gt;&gt;&gt; was terminated for reason: {shutdown,{error,enotconn}}
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; During the last 5 days, there's no changes of the "riak-admin member
&gt;&gt;&gt;&gt; status" output.
&gt;&gt;&gt;&gt; 3. how to accelerate the data balance?
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; On Fri, Aug 7, 2015 at 6:41 AM, Dmitri Zagidulin 
&gt;&gt;&gt;&gt; wrote:
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; Ok, I think I understand so far. So what's the question?
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; On Thursday, August 6, 2015, Changmao.Wang 
&gt;&gt;&gt;&gt;&gt; wrote:
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt; Hi Riak users,
&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt; Before adding new nodes, the cluster only have five nodes. The member
&gt;&gt;&gt;&gt;&gt;&gt; list are as below:
&gt;&gt;&gt;&gt;&gt;&gt; 10.21.136.66,10.21.136.71,10.21.136.76,10.21.136.81,10.21.136.86.
&gt;&gt;&gt;&gt;&gt;&gt; We did not setup http proxy for the cluster, only one node of the
&gt;&gt;&gt;&gt;&gt;&gt; cluster provide the http service. so the CPU load is always high on this
&gt;&gt;&gt;&gt;&gt;&gt; node.
&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt; After that, I added four nodes (10.21.136.[91-94]) to those cluster.
&gt;&gt;&gt;&gt;&gt;&gt; During the ring/data balance progress, each node failed(riak stopped)
&gt;&gt;&gt;&gt;&gt;&gt; because of disk 100% full.
&gt;&gt;&gt;&gt;&gt;&gt; I used multi-disk path to "data\\_root" parameter in
&gt;&gt;&gt;&gt;&gt;&gt; '/etc/riak/app.config'. Each disk is only 580MB size.
&gt;&gt;&gt;&gt;&gt;&gt; As you know, bitcask storage engine did not support multi-disk path.
&gt;&gt;&gt;&gt;&gt;&gt; After one of the disks is 100% full, it can not switch next idle disk. So
&gt;&gt;&gt;&gt;&gt;&gt; the "riak" service is down.
&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt; After that, I removed the new add four nodes at active nodes with
&gt;&gt;&gt;&gt;&gt;&gt; "riak-admin cluster leave riak@'10.21.136.91'".
&gt;&gt;&gt;&gt;&gt;&gt; and then stop "riak" service on other active new nodes, reformat the
&gt;&gt;&gt;&gt;&gt;&gt; above new nodes with LVM disk management (bind 6 disk with virtual disk
&gt;&gt;&gt;&gt;&gt;&gt; group).
&gt;&gt;&gt;&gt;&gt;&gt; Replace the "data-root" parameter with one folder, and then start
&gt;&gt;&gt;&gt;&gt;&gt; "riak" service again. After that, the cluster began the data balance 
&gt;&gt;&gt;&gt;&gt;&gt; again.
&gt;&gt;&gt;&gt;&gt;&gt; That's the whole story.
&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt; Amao
&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt; ------------------------------
&gt;&gt;&gt;&gt;&gt;&gt; \\*From: \\*"Dmitri Zagidulin" 
&gt;&gt;&gt;&gt;&gt;&gt; \\*To: \\*"Changmao.Wang" 
&gt;&gt;&gt;&gt;&gt;&gt; \\*Sent: \\*Thursday, August 6, 2015 10:46:59 PM
&gt;&gt;&gt;&gt;&gt;&gt; \\*Subject: \\*Re: why leaving riak cluster so slowly and how to
&gt;&gt;&gt;&gt;&gt;&gt; accelerate the speed
&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt; Hi Amao,
&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt; Can you explain a bit more which steps you've taken, and what the
&gt;&gt;&gt;&gt;&gt;&gt; problem is?
&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt; Which nodes have been added, and which nodes are leaving the cluster?
&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt; On Tue, Jul 28, 2015 at 11:03 PM, Changmao.Wang &lt;
&gt;&gt;&gt;&gt;&gt;&gt; changmao.w...@datayes.com&gt; wrote:
&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Hi Raik user group,
&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt; I'm using riak and riak-cs 1.4.2. Last weekend, I added four nodes
&gt;&gt;&gt;&gt;&gt;&gt;&gt; to cluster with 5 nodes. However, it's failed with one of disks 100% 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; full.
&gt;&gt;&gt;&gt;&gt;&gt;&gt; As you know bitcask storage engine can not support multifolders.
&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt; After that, I restarted the "riak" and leave the cluster with the
&gt;&gt;&gt;&gt;&gt;&gt;&gt; command "riak-admin cluster leave" and "riak-admin cluster plan", and 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; the
&gt;&gt;&gt;&gt;&gt;&gt;&gt; commit.
&gt;&gt;&gt;&gt;&gt;&gt;&gt; However, riak is always doing KV balance after my submit leaving
&gt;&gt;&gt;&gt;&gt;&gt;&gt; command. I guess that it's doing join cluster progress.
&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Could you show us how to accelerate the leaving progress? I have
&gt;&gt;&gt;&gt;&gt;&gt;&gt; tuned the "transfer-limit" parameters on 9 nodes.
&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt; below is some commands output:
&gt;&gt;&gt;&gt;&gt;&gt;&gt; riak-admin member-status
&gt;&gt;&gt;&gt;&gt;&gt;&gt; ================================= Membership
&gt;&gt;&gt;&gt;&gt;&gt;&gt; ==================================
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Status Ring Pending Node
&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt; -------------------------------------------------------------------------------
&gt;&gt;&gt;&gt;&gt;&gt;&gt; leaving 6.3% 10.9% 'riak@10.21.136.91'
&gt;&gt;&gt;&gt;&gt;&gt;&gt; leaving 9.4% 10.9% 'riak@10.21.136.92'
&gt;&gt;&gt;&gt;&gt;&gt;&gt; leaving 6.3% 10.9% 'riak@10.21.136.93'
&gt;&gt;&gt;&gt;&gt;&gt;&gt; leaving 6.3% 10.9% 'riak@10.21.136.94'
&gt;&gt;&gt;&gt;&gt;&gt;&gt; valid 10.9% 10.9% 'riak@10.21.136.66'
&gt;&gt;&gt;&gt;&gt;&gt;&gt; valid 12.5% 10.9% 'riak@10.21.136.71'
&gt;&gt;&gt;&gt;&gt;&gt;&gt; valid 18.8% 10.9% 'riak@10.21.136.76'
&gt;&gt;&gt;&gt;&gt;&gt;&gt; valid 18.8% 12.5% 'riak@10.21.136.81'
&gt;&gt;&gt;&gt;&gt;&gt;&gt; valid 10.9% 10.9% 'riak@10.21.136.86'
&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt; riak-admin transfer\\_limit
&gt;&gt;&gt;&gt;&gt;&gt;&gt; =============================== Transfer Limit
&gt;&gt;&gt;&gt;&gt;&gt;&gt; ================================
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Limit Node
&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt; -------------------------------------------------------------------------------
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 200 'riak@10.21.136.66'
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 200 'riak@10.21.136.71'
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 100 'riak@10.21.136.76'
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 100 'riak@10.21.136.81'
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 200 'riak@10.21.136.86'
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 500 'riak@10.21.136.91'
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 500 'riak@10.21.136.92'
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 500 'riak@10.21.136.93'
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 500 'riak@10.21.136.94'
&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Any more details for your diagnosing the problem?
&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Amao
&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt;&gt;&gt;&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt;&gt;&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt;&gt;&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt;&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; --
&gt;&gt;&gt;&gt; Amao Wang
&gt;&gt;&gt;&gt; Best & Regards
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt; --
&gt;&gt; Amao Wang
&gt;&gt; Best & Regards
&gt;&gt;
&gt;
&gt;


-- 
Amao Wang
Best & Regards
\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

