---
title: "Re: Help with handling Riak disk failure"
description: ""
project: community
lastmod: 2017-09-19T15:25:40-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg18445"
mailinglist_parent_id: "msg18444"
author_name: "Leo"
project_section: "mailinglistitem"
sent_date: 2017-09-19T15:25:40-07:00
---


Okay. Please let me know the riak config parameters or other
parameters you think could make the recovery faster. For example, the
transfer-limit which can be changed used the riak-admin transfer-limit
command.

Thanks,
Leo

On Tue, Sep 19, 2017 at 2:23 PM, Bryan Hunt
 wrote:
&gt; Sorry Leo,
&gt;
&gt; That’s completely impossible to guess :-D
&gt;
&gt; Factors include - I/O, Network cards, network switch, selinux, block size, 
&gt; CPU, size of objects, number of objects, CRDT, Riak version, etc…
&gt;
&gt; Best,
&gt;
&gt; Bryan
&gt;
&gt;&gt; On 19 Sep 2017, at 18:53, Leo  wrote:
&gt;&gt;
&gt;&gt; Dear Bryan,
&gt;&gt;
&gt;&gt; Thank you very much for your answers. They are very helpful to me.
&gt;&gt; I will use more nodes (&gt;=5) in future.
&gt;&gt;
&gt;&gt; From your experience with using Riak, what would your guess be for the
&gt;&gt; time taken to finish all the AAE transfers and be done with the
&gt;&gt; recovery for about 1 TB worth of data (assuming my cluster is
&gt;&gt; otherwise completely idle without any user accessing the cluster
&gt;&gt; during this process and that I am continuously watching the transfers
&gt;&gt; and re-enabling disabled AAE trees gradually )? I am just asking for
&gt;&gt; rough estimate from your past experience ( please quote from your
&gt;&gt; experience with a difference sized cluster / data size too ). My guess
&gt;&gt; is that it will take approx. 2 days or more. Do you concur?
&gt;&gt;
&gt;&gt; Thanks,
&gt;&gt; Leo
&gt;&gt;
&gt;&gt;
&gt;&gt; On Tue, Sep 19, 2017 at 12:41 PM, Bryan Hunt
&gt;&gt;  wrote:
&gt;&gt;&gt; (0) Three nodes are insufficient, you should have 5 nodes
&gt;&gt;&gt; (1) You could iterate and read every object in the cluster - this would also
&gt;&gt;&gt; trigger read repair for every object
&gt;&gt;&gt; (2) - copied from Engel Sanchez response to a similar question April 10th
&gt;&gt;&gt; 2014 )
&gt;&gt;&gt;
&gt;&gt;&gt; \\* If AAE is disabled, you don't have to stop the node to delete the data in
&gt;&gt;&gt; the anti\\_entropy directories
&gt;&gt;&gt; \\* If AAE is enabled, deleting the AAE data in a rolling manner may trigger
&gt;&gt;&gt; an avalanche of read repairs between nodes with the bad trees and nodes
&gt;&gt;&gt; with good trees as the data seems to diverge.
&gt;&gt;&gt;
&gt;&gt;&gt; If your nodes are already up, with AAE enabled and with old incorrect trees
&gt;&gt;&gt; in the mix, there is a better way. You can dynamically disable AAE with
&gt;&gt;&gt; some console commands. At that point, without stopping the nodes, you can
&gt;&gt;&gt; delete all AAE data across the cluster. At a convenient time, re-enable
&gt;&gt;&gt; AAE. I say convenient because all trees will start to rebuild, and that
&gt;&gt;&gt; can be problematic in an overloaded cluster. Doing this over the weekend
&gt;&gt;&gt; might be a good idea unless your cluster can take the extra load.
&gt;&gt;&gt;
&gt;&gt;&gt; To dynamically disable AAE from the Riak console, you can run this command:
&gt;&gt;&gt;
&gt;&gt;&gt;&gt; riak\\_core\\_util:rpc\\_every\\_member\\_ann(riak\\_kv\\_entropy\\_manager, disable, [],
&gt;&gt;&gt; 60000).
&gt;&gt;&gt;
&gt;&gt;&gt; and enable with the similar:
&gt;&gt;&gt;
&gt;&gt;&gt;&gt; riak\\_core\\_util:rpc\\_every\\_member\\_ann(riak\\_kv\\_entropy\\_manager, enable, [],
&gt;&gt;&gt; 60000).
&gt;&gt;&gt;
&gt;&gt;&gt; That last number is just a timeout for the RPC operation. I hope this
&gt;&gt;&gt; saves you some extra load on your clusters.
&gt;&gt;&gt;
&gt;&gt;&gt; (3) That’s going to be :
&gt;&gt;&gt; (3a) List all keys using the client of your choice
&gt;&gt;&gt; (3b) Fetch each object
&gt;&gt;&gt;
&gt;&gt;&gt; https://www.tiot.jp/riak-docs/riak/kv/2.2.3/developing/usage/reading-objects/
&gt;&gt;&gt;
&gt;&gt;&gt; https://www.tiot.jp/riak-docs/riak/kv/2.2.3/developing/usage/secondary-indexes/
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; On 19 Sep 2017, at 18:31, Leo  wrote:
&gt;&gt;&gt;
&gt;&gt;&gt; Dear Riak users and experts,
&gt;&gt;&gt;
&gt;&gt;&gt; I really appreciate any help with my questions below.
&gt;&gt;&gt;
&gt;&gt;&gt; I have a 3 node Riak cluster with each having approx. 1 TB disk usage.
&gt;&gt;&gt; All of a sudden, one node's hard disk failed unrecoverably. So, I
&gt;&gt;&gt; added a new node using the following steps:
&gt;&gt;&gt;
&gt;&gt;&gt; 1) riak-admin cluster join 2) down the failed node 3) riak-admin
&gt;&gt;&gt; force-replace failed-node new-node 4) riak-admin cluster plan 5)
&gt;&gt;&gt; riak-admin cluster commit.
&gt;&gt;&gt;
&gt;&gt;&gt; This almost fixed the problem except that after lots of data transfers
&gt;&gt;&gt; and handoffs, now not all three nodes have 1 TB disk usage. Only two
&gt;&gt;&gt; of them have 1 TB disk usage. The other one is almost empty (few 10s
&gt;&gt;&gt; of GBs). This means there are no longer 3 copies on disk anymore. My
&gt;&gt;&gt; data is completely random (no two keys have same data associated with
&gt;&gt;&gt; them. So, compression of data cannot be the reason for less data on
&gt;&gt;&gt; disk),
&gt;&gt;&gt;
&gt;&gt;&gt; I also tried using the "riak-admin cluster replace failednode newnode"
&gt;&gt;&gt; command so that the leaving node handsoff data to the joining node.
&gt;&gt;&gt; This however is not helpful if the leaving node has a failed hard
&gt;&gt;&gt; disk. I want the remaining live vnodes to help the new node recreate
&gt;&gt;&gt; the lost data using their replica copies.
&gt;&gt;&gt;
&gt;&gt;&gt; I have three questions:
&gt;&gt;&gt;
&gt;&gt;&gt; 1) What commands should I run to forcefully make sure there are three
&gt;&gt;&gt; replicas on disk overall without waiting for read-repair or
&gt;&gt;&gt; anti-entropy to make three copies ? Bandwidth usage or CPU usage is
&gt;&gt;&gt; not a huge concern for me.
&gt;&gt;&gt;
&gt;&gt;&gt; 2) Also, I will be very grateful if someone lists the commands that I
&gt;&gt;&gt; can run using "riak attach" so that I can clear the AAE trees and
&gt;&gt;&gt; forcefully make sure all data has 3 copies.
&gt;&gt;&gt;
&gt;&gt;&gt; 3) I will be very thankful if someone helps me with the commands that
&gt;&gt;&gt; I should run to ensure that all data has 3 replicas on disk after the
&gt;&gt;&gt; disk failure (instead of just looking at the disk space usage in all
&gt;&gt;&gt; the nodes as hints)?
&gt;&gt;&gt;
&gt;&gt;&gt; Thanks,
&gt;&gt;&gt; Leo
&gt;&gt;&gt;
&gt;&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;

\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

