---
title: "Re: Bigger data than disk space?"
description: ""
project: community
lastmod: 2013-03-14T15:03:36-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg10445"
mailinglist_parent_id: "msg10439"
author_name: "Jeremiah Peschka"
project_section: "mailinglistitem"
sent_date: 2013-03-14T15:03:36-07:00
---


---
Jeremiah Peschka - Founder, Brent Ozar Unlimited
MCITP: SQL Server 2008, MVP
Cloudera Certified Developer for Apache Hadoop


On Thu, Mar 14, 2013 at 2:39 PM, Kevin Burton wrote:

&gt; One more question. Then how many physical nodes can go down before I loose
&gt; data? Again assuming that I have 4 physical nodes.


Assuming 4 nodes and n = 3, you could have two nodes fail (maybe) before
there's a problem. Ideally you'll have a minimum of 5 nodes in production.


&gt; In a crude sense it seems
&gt; that it would depend on how the replicas are spread out. But if you lost
&gt; the
&gt; two physical nodes that could possibly take out enough of the replicas to
&gt; cause data loss. Right? Also if you had more physical nodes although the
&gt; probability is smaller that you could lose the critical nodes you could
&gt; still lose two critical nodes and lose data. Am I understanding the
&gt; trade-offs correctly?
&gt;

Yes. I believe there are changes present in Riak 1.3 that make it less
likely that multiple vnodes storing the same bucket/key combination are on
the same physical nodes. If you need more paranoia, add more nodes.


&gt;
&gt; -----Original Message-----
&gt; From: Alexander Sicular [mailto:sicul...@gmail.com]
&gt; Sent: Thursday, March 14, 2013 4:17 PM
&gt; To: Kevin Burton
&gt; Cc: 'Mark Phillips'; riak-users@lists.basho.com
&gt; Subject: Re: Bigger data than disk space?
&gt;
&gt; You have to think at the cluster level. If you have 10 GB of data and if
&gt; your replication factor is three then your total data across the cluster
&gt; will be
&gt;
&gt; 10 GB x 3 replicas = 30 GB across the cluster
&gt;
&gt; Now, if you have four physical machines in your cluster each will be
&gt; responsible for 1/4 that data.
&gt;
&gt; 0.25 x 30 GB = 7.5 GB
&gt;
&gt; That is because the vnodes are evenly divided amongst physical machines in
&gt; the cluster.
&gt;
&gt; -Alexander Sicular
&gt;
&gt; @siculars
&gt;
&gt; On Mar 14, 2013, at 5:08 PM, "Kevin Burton" 
&gt; wrote:
&gt;
&gt; &gt; Then that is not quite as bad but still if I have 10 GB of data and to
&gt; &gt; support replication that requires 30 GB of disk space, what if I only
&gt; &gt; have
&gt; &gt; 20 GB of disk space per physical node?
&gt; &gt;
&gt; &gt; -----Original Message-----
&gt; &gt; From: Mark Phillips [mailto:m...@basho.com]
&gt; &gt; Sent: Thursday, March 14, 2013 4:05 PM
&gt; &gt; To: Kevin Burton
&gt; &gt; Cc: Alexander Sicular; riak-users@lists.basho.com
&gt; &gt; Subject: Re: Bigger data than disk space?
&gt; &gt;
&gt; &gt; Kevin,
&gt; &gt;
&gt; &gt; On Thu, Mar 14, 2013 at 1:56 PM, Kevin Burton
&gt; &gt; 
&gt; &gt; wrote:
&gt; &gt;&gt; So that is what I am missing. If each vnode keeps an entire copy of
&gt; &gt;&gt; my data and I have 4 physical node then there are 16 vnodes per
&gt; &gt;&gt; physical node. That would mean I have the data replicated 16 times
&gt; &gt;&gt; per physical node. 10 GB turns into 160GB etc. Right? So won’t I run out
&gt; of disk space?
&gt; &gt;&gt;
&gt; &gt;
&gt; &gt; Your raw data set is replicated 3 times by default. Three different
&gt; &gt; vnodes of your total (by default 64) will be responsible for each
&gt; &gt; replica. So, 10GB raw = 30GB replicated.
&gt; &gt;
&gt; &gt; Mark
&gt; &gt;
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt; From: Alexander Sicular [mailto:sicul...@gmail.com]
&gt; &gt;&gt; Sent: Thursday, March 14, 2013 3:51 PM
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt; To: Kevin Burton
&gt; &gt;&gt; Cc: riak-users@lists.basho.com
&gt; &gt;&gt; Subject: Re: Bigger data than disk space?
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt; Each vnode keeps \\_an entire copy\\_ of your data. There is no striping,
&gt; &gt;&gt; which I think you are conflating with RAID. Default replication (also
&gt; &gt;&gt; configured in etc/app.config) is set to three. In which case, three
&gt; &gt;&gt; entire copies of your data are kept on three different vnodes and if
&gt; &gt;&gt; you indeed have five physical nodes in your cluster you are
&gt; &gt;&gt; guaranteed to have each of those three vnodes on different physical
&gt; machines.
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt; -Alexander Sicular
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt; @siculars
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt; On Mar 14, 2013, at 4:42 PM, "Kevin Burton"
&gt; &gt;&gt; 
&gt; &gt;&gt; wrote:
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt; Thank you. Let me get it straight. I have a 4 node cluster (4
&gt; &gt;&gt; physical machines). If I have not made any changes to the ring size
&gt; &gt;&gt; then I have
&gt; &gt;&gt; 16
&gt; &gt;&gt; (64/4) vnodes. Each physical node stores the actual data (the value)
&gt; &gt;&gt; of about ¼ of the data size. So when querying the data with a key
&gt; &gt;&gt; given the number of vnodes it can be determined which physical
&gt; &gt;&gt; machine the
&gt; &gt; data is on.
&gt; &gt;&gt; There must be enough redundancy built in so that if one or more of
&gt; &gt;&gt; the physical machines go down the remaining physical machines can
&gt; &gt;&gt; reconstruct the values lost by the lost vnodes. Correct so far? Now
&gt; &gt;&gt; where does replication some in? The documentation indicates that
&gt; &gt;&gt; there are 3 copies of the data (default) made. How is this changed
&gt; &gt;&gt; and how can this replication of the data be taken advantage of?
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt; From: Alexander Sicular [mailto:sicul...@gmail.com]
&gt; &gt;&gt; Sent: Thursday, March 14, 2013 3:28 PM
&gt; &gt;&gt; To: Kevin Burton
&gt; &gt;&gt; Cc: riak-users@lists.basho.com
&gt; &gt;&gt; Subject: Re: Bigger data than disk space?
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt; Hi Kevin,
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt; The Riak distribution model is not based on "buckets" but rather the
&gt; &gt;&gt; hash of the bucket/key combination. That hash (and associated data)
&gt; &gt;&gt; is then allocated against a "vnode". A vnode, in turn, is one of n
&gt; &gt;&gt; where n is the ring\\_creation\\_size (default is 64, modify in
&gt; etc/app.config).
&gt; &gt;&gt; Each physical machine in a Riak cluster claims an equal share of the
&gt; &gt;&gt; ring. For example, a cluster with five machines (the recommended
&gt; &gt;&gt; minimum for a production
&gt; &gt;&gt; cluster) and the default ring\\_creation\\_size will have 64/5 vnodes per
&gt; &gt;&gt; physical machine (not sure if they round down or up but all machines
&gt; &gt;&gt; will have about the same number of vnodes). What you would do to make
&gt; &gt;&gt; more data available is either add a machine to the cluster whose
&gt; &gt;&gt; available disk space is equal or greater than the cluster member with
&gt; &gt;&gt; the least amount of total space or increase the space on all machines
&gt; &gt; already in the cluster.
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt; tl;dr add a machine to your cluster.
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt; -Alexander Sicular
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt; @siculars
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt; On Mar 14, 2013, at 3:41 PM, Kevin Burton 
&gt; &gt; wrote:
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt; I am relatively new to Riak so forgive me if this has been asked
&gt; &gt;&gt; before. I have a very thin understanding of a Riak cluster and
&gt; &gt;&gt; understand somewhat about replication. In planning I foresee a time
&gt; &gt;&gt; when the amount of data exceeds the disk space that is available to a
&gt; &gt;&gt; single node. What facilities are there to essentially “split” a
&gt; &gt;&gt; bucket across several servers? How is this handled?
&gt; &gt;&gt;
&gt; &gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; &gt;&gt; riak-users mailing list
&gt; &gt;&gt; riak-users@lists.basho.com
&gt; &gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; &gt;&gt; riak-users mailing list
&gt; &gt;&gt; riak-users@lists.basho.com
&gt; &gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt; &gt;&gt;
&gt; &gt;
&gt;
&gt;
&gt;
&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;


---
Jeremiah Peschka - Founder, Brent Ozar Unlimited
MCITP: SQL Server 2008, MVP
Cloudera Certified Developer for Apache Hadoop


On Thu, Mar 14, 2013 at 2:39 PM, Kevin Burton wrote:

&gt; One more question. Then how many physical nodes can go down before I loose
&gt; data? Again assuming that I have 4 physical nodes. In a crude sense it
&gt; seems
&gt; that it would depend on how the replicas are spread out. But if you lost
&gt; the
&gt; two physical nodes that could possibly take out enough of the replicas to
&gt; cause data loss. Right? Also if you had more physical nodes although the
&gt; probability is smaller that you could lose the critical nodes you could
&gt; still lose two critical nodes and lose data. Am I understanding the
&gt; trade-offs correctly?
&gt;
&gt; -----Original Message-----
&gt; From: Alexander Sicular [mailto:sicul...@gmail.com]
&gt; Sent: Thursday, March 14, 2013 4:17 PM
&gt; To: Kevin Burton
&gt; Cc: 'Mark Phillips'; riak-users@lists.basho.com
&gt; Subject: Re: Bigger data than disk space?
&gt;
&gt; You have to think at the cluster level. If you have 10 GB of data and if
&gt; your replication factor is three then your total data across the cluster
&gt; will be
&gt;
&gt; 10 GB x 3 replicas = 30 GB across the cluster
&gt;
&gt; Now, if you have four physical machines in your cluster each will be
&gt; responsible for 1/4 that data.
&gt;
&gt; 0.25 x 30 GB = 7.5 GB
&gt;
&gt; That is because the vnodes are evenly divided amongst physical machines in
&gt; the cluster.
&gt;
&gt; -Alexander Sicular
&gt;
&gt; @siculars
&gt;
&gt; On Mar 14, 2013, at 5:08 PM, "Kevin Burton" 
&gt; wrote:
&gt;
&gt; &gt; Then that is not quite as bad but still if I have 10 GB of data and to
&gt; &gt; support replication that requires 30 GB of disk space, what if I only
&gt; &gt; have
&gt; &gt; 20 GB of disk space per physical node?
&gt; &gt;
&gt; &gt; -----Original Message-----
&gt; &gt; From: Mark Phillips [mailto:m...@basho.com]
&gt; &gt; Sent: Thursday, March 14, 2013 4:05 PM
&gt; &gt; To: Kevin Burton
&gt; &gt; Cc: Alexander Sicular; riak-users@lists.basho.com
&gt; &gt; Subject: Re: Bigger data than disk space?
&gt; &gt;
&gt; &gt; Kevin,
&gt; &gt;
&gt; &gt; On Thu, Mar 14, 2013 at 1:56 PM, Kevin Burton
&gt; &gt; 
&gt; &gt; wrote:
&gt; &gt;&gt; So that is what I am missing. If each vnode keeps an entire copy of
&gt; &gt;&gt; my data and I have 4 physical node then there are 16 vnodes per
&gt; &gt;&gt; physical node. That would mean I have the data replicated 16 times
&gt; &gt;&gt; per physical node. 10 GB turns into 160GB etc. Right? So won’t I run out
&gt; of disk space?
&gt; &gt;&gt;
&gt; &gt;
&gt; &gt; Your raw data set is replicated 3 times by default. Three different
&gt; &gt; vnodes of your total (by default 64) will be responsible for each
&gt; &gt; replica. So, 10GB raw = 30GB replicated.
&gt; &gt;
&gt; &gt; Mark
&gt; &gt;
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt; From: Alexander Sicular [mailto:sicul...@gmail.com]
&gt; &gt;&gt; Sent: Thursday, March 14, 2013 3:51 PM
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt; To: Kevin Burton
&gt; &gt;&gt; Cc: riak-users@lists.basho.com
&gt; &gt;&gt; Subject: Re: Bigger data than disk space?
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt; Each vnode keeps \\_an entire copy\\_ of your data. There is no striping,
&gt; &gt;&gt; which I think you are conflating with RAID. Default replication (also
&gt; &gt;&gt; configured in etc/app.config) is set to three. In which case, three
&gt; &gt;&gt; entire copies of your data are kept on three different vnodes and if
&gt; &gt;&gt; you indeed have five physical nodes in your cluster you are
&gt; &gt;&gt; guaranteed to have each of those three vnodes on different physical
&gt; machines.
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt; -Alexander Sicular
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt; @siculars
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt; On Mar 14, 2013, at 4:42 PM, "Kevin Burton"
&gt; &gt;&gt; 
&gt; &gt;&gt; wrote:
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt; Thank you. Let me get it straight. I have a 4 node cluster (4
&gt; &gt;&gt; physical machines). If I have not made any changes to the ring size
&gt; &gt;&gt; then I have
&gt; &gt;&gt; 16
&gt; &gt;&gt; (64/4) vnodes. Each physical node stores the actual data (the value)
&gt; &gt;&gt; of about ¼ of the data size. So when querying the data with a key
&gt; &gt;&gt; given the number of vnodes it can be determined which physical
&gt; &gt;&gt; machine the
&gt; &gt; data is on.
&gt; &gt;&gt; There must be enough redundancy built in so that if one or more of
&gt; &gt;&gt; the physical machines go down the remaining physical machines can
&gt; &gt;&gt; reconstruct the values lost by the lost vnodes. Correct so far? Now
&gt; &gt;&gt; where does replication some in? The documentation indicates that
&gt; &gt;&gt; there are 3 copies of the data (default) made. How is this changed
&gt; &gt;&gt; and how can this replication of the data be taken advantage of?
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt; From: Alexander Sicular [mailto:sicul...@gmail.com]
&gt; &gt;&gt; Sent: Thursday, March 14, 2013 3:28 PM
&gt; &gt;&gt; To: Kevin Burton
&gt; &gt;&gt; Cc: riak-users@lists.basho.com
&gt; &gt;&gt; Subject: Re: Bigger data than disk space?
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt; Hi Kevin,
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt; The Riak distribution model is not based on "buckets" but rather the
&gt; &gt;&gt; hash of the bucket/key combination. That hash (and associated data)
&gt; &gt;&gt; is then allocated against a "vnode". A vnode, in turn, is one of n
&gt; &gt;&gt; where n is the ring\\_creation\\_size (default is 64, modify in
&gt; etc/app.config).
&gt; &gt;&gt; Each physical machine in a Riak cluster claims an equal share of the
&gt; &gt;&gt; ring. For example, a cluster with five machines (the recommended
&gt; &gt;&gt; minimum for a production
&gt; &gt;&gt; cluster) and the default ring\\_creation\\_size will have 64/5 vnodes per
&gt; &gt;&gt; physical machine (not sure if they round down or up but all machines
&gt; &gt;&gt; will have about the same number of vnodes). What you would do to make
&gt; &gt;&gt; more data available is either add a machine to the cluster whose
&gt; &gt;&gt; available disk space is equal or greater than the cluster member with
&gt; &gt;&gt; the least amount of total space or increase the space on all machines
&gt; &gt; already in the cluster.
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt; tl;dr add a machine to your cluster.
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt; -Alexander Sicular
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt; @siculars
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt; On Mar 14, 2013, at 3:41 PM, Kevin Burton 
&gt; &gt; wrote:
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt; I am relatively new to Riak so forgive me if this has been asked
&gt; &gt;&gt; before. I have a very thin understanding of a Riak cluster and
&gt; &gt;&gt; understand somewhat about replication. In planning I foresee a time
&gt; &gt;&gt; when the amount of data exceeds the disk space that is available to a
&gt; &gt;&gt; single node. What facilities are there to essentially “split” a
&gt; &gt;&gt; bucket across several servers? How is this handled?
&gt; &gt;&gt;
&gt; &gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; &gt;&gt; riak-users mailing list
&gt; &gt;&gt; riak-users@lists.basho.com
&gt; &gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; &gt;&gt; riak-users mailing list
&gt; &gt;&gt; riak-users@lists.basho.com
&gt; &gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt; &gt;&gt;
&gt; &gt;
&gt;
&gt;
&gt;
&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;
\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

