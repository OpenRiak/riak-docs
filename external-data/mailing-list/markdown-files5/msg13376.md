---
title: "Re: Single node causing cluster to be extremely slow (leveldb)"
description: ""
project: community
lastmod: 2014-01-10T06:55:57-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg13376"
mailinglist_parent_id: "msg13374"
author_name: "Matthew Von-Maszewski"
project_section: "mailinglistitem"
sent_date: 2014-01-10T06:55:57-08:00
---


Martin,

Assuming your business continues to grow, this problem will come back under 1.4 
… but not for a while. We can push the cache\\_size as far down as 8Mbytes to 
make room for a little more file cache space if needed. 

The manual tunings I gave you and the subsequent block\\_size tuning I mentioned 
are all automated in the leveldb for Riak 2.0. You should consider that 
upgrade once its available (we are code complete and testing now).

The cache sizing considerations are discussed here: 

 https://github.com/basho/leveldb/wiki/mv-flexcache

The block size considerations are discussed here:

 https://github.com/basho/leveldb/wiki/mv-dynamic-block-size

And sooner or later you are going to be asking why deletes do not free up space 
(which implies freeing up file cache space):

 https://github.com/basho/leveldb/wiki/mv-aggressive-delete


Let me know if you have further questions or concerns.

Matthew




On Jan 10, 2014, at 9:41 AM, Martin May  wrote:

&gt; Hi Matthew,
&gt; 
&gt; We applied this change to node 4, started it up, and it seems much happier 
&gt; (no crazy CPU). We’re going to keep an eye on it for a little while, and then 
&gt; apply this setting to all the other nodes as well.
&gt; 
&gt; Is there anything we can do to prevent this scenario in the future, or should 
&gt; the settings you suggested take care of that?
&gt; 
&gt; Thanks,
&gt; Martin
&gt; 
&gt; On Jan 10, 2014, at 6:42 AM, Matthew Von-Maszewski  wrote:
&gt; 
&gt;&gt; Sean,
&gt;&gt; 
&gt;&gt; I did some math based upon the app.config and LOG files. I am guessing that 
&gt;&gt; you are starting to thrash your file cache.
&gt;&gt; 
&gt;&gt; This theory should be easy to prove / disprove. On that one node, change 
&gt;&gt; the cache\\_size and max\\_open\\_files to:
&gt;&gt; 
&gt;&gt; cache\\_size 68435456
&gt;&gt; max\\_open\\_files 425
&gt;&gt; 
&gt;&gt; If I am correct, the node should come up and not cause problems. We are 
&gt;&gt; trading block cache space for file cache space. A miss in the file cache is 
&gt;&gt; far more costly than a miss in the block cache.
&gt;&gt; 
&gt;&gt; Let me know how this works for you. It is possible that we might want to 
&gt;&gt; talk about raising your block size slightly to reduce file cache overhead.
&gt;&gt; 
&gt;&gt; Matthew
&gt;&gt; 
&gt;&gt; On Jan 9, 2014, at 9:33 PM, Sean McKibben  wrote:
&gt;&gt; 
&gt;&gt;&gt; We have a 5 node cluster using elevelDB (1.4.2) and 2i, and this afternoon 
&gt;&gt;&gt; it started responding extremely slowly. CPU on member 4 was extremely high 
&gt;&gt;&gt; and we restarted that process, but it didn’t help. We temporarily shut down 
&gt;&gt;&gt; member 4 and cluster speed returned to normal, but as soon as we boot 
&gt;&gt;&gt; member 4 back up, the cluster performance goes to shit.
&gt;&gt;&gt; 
&gt;&gt;&gt; We’ve run in to this before but were able to just start with a fresh set of 
&gt;&gt;&gt; data after wiping machines as it was before we migrated to this bare-metal 
&gt;&gt;&gt; cluster. Now it is causing some pretty significant issues and we’re not 
&gt;&gt;&gt; sure what we can do to get it back to normal, many of our queues are 
&gt;&gt;&gt; filling up and we’ll probably have to take node 4 off again just so we can 
&gt;&gt;&gt; provide a regular quality of service.
&gt;&gt;&gt; 
&gt;&gt;&gt; We’ve turned off AAE on node 4 but it hasn’t helped. We have some transfers 
&gt;&gt;&gt; that need to happen but they are going very slowly.
&gt;&gt;&gt; 
&gt;&gt;&gt; 'riak-admin top’ on node 4 reports this:
&gt;&gt;&gt; Load: cpu 610 Memory: total 503852 binary 
&gt;&gt;&gt; 231544
&gt;&gt;&gt; procs 804 processes 179850 code 
&gt;&gt;&gt; 11588
&gt;&gt;&gt; runq 134 atom 533 ets 
&gt;&gt;&gt; 4581
&gt;&gt;&gt; 
&gt;&gt;&gt; Pid Name or Initial Func Time Reds Memory 
&gt;&gt;&gt; MsgQ Current Function
&gt;&gt;&gt; -------------------------------------------------------------------------------------------------------------------------------
&gt;&gt;&gt; &lt;6175.29048.3&gt; proc\\_lib:init\\_p/5 '-' 462231 51356760 
&gt;&gt;&gt; 0 mochijson2:json\\_bin\\_is\\_safe/1
&gt;&gt;&gt; &lt;6175.12281.6&gt; proc\\_lib:init\\_p/5 '-' 307183 64195856 
&gt;&gt;&gt; 1 gen\\_fsm:loop/7
&gt;&gt;&gt; &lt;6175.1581.5&gt; proc\\_lib:init\\_p/5 '-' 286143 41085600 
&gt;&gt;&gt; 0 mochijson2:json\\_bin\\_is\\_safe/1
&gt;&gt;&gt; &lt;6175.6659.0&gt; proc\\_lib:init\\_p/5 '-' 281845 13752 
&gt;&gt;&gt; 0 sext:decode\\_binary/3
&gt;&gt;&gt; &lt;6175.6666.0&gt; proc\\_lib:init\\_p/5 '-' 209113 21648 
&gt;&gt;&gt; 0 sext:decode\\_binary/3
&gt;&gt;&gt; &lt;6175.12219.6&gt; proc\\_lib:init\\_p/5 '-' 168832 16829200 
&gt;&gt;&gt; 0 riak\\_client:wait\\_for\\_query\\_results/4
&gt;&gt;&gt; &lt;6175.8403.0&gt; proc\\_lib:init\\_p/5 '-' 133333 13880 
&gt;&gt;&gt; 1 eleveldb:iterator\\_move/2
&gt;&gt;&gt; &lt;6175.8813.0&gt; proc\\_lib:init\\_p/5 '-' 119548 9000 
&gt;&gt;&gt; 1 eleveldb:iterator/3
&gt;&gt;&gt; &lt;6175.8411.0&gt; proc\\_lib:init\\_p/5 '-' 115759 34472 
&gt;&gt;&gt; 0 riak\\_kv\\_vnode:'-result\\_fun\\_ack/2-fun-0-'
&gt;&gt;&gt; &lt;6175.5679.0&gt; proc\\_lib:init\\_p/5 '-' 109577 8952 
&gt;&gt;&gt; 0 riak\\_kv\\_vnode:'-result\\_fun\\_ack/2-fun-0-'
&gt;&gt;&gt; Output server crashed: connection\\_lost
&gt;&gt;&gt; 
&gt;&gt;&gt; Based on that, is there anything anyone can think to do to try to bring 
&gt;&gt;&gt; performance back in to the land of usability? Does this thing appear to be 
&gt;&gt;&gt; something that may have been resolved in 1.4.6 or 1.4.7?
&gt;&gt;&gt; 
&gt;&gt;&gt; Only thing we can think of at this point might be to remove or force remove 
&gt;&gt;&gt; the member and join in a new freshly built one, but last time we attempted 
&gt;&gt;&gt; that (on a different cluster) our secondary indexes got irreparably damaged 
&gt;&gt;&gt; and only regained consistency when we copied every individual key to (this) 
&gt;&gt;&gt; new cluster! Not a good experience :( but i’m hopeful that 1.4.6 may have 
&gt;&gt;&gt; addressed some of our issues.
&gt;&gt;&gt; 
&gt;&gt;&gt; Any help is appreciated.
&gt;&gt;&gt; 
&gt;&gt;&gt; Thank you,
&gt;&gt;&gt; Sean McKibben
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt; 
&gt;&gt; 
&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt; riak-users mailing list
&gt;&gt; riak-users@lists.basho.com
&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt; 


\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

