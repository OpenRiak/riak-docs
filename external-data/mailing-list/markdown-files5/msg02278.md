---
title: "Re: Schema Architecture, Map Reduce & Key Lists"
description: ""
project: community
lastmod: 2011-02-11T03:13:02-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg02278"
mailinglist_parent_id: "msg02268"
author_name: "Nico Meyer"
project_section: "mailinglistitem"
sent_date: 2011-02-11T03:13:02-08:00
---


Hi Jeremiah!

Actually there should be no compaction at all if he only ever inserts
new keys, so the expire feature of bitcask won't help in this case.
Compactions/Merges only happen if keys have been updated or deleted.

Cheers,
Nico

Am Donnerstag, den 10.02.2011, 09:52 -0800 schrieb Jeremiah Peschka:
&gt; Riak 0.14 brings key filters - it's still going to take time to filter
&gt; the keys in memory, but it's an in memory operation. Using 'smart
&gt; keys' along the lines of UNIXTIMESTAMP:placement:campaign:customer you
&gt; can rapidly filter your keys using meaningful criteria and perform
&gt; MapReduce jobs on the results.
&gt; 
&gt; 
&gt; Nothing says you can't also store the same data in multiple buckets in
&gt; multiple formats to make querying easier.
&gt; 
&gt; 
&gt; In response to number 2 - there's a way to set Riak to auto expire
&gt; data from a bucket. It'll only be removed when compactions occur, but
&gt; if you're storing clickstream data that should be happen often enough.
&gt; 
&gt; -- 
&gt; Jeremiah Peschka
&gt; Microsoft SQL Server MVP
&gt; MCITP: Database Developer, DBA
&gt; 
&gt; 
&gt; On Thursday, February 10, 2011 at 9:35 AM, Mat Ellis wrote:
&gt; 
&gt; &gt; We are converting a mysql based schema to Riak using Ripple. We're
&gt; &gt; tracking a lot of clicks, and each click belongs to a cascade of
&gt; &gt; other objects:
&gt; &gt; 
&gt; &gt; 
&gt; &gt; click -&gt; placement -&gt; campaign -&gt; customer
&gt; &gt; 
&gt; &gt; 
&gt; &gt; i.e. we do a lot of operations on these clicks grouped by placement
&gt; &gt; or sets of placements.
&gt; &gt; 
&gt; &gt; 
&gt; &gt; Reading
&gt; &gt; this 
&gt; &gt; http://lists.basho.com/pipermail/riak-users\\_lists.basho.com/2010-July/001591.html
&gt; &gt; gave me pause for thought. I was hoping the time needed to crunch each 
&gt; &gt; day's data would be proportional to the volume of clicks on that day but it 
&gt; &gt; seems that it would be proportional to the total number of clicks ever.
&gt; &gt; 
&gt; &gt; 
&gt; &gt; What's the best approach here? I can see a number of 'solutions'
&gt; &gt; each of them complicated:
&gt; &gt; 
&gt; &gt; 
&gt; &gt; (1) Maintain an index of clicks by day so that we can focus our
&gt; &gt; operations on a time bound set of clicks
&gt; &gt; 
&gt; &gt; 
&gt; &gt; (2) Delete or archive clicks once they have been processed or after
&gt; &gt; a certain number of days
&gt; &gt; 
&gt; &gt; 
&gt; &gt; (3) Add many links to each placement, one per click (millions
&gt; &gt; potentially)
&gt; &gt; 
&gt; &gt; 
&gt; &gt; On a related noob-note, what would be the best way of creating a set
&gt; &gt; of the clicks for a given placement? Map Reduce or Riak Search or
&gt; &gt; some other method?
&gt; &gt; 
&gt; &gt; 
&gt; &gt; Thanks in advance.
&gt; &gt; 
&gt; &gt; 
&gt; &gt; M.
&gt; &gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; &gt; riak-users mailing list
&gt; &gt; riak-users@lists.basho.com
&gt; &gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt; &gt; 
&gt; 
&gt; 
&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com



\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

