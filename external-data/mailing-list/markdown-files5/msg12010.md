---
title: "Re: Practical Riak cluster choices in AWS (number of nodes? AZ's?)"
description: ""
project: community
lastmod: 2013-08-13T13:29:37-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg12010"
mailinglist_parent_id: "msg12009"
author_name: "Brady Wetherington"
project_section: "mailinglistitem"
sent_date: 2013-08-13T13:29:37-07:00
---


That \\*does\\* sound like an interesting way to do it. Kinda
best-of-both-worlds, depending on your backup schemes and whatnot. I'm
definitely curious to hear about how it works out for you.

-B.


On Tue, Aug 13, 2013 at 4:03 PM, Dave Martorana  wrote:

&gt; An interesting hybrid that I'm coming around to seems to be using a Unix
&gt; release - OmniOS has an AMI, for instance - and ZFS. With a large-enough
&gt; store, I can run without EBS on my nodes, and have a single ZFS backup
&gt; instance with a huge amount of slow-EBS storage for accepting ZFS snapshots.
&gt;
&gt; I'm still learning all the pieces, but luckily I have a company upstairs
&gt; from me that does a very similar thing with &gt; 300TB and is willing to help
&gt; me set up my ZFS backup infrastructure.
&gt;
&gt; Dave
&gt;
&gt;
&gt; On Mon, Aug 12, 2013 at 10:00 PM, Brady Wetherington  &gt; wrote:
&gt;
&gt;&gt; I will probably stick with EBS-store for now. I don't know how
&gt;&gt; comfortable I can get with a replica that could disappear with simply an
&gt;&gt; unintended reboot (one of my nodes just did that randomly today, for
&gt;&gt; example). Sure, I would immediately start rebuilding it as soon as that
&gt;&gt; were to happen, but we could be talking a pretty huge chunk of data that
&gt;&gt; would have to get rebuilt out of the cluster. And that sounds scary. Even
&gt;&gt; though, logically, I understand that it should not be.
&gt;&gt;
&gt;&gt; I will get there; I'm just a little cautious. As I learn Riak better and
&gt;&gt; get more comfortable with it, maybe I would be able to start to move in a
&gt;&gt; direction like that. And certainly as the performance characteristics of
&gt;&gt; EBS-volumes start to bite me in the butt; that might force me to get
&gt;&gt; comfortable with instance-store real quick. I would at least hope to be
&gt;&gt; serving a decent-sized chunk of my data from memory, however.
&gt;&gt;
&gt;&gt; As for throwing my instances in one AZ - I don't feel comfortable with
&gt;&gt; that either. I'll try out the way I'm saying and will report back - do I
&gt;&gt; end up with crazy latencies all over the map, or does it seem to "just
&gt;&gt; work?" We'll see.
&gt;&gt;
&gt;&gt; In the meantime, I still feel funny about "breaking the rules" on the
&gt;&gt; 5-node cluster policy. Given my other choices as having been kinda
&gt;&gt; nailed-down for now, what do you guys think of that?
&gt;&gt;
&gt;&gt; E.g. - should I take the risk of putting a 5th instance up in the same AZ
&gt;&gt; as one of the others, or should I just "be ok" with having 4? Or should I
&gt;&gt; do something weird like changing my 'n' value to be one fewer or something
&gt;&gt; like that? (I think, as I understand it so far, I'm really liking "n=3,
&gt;&gt; w=2, r=2" - but I could change it if it made more sense with the topology
&gt;&gt; I've selected.)
&gt;&gt;
&gt;&gt; -B.
&gt;&gt;
&gt;&gt;
&gt;&gt; Date: Sun, 11 Aug 2013 18:57:11 -0600
&gt;&gt;&gt; From: Jared Morrow 
&gt;&gt;&gt; To: Jeremiah Peschka 
&gt;&gt;&gt; Cc: riak-users 
&gt;&gt;&gt; Subject: Re: Practical Riak cluster choices in AWS (number of nodes?
&gt;&gt;&gt; AZ's?)
&gt;&gt;&gt; Message-ID:
&gt;&gt;&gt; &lt;
&gt;&gt;&gt; cacusovelpu8yfcivykexm9ztkhq-kdnowk1afvpflcsip2h...@mail.gmail.com&gt;
&gt;&gt;&gt; Content-Type: text/plain; charset="iso-8859-1"
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; +1 to what Jeremiah said, putting a 4 or 5 node cluster in each US West
&gt;&gt;&gt; and
&gt;&gt;&gt; US East using MDC between them would be the optimum solution. I'm also
&gt;&gt;&gt; not
&gt;&gt;&gt; buying consistent latencies between AZ's, but I've also not tested it
&gt;&gt;&gt; personally in a production environment. We have many riak-users members
&gt;&gt;&gt; on
&gt;&gt;&gt; AWS, so hopefully more experienced people will chime in.
&gt;&gt;&gt;
&gt;&gt;&gt; If you haven't seen them already, here's what I have in my "Riak on AWS"
&gt;&gt;&gt; bookmark folder:
&gt;&gt;&gt;
&gt;&gt;&gt; http://media.amazonwebservices.com/AWS\\_NoSQL\\_Riak.pdf
&gt;&gt;&gt; http://docs.basho.com/riak/latest/ops/tuning/aws/
&gt;&gt;&gt; http://basho.com/riak-on-aws-deployment-options/
&gt;&gt;&gt;
&gt;&gt;&gt; -Jared
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; On Sun, Aug 11, 2013 at 6:11 PM, Jeremiah Peschka &lt;
&gt;&gt;&gt; jeremiah.pesc...@gmail.com&gt; wrote:
&gt;&gt;&gt;
&gt;&gt;&gt; &gt; I'd be wary of using EBS backed nodes for Riak - with only a single
&gt;&gt;&gt; &gt; ethernet connection, it wil be very easy to saturate the max of
&gt;&gt;&gt; 1000mbps
&gt;&gt;&gt; &gt; available in a single AWS NIC (unless you're using cluster compute
&gt;&gt;&gt; &gt; instances). I'd be more worried about temporarily losing contact with a
&gt;&gt;&gt; &gt; node through network saturation than through AZ failure, truthfully.
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; The beauty of Riak is that a node can drop and you can replace it with
&gt;&gt;&gt; &gt; minimal fuss. Use that to your advantage and make every node in the
&gt;&gt;&gt; cluster
&gt;&gt;&gt; &gt; disposable.
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; As far as doubling up in one AZ goes - if you're worried about AZ
&gt;&gt;&gt; failure,
&gt;&gt;&gt; &gt; you should treat each AZ as a separate data center and design your
&gt;&gt;&gt; failure
&gt;&gt;&gt; &gt; scenarios accordingly. Yes, Amazon say you should put one Riak node in
&gt;&gt;&gt; each
&gt;&gt;&gt; &gt; AZ; I'm not buying that. With no guarantee around latency, and no
&gt;&gt;&gt; control
&gt;&gt;&gt; &gt; around between DCs, you need to be very careful how much of that
&gt;&gt;&gt; latency
&gt;&gt;&gt; &gt; you're willing to introduce into your application.
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; Were I in your position, I'd stand up a 5 node cluster in US-WEST-2
&gt;&gt;&gt; and be
&gt;&gt;&gt; &gt; done with it. I'd consider Riak EE for my HA/DR solution once the
&gt;&gt;&gt; business
&gt;&gt;&gt; &gt; decides that off-site HA/DR is something it wants/needs.
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; ---
&gt;&gt;&gt; &gt; Jeremiah Peschka - Founder, Brent Ozar Unlimited
&gt;&gt;&gt; &gt; MCITP: SQL Server 2008, MVP
&gt;&gt;&gt; &gt; Cloudera Certified Developer for Apache Hadoop
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; On Sun, Aug 11, 2013 at 1:52 PM, Brady Wetherington &lt;
&gt;&gt;&gt; br...@bespincorp.com&gt;wrote:
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt;&gt; Hi all -
&gt;&gt;&gt; &gt;&gt;
&gt;&gt;&gt; &gt;&gt; I have some questions about how I want my Riak stuff to work - I've
&gt;&gt;&gt; &gt;&gt; already asked these questions of some Basho people and gotten some
&gt;&gt;&gt; answers,
&gt;&gt;&gt; &gt;&gt; but thought I would toss it out into the wider world to see what you
&gt;&gt;&gt; all
&gt;&gt;&gt; &gt;&gt; have to say, too:
&gt;&gt;&gt; &gt;&gt;
&gt;&gt;&gt; &gt;&gt; First off - I know 5 instances is the "magic number" of instances to
&gt;&gt;&gt; &gt;&gt; have. If I understand the thinking here, it's that at the default
&gt;&gt;&gt; &gt;&gt; redundancy level ('n'?) of 3, it is most likely to start getting me
&gt;&gt;&gt; some
&gt;&gt;&gt; &gt;&gt; scaling (e.g., performance &gt; just that of a single node), and yet
&gt;&gt;&gt; also have
&gt;&gt;&gt; &gt;&gt; redundancy; whereby I can lose one box and not start to take a
&gt;&gt;&gt; performance
&gt;&gt;&gt; &gt;&gt; hit.
&gt;&gt;&gt; &gt;&gt;
&gt;&gt;&gt; &gt;&gt; My question is - I think I can only do 4 in a way that makes sense. I
&gt;&gt;&gt; &gt;&gt; only have 4 AZ's that I can use right now; AWS won't let me boot
&gt;&gt;&gt; instances
&gt;&gt;&gt; &gt;&gt; in 1a. My concern is if I try to do 5, I will be "doubling up" in one
&gt;&gt;&gt; AZ -
&gt;&gt;&gt; &gt;&gt; and in AWS you're almost as likely to lose an entire AZ as you are a
&gt;&gt;&gt; single
&gt;&gt;&gt; &gt;&gt; instance. And so, if I have instances doubled-up in one AZ (let's say
&gt;&gt;&gt; &gt;&gt; us-east-1e), and then I lose 1e, I've now lost two instances. What
&gt;&gt;&gt; are the
&gt;&gt;&gt; &gt;&gt; chances that all three of my replicas of some chunk of my data are on
&gt;&gt;&gt; those
&gt;&gt;&gt; &gt;&gt; two instances? I know that it's not guaranteed that all replicas are
&gt;&gt;&gt; on
&gt;&gt;&gt; &gt;&gt; separate nodes.
&gt;&gt;&gt; &gt;&gt;
&gt;&gt;&gt; &gt;&gt; So is it better for me to ignore the recommendation of 5 nodes, and
&gt;&gt;&gt; just
&gt;&gt;&gt; &gt;&gt; do 4? Or to ignore the fact that I might be doubling-up in one AZ?
&gt;&gt;&gt; Also,
&gt;&gt;&gt; &gt;&gt; another note. These are designed to be 'durable' nodes, so if one
&gt;&gt;&gt; should go
&gt;&gt;&gt; &gt;&gt; down I would expect to bring it back up \\*with\\* its data - or, if I
&gt;&gt;&gt; &gt;&gt; couldn't, I would do a force-replace or replace and rebuild it from
&gt;&gt;&gt; the
&gt;&gt;&gt; &gt;&gt; other replicas. I'm definitely not doing instance-store. So I don't
&gt;&gt;&gt; know if
&gt;&gt;&gt; &gt;&gt; that mitigates my need for a full 5 nodes. I would also consider
&gt;&gt;&gt; losing one
&gt;&gt;&gt; &gt;&gt; node to be "degraded" and would probably seek to fix that problem as
&gt;&gt;&gt; soon
&gt;&gt;&gt; &gt;&gt; as possible, so I wouldn't expect to be in that situation for long. I
&gt;&gt;&gt; would
&gt;&gt;&gt; &gt;&gt; probably tolerate a drop in performance during that time, too. (Not a
&gt;&gt;&gt; &gt;&gt; super-severe one, but 20-30 percent? Sure.)
&gt;&gt;&gt; &gt;&gt;
&gt;&gt;&gt; &gt;&gt; What do you folks think?
&gt;&gt;&gt; &gt;&gt;
&gt;&gt;&gt; &gt;&gt; -B.
&gt;&gt;&gt; &gt;&gt;
&gt;&gt;&gt; &gt;&gt;
&gt;&gt;&gt; &gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt;&gt; &gt;&gt; riak-users mailing list
&gt;&gt;&gt; &gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt; &gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt;&gt; &gt;&gt;
&gt;&gt;&gt; &gt;&gt;
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt;&gt; &gt; riak-users mailing list
&gt;&gt;&gt; &gt; riak-users@lists.basho.com
&gt;&gt;&gt; &gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; -------------- next part --------------
&gt;&gt;&gt; An HTML attachment was scrubbed...
&gt;&gt;&gt; URL: &lt;
&gt;&gt;&gt; http://lists.basho.com/pipermail/riak-users\\_lists.basho.com/attachments/20130811/d350b1f1/attachment-0001.html
&gt;&gt;&gt; &gt;
&gt;&gt;&gt;
&gt;&gt;
&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt; riak-users mailing list
&gt;&gt; riak-users@lists.basho.com
&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt;
&gt;&gt;
&gt;
\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

