---
title: "Re: Reindexing solr after backup restore"
description: ""
project: community
lastmod: 2015-04-24T18:09:21-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg16076"
mailinglist_parent_id: "msg16075"
author_name: "Zeeshan Lakhani"
project_section: "mailinglistitem"
sent_date: 2015-04-24T18:09:21-07:00
---


Awesome. Thanks.

Zeeshan Lakhani
programmer | 
software engineer at @basho | 
org. member/founder of @papers\\_we\\_love |
twitter =&gt; @zeeshanlakhani

&gt; On Apr 24, 2015, at 8:56 PM, Jason Campbell  wrote:
&gt; 
&gt; This may be a case of force-replace vs replace vs reip. I'm happy to see if 
&gt; I can get new cluster from backup to keep the Solr indexes.
&gt; 
&gt; The disk backup was all of /var/lib/riak, so definitely included the YZ 
&gt; indexes before the force-replace, and they were kept on the first node that 
&gt; was changed with reip. I stopped each node before the snapshot to ensure 
&gt; consistency. So I would expect the final restored cluster to be somewhere 
&gt; between the first and last node snapshot in terms of data, and AAE to repair 
&gt; things to a consistent state for that few minute gap.
&gt; 
&gt; I'll experiment with different methods of rebuilding the cluster on Monday 
&gt; and see if I can get it to keep the Solr indexes. Maybe moving the YZ 
&gt; indexes out of the way during the force-replace, then stopping the node and 
&gt; putting them back could help as well. I'll let you know the results of the 
&gt; experiments either way.
&gt; 
&gt; Thanks,
&gt; Jason
&gt; 
&gt;&gt; On 25 Apr 2015, at 09:25, Zeeshan Lakhani  wrote:
&gt;&gt; 
&gt;&gt; Hey Jason,
&gt;&gt; 
&gt;&gt; Yeah, nodes can normally be joined without a cluster dropping its Solr Index 
&gt;&gt; and AAE normally rebuilds the missing KV bits.
&gt;&gt; 
&gt;&gt; In the case of restoring from a backup and having missing data, we can only 
&gt;&gt; recommend a reindex (the indexes that have the issue) with aggressive AAE 
&gt;&gt; settings to speed things up. It can be pretty fast. Recreating indexes are 
&gt;&gt; cheap in Yokozuna, but are the `data/yz` directories missing from the nodes 
&gt;&gt; that were force-replaced? Unless someone else wants to chime in, I’ll gather 
&gt;&gt; more info on what occurred from the reip vs the force-replace. 
&gt;&gt; 
&gt;&gt; Zeeshan Lakhani
&gt;&gt; programmer | 
&gt;&gt; software engineer at @basho | 
&gt;&gt; org. member/founder of @papers\\_we\\_love | paperswelove.org
&gt;&gt; twitter =&gt; @zeeshanlakhani
&gt;&gt; 
&gt;&gt;&gt; On Apr 24, 2015, at 7:02 PM, Jason Campbell  wrote:
&gt;&gt;&gt; 
&gt;&gt;&gt; Is there a way to do a restore without rebuilding these indexes though? 
&gt;&gt;&gt; Obviously this could take a long time depending on the amount of indexed 
&gt;&gt;&gt; data in the cluster. It's a fairly big gotcha to say that Yokozuna fixes a 
&gt;&gt;&gt; lot of the data access issues that Riak has, but if you restore from a 
&gt;&gt;&gt; backup, it could be useless for days or weeks.
&gt;&gt;&gt; 
&gt;&gt;&gt; As far as disk consistency, the nodes were stopped during the snapshot, so 
&gt;&gt;&gt; I'm assuming on-disk it would be consistent within a single node. And 
&gt;&gt;&gt; cluster wide, I would expect the overall data to fall somewhere between the 
&gt;&gt;&gt; first and last node snapshot. AAE should still repair the bits left over, 
&gt;&gt;&gt; but it shouldn't have to rebuild the entire Solr index.
&gt;&gt;&gt; 
&gt;&gt;&gt; So the heart of the question can I join a node to a cluster without 
&gt;&gt;&gt; dropping it's Solr index? force-replace obviously doesn't work, what is 
&gt;&gt;&gt; the harm in running reip on every node instead of just the first?
&gt;&gt;&gt; 
&gt;&gt;&gt; Thanks for the help,
&gt;&gt;&gt; Jason
&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; On 25 Apr 2015, at 00:36, Zeeshan Lakhani  wrote:
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Hey Jason,
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Here’s a little more discussion on Yokozuna backup strategies: 
&gt;&gt;&gt;&gt; http://lists.basho.com/pipermail/riak-users\\_lists.basho.com/2014-January/014514.html.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Nonetheless, I wouldn’t say the behavior’s expected, but we’re going to be 
&gt;&gt;&gt;&gt; adding more to the docs on how to rebuild indexes.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; To do so, you could just remove the yz\\_anti\\_entropy directory, and make 
&gt;&gt;&gt;&gt; AAE more aggressive, via
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; ```
&gt;&gt;&gt;&gt; rpc:multicall([node() | nodes()], application, set\\_env, [yokozuna, 
&gt;&gt;&gt;&gt; anti\\_entropy\\_build\\_limit, {100, 1000}]).
&gt;&gt;&gt;&gt; rpc:multicall([node() | nodes()], application, set\\_env, [yokozuna, 
&gt;&gt;&gt;&gt; anti\\_entropy\\_concurrency, 4])
&gt;&gt;&gt;&gt; ```
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; and the indexes will rebuild. You can try to initialize the building of 
&gt;&gt;&gt;&gt; trees with `yz\\_entropy\\_mgr:init([])` via `riak attach`, but a restart 
&gt;&gt;&gt;&gt; would also kick AAE into gear. There’s a bit more related info on this 
&gt;&gt;&gt;&gt; thread: 
&gt;&gt;&gt;&gt; http://lists.basho.com/pipermail/riak-users\\_lists.basho.com/2015-March/016929.html.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Thanks.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Zeeshan Lakhani
&gt;&gt;&gt;&gt; programmer | 
&gt;&gt;&gt;&gt; software engineer at @basho | 
&gt;&gt;&gt;&gt; org. member/founder of @papers\\_we\\_love | paperswelove.org
&gt;&gt;&gt;&gt; twitter =&gt; @zeeshanlakhani
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; On Apr 24, 2015, at 1:34 AM, Jason Campbell  wrote:
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; I think I figured it out.
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; I followed this guide: 
&gt;&gt;&gt;&gt;&gt; http://docs.basho.com/riak/latest/ops/running/nodes/renaming/#Clusters-from-Backups
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; The first Riak node (changed with riak-admin reip) kept it's Solr index. 
&gt;&gt;&gt;&gt;&gt; However, the other nodes when joined via riak-admin cluster 
&gt;&gt;&gt;&gt;&gt; force-replace, dropped their Solr indexes.
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; Is this expected? If so, it should really be in the docs, and there 
&gt;&gt;&gt;&gt;&gt; should be another way to restore a cluster keeping Solr intact.
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; Also, is there a way to rebuild a Solr index?
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; Thanks,
&gt;&gt;&gt;&gt;&gt; Jason
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; On 24 Apr 2015, at 15:16, Jason Campbell  wrote:
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; I've just done a backup and restore of our production Riak cluster, and 
&gt;&gt;&gt;&gt;&gt;&gt; Yokozuna has dropped from around 125 million records to 25million. 
&gt;&gt;&gt;&gt;&gt;&gt; Obviously the IPs have changed, and although the Riak cluster is stable, 
&gt;&gt;&gt;&gt;&gt;&gt; I'm not sure Solr handled the transition as nicely.
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; Is there a way to force Solr to rebuild the indexes, or at least get 
&gt;&gt;&gt;&gt;&gt;&gt; back to the state it was in before the backup?
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; Also, is this expected behaviour?
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; Thanks,
&gt;&gt;&gt;&gt;&gt;&gt; Jason
&gt;&gt;&gt;&gt;&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt;&gt;&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt;&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt;&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt;&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt; 
&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt; riak-users mailing list
&gt;&gt; riak-users@lists.basho.com
&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt; 

\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

