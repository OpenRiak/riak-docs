---
title: "Re: Memory-backend TTL"
description: ""
project: community
lastmod: 2014-10-20T01:58:12-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg15049"
mailinglist_parent_id: "msg15023"
author_name: "Lucas Grijander"
project_section: "mailinglistitem"
sent_date: 2014-10-20T01:58:12-07:00
---


Hi Luke,

Indeed, when removed the thousands of requests, the memory is stabilized.
However the memory consumption is still very high:

riak-admin status |grep memory
memory\_total : 18494760128
memory\_processes : 145363184
memory\_processes\_used : 142886424
memory\_system : 18349396944
memory\_atom : 561761
memory\_atom\_used : 554496
memory\_binary : 7108243240
memory\_code : 13917820
memory\_ets : 11200328880

I have test also with Riak 1.4.10 and the performance is the same.

Is it normal that the "memory\_ets" has more than 10GB when we have a
"ring\_size" of 16 and a max\_memory\_per\_vnode = 250MB?

2014-10-15 20:50 GMT+02:00 Lucas Grijander :

> Hi Luke.
>
> About the first issue:
>
> - From the beginning, the servers are all running ntpd. They are Ubuntu
> 14.04 and the ntpd service is installed and running by default.
> - Anti-entropy was also disabled from the beginning:
>
> {anti\_entropy,{off,[]}},
>
>
> About the second issue, I am perplex because, after 2 restarts of the Riak
> server, just now there is a big memory consumption but is not growing like
> the previous days. The only change was to remove this code (it was used
> thousands of times/s). It was a possible workaround about the previous
> problem with the TTL but this code now is useless because the TTL is
> working fine with this node alone:
>
> self.db.delete((key)
> self.db.get(key, r=1)
>
>
> # riak-admin status|grep memory
> memory\_total : 18617871264
> memory\_processes : 224480232
> memory\_processes\_used : 222700176
> memory\_system : 18393391032
> memory\_atom : 561761
> memory\_atom\_used : 552862
> memory\_binary : 7135206080
> memory\_code : 13779729
> memory\_ets : 11209256232
>
> The problem is that I don't remember if the code change was after or
> before the second restart. I am going to restart the riak server again and
> I will report you about if the "possible memory leak" is reproduced.
>
> This is the props of the bucket:
>
> {"props":{"allow\_mult":false,"backend":"ttl\_stg","basic\_quorum":false,"big\_vclock":50,"chash\_keyfun":{"mod":"riak\_core\_util","fun":"chash\_std\_keyfun"},"dvv\_enabled":false,"dw":"quorum","last\_write\_wins":true,"linkfun":{"mod":"riak\_kv\_wm\_link\_walker","fun":"mapreduce\_linkfun"},"n\_val":1,"name":"ttl\_stg","notfound\_ok":true,"old\_vclock":86400,"postcommit":[],"pr":0,"precommit":[],"pw":0,"r":1,"rw":"quorum","small\_vclock":50,"w":1,"young\_vclock":20}}
>
> About the data that we put into the bucket are all with this schema:
>
> KEY: Alphanumeric with a length of 47
> DATA: Long integer.
>
> # riak-admin status|grep puts
> vnode\_puts : 84708
> vnode\_puts\_total : 123127430
> node\_puts : 83169
> node\_puts\_total : 123128062
>
> # riak-admin status|grep gets
> vnode\_gets : 162314
> vnode\_gets\_total : 240433213
> node\_gets : 162317
> node\_gets\_total : 240433216
>
> 2014-10-14 16:26 GMT+02:00 Luke Bakken :
>
>> Hi Lucas,
>>
>> With regard to the mysterious key deletion / resurrection, please do
>> the following:
>>
>> \* Ensure your servers are all running ntpd and have their time
>> synchronized as closely as possible.
>> \* Disable anti-entropy. I suspect this is causing the strange behavior
>> you're seeing with keys.
>>
>> Your single node cluster memory consumption issue is a bit of a
>> puzzler. I'm assuming you're using default bucket settings and not
>> using bucket types based on your previous emails, and that allow\_mult
>> is still false for your ttl\_stg bucket. Can you tell me more about the
>> data you're putting into that bucket for testing? I'll try and
>> reproduce it with my single node cluster.
>>
>> --
>> Luke Bakken
>> Engineer / CSE
>> lbak...@basho.com
>>
>>
>> On Mon, Oct 13, 2014 at 5:02 PM, Lucas Grijander
>>  wrote:
>> > Hi Luke.
>> >
>> > I really appreciate your efforts to attempt to reproduce the problem. I
>> > think that the configs are right. I have been doing also a lot of tests
>> and
>> > with 1 server/node, the memory bucket works flawlessly, as your test.
>> The
>> > Riak cluster where we have the problem has a multi\_backend with 1 memory
>> > backend, 2 bitcask backends and 2 leveldb backends. I have only changed
>> the
>> > parameter connection of the memory backend in our production code to
>> another
>> > new "cluster" with only 1 node, with the same config of Riak but with
>> only 1
>> > memory backend under the multi configuration and, as I said, all fine,
>> the
>> > problem vanished. I deduce that the problem appears only with more than
>> 1
>> > node and with a lot of requests.
>> >
>> > In my tests with the production cluster with the problem ( 4 nodes),
>> finally
>> > I realized that the TTL is working but, randomly and suddenly, KEYS
>> already
>> > deleted appear, and KEYS with correct TTL disappear :-? (Maybe something
>> > related with the some ETS internal table? ) This is the moment when I
>> can
>> > obtain KEYS already expired.
>> >
>> > In summary:
>> >
>> > - With cluster with 4 nodes (config below): All OK for a while and
>> suddenly
>> > we lost the last 20 seconds approx. of keys and OLD keys appear in the
>> list:
>> > curl -X GET http://localhost:8098/buckets/ttl\_stg/keys?keys=true
>> >
>> > buckets.default.last\_write\_wins = true
>> > bitcask.io\_mode = erlang
>> > multi\_backend.ttl\_stg.storage\_backend = memory
>> > multi\_backend.ttl\_stg.memory\_backend.ttl = 90s
>> > multi\_backend.ttl\_stg.memory\_backend.max\_memory\_per\_vnode = 25MB
>> > anti\_entropy = passive
>> > ring\_size = 256
>> >
>> > - With 1 node: All OK
>> >
>> > buckets.default.n\_val = 1
>> > buckets.default.last\_write\_wins = true
>> > buckets.default.r = 1
>> > buckets.default.w = 1
>> > multi\_backend. ttl\_stg.storage\_backend = memory
>> > multi\_backend. ttl\_stg.memory\_backend.ttl = 90s
>> > multi\_backend. ttl\_stg.memory\_backend.max\_memory\_per\_vnode = 250MB
>> > ring\_size = 16
>> >
>> >
>> >
>> > Another note: With this 1 node (32GB RAM) and only activated the memory
>> > backend I have realized than the memory consumption grows without
>> control:
>> >
>> >
>> > # riak-admin status|grep memory
>> > memory\_total : 17323130960
>> > memory\_processes : 235043016
>> > memory\_processes\_used : 233078456
>> > memory\_system : 17088087944
>> > memory\_atom : 561761
>> > memory\_atom\_used : 561127
>> > memory\_binary : 6737787976
>> > memory\_code : 14370908
>> > memory\_ets : 10295224544
>> >
>> > # # riak-admin diag -d debug
>> > [debug] Local RPC: os:getpid([]) [5000]
>> > [debug] Running shell command: ps -o pmem,rss -p 17521
>> > [debug] Shell command output:
>> > %MEM RSS
>> > 60.5 19863800
>> >
>> > Wow 18.9GB when the max\_memory\_per\_vnode = 250MB. Is far away from the
>> > value, 250\*16vnodes = 4000MB. Is it that correct?
>> >
>> > This is the riak-admin vnode-status of 1 vnode, the other 15 are with
>> > similar data:
>> >
>> > VNode: 1370157784997721485815954530671515330927436759040
>> > Backend: riak\_kv\_multi\_backend
>> > Status:
>> > [{<<"ttl\_stg">>,
>> > [{mod,riak\_kv\_memory\_backend},
>> > {data\_table\_status,[{compressed,false},
>> > {memory,1156673},
>> > {owner,<8343.9466.104>},
>> > {heir,none},
>> >
>> > {name,riak\_kv\_1370157784997721485815954530671515330927436759040},
>> > {size,29656},
>> > {node,'riak@xxxxxxxx'},
>> > {named\_table,false},
>> > {type,ordered\_set},
>> > {keypos,1},
>> > {protection,protected}]},
>> > {index\_table\_status,[{compressed,false},
>> > {memory,89},
>> > {owner,<8343.9466.104>},
>> > {heir,none},
>> >
>> > {name,riak\_kv\_1370157784997721485815954530671515330927436759040\_i},
>> > {size,0},
>> > {node,'riak@xxxxxxxxx'},
>> > {named\_table,false},
>> > {type,ordered\_set},
>> > {keypos,1},
>> > {protection,protected}]},
>> > {time\_table\_status,[{compressed,false},
>> > {memory,75968936},
>> > {owner,<8343.9466.104>},
>> > {heir,none},
>> >
>> > {name,riak\_kv\_1370157784997721485815954530671515330927436759040\_t},
>> > {size,2813661},
>> > {node,'riak@xxxxxxxxx'},
>> > {named\_table,false},
>> > {type,ordered\_set},
>> > {keypos,1},
>> > {protection,protected}]}]}]
>> >
>> > Thanks!
>> >
>> > 2014-10-13 22:30 GMT+02:00 Luke Bakken :
>> >>
>> >> Hi Lucas,
>> >>
>> >> I've tried reproducing this using a local Riak 2.0.1 node, however TTL
>> >> is working as expected.
>> >>
>> >> Here is the configuration I have in /etc/riak/riak.conf:
>> >>
>> >> storage\_backend = multi
>> >> multi\_backend.default = bc\_default
>> >>
>> >> multi\_backend.ttl\_stg.storage\_backend = memory
>> >> multi\_backend.ttl\_stg.memory\_backend.ttl = 90s
>> >> multi\_backend.ttl\_stg.memory\_backend.max\_memory\_per\_vnode = 4MB
>> >>
>> >> multi\_backend.bc\_default.storage\_backend = bitcask
>> >> multi\_backend.bc\_default.bitcask.data\_root = /var/lib/riak/bc\_default
>> >> multi\_backend.bc\_default.bitcask.io\_mode = erlang
>> >>
>> >> This translates to the following in
>> >> /var/lib/riak/generated.configs/app.2014.10.13.13.13.29.config:
>> >>
>> >> {multi\_backend\_default,<<"bc\_default">>},
>> >> {multi\_backend,
>> >> [{<<"ttl\_stg">>,riak\_kv\_memory\_backend,[{ttl,90},{max\_memory,4}]},
>> >> {<<"bc\_default">>,riak\_kv\_bitcask\_backend,
>> >> [{io\_mode,erlang},
>> >> {expiry\_grace\_time,0},
>> >> {small\_file\_threshold,10485760},
>> >> {dead\_bytes\_threshold,134217728},
>> >> {frag\_threshold,40},
>> >> {dead\_bytes\_merge\_trigger,536870912},
>> >> {frag\_merge\_trigger,60},
>> >> {max\_file\_size,2147483648},
>> >> {open\_timeout,4},
>> >> {data\_root,"/var/lib/riak/bc\_default"},
>> >> {sync\_strategy,none},
>> >> {merge\_window,always},
>> >> {max\_fold\_age,-1},
>> >> {max\_fold\_puts,0},
>> >> {expiry\_secs,-1},
>> >> {require\_hint\_crc,true}]}]}]},
>> >>
>> >> I set the bucket properties to use the ttl\_stg backend:
>> >>
>> >> root@UBUNTU-12-1:~# cat ttl\_stg-props.json
>> >> {"props":{"name":"ttl\_stg","backend":"ttl\_stg"}}
>> >>
>> >> root@UBUNTU-12-1:~# curl -XPUT -H'Content-type: application/json'
>> >> localhost:8098/buckets/ttl\_stg/props --data-ascii @ttl\_stg-props.json
>> >>
>> >> root@UBUNTU-12-1:~# curl -XGET localhost:8098/buckets/ttl\_stg/props
>> >>
>> >>
>> {"props":{"allow\_mult":false,"backend":"ttl\_stg","basic\_quorum":false,"big\_vclock":50,"chash\_keyfun":{"mod":"riak\_core\_util","fun":"chash\_std\_keyfun"},"dvv\_enabled":false,"dw":"quorum","last\_write\_wins":false,"linkfun":{"mod":"riak\_kv\_wm\_link\_walker","fun":"mapreduce\_linkfun"},"n\_val":3,"name":"ttl\_stg","notfound\_ok":true,"old\_vclock":86400,"postcommit":[],"pr":0,"precommit":[],"pw":0,"r":"quorum","rw":"quorum","small\_vclock":50,"w":"quorum","young\_vclock":20}}
>> >>
>> >>
>> >> And used the following statement to PUT test data:
>> >>
>> >> curl -XPUT localhost:8098/buckets/ttl\_stg/keys/1 -d "TEST $(date)"
>> >>
>> >> After 90 seconds, this is the response I get from Riak:
>> >>
>> >> root@UBUNTU-12-1:~# curl -XGET localhost:8098/buckets/ttl\_stg/keys/1
>> >> not found
>> >>
>> >> I would carefully check all of the app.config / riak.conf files in
>> >> your cluster, the output of "riak config effective" and the bucket
>> >> properties for those buckets you expect to be using the memory backend
>> >> with TTL. I also recommend using the localhost:8098/buckets/ endpoint
>> >> instead of the deprecated riak/ endpoint.
>> >>
>> >> Please let me know if you have additional questions.
>> >> --
>> >> Luke Bakken
>> >> Engineer / CSE
>> >> lbak...@basho.com
>> >>
>> >>
>> >> On Fri, Oct 3, 2014 at 11:32 AM, Lucas Grijander
>> >>  wrote:
>> >> > Hello,
>> >> >
>> >> > I have a memory backend in production with Riak 2.0.1, 4 servers and
>> 256
>> >> > vnodes. The servers have the same date and time.
>> >> >
>> >> > I have seen an odd performance with the ttl.
>> >> >
>> >> > This is the config:
>> >> >
>> >> > {<<"ttl\_stg">>,riak\_kv\_memory\_backend,
>> >> > [{ttl,90},{max\_memory,25}]},
>> >> >
>> >> > For example, see this GET response in one of the riak servers:
>> >> >
>> >> > < HTTP/1.1 200 OK
>> >> > < X-Riak-Vclock: a85hYGBgzGDKBVIc4otdfgR/7bfIYEpkzGNlKI1efJYvCwA=
>> >> > < Vary: Accept-Encoding
>> >> > \* Server MochiWeb/1.1 WebMachine/1.10.5 (jokes are better explained)
>> is
>> >> > not
>> >> > blacklisted
>> >> > < Server: MochiWeb/1.1 WebMachine/1.10.5 (jokes are better explained)
>> >> > < Link: ; rel="up"
>> >> > < Last-Modified: Fri, 03 Oct 2014 17:40:05 GMT
>> >> > < ETag: "3c8bGoifWcOCSVn0otD5nI"
>> >> > < Date: Fri, 03 Oct 2014 17:47:50 GMT
>> >> > < Content-Type: application/json
>> >> > < Content-Length: 17
>> >> >
>> >> > If the TTL is 90 seconds, Why the GET doesn't return "not found" if
>> the
>> >> > difference between "Last-Modified" and "Date" (of the curl request)
>> is
>> >> > greater than the TTL?
>> >> >
>> >> > Thanks in advance!
>> >> >
>> >> >
>> >> > \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
>> >> > riak-users mailing list
>> >> > riak-users@lists.basho.com
>> >> > http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
>> >> >
>> >
>> >
>> >
>> > \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
>> > riak-users mailing list
>> > riak-users@lists.basho.com
>> > http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
>> >
>>
>
>
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

