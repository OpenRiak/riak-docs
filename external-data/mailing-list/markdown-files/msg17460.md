---
title: "Re: How to cold (re)boot a cluster with already existing node data"
description: ""
project: community
lastmod: 2016-06-06T14:11:57-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg17460"
mailinglist_parent_id: "msg17459"
author_name: "DeadZen"
project_section: "mailinglistitem"
sent_date: 2016-06-06T14:11:57-07:00
---


I wasn't referring to a cluster replace. node name/reip change can be
done on all offline nodes before starting them.
They still have a cluster if you dont delete the ring data.
Having done that you actually deleted the cluster, (but not the data) when
all that occurred was an ip address change such that the node its looking
for in the dict. cant be found.

orddict,fetch,['riak@10.44.2.8'...
Youd want a reip of the old r...@10.xx to riak@10.44.2.8 iirc

then the node would boot as if nothing happened.

personally I think this state could be detected and a friendlier, I cant
find x have you recently transferred to a new node or ip? etc etc


On Monday, June 6, 2016, Sargun Dhillon  wrote:

> Two suggestions:
> 1. Use Riak-EE, and have two rings. When you do an update, copy over one
> ring to the other side after you do a "cold reboot"
> 2. Use the Riak Mesos Framework. Mesos is like K8s, but it has stateful
> storage primitives. (Link: https://github.com/basho-labs/riak-mesos)
>
> On Mon, Jun 6, 2016 at 10:37 AM, Jan-Philip Loos  > wrote:
>
>>
>>
>> On Mon, 6 Jun 2016 at 16:52 Alex Moore > > wrote:
>>
>>> Hi Jan,
>>>
>>> When you update the Kubernates nodes, do you have to do them all at once
>>> or can they be done in a rolling fashion (one after another)?
>>>
>>
>> Thnaks for your reply,
>>
>> sadly this is not possible. Kubernetes with GKE just tears all nodes
>> down, creating new nodes with new kubernets version and reschedule all
>> services on these nodes. So after an upgrade, all riak nodes are
>> stand-alone (when starting after deleting /var/lib/riak/ring)
>>
>> Greetings
>>
>> Jan
>>
>>
>>> If you can do them rolling-wise, you should be able to:
>>>
>>> For each node, one at a time:
>>> 1. Shut down Riak
>>> 2. Shutdown/restart/upgrade Kubernates
>>> 3. Start Riak
>>> 4. Use `riak-admin force-replace` to rename the old node name to the new
>>> node name
>>> 5. Repeat on remaining nodes.
>>>
>>> This is covered in "Renaming Multi-node clusters
>>> "
>>> doc.
>>>
>>> As for your current predicament, have you created any new
>>> buckets/changed bucket props in the default namespace since you restarted?
>>> Or have you only done regular operations since?
>>>
>>> Thanks,
>>> Alex
>>>
>>>
>>> On Mon, Jun 6, 2016 at 5:25 AM Jan-Philip Loos >> > wrote:
>>>
>>>> Hi,
>>>>
>>>> we are using riak in a kuberentes cluster (on GKE). Sometimes it's
>>>> necessary to reboot the complete cluster to update the kubernetes-nodes.
>>>> This results in a complete shutdown of the riak cluster and the riak-nodes
>>>> are rescheduled with a new IP. So how can I handle this situation? How can
>>>> I form a new riak cluster out of the old nodes with new names?
>>>>
>>>> The /var/lib/riak directory is persisted. I had to delete the
>>>> /var/lib/riak/ring folder otherwise "riak start" crashed with this message
>>>> (but saved the old ring state in a tar):
>>>>
>>>> {"Kernel pid
>>>>> terminated",application\_controller,"{application\_start\_failure,riak\_core,{{shutdown,{failed\_to\_start\_child,riak\_core\_broadcast,{'EXIT',{function\_clause,[{orddict,fetch,['
>>>>> riak@10.44.2.8 
>>>>> ',[]],[{file,\"orddict.erl\"},{line,72}]},{riak\_core\_broadcast,init\_peers,1,[{file,\"src/riak\_core\_broadcast.erl\"},{line,616}]},{riak\_core\_broadcast,start\_link,0,[{file,\"src/riak\_core\_broadcast.erl\"},{line,116}]},{supervisor,do\_start\_child,2,[{file,\"supervisor.erl\"},{line,310}]},{supervisor,start\_children,3,[{file,\"supervisor.erl\"},{line,293}]},{supervisor,init\_children,2,[{file,\"supervisor.erl\"},{line,259}]},{gen\_server,init\_it,6,[{file,\"gen\_server.erl\"},{line,304}]},{proc\_lib,init\_p\_do\_apply,3,[{file,\"proc\_lib.erl\"},{line,239}]}]}}}},{riak\_core\_app,start,[normal,[]]}}}"}
>>>>> Crash dump was written to: /var/log/riak/erl\_crash.dump
>>>>> Kernel pid terminated (application\_controller)
>>>>> ({application\_start\_failure,riak\_core,{{shutdown,{failed\_to\_start\_child,riak\_core\_broadcast,{'EXIT',{function\_clause,[{orddict,fetch,['
>>>>> riak@10.44.2.8 ',
>>>>
>>>>
>>>> The I formed a new cluster via join & plan & commit.
>>>>
>>>> But now, I discovered a problems with incomplete and inconsistent
>>>> partitions:
>>>>
>>>> \*$ \*curl -Ss "
>>>> http://riak.default.svc.cluster.local:8098/buckets/users/keys?keys=true"
>>>> | jq '.[] | length'
>>>>
>>>> 3064
>>>>
>>>> \*$\* curl -Ss "
>>>> http://riak.default.svc.cluster.local:8098/buckets/users/keys?keys=true"
>>>> | jq '.[] | length'
>>>>
>>>> 2987
>>>>
>>>> \*$\* curl -Ss "
>>>> http://riak.default.svc.cluster.local:8098/buckets/users/keys?keys=true"
>>>> | jq '.[] | length'
>>>>
>>>> 705
>>>>
>>>> \*$\* curl -Ss "
>>>> http://riak.default.svc.cluster.local:8098/buckets/users/keys?keys=true"
>>>> | jq '.[] | length'
>>>> 3064
>>>>
>>>> Is there a way to fix this? I guess this is caused by the missing old
>>>> ring-state?
>>>>
>>>> Greetings
>>>>
>>>> Jan
>>>>
>>> \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
>>>> riak-users mailing list
>>>> riak-users@lists.basho.com
>>>> 
>>>> http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
>>>>
>>>
>> \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
>> riak-users mailing list
>> riak-users@lists.basho.com
>> 
>> http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
>>
>>
>
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

