---
title: "Re: Handoff stalled on 1.0.2 riak cluster"
description: ""
project: community
lastmod: 2012-06-03T19:43:48-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg07599"
mailinglist_parent_id: "msg07598"
author_name: "Mark Phillips"
project_section: "mailinglistitem"
sent_date: 2012-06-03T19:43:48-07:00
---


Hi John,

Assuming things aren't back to normal... A few things:

Attach to any running node and run this:

rpc:multicall([node() | nodes()], riak\_core\_vnode\_manager, force\_handoffs, []).

This will attempt to force handoff. If this restarts handoff, you've
got new issue that we'll need to track down. Please report back if
this gets handoffs running again .

Another possible fix:

Take a look at https://github.com/basho/riak\_core/pull/153

This was fixed on 1.1, but it might be what's hitting you (though,
admittedly, your issue does seem like a perfect match for the issue
from the 1.0.2 release notes).

If this is what's ailing you, there's a work-around here:
https://github.com/basho/riak\_core/pull/153#issuecomment-4527706

If neither of these work, let us know and we'll take a deeper look.
Specifically:

a) any log files you could send along would be helpful
b) the output of the following diagnostic:

f(Members).
Members = riak\_core\_ring:all\_members(element(2,
riak\_core\_ring\_manager:get\_raw\_ring())).
[{N, rpc:call(N, riak\_core\_handoff\_manager, status, [])} || N <- Members].

Thanks, John.

Mark



On Sun, Jun 3, 2012 at 5:06 AM, John Axel Eriksson  wrote:

> Hi.
>
> We had an issue where one of the riak servers died (had to be force
> removed from cluster). After we did that things got really bad and most
> data was unreachable for hours. I added a new node to replace the old one
> at one point as well - that never got any data and even now about a day
> later it hasn't gotten any data.
> What seems to be the issue now is that there are a few nodes are waiting
> on handoff of 1 partition. When I look at ring\_status I see this:
>
> Attempting to restart script through sudo -u riak
> ================================== Claimant
> ===================================
> Claimant: 'riak@r-001.x.x.x
> Status: up
> Ring Ready: true
>
> ============================== Ownership Handoff
> ==============================
> Owner: riak@r-004.x.x.x
> Next Owner: riak@r-003.x.x.x
>
> Index: 930565495644285842450002452081070828921550798848
> Waiting on: []
> Complete: [riak\_kv\_vnode,riak\_pipe\_vnode,riak\_search\_vnode]
>
>
> -------------------------------------------------------------------------------
>
> ============================== Unreachable Nodes
> ==============================
> All nodes are up and reachable
>
>
> Ok, so it looks like the problem described in the Release Notes for 1.0.2
> here https://github.com/basho/riak/blob/1.0.2-release/RELEASE-NOTES.org.
> Unfortunately I've run that code (through riak attach) with no result.
>
> It's been in this state for 12 hours now I think. What can we do to fix
> our cluster?
>
> I upgraded to 1.0.3 hoping it would fix our problems but that didn't help.
> I cannot upgrade to 1.1.x because we mainly use Luwak for large object
> support
> and that's discontinued in 1.1.x as far as I know.
>
> Thanks for your help,
> John
>
> \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
> riak-users mailing list
> riak-users@lists.basho.com
> http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
>
>
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

