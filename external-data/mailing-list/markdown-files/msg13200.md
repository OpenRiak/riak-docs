---
title: "Re: Stalled handoffs on a prod cluster after server crash"
description: ""
project: community
lastmod: 2013-12-10T20:10:04-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg13200"
mailinglist_parent_id: "msg13198"
author_name: "Mark Phillips"
project_section: "mailinglistitem"
sent_date: 2013-12-10T20:10:04-08:00
---


Hi Ivaylo,

Is there anything useful in console.log of any (or all) the nodes? If
so, throw it in a gist and we'll take a look at it.

Mark

On Tue, Dec 10, 2013 at 1:13 PM, Jeppe Toustrup  wrote:
> Try to take a look at this thread from November where I experienced a
> similar problem:
> http://lists.basho.com/pipermail/riak-users\_lists.basho.com/2013-November/014027.html
>
> The following mails in the thread mentions things you try to correct
> the problem, and what I ended up doing with the help of Basho
> employees.
>
> --
> Jeppe Fihl Toustrup
> Operations Engineer
> Falcon Social
>
> On 10 December 2013 22:03, Ivaylo Panitchkov  wrote:
>> Hello,
>> Below is the transfers info:
>>
>> ~# riak-admin transfers
>>
>> Attempting to restart script through sudo -u riak
>> 'r...@ccc.ccc.ccc.ccc' waiting to handoff 7 partitions
>> 'r...@bbb.bbb.bbb.bbb' waiting to handoff 7 partitions
>> 'r...@aaa.aaa.aaa.aaa' waiting to handoff 5 partitions
>>
>>
>> ~# riak-admin member\_status
>> Attempting to restart script through sudo -u riak
>> ================================= Membership
>> ==================================
>> Status Ring Pending Node
>> -------------------------------------------------------------------------------
>> valid 45.3% 34.4% 'r...@aaa.aaa.aaa.aaa'
>> valid 26.6% 32.8% 'r...@bbb.bbb.bbb.bbb'
>> valid 28.1% 32.8% 'r...@ccc.ccc.ccc.ccc'
>> -------------------------------------------------------------------------------
>>
>> It's stuck with all those handoffs for few days now.
>> riak-admin ring\_status gives me the same info like the one I mentioned when
>> opened the case.
>> I noticed AAA.AAA.AAA.AAA experience more load than other servers as it's
>> responsible for almost half of the data.
>> Is it safe to add another machine to the cluster in order to relief
>> AAA.AAA.AAA.AAA even when the issue with handoffs is not yet resolved?
>>
>> Thanks,
>> Ivaylo
>
> \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
> riak-users mailing list
> riak-users@lists.basho.com
> http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

