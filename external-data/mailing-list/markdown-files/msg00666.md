---
title: "Re: Retrieving very large numbers of objects at once from Riak - wise?"
description: ""
project: community
lastmod: 2010-06-29T16:25:40-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg00666"
mailinglist_parent_id: "msg00665"
author_name: "Alexander Sicular"
project_section: "mailinglistitem"
sent_date: 2010-06-29T16:25:40-07:00
---


Theoretically, Riak can store trillions of objects because it hashes 
keys to a 160b number space. But practically there are limitations.


First is your backend store. The current default is bitcask which, 
although it has performance benefits over other backends, is limited 
by the number of keys (and other metadata) it can store in memory. 
Currently, each physical node loads all it's assigned keys - but not 
values - into ram. This means each server is limited by number of keys 
and key length (there are ways to calculate this). Other backends, 
like inno have their own issues.


Backends aside, the only way to do key retrieval/traversal is via map/ 
reduce. Of course, getting any individual record by key is very fast. 
With M/R, the map phase is executed in parallel on the machine where 
the data lives. However, all the matched data via the map phase is 
reduced on the machine that initiated the query. So basically the 
result set coming back to your reduce phase could tax memory on the 
machine coordinating the reduce. Pro tip - put as much logic as you 
can in the map phase.


What all this basically boils down to is that the more physical nodes 
you have, the smaller keyspace each individual node may claim which 
means you have more I/O to the data on disk and the less keys bitcask 
needs to load into ram. I'm sure someone from Basho will talk about 
how they are working on making bitcask better going forward.


Now the general data/schema design consideration is to chop your data 
up into buckets. Because the bucket is the main logical unit that 
wraps your keys (like a table) and is declared in the "inputs" phase 
of your m/r (you may also specify a list of keys). For instance, if 
you have log data, chop by time. One bucket for some frequency (day, 
hr, min, sec) based on volume in your use case, deterministically 
named. I would say this is the primary mechanism to manage and 
accelerate data retrieval.


Best, Alexander


@siculars on twitter
http://siculars.posterous.com

Sent from my iPhone

On Jun 29, 2010, at 16:58, Ian Clarke  wrote:

Hi, I'm evaluating Riak for an application where I need to store 
tens of millions of JSON objects, which periodically need to be 
updated in a more-or-less random access manner.


Here is the thing, these objects form the input to a machine 
learning algorithm, and so I will periodically need to retrieve all 
of them at once, so that they can be fed to this algorithm.


Is Riak well suited to this task? If not, does anyone have any 
suggestions?


Thanks,

Ian.

--
Ian Clarke
CEO, SenseArray
Email: i...@sensearray.com
Ph: +1 512 422 3588
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

