---
title: "Re: Strange spike"
description: ""
project: community
lastmod: 2012-06-01T09:45:21-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg07594"
mailinglist_parent_id: "msg07587"
author_name: "Nam Nguyen"
project_section: "mailinglistitem"
sent_date: 2012-06-01T09:45:21-07:00
---


Sounds like a good plan, Greg.

By the way, I joined back the old node into the cluster (making it 6-node 
cluster now). And it seems to me that the spike occurs only on that node.

To summarize the problem so far, for the benefits of other list members:

1. The latency spike occurred on one particular node.

2. I took that node out of the cluster, and put in another one. Spikes seemed 
to happen on all nodes when the cluster were converging and shortly after that. 
The new node seemed to perform a little bit worse than the rest of the cluster.

3. I rejoin the old node to the cluster. Spike seems to be localized to it 
again. The spike is, however, less intensive and a little bit shorter in 
duration. New node seems to be doing on par with the rest.

I'll wait for version 1.2.

Cheers,
Nam


On Jun 1, 2012, at 2:36 AM, Greg Burd wrote:

> Hey Nam,
> 
> It is safe to restart but my advice is to wait until we release 1.2.0 in the 
> next week or two. It has a boat-load of fixes to LevelDB one of which I'm 
> fairly sure is impacting you. Changing these settings as you've indicated is 
> unlikely to make a difference if my intuition is correct.
> 
> -greg 
> 
> @gregburd
> Developer Advocate, Basho Technologies | http://basho.com | @basho
> 
> 
> On Thursday, May 31, 2012 at 9:22 PM, Nam Nguyen wrote:
> 
>> Hi Seth,
>> 
>> Yes, I am using the default config.
>> 
>> Is it safe to change these values and restart riak?
>> 
>> Nam
>> 
>> On May 31, 2012, at 11:24 AM, Seth Benton wrote:
>>> Hey,
>>> 
>>> Apologies if this is the wrong place for this, but I just updated the 
>>> eLevelDB wiki page to mention randomization of the write buffer length (via 
>>> setting write\_buffer\_size\_min and write\_buffer\_size\_max). Before there was 
>>> no mention of these config parameters. Perhaps people were just using 
>>> levelDB's 4MB default buffer size, causing all the vnodes to compact at the 
>>> same time? Or are there default write\_buffer\_size\_min and 
>>> write\_buffer\_size\_max parameters under the hood?
>>> 
>>> http://wiki.basho.com/LevelDB.html
>>> 
>>> P.S. Mathew V is getting back to me shortly on changes to this page due to 
>>> changes in 1.2.
>>> 
>>> Seth
>>> (Tech Writer)
>>> 
>>> 
>>> On Thu, May 31, 2012 at 9:26 AM, Nam Nguyen >> (mailto:n...@tinyco.com)> wrote:
>>>> Hi Sean,
>>>> 
>>>> You are right. At first I thought it was localized to that one particular 
>>>> node. Now others are also exhibiting the same symptom.
>>>> 
>>>> I am putting in another node. 
>>>> 
>>>> Cheers,
>>>> Nam
>>>> 
>>>> 
>>>> On May 30, 2012, at 11:23 PM, Sean Cribbs wrote:
>>>>> Nam,
>>>>> 
>>>>> The LevelDB storage backend has a known issue where compaction can stall 
>>>>> a heavily-loaded node for a long time (we've seen 60 seconds or more in 
>>>>> production clusters). We're very sorry about this, but an improvement 
>>>>> will be available in the next release. In the meantime, DO NOT make the 
>>>>> node leave the cluster - this will only make things worse! It might be 
>>>>> worth adding another node to the cluster, but I suggest you wait until 
>>>>> the node finishes compaction.
>>>>> 
>>>>> On Wed, May 30, 2012 at 10:43 PM, Nam Nguyen >>>> (mailto:n...@tinyco.com)> wrote:
>>>>>> Hi,
>>>>>> 
>>>>>> My 5-node cluster exhibits a strange spike on one particular node.
>>>>>> 
>>>>>> Overall, the mean get time is about 1ms. This node occasionally shoots 
>>>>>> up to 40ms.
>>>>>> 
>>>>>> During those times, %iowait is still the same as it is before the spike. 
>>>>>> No error. Console log shows many lines like the below, which I don't 
>>>>>> think relevant to the spike.
>>>>>> 
>>>>>> 2012-05-30 21:29:50.591 [info] 
>>>>>> <0.72.0>@riak\_core\_sysmon\_handler:handle\_event:85 monitor long\_gc 
>>>>>> <0.938.0> 
>>>>>> [{initial\_call,{riak\_core\_vnode,init,1}},{almost\_current\_function,{gen\_fsm,loop,7}},{message\_queue\_len,0}]
>>>>>> 
>>>>>> [{timeout,185},{old\_heap\_block\_size,0},{heap\_block\_size,2584},{mbuf\_size,0},{stack\_size,55},{old\_heap\_size,0},{heap\_size,804}]
>>>>>> 
>>>>>> The cluster is set up uniformly. Ubuntu 64bit, m2.2xlarge instance. Riak 
>>>>>> 1.1.2 with LevelDB backend.
>>>>>> 
>>>>>> What would be the best course of actions for me?
>>>>>> 
>>>>>> I plan to:
>>>>>> 
>>>>>> - riak-admin leave on that node
>>>>>> - set up new instance
>>>>>> - riak-admin reip the new instance
>>>>>> - riak-admin join it to the cluster
>>>>>> 
>>>>>> Cheers,
>>>>>> Nam
>>>>>> 
>>>>>> 
>>>>>> \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
>>>>>> riak-users mailing list
>>>>>> riak-users@lists.basho.com (mailto:riak-users@lists.basho.com)
>>>>>> http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
>>>>> 
>>>>> 
>>>>> 
>>>>> 
>>>>> -- 
>>>>> Sean Cribbs 
>>>>> Software Engineer
>>>>> Basho Technologies, Inc.
>>>>> http://basho.com/
>>>> 
>>>> 
>>>> 
>>>> \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
>>>> riak-users mailing list
>>>> riak-users@lists.basho.com (mailto:riak-users@lists.basho.com)
>>>> http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
>>> 
>> 
>> 
>> \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
>> riak-users mailing list
>> riak-users@lists.basho.com (mailto:riak-users@lists.basho.com)
>> http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
> 
> 
> 


\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

