---
title: "Re: WARNING: Not all replicas will be on distinct nodes (with 5 nodes)"
description: ""
project: community
lastmod: 2013-08-08T13:39:49-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg11944"
mailinglist_parent_id: "msg11925"
author_name: "Joe Caswell"
project_section: "mailinglistitem"
sent_date: 2013-08-08T13:39:49-07:00
---


There are 2 options for that situation when riak-admin cluster plan gives
you the Not all replicas will be on distinct nodes warning.

1. You can riak-admin cluster clear, which will wipe out the plan, and the
riak-admin cluster plan again. You will get the same effect of
redistributing the vnodes without needing to join/leave.
2. Or you could change the claim algorithm. Set {choose\_claim\_fun,
{riak\_core\_claim,choose\_claim\_v3}} and
{wants\_claim\_fun,{riak\_core\_claim,wants\_claim\_v3}} in the riak\_core section
of app.config on each node and restart. Then clear and plan again, the new
plan will use the version 3 claim functions.

Joe

From: Guillermo 
Date: Wednesday, August 7, 2013 5:17 AM
To: 
Subject: WARNING: Not all replicas will be on distinct nodes (with 5 nodes)

Hi. I saw before this warning with clusters with not enough nodes.

On this case, the environment is amazon ec2 c1.xlarge machines in the same
availability zone. 5 machines. Manage with chef and the oficial riak
cookbook 2.2.0 that installs riak 1.4.0 build 1.

The process:
> 
> knife ssh "roles:riak" "riak-admin cluster join riak@node01"
> ssh node01 riak-admin cluster plan
> ssh node01 riak-admin cluster commit

In the plan face, it already says:
> 
> WARNING: Not all replicas will be on distinct nodes

And after commit, riak-admin diag confirms that:

> [warning] The following preflists do not satisfy the n\_val: [[{0,
> 'riak@node02'},
> 
> {2854495385411919762116571938898990272765493248,
> 'riak@node02'},
> 
> {5708990770823839524233143877797980545530986496,
> [....] (with
> metions to all the nodes)
> 

The solution I found was:

In node01 (or probably any). riak-admin cluster leave. Wait till transfers
finish, and join again.
After this tedious process (30 minuts every cluster commit), riak-diag no
longer mentions nothing.

I do this process to times, with the same result, and needing to apply the
same solution. 
Any way to skip this? I am doing something wrong?

Thanks.


-- 
Guillermo √Ålvarez 
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

