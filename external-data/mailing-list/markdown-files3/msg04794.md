---
title: "Re: Dropping a search index"
description: ""
project: community
lastmod: 2011-09-20T07:02:01-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg04794"
mailinglist_parent_id: "msg04728"
author_name: "Ryan Zezeski"
project_section: "mailinglistitem"
sent_date: 2011-09-20T07:02:01-07:00
---


Elias,

Index entries are removed by the Search hook during object deletion. It
sounds like you removed the Search hook, then removed the objects, and now
you have entires in your Search index for non-existing objects. If you
don't care about losing \*\*all\*\* of your indexes then you should be able to
take the cluster down, delete all the `data/merge\_index` dirs, and then
restart the cluster. At that point all your objects should be in tact but
the indexes should be gone. It's important that you take the cluster down
for this to work as merge\_index keeps some data in memory that can survive.


If you know the object ids then you could delete only the necessary indexes
via a small bit of Erlang at the riak console via `search:delete\_docs/1`.

search:delete\_docs([{<<"bucket">>, <<"key1">>}, ...])

-Ryan

On Thu, Sep 15, 2011 at 1:32 AM, Elias Levy wrote:

> Is there some way to drop a search index, other that forcibly removing the
> merge\_index data directory on each node?
>
> Change a bucket's search property to false or executing search-cmd
> uninstall do not appear to have this effect. I can continue to execute
> queries against the index. They appear to merely remove the search
> pre-commit hook on the bucket.
>
> After playing around with binary keys and determining that if you use then
> you really can't make use of any JavaScript functions, and that they lead to
> encoding errors elsewhere, I decided to switch to string keys and switch to
> the leveldb backend as its memory requirements have a less pronounced
> relationship with the number and size of keys.
>
> Alas, I must have not removed all the data from the bitcask backend before
> switching, as after switching I could query the index through the Solr API,
> and although it did not return any values, it did say there were matching
> rows.
>
> And if I execute a MR job with a JS function I get an encoding error, as
> the old binary keys are being retrieved from the index, but they cannot be
> passed to the JS function.
>
> So, is there a way to clear such an index?
>
> Elias Levy
>
> \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
> riak-users mailing list
> riak-users@lists.basho.com
> http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
>
>
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

