---
title: "Re: How to cold (re)boot a cluster with already existing node data"
description: ""
project: community
lastmod: 2016-06-06T07:35:45-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg17455"
mailinglist_parent_id: "msg17454"
author_name: "DeadZen"
project_section: "mailinglistitem"
sent_date: 2016-06-06T07:35:45-07:00
---


this might be helpful, an Omniti article.
https://omniti.com/seeds/migrating-riak-do-it-live

As to fixing this specific error. That iirc can be done doing a name change
in the ring to match your new node name. renaming the node will make that
orddict lookup succeed.
Theres a supplied admin utility for that.


On Sunday, June 5, 2016, Jan-Philip Loos  wrote:

> Hi,
>
> we are using riak in a kuberentes cluster (on GKE). Sometimes it's
> necessary to reboot the complete cluster to update the kubernetes-nodes.
> This results in a complete shutdown of the riak cluster and the riak-nodes
> are rescheduled with a new IP. So how can I handle this situation? How can
> I form a new riak cluster out of the old nodes with new names?
>
> The /var/lib/riak directory is persisted. I had to delete the
> /var/lib/riak/ring folder otherwise "riak start" crashed with this message
> (but saved the old ring state in a tar):
>
> {"Kernel pid
>> terminated",application\_controller,"{application\_start\_failure,riak\_core,{{shutdown,{failed\_to\_start\_child,riak\_core\_broadcast,{'EXIT',{function\_clause,[{orddict,fetch,['
>> riak@10.44.2.8 
>> ',[]],[{file,\"orddict.erl\"},{line,72}]},{riak\_core\_broadcast,init\_peers,1,[{file,\"src/riak\_core\_broadcast.erl\"},{line,616}]},{riak\_core\_broadcast,start\_link,0,[{file,\"src/riak\_core\_broadcast.erl\"},{line,116}]},{supervisor,do\_start\_child,2,[{file,\"supervisor.erl\"},{line,310}]},{supervisor,start\_children,3,[{file,\"supervisor.erl\"},{line,293}]},{supervisor,init\_children,2,[{file,\"supervisor.erl\"},{line,259}]},{gen\_server,init\_it,6,[{file,\"gen\_server.erl\"},{line,304}]},{proc\_lib,init\_p\_do\_apply,3,[{file,\"proc\_lib.erl\"},{line,239}]}]}}}},{riak\_core\_app,start,[normal,[]]}}}"}
>> Crash dump was written to: /var/log/riak/erl\_crash.dump
>> Kernel pid terminated (application\_controller)
>> ({application\_start\_failure,riak\_core,{{shutdown,{failed\_to\_start\_child,riak\_core\_broadcast,{'EXIT',{function\_clause,[{orddict,fetch,['
>> riak@10.44.2.8 ',
>
>
> The I formed a new cluster via join & plan & commit.
>
> But now, I discovered a problems with incomplete and inconsistent
> partitions:
>
> \*$ \*curl -Ss "
> http://riak.default.svc.cluster.local:8098/buckets/users/keys?keys=true"
> | jq '.[] | length'
>
> 3064
>
> \*$\* curl -Ss "
> http://riak.default.svc.cluster.local:8098/buckets/users/keys?keys=true"
> | jq '.[] | length'
>
> 2987
>
> \*$\* curl -Ss "
> http://riak.default.svc.cluster.local:8098/buckets/users/keys?keys=true"
> | jq '.[] | length'
>
> 705
>
> \*$\* curl -Ss "
> http://riak.default.svc.cluster.local:8098/buckets/users/keys?keys=true"
> | jq '.[] | length'
> 3064
>
> Is there a way to fix this? I guess this is caused by the missing old
> ring-state?
>
> Greetings
>
> Jan
>
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

