---
title: "Re: LevelDB parameter planning - max_open_files"
description: ""
project: community
lastmod: 2014-04-04T08:43:44-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg13998"
mailinglist_parent_id: "msg13997"
author_name: "Matthew Von-Maszewski"
project_section: "mailinglistitem"
sent_date: 2014-04-04T08:43:44-07:00
---



Copying back to mailing list for others and archive.

On Apr 4, 2014, at 11:37 AM, Oleksiy Krivoshey  wrote:

> Great! 
> 
> I'm trying 2.0 right now and have found that 'total\_leveldb\_mem' and 
> 'total\_leveldb\_mem\_percent' are really much easier to use and understand. 
> 
> Thanks!
> 
> 
> On 4 April 2014 18:14, Matthew Von-Maszewski  wrote:
> Oleksiy,
> 
> Go to step 6: "Compare Step 2 and Step 5 â€¦". There is a link to an Excel 
> spreadsheet at the end of the sentence "The above calculations are automated 
> in this memory model spreadsheet.". Forget the text and use the spreadsheet 
> (memory model spreadsheet).
> 
> Much of that text is still related to memory management of 1.2 and 1.3. 
> Seems it did not get updated to 1.4. Hmm, that might be my fault.
> 
> Answers to your comments/questions below:
> 
> 1. Step 3 on the page is just wrong with 1.4: open\_file\_memory = 
> (max\_open\_files -10) \* 4194304
> 
> 2. average\_sst\_filesize is not relevant with 1.4. It was used to estimate 
> the size of the bloom filter attached to each .sst file. There is now a 
> fixed maximum of 150,001 bytes for the bloom filter, and it is the typical 
> size for all files in levels 2 through 6.
> 
> 3. The page is attempting to estimate the total memory usage of one vnode. 
> The spreadsheet does the same. Therefore the maximum memory per either model 
> is the "working memory per vnode" in Step 5, or the "working per vnode" line 
> in the spreadsheet, multiplied by the number of vnodes active on the node 
> (server).
> 
> 
> Now let me make a related note / sales pitch. The upcoming Riak 2.0 
> eliminates all the manual calculations / planning. You tell Riak what 
> percentage of memory is allocated to leveldb. leveldb then dynamically 
> adjusts each vnode's allocation as your dataset changes and/or vnodes are 
> moved to and from the node (server). 
> 
> Matthew
> 
> 
> On Apr 4, 2014, at 5:08 AM, Oleksiy Krivoshey  wrote:
> 
>> Can someone please suggest how to understand the formula for 
>> open\_file\_memory on this page: 
>> http://docs.basho.com/riak/latest/ops/advanced/backends/leveldb/#Parameter-Planning
>> 
>> 1. It definitely lacks some brackets, the correct formula is:
>> 
>> OPEN\_FILE\_MEMORY = (max\_open\_files-10) \* (184 + (average\_sst\_filesize/2048) 
>> \* (8+((key\_size+value\_size)/2048 +1)\*0.6))
>> 
>> 
>> 2. How to estimate average\_sst\_filesize?
>> 
>> 
>> 3. does the result estimate the memory used by a single open file in any 
>> particular vnode? Or by a single vnode with max\_open\_files open? As 
>> max\_open\_files is a per vnode parameter then how to estimate the maximum 
>> memory used by leveldb if all vnodes have all max\_open\_files open? is it 
>> result\*ring\_size or result\*ring\_size\*max\_open\_files?
>> 
>> 
>> Thanks!
>> 
>> -- 
>> Oleksiy
>> \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
>> riak-users mailing list
>> riak-users@lists.basho.com
>> http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
> 
> 
> 
> 
> -- 
> Oleksiy Krivoshey

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

