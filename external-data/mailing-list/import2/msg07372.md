---
title: "Re: Reip(ing) riak node created two copies in the cluster"
description: ""
project: community
lastmod: 2012-05-02T08:51:15-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg07372"
mailinglist_parent_id: "msg07371"
author_name: "Nitish Sharma"
project_section: "mailinglistitem"
sent_date: 2012-05-02T08:51:15-07:00
---


Hi Jon,
Thanks for your input. I've already started working on that lines. 
I stopped all the nodes, moved ring directory from one node, brought that one 
up, and issued join command to one other node (after moving the ring directory) 
- node2. While they were busy re-distributing the partitions, I started another 
node (node3) and issued join command (before risk\_kv was running, since it 
takes some time to load existing data).
But after this, data handoffs are occurring only between node1 and node2. 
"member\_status" says that node 3 owns 0% of the ring and 0% are pending.
We have a lot of data - each node serves around 200 million documents. Riak 
cluster is running 1.1.2.
Any suggestions?

Cheers
Nitish
On May 2, 2012, at 5:31 PM, Jon Meredith wrote:

> Hi Nitish,
> 
> If you rebuild the cluster with the same ring size, the data will eventually 
> get back to the right place. While the rebuild is taking place you may have 
> notfounds for gets until the data has been handed off to the newly assigned 
> owner (as it will be secondary handoff, not primary ownership handoff to get 
> teh data back). If you don't have a lot of data stored in the cluster it 
> shouldn't take too long.
> 
> The process would be to stop all nodes, move the files out of the ring 
> directory to a safe place, start all nodes and rejoin. If you're using 1.1.x 
> and you have capacity in your hardware you may want to increase 
> handoff\_concurrency to something like 4 to permit more transfers to happen 
> across the cluster.
> 
> 
> Jon.
> 
> 
> 
> On Wed, May 2, 2012 at 9:05 AM, Nitish Sharma  
> wrote:
> Hi,
> We have a 12-node Riak cluster. Until now we were naming every new node as 
> riak@. We then decided to rename the all the nodes to 
> riak@, which makes troubleshooting easier.
> After issuing reip command to two nodes, we noticed in the "status" that 
> those 2 nodes were now appearing in the cluster with the old name as well as 
> the new name. Other nodes were trying to handoff partitions to the "new" 
> nodes, but apparently they were not able to. After this the whole cluster 
> went down and completely stopped responding to any read/write requests.
> member\_status displayed old Riak name in "legacy" mode. Since this is our 
> production cluster, we are desperately looking for some quick remedies. 
> Issuing "force-remove" to the old names, restarting all the nodes, changing 
> the riak names back to the old ones - none of it helped.
> Currently, we are hosting limited amount of data. Whats an elegant way to 
> recover from this mess? Would shutting off all the nodes, deleting the ring 
> directory, and again forming the cluster work?
> 
> Cheers
> Nitish
> \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
> riak-users mailing list
> riak-users@lists.basho.com
> http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
> 
> 
> 
> -- 
> Jon Meredith
> Platform Engineering Manager
> Basho Technologies, Inc.
> jmered...@basho.com
> 

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

