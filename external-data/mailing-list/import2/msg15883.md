---
title: "Re: Different numFound request to riak search"
description: ""
project: community
lastmod: 2015-03-12T08:51:29-0700
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg15883"
mailinglist_parent_id: "msg15882"
author_name: "Roma Lakotko"
project_section: "mailinglistitem"
sent_date: 2015-03-12T08:51:29-0700
---


I don't see any solr errors. But each 10-20 minutes on prod and once a day
on dev i see strange errors:

2015-03-11 09:18:10.668 [error] <0.234.0> Supervisor riak\_pipe\_fitting\_sup
had child undefined started with riak\_pipe\_fitting:start\_link() at
<0.12060.2> exit with reason noproc in context shutdown\_error
2015-03-12 13:12:05.200 [error] <0.379.0> Supervisor riak\_kv\_mrc\_sink\_sup
had child undefined started with riak\_kv\_mrc\_sink:start\_link() at
<0.6601.1> exit with reason noproc in context shutdown\_error

For both prod and dev instance values are:

anti\_entropy\_build\_limit -> {ok,{1,3600000}}
anti\_entropy\_concurrency -> {ok,2}
anti\_entropy\_tick - > undefined

I delete data folder and run init method, i'll results after it rebuild
trees.

2015-03-12 18:22 GMT+03:00 Zeeshan Lakhani :

> Are you noticing any Solr errors in the logs?
>
> For your container instance, you can attempt to clear the AAE trees and
> force a rebuild by removing the entropy directories in 
> `./data/yz\_anti\_entropy`
> and running `yz\_entropy\_mgr:init([])` via `riak attach`. Or, you can let
> AAE occur naturally (after removing the entropy data) and up the
> concurrency/build\_limit/tick (using set\_env). You can see what you’re
> current settings are by calling...
>
> ```
> riak\_core\_util:rpc\_every\_member\_ann(application, get\_env, [riak\_kv,
> anti\_entropy\_build\_limit],infinity).
> riak\_core\_util:rpc\_every\_member\_ann(application, get\_env, [riak\_kv,
> anti\_entropy\_concurrency],infinity).
> riak\_core\_util:rpc\_every\_member\_ann(application, get\_env, [yokozuna,
> anti\_entropy\_tick],infinity).
> ```
>
> … on any of the nodes. Query coverage is R=1, but the values should be
> replicated across.
>
> Thanks.
>
>
> On Mar 12, 2015, at 9:51 AM, Roma Lakotko  wrote:
>
> Hello Zeeshan.
>
> While i run queries no delete object is occurs.
>
> Stats on production and developer nodes output something like this:
> https://gist.github.com/romulka/d0254aa193a9dbb52b67
>
> On dev container:
>
> /etc/riak# grep anti\_entropy \*
> riak.conf:anti\_entropy = active
> riak.conf.dpkg-dist:anti\_entropy = active
>
> ll -h /var/lib/riak/yz\_anti\_entropy/
> total 264K
> drwxrwxr-x 66 riak riak 4.0K Sep 25 12:08 ./
> drwxr-xr-x 12 riak riak 4.0K Dec 9 12:19 ../
> drwxr-xr-x 9 riak riak 4.0K Mar 12 12:01 0/
> drwxr-xr-x 9 riak riak 4.0K Mar 12 12:01
> 1004782375664995756265033322492444576013453623296/
> drwxr-xr-x 9 riak riak 4.0K Mar 12 12:01
> 1027618338748291114361965898003636498195577569280/
> ....
>
> On prod:
>
> grep anti\_entropy \* /etc/riak/ -> empty
>
> root@riak-21:/var/lib/riak/yz\_anti\_entropy# ll -h
> total 64K
> drwxrwxr-x 16 riak riak 4.0K Dec 4 03:44 ./
> drwxr-xr-x 14 riak riak 4.0K Dec 9 12:10 ../
> drwxr-xr-x 9 riak riak 4.0K Dec 4 03:44 0/
> drwxr-xr-x 9 riak riak 4.0K Mar 12 12:57
> 1027618338748291114361965898003636498195577569280/
> ....
>
> I'm already try re-save all keys, it doesn't helps.
>
> Production cluster have 7 node, start from 3. So yes, nodes was
> added/delete sometimes.
>
> On dev, i have 1 instance in docker container, never added to cluster. But
> data in that riak is imported from production cluster a while ago.
>
> I can give you a copy of container, if you need to.
>
> Thanks,
> Ronan Lakotko
>
>
>
> 2015-03-12 16:36 GMT+03:00 Zeeshan Lakhani :
>
>> Hello Roma,
>>
>> Have you deleted this object at some point in your runs? Please make sure
>> AAE is running by checking search’s AAE status, `riak-admin search
>> aae-status`, and that data exists in the correct directory,
>> `./data/yz\_anti\_entropy` (
>> http://docs.basho.com/riak/latest/ops/advanced/configs/search/). You may
>> just need to perform a read-repair by performing a fetch of the object
>> itself first, before performing search queries again.
>>
>> Also, have you left or added nodes? I’m guessing that even your 1 node
>> instance is still running a cluster on that one node, right?
>>
>> Thanks.
>>
>> Zeeshan Lakhani
>> programmer |
>> software engineer at @basho |
>> org. member/founder of @papers\_we\_love | paperswelove.org
>> twitter => @zeeshanlakhani
>>
>> On Mar 12, 2015, at 5:59 AM, Roma Lakotko  wrote:
>>
>> Each request to riak search return different results. It's return
>> different numFound.
>>
>> I use request like this:
>>
>>
>> http://localhost:8098/search/query/assets?wt=json&q=type:\*&sort=\_yz\_rk%20asc
>>
>> If add start offset it can return:
>>
>>
>> http://localhost:8098/search/query/assets?wt=json&q=type:\*&sort=\_yz\_rk%20asc&start=1247
>>
>> "response": {
>> "numFound": 1248,
>> "start": 1247,
>> "docs": [
>> {
>> "\_yz\_id": 
>> "1\*default\*assets\*fff63ecf-a0c4-4ecf-b24d-c493ca3a302f\*44",
>> "\_yz\_rk": "fff63ecf-a0c4-4ecf-b24d-c493ca3a302f",
>> "\_yz\_rt": "default",
>> "\_yz\_rb": "assets"
>> }
>> ]
>> }
>>
>>
>> On next request it return something like this
>>
>>
>> "numFound": 1224,
>> "start": 1247,
>> "docs": []
>>
>>
>> I have 1 node installation, and no process write to Riak.
>>
>> I have same problem this production cluster with 7 nodes.
>>
>>
>> Scheme for document
>>
>>
>> xml version="1.0" encoding="UTF-8" ?
>> 
>> 
>> > stored="false" />
>> 
>> > stored="false" />
>>
>> > stored="false" />
>> > stored="false" multiValued="true" />
>> 
>> > stored="false" />
>> > stored="false" />
>> 
>> 
>>
>> 
>> > multiValued="false" required="true"/>
>> > multiValued="false"/>
>> > multiValued="false"/>
>> > multiValued="false"/>
>> > multiValued="false"/>
>> > multiValued="false"/>
>> > multiValued="false"/>
>> > multiValued="false"/>
>> > multiValued="false"/>
>>
>> 
>> 
>>
>> \_yz\_id
>>
>> 
>> 
>> > omitNorms="true"/>
>> > sortMissingLast="true" omitNorms="true"/>
>> > omitNorms="true"/>
>>
>> > omitNorms="true"/>
>> 
>> > multiValued="true" class="solr.StrField" />
>> > omitNorms="true">
>> 
>> 
>> 
>> > replacement='е' replace='all'/>
>> 
>> 
>> 
>>
>> 
>>
>>
>> Best regards,
>>
>> Roman
>>
>> \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
>> riak-users mailing list
>> riak-users@lists.basho.com
>> http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
>>
>>
>>
>
>
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

