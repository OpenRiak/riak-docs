---
title: "Re: Bitcask node won't restart"
description: ""
project: community
lastmod: 2011-04-01T16:42:00-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg02852"
mailinglist_parent_id: "msg02850"
author_name: "Dan Reverri"
project_section: "mailinglistitem"
sent_date: 2011-04-01T16:42:00-07:00
---


Hi Keith,

Can you try attaching to node 2 using "riak attach"? If that doesn't work,
manually kill node 2 and run "riak console".

Once you have access to the console, type the following:
1> node().
% the console will output the node name here

2> erlang:get\_cookie().
% the console will output the cookie here

Let me know what those commands output.

Thanks,
Dan

Daniel Reverri
Developer Advocate
Basho Technologies, Inc.
d...@basho.com


On Fri, Apr 1, 2011 at 2:34 PM, Keith Dreibelbis  wrote:

> Thanks for the response, Dan. Yes, the problem is that it \*looks\* like
> starting node 2 was successful (says it's ALIVE, shows up in ps). But it's
> not responding to pings, isn't usable, and nodes 1 and 3 say node 2 isn't
> connected.
>
> As you suggested, here is the output of riak-admin status for the 3 nodes,
> and I'll attach a tarball for node 2's log directory.
>
> Keith
>
> kratos:dev1 keith$ bin/riak-admin status
> 1-minute stats for 'dev1@127.0.0.1'
> -------------------------------------------
> vnode gets : 0
> vnode\_puts : 0
> read\_repairs : 0
> vnode\_gets\_total : 6251
> vnode\_puts\_total : 1064
> node\_gets : 0
> node\_gets\_total : 4786
> node\_get\_fsm\_time\_mean : 0
> node\_get\_fsm\_time\_median : 0
> node\_get\_fsm\_time\_95 : 0
> node\_get\_fsm\_time\_99 : 0
> node\_get\_fsm\_time\_100 : 0
> node\_puts : 0
> node\_puts\_total : 774
> node\_put\_fsm\_time\_mean : 0
> node\_put\_fsm\_time\_median : 0
> node\_put\_fsm\_time\_95 : 0
> node\_put\_fsm\_time\_99 : 0
> node\_put\_fsm\_time\_100 : 0
> read\_repairs\_total : 354
> cpu\_nprocs : 127
> cpu\_avg1 : 164
> cpu\_avg5 : 202
> cpu\_avg15 : 205
> mem\_total : 3264444000
> mem\_allocated : 3155680000
> disk : [{"/",488050672,13}]
> nodename : 'dev1@127.0.0.1'
> connected\_nodes : ['dev3@127.0.0.1']
> sys\_driver\_version : <<"1.5">>
> sys\_global\_heaps\_size : 0
> sys\_heap\_type : private
> sys\_logical\_processors : 2
> sys\_otp\_release : <<"R14B01">>
> sys\_process\_count : 206
> sys\_smp\_support : true
> sys\_system\_version : <<"Erlang R14B01 (erts-5.8.2) [source] [64-bit]
> [smp:2:2] [rq:2] [async-threads:64] [hipe] [kernel-poll:true]">>
> sys\_system\_architecture : <<"i386-apple-darwin10.7.0">>
> sys\_threads\_enabled : true
> sys\_thread\_pool\_size : 64
> sys\_wordsize : 8
> ring\_members : ['dev1@127.0.0.1','dev2@127.0.0.1','dev3@127.0.0.1']
> ring\_num\_partitions : 64
> ring\_ownership : <<"[{'dev3@127.0.0.1',21},{'dev2@127.0.0.1',21},{'
> dev1@127.0.0.1',22}]">>
> ring\_creation\_size : 64
> storage\_backend : riak\_kv\_bitcask\_backend
> pbc\_connects\_total : 350
> pbc\_connects : 0
> pbc\_active : 0
> riak\_err\_version : <<"1.0.1">>
> runtime\_tools\_version : <<"1.8.4.1">>
> basho\_stats\_version : <<"1.0.1">>
> luwak\_version : <<"1.0.0">>
> skerl\_version : <<"1.0.0">>
> riak\_kv\_version : <<"0.14.0">>
> bitcask\_version : <<"1.1.5">>
> riak\_core\_version : <<"0.14.0">>
> riak\_sysmon\_version : <<"0.9.0">>
> luke\_version : <<"0.2.3">>
> erlang\_js\_version : <<"0.5.0">>
> mochiweb\_version : <<"1.7.1">>
> webmachine\_version : <<"1.8.0">>
> crypto\_version : <<"2.0.2">>
> os\_mon\_version : <<"2.2.5">>
> cluster\_info\_version : <<"1.1.0">>
> sasl\_version : <<"2.1.9.2">>
> stdlib\_version : <<"1.17.2">>
> kernel\_version : <<"2.14.2">>
> executing\_mappers : 0
>
> kratos:dev2 keith$ bin/riak-admin status
> Node is not running!
>
> kratos:dev3 keith$ bin/riak-admin status
> 1-minute stats for 'dev3@127.0.0.1'
> -------------------------------------------
> vnode gets : 0
> vnode\_puts : 0
> read\_repairs : 0
> vnode\_gets\_total : 7061
> vnode\_puts\_total : 1198
> node\_gets : 0
> node\_gets\_total : 0
> node\_get\_fsm\_time\_mean : 0
> node\_get\_fsm\_time\_median : 0
> node\_get\_fsm\_time\_95 : 0
> node\_get\_fsm\_time\_99 : 0
> node\_get\_fsm\_time\_100 : 0
> node\_puts : 0
> node\_puts\_total : 0
> node\_put\_fsm\_time\_mean : 0
> node\_put\_fsm\_time\_median : 0
> node\_put\_fsm\_time\_95 : 0
> node\_put\_fsm\_time\_99 : 0
> node\_put\_fsm\_time\_100 : 0
> read\_repairs\_total : 0
> cpu\_nprocs : 134
> cpu\_avg1 : 118
> cpu\_avg5 : 161
> cpu\_avg15 : 184
> mem\_total : 3264252000
> mem\_allocated : 3189744000
> disk : [{"/",488050672,13}]
> nodename : 'dev3@127.0.0.1'
> connected\_nodes : ['dev1@127.0.0.1']
> sys\_driver\_version : <<"1.5">>
> sys\_global\_heaps\_size : 0
> sys\_heap\_type : private
> sys\_logical\_processors : 2
> sys\_otp\_release : <<"R14B01">>
> sys\_process\_count : 205
> sys\_smp\_support : true
> sys\_system\_version : <<"Erlang R14B01 (erts-5.8.2) [source] [64-bit]
> [smp:2:2] [rq:2] [async-threads:64] [hipe] [kernel-poll:true]">>
> sys\_system\_architecture : <<"i386-apple-darwin10.7.0">>
> sys\_threads\_enabled : true
> sys\_thread\_pool\_size : 64
> sys\_wordsize : 8
> ring\_members : ['dev1@127.0.0.1','dev2@127.0.0.1','dev3@127.0.0.1']
> ring\_num\_partitions : 64
> ring\_ownership : <<"[{'dev3@127.0.0.1',21},{'dev2@127.0.0.1',21},{'
> dev1@127.0.0.1',22}]">>
> ring\_creation\_size : 64
> storage\_backend : riak\_kv\_bitcask\_backend
> pbc\_connects\_total : 0
> pbc\_connects : 0
> pbc\_active : 0
> riak\_err\_version : <<"1.0.1">>
> runtime\_tools\_version : <<"1.8.4.1">>
> basho\_stats\_version : <<"1.0.1">>
> luwak\_version : <<"1.0.0">>
> skerl\_version : <<"1.0.0">>
> riak\_kv\_version : <<"0.14.0">>
> bitcask\_version : <<"1.1.5">>
> riak\_core\_version : <<"0.14.0">>
> riak\_sysmon\_version : <<"0.9.0">>
> luke\_version : <<"0.2.3">>
> erlang\_js\_version : <<"0.5.0">>
> mochiweb\_version : <<"1.7.1">>
> webmachine\_version : <<"1.8.0">>
> crypto\_version : <<"2.0.2">>
> os\_mon\_version : <<"2.2.5">>
> cluster\_info\_version : <<"1.1.0">>
> sasl\_version : <<"2.1.9.2">>
> stdlib\_version : <<"1.17.2">>
> kernel\_version : <<"2.14.2">>
> executing\_mappers : 0
>
>
>
> On Fri, Apr 1, 2011 at 2:17 PM, Dan Reverri  wrote:
>
>> Hi Keith,
>>
>> The first set of errors you saw ("Protocol: ~p: register error: ~p~n")
>> indicate an Erlang node was already running with this name; node 2 may have
>> been running in the background without you realizing it.
>>
>> The second error which occurred when choosing a different name was
>> probably due to a port binding issue; this means the ports node 2 tried
>> binding to (handoff, web, pb) were already occupied. Again, node 2 may have
>> already been running in the background.
>>
>> After rebooting the machine it looks like starting node 2 was successful.
>> Regarding the ringready failure, can you run "riak-admin status" on all
>> three nodes? Also, can you send in the log files for node 2 (the entire log
>> directory would be great)?
>>
>> Thanks,
>> Dan
>>
>> Daniel Reverri
>> Developer Advocate
>> Basho Technologies, Inc.
>> d...@basho.com
>>
>>
>> On Fri, Apr 1, 2011 at 1:57 PM, Keith Dreibelbis wrote:
>>
>>> Hi riak-users,
>>>
>>> I have a node in a cluster of 3 that failed and won't come back up. This
>>> is in a dev environment, so it's not like there's critical data on there.
>>> However, rather than start over with a new install, I want to learn how to
>>> recover from such a failure in production. I figured there was enough
>>> redundancy such that node 2 could recover with (at worst) a little help from
>>> nodes 1 and 3.
>>>
>>> When I tried to restart/reboot (I tried both), this showed up in
>>> erlang.log.1:
>>>
>>> Exec: /Users/keith/src/riak/dev/dev2/erts-5.8.2/bin/erlexec -boot
>>> /Users/keith/src/riak/dev/dev2/releases/0.14.0/riak -embedded
>>> -config /Users/keith/src/riak/dev/dev2/etc/app.confi
>>>
>>> g -args\_file /Users/keith/src/riak/dev/dev2/etc/vm.args --
>>> console
>>>
>>> Root: /Users/keith/src/riak/dev/dev2
>>>
>>> {error\_logger,{{2011,3,31},{16,43,35}},"Protocol: ~p: register error:
>>> ~p~n",["inet\_tcp",{{badmatch,{error,duplicate\_name}},[{inet\_tcp\_dist,listen,1},{net\_kernel,start\_protos,4},{net\_kernel,sta
>>>
>>>
>>> rt\_protos,3},{net\_kernel,init\_node,2},{net\_kernel,init,1},{gen\_server,init\_it,6},{proc\_lib,init\_p\_do\_apply,3}]}]}^M
>>>
>>>
>>> {error\_logger,{{2011,3,31},{16,43,35}},crash\_report,[[{initial\_call,{net\_kernel,init,['Argument\_\_1']}},{pid,<0.20.0>},{registered\_name,[]},{error\_info,{exit,{error,badarg},[{gen\_server,init\_it
>>>
>>>
>>> ,6},{proc\_lib,init\_p\_do\_apply,3}]}},{ancestors,[net\_sup,kernel\_sup,<0.10.0>]},{messages,[]},{links,[#Port<0.138>,<0.17.0>]},{dictionary,[{longnames,true}]},{trap\_exit,true},{status,running},{h
>>>
>>> eap\_size,377},{stack\_size,24},{reductions,456}],[]]}^M
>>>
>>>
>>> {error\_logger,{{2011,3,31},{16,43,35}},supervisor\_report,[{supervisor,{local,net\_sup}},{errorContext,start\_error},{reason,{'EXIT',nodistribution}},{offender,[{pid,undefined},{name,net\_kernel},
>>>
>>> {mfargs,{net\_kernel,start\_link,[['dev2@127.0.0.1
>>> ',longnames]]}},{restart\_type,permanent},{shutdown,2000},{child\_type,worker}]}]}^M
>>>
>>>
>>> {error\_logger,{{2011,3,31},{16,43,35}},supervisor\_report,[{supervisor,{local,kernel\_sup}},{errorContext,start\_error},{reason,shutdown},{offender,[{pid,undefined},{name,net\_sup},{mfargs,{erl\_di
>>>
>>>
>>> stribution,start\_link,[]}},{restart\_type,permanent},{shutdown,infinity},{child\_type,supervisor}]}]}^M
>>>
>>>
>>> {error\_logger,{{2011,3,31},{16,43,35}},std\_info,[{application,kernel},{exited,{shutdown,{kernel,start,[normal,[]]}}},{type,permanent}]}^M
>>>
>>> {"Kernel pid
>>> terminated",application\_controller,"{application\_start\_failure,kernel,{shutdown,{kernel,start,[normal,[]]}}}"}^M
>>>
>>> ^M
>>>
>>> Crash dump was written to: erl\_crash.dump^M
>>>
>>> Kernel pid terminated (application\_controller)
>>> ({application\_start\_failure,kernel,{shutdown,{kernel,start,[normal,[]]}}})^M
>>>
>>>
>>> http://wiki.basho.com/Recovering-a-failed-node.html suggests starting
>>> the node in console mode with a different name. This didn't help, it just
>>> crashed again. I'm using bitcask (the default) while the example on that
>>> page gives output like InnoDB would return.
>>>
>>> kratos:dev2 keith$ bin/riak console -name differentname@nohost
>>> Exec: /Users/keith/src/riak/dev/dev2/erts-5.8.2/bin/erlexec -boot
>>> /Users/keith/src/riak/dev/dev2/releases/0.14.0/riak -embedded
>>> -config /Users/keith/src/riak/dev/dev2/etc/app.config -args\_file
>>> /Users/keith/src/riak/dev/dev2/etc/vm.args -- console -name
>>> differentname@nohost
>>> Root: /Users/keith/src/riak/dev/dev2
>>> Erlang R14B01 (erts-5.8.2) [source] [64-bit] [smp:2:2] [rq:2]
>>> [async-threads:64] [hipe] [kernel-poll:true]
>>>
>>>
>>> =INFO REPORT==== 31-Mar-2011::17:35:05 ===
>>> alarm\_handler: {set,{system\_memory\_high\_watermark,[]}}
>>> \*\* Found 0 name clashes in code paths
>>>
>>> =INFO REPORT==== 31-Mar-2011::17:35:05 ===
>>> application: riak\_core
>>> exited: {shutdown,{riak\_core\_app,start,[normal,[]]}}
>>> type: permanent
>>>
>>> =INFO REPORT==== 31-Mar-2011::17:35:05 ===
>>> alarm\_handler: {clear,system\_memory\_high\_watermark}
>>> {"Kernel pid
>>> terminated",application\_controller,"{application\_start\_failure,riak\_core,{shutdown,{riak\_core\_app,start,[normal,[]]}}}"}
>>>
>>> Crash dump was written to: erl\_crash.dump
>>> Kernel pid terminated (application\_controller)
>>> ({application\_start\_failure,riak\_core,{shutdown,{riak\_core\_app,start,[normal,[]]}}})
>>> kratos:dev2 keith$
>>>
>>> After I rebooted my machine and tried starting the trio of riak nodes,
>>> again node 2 is not responding to pings, and "riak-admin ringready" from
>>> nodes 1 and 3 complain that node 2 is down. But in the log, node 2 is
>>> saying it's ALIVE. Also, I can see processes for all 3 nodes in ps:
>>>
>>> kratos:~ keith$ ps auxww | grep riak
>>> keith 360 0.2 3.4 2606932 143044 s006 Ss+ 12:05PM 3:21.61
>>> /Users/keith/src/riak/dev/dev1/erts-5.8.2/bin/beam.smp -K true -A 64 --
>>> -root /Users/keith/src/riak/dev/dev1 -progname riak -- -home /Users/keith --
>>> -boot /Users/keith/src/riak/dev/dev1/releases/0.14.0/riak -embedded -config
>>> /Users/keith/src/riak/dev/dev1/etc/app.config -name 
>>> dev1@127.0.0.1-setcookie riak -- console
>>> keith 580 0.1 2.0 2549924 85492 s008 Ss+ 12:05PM 2:24.08
>>> /Users/keith/src/riak/dev/dev3/erts-5.8.2/bin/beam.smp -K true -A 64 --
>>> -root /Users/keith/src/riak/dev/dev3 -progname riak -- -home /Users/keith --
>>> -boot /Users/keith/src/riak/dev/dev3/releases/0.14.0/riak -embedded -config
>>> /Users/keith/src/riak/dev/dev3/etc/app.config -name 
>>> dev3@127.0.0.1-setcookie riak -- console
>>> keith 380 0.0 0.0 2435004 268 ?? S 12:05PM 0:00.08
>>> /Users/keith/src/riak/dev/dev1/erts-5.8.2/bin/epmd -daemon
>>> keith 358 0.0 0.0 2434988 264 ?? S 12:05PM 0:00.01
>>> /Users/keith/src/riak/dev/dev1/erts-5.8.2/bin/run\_erl -daemon
>>> /tmp//Users/keith/src/riak/dev/dev1// /Users/keith/src/riak/dev/dev1/log
>>> exec /Users/keith/src/riak/dev/dev1/bin/riak console
>>> keith 1633 0.0 0.0 2435548 0 s010 R+ 1:34PM 0:00.00
>>> grep riak
>>> keith 578 0.0 0.0 2434988 264 ?? S 12:05PM 0:00.00
>>> /Users/keith/src/riak/dev/dev3/erts-5.8.2/bin/run\_erl -daemon
>>> /tmp//Users/keith/src/riak/dev/dev3// /Users/keith/src/riak/dev/dev3/log
>>> exec /Users/keith/src/riak/dev/dev3/bin/riak console
>>> keith 470 0.0 2.0 2548688 83584 s007 Ss+ 12:05PM 0:33.41
>>> /Users/keith/src/riak/dev/dev2/erts-5.8.2/bin/beam.smp -K true -A 64 --
>>> -root /Users/keith/src/riak/dev/dev2 -progname riak -- -home /Users/keith --
>>> -boot /Users/keith/src/riak/dev/dev2/releases/0.14.0/riak -embedded -config
>>> /Users/keith/src/riak/dev/dev2/etc/app.config -name 
>>> dev2@127.0.0.1-setcookie riak -- console
>>> keith 468 0.0 0.0 2434988 264 ?? S 12:05PM 0:00.01
>>> /Users/keith/src/riak/dev/dev2/erts-5.8.2/bin/run\_erl -daemon
>>> /tmp//Users/keith/src/riak/dev/dev2// /Users/keith/src/riak/dev/dev2/log
>>> exec /Users/keith/src/riak/dev/dev2/bin/riak console
>>> kratos:~ keith$
>>>
>>> I've attached the erl\_crash.dump file. Anyone have an explanation or
>>> suggestions on how to proceed?
>>>
>>>
>>> Keith
>>>
>>>
>>> \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
>>> riak-users mailing list
>>> riak-users@lists.basho.com
>>> http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
>>>
>>>
>>
>
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

