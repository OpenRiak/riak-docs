---
title: "Re: Uneven distribution of partitions in RIAK cluster"
description: ""
project: community
lastmod: 2016-11-29T12:44:11-0800
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg17791"
mailinglist_parent_id: "msg17747"
author_name: "Semov, Raymond"
project_section: "mailinglistitem"
sent_date: 2016-11-29T12:44:11-0800
---


Drew,
Thank you for the response! I would love to consider the claim\_v2 -> claim\_v3 
but since it’s experimental I’d rather not, I’m dealing with a RIAK cluster 
that is in production.
What I will end up doing is (after our team cleans up all the junk in the 
cluster) have a node leave the cluster and then rejoin. That’ll fix the 
fragmentation that will happen after the old data purge as well.

From: Drew Pirrone-Brusse 
>
Date: Monday, November 14, 2016 at 10:05 AM
To: "Semov, Raymond" >
Cc: "riak-users@lists.basho.com" 
>
Subject: Re: Uneven distribution of partitions in RIAK cluster

Hi Ray,

Riak's partition distribution is automatically calculated using our 
nondeterministic `claim` algorithm. That system is able to re-balance clusters, 
but is typically only run during membership operations; joining, leaving, or 
replacing nodes. The uneven partition distribution won't self-heal unless you 
add a new node to this cluster.

We can force a re-balance of this sort of uneven distribution by temporarily 
switching from `claim\_v2` to `claim\_v3`, and triggering a membership 
recalculation. `claim\_v3` is still an experimental system that is much more 
aggressive about avoiding preflist violations and lumpy claims, without much 
regard for limiting the scope of membership changes. With `claim\_v2`, the 
addition of a new node to an existing cluster will almost always only involve 
moving partitions off of existing nodes and onto the new node. With `claim\_v3`, 
it's somewhat common to see partitions also being moved between existing 
partitions in order to prevent lumpy claims.

These unpredictable spikes in membership changes have caused serious problems 
for our customers in the past, and they are nearly impossible to plan for, so 
we don't advise using `claim\_v3` for the majority of operations.

To enable `claim\_v3` and trigger a re-balance of the ring,

1. Enable the use of `claim\_v3` by opening a `riak attach` session on any node 
in this cluster, and running the below snippets,

 rpc:multicall(application, set\_env, [riak\_core, wants\_claim\_fun, 
{riak\_core\_claim, wants\_claim\_v3}]).
 rpc:multicall(application, set\_env, [riak\_core, choose\_claim\_fun, 
{riak\_core\_claim, choose\_claim\_v3}]).

(Please note, the `.`s are syntactically significant in Erlang, and you can 
exit `attach` sessions with `ctrl+g, q, enter`.)

2. Determine which node is currently the Claimant by running `riak-admin 
ring-status` on any node in the cluster. Look for the line similar to 
`Claimant: 'dev2@127.0.0.1'`.

3. Stop the claimant. In this case I would run `riak stop` on 
dev2@127.0.0.1.

4. Trigger the election of a new claimant by marking the current claimant DOWN 
in the ring. In this case, I would run `riak-admin down 
dev2@127.0.0.1` on any active node in this cluster.

5. Verify the reelection with `riak-admin ring-status` (checking to make sure 
the claimant has changed), and restart the node that was previously stopped.

At this time the rebalance should have occurred and membership transfers 
started.

6. To disable `claim\_v3`, open another `riak attach` session on any node in 
this cluster, and run the below snippets,

 rpc:multicall(application, set\_env, [riak\_core, wants\_claim\_fun, 
{riak\_core\_claim, default\_wants\_claim}]).
 rpc:multicall(application, set\_env, [riak\_core, choose\_claim\_fun, 
{riak\_core\_claim, default\_choose\_claim}]).

This can be done while the transfers are in-flight. The new plan will have 
already been injected into the ring.

I hope this helps.
Best regards,
-Drew

On Fri, Nov 11, 2016 at 2:13 PM, Semov, Raymond 
> wrote:
I have a 5-node cluster with 12 partitions in 4 of the nodes and 16 partitions 
in node #5. That is causing dangerously high disk utilization in that node. I 
plowed thru the documentation and Googled the hell out of it but I can’t find 
info on how rebalance the extra 4 partitions on the 4 underutilized nodes. The 
docs say the cluster balances itself but that’s apparently not the case here. 
Can anyone give any suggestions?
I run RIAK version 1.4.8 on Linux kernel 3.13
Ray

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com


\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

