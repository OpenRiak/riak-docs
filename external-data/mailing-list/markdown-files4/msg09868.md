---
title: "Re: Can my pagination approach scale?"
description: ""
project: community
lastmod: 2013-01-22T06:47:56-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg09868"
mailinglist_parent_id: "msg09867"
author_name: "Anton"
project_section: "mailinglistitem"
sent_date: 2013-01-22T06:47:56-08:00
---


You can check roughly how well your approach will work with
basho\_bench. If you estimate roughly how big your pages will be, set
up an appropriate benchmark and run it against the cluster or a
staging setup so you can get an idea of what performance you should
expect.

I don't think there's anything fundamentally wrong with your approach.
In fact I'm working on a similar storage scheme and I'm fairly
comfortable with it. You can find examples of real-world applications
in http://docs.basho.com/riak/latest/cookbooks/use-cases/. The Yammer
presentation, linked here,
http://docs.basho.com/riak/latest/cookbooks/use-cases/user-events-timelines/
also has similar ideas, it's worth checking out.



On 22 January 2013 14:56, Bach Le  wrote:
&gt; Hi, I'm currently using Riak for my project. It works well for single
&gt; documents, however I often need to present to users a stream of (loosely)
&gt; time ordered documents, Riak's keys are unordered by nature so there's no
&gt; straight forward way of traversing data. I came up with the following
&gt; approach:
&gt;
&gt; Make a bucket (i.e: "pages"), set allow\_mult to true. Inside this bucket
&gt; store a number that points to the "current" page, this number is initialized
&gt; to 0, I call this a cursor. For every "page" of data, create an object in
&gt; the same bucket, e.g: first page is associated with the key page\_0, second
&gt; page: page\_1 etc... These page objects are sets modeled using statebox for
&gt; conflict resolution.
&gt;
&gt; When a document is inserted, read the cursor value. Since the cursor can
&gt; only be increasing, we resolve conflicts by choosing the largest value among
&gt; the siblings. Next, read the page that it points to (if cursor is 0, read
&gt; the key "page\_0", if it is 1, read "page\_1" etc). If the number of objects
&gt; inside this set exceeds the page size, increment the counter and create a
&gt; new page to insert the object into, otherwise, leave the counter be and
&gt; insert into this page.
&gt;
&gt; To retrieve data in reverse chronological order, read the cursor to find out
&gt; the current page and then read the last page (which is shown to users as the
&gt; first page).
&gt;
&gt; Currently, my document's ids are monotonically increasing using this:
&gt; https://github.com/boundary/flake so I can sort documents within a page.
&gt;
&gt; I do realize that a page size can exceed its limit however, I don't know how
&gt; badly it can be with respect to writing rate. All I need is some form of
&gt; bulk get and chunking without resorting to 2i which can cover the whole
&gt; cluster.
&gt;
&gt; So, is there any major problem with this approach? Thanks.
&gt;
&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

