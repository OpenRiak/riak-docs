---
title: "Re: Performance Tuning in OmniOS"
description: ""
project: community
lastmod: 2014-03-13T04:04:23-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg13793"
mailinglist_parent_id: "msg13450"
author_name: "Hari John Kuriakose"
project_section: "mailinglistitem"
sent_date: 2014-03-13T04:04:23-07:00
---


Thought I would clarify my suspicion regarding the performance difference
between Linux and OmniOS.

Turns out during my testing with OmniOS, I did not launch some nodes as
Amazon "EBS Optimized". This was the primary reason for the comparitively
very poor IOPS obtained in OmniOS setup.

I would also like to add that even though I had a nice run with Riak on
OmniOS with ZFS, I finally settled with Ubuntu itself. Linux has always
been familiar territory.

Ofcourse other things like snapshots (I used LevelDB as backend), RAID
config, etc. were taken care of in other ways but atleast there was nothing
unknown or weird anymore.

Hope this would clear of any wrong impression I may have created, and may
help someone in some way.

Thanks once again to all of you for your help.

Regards,
Hari John Kuriakose.


On Wed, Jan 22, 2014 at 4:15 PM, Hari John Kuriakose wrote:

&gt; Thanks again for the quick reply.
&gt;
&gt; As you said, I am wondering about apples and oranges primarily. The same
&gt; EBS volume backed setup on Linux beat the OmniOS setup my more than 2x.
&gt; That is more weird to me than normal. This I tested with simple RAID0 (40Gb
&gt; \* 5 EBS devices) on both the operating systems.
&gt;
&gt; Also, I had followed the LevelDB tuning docs and set 50% of 8GB ram for
&gt; LevelDB, and rest was supposed to be for os. But the ZFS ARC cache seems to
&gt; have a mind of its own. This is unlike how LInux efficiently manages a disk
&gt; cache automatically. Again quoting your words, ZFS do needs ram, but I find
&gt; a lack of proper documentation on the ARC cache atleast so that I could
&gt; tune it specifically for AWS.
&gt;
&gt; Finally, I would want my RAID setup to reduce the latency rather than to
&gt; be fault tolerant, since I hope Riak will be available. Also if RAIDZ is
&gt; going to have trade-offs of its own, I would not want that to add up to the
&gt; unreliability of standard EBS volumes. As summarized, I would repeat my
&gt; tests focusing on maybe PIOPS because price is also a concern.
&gt;
&gt; Ultimately, I just wish to avoid any common pitfalls or dead-ends.
&gt;
&gt; Regards,
&gt; Hari John Kuriakose.
&gt;
&gt;
&gt; On Wed, Jan 22, 2014 at 3:26 AM, Jason Campbell  wrote:
&gt;
&gt;&gt; First off, make sure you are comparing apples to apples. I am assuming
&gt;&gt; the "default" RAID is RAIDZ on ZFS, so make sure the LVM raid is using
&gt;&gt; RAID5 for comparable performance.
&gt;&gt;
&gt;&gt; Generally, I wouldn't be using RAIDZ on AWS at all. The primary issue
&gt;&gt; with RAID5/RAIDZ is that the IO speeds are not only limited to that of a
&gt;&gt; single EBS volume, but that of the slowest EBS volume. That speed can vary
&gt;&gt; with which host you get assigned to, time of day, contention from other
&gt;&gt; customers, and seemingly the phase of the moon. I don't think anyone would
&gt;&gt; be surprised if you ran the same test again and got completely different
&gt;&gt; results. Provisioned IOPS disks will help even out extremely slow disks,
&gt;&gt; but you can still get quite a bit of variance.
&gt;&gt;
&gt;&gt; I would suggest moving to mirrored disks (RAID1) in both ZFS and Ubuntu.
&gt;&gt; I'm not sure about LVM, but ZFS will use the mirror to even out reads
&gt;&gt; (writes are harder) which should fix some of the high latency, even on
&gt;&gt; normal EBS volumes. I would suggest 4 disks in a RAID 10 configuration
&gt;&gt; (striping 2 mirrored pairs). Even better would be a 4-way mirror in ZFS if
&gt;&gt; price isn't much of a concern. This will limit you to the capacity of a
&gt;&gt; single EBS volume, but reads will use the fastest disk of the 4 disks,
&gt;&gt; instead of the slowest. It also has a nice side effect of being extremely
&gt;&gt; fault tolerant.
&gt;&gt;
&gt;&gt; The other thing to keep in mind is that ZFS is extremely RAM hungry and
&gt;&gt; read performance drops considerably when it is RAM starved, so I would
&gt;&gt; ensure that Riak doesn't use the last 1 - 1.5 GB of RAM so ZFS can use it
&gt;&gt; for caches.
&gt;&gt;
&gt;&gt; So in summary:
&gt;&gt; - Test, test and test again on different instances, this will help even
&gt;&gt; out EBS issues.
&gt;&gt; - Use mirrors, not RAID5/RAIDZ
&gt;&gt; - Use dedicated IOPS (at least for testing purposes)
&gt;&gt; - Ensure ZFS has some RAM to play with
&gt;&gt;
&gt;&gt; Hope this helps,
&gt;&gt; Jason Campbell
&gt;&gt;
&gt;&gt; ----- Original Message -----
&gt;&gt; From: "Jared Morrow" 
&gt;&gt; To: ejh...@gmail.com
&gt;&gt; Cc: "riak-users" 
&gt;&gt; Sent: Wednesday, 22 January, 2014 7:51:19 AM
&gt;&gt; Subject: Re: Performance Tuning in OmniOS
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt; Oh I think OmniOS is far from hopeless. The problem you are having is the
&gt;&gt; same problem you'd have if you were on ubuntu and you made a LVM raid on
&gt;&gt; vanilla EBS. EBS is the problem when it comes to predictable write / read
&gt;&gt; speed. People still use it, but not without careful thought and
&gt;&gt; consideration. You can try using provisioned IOPS for EBS, which the
&gt;&gt; m1.large supports, or ask in risk-users what other AWS users have setup. I
&gt;&gt; know we have a lot of customers and OSS users running on AWS, so they are
&gt;&gt; far more knowledgeable about real-world performance than I am.
&gt;&gt;
&gt;&gt;
&gt;&gt; Good luck,
&gt;&gt; Jared
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt; On Tue, Jan 21, 2014 at 12:05 PM, Hari John Kuriakose &lt; ejh...@gmail.com&gt; 
&gt;&gt; wrote:
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt; I am using the default raid itself.
&gt;&gt;
&gt;&gt; Well, if this is the case, I will run the tests again with a different
&gt;&gt; setup as you said, and get back as soon as possible. I would just like to
&gt;&gt; believe that OmniOS is not too hopeless.
&gt;&gt;
&gt;&gt; Thank you.
&gt;&gt;
&gt;&gt;
&gt;&gt; On Jan 21, 2014 11:17 PM, "Jared Morrow" &lt; ja...@basho.com &gt; wrote:
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt; What type of RAID did you chose for your spool of 5 volumes? If you chose
&gt;&gt; the default of raidz, you will not be getting much of a performance boost
&gt;&gt; over vanilla EBS, just a big integrity boost. Also, unless you are using
&gt;&gt; provisioned IOPS for EBS, you are starting from an extremely slow
&gt;&gt; base-case, so adding ZFS on top might not help matters much.
&gt;&gt;
&gt;&gt;
&gt;&gt; If speed is the concern, as a test I'm willing to bet if you do another
&gt;&gt; test run against the two instance storage disks on that m1.large, you will
&gt;&gt; probably beat those 5 EBS volumes pretty easily.
&gt;&gt;
&gt;&gt;
&gt;&gt; -Jared
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt; On Tue, Jan 21, 2014 at 9:22 AM, Hari John Kuriakose &lt; ejh...@gmail.com&gt; 
&gt;&gt; wrote:
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt; Hello,
&gt;&gt;
&gt;&gt;
&gt;&gt; I am using standard EBS devices, with a zpool in an instance comprising
&gt;&gt; of five 40GB volumes.
&gt;&gt; Each of the Riak instance is of m1.large type.
&gt;&gt;
&gt;&gt;
&gt;&gt; I have made the following changes in zfs properties:
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt; # My reason: the default sst block size for leveldb is 4k.
&gt;&gt; zfs set recordsize=4k tank/riak
&gt;&gt; # My reason: by default, leveldb verifies checksums automatically.
&gt;&gt; zfs set checksum=off tank/riak
&gt;&gt; zfs set atime=off tank/riak
&gt;&gt; zfs set snapdir=visible tank/riak
&gt;&gt;
&gt;&gt;
&gt;&gt; And I did the following with help from Basho AWS tuning docs:
&gt;&gt;
&gt;&gt;
&gt;&gt; projadd -c "riak" -K "process.max-file-descriptor=(basic,65536,deny)"
&gt;&gt; user.riak
&gt;&gt;
&gt;&gt; bash -c "echo 'set rlim\_fd\_max=65536' &gt;&gt; /etc/system"
&gt;&gt;
&gt;&gt;
&gt;&gt; bash -c "echo 'set rlim\_fd\_cur=65536' &gt;&gt; /etc/system"
&gt;&gt; ndd -set /dev/tcp tcp\_conn\_req\_max\_q0 40000
&gt;&gt;
&gt;&gt;
&gt;&gt; ndd -set /dev/tcp tcp\_conn\_req\_max\_q 4000
&gt;&gt; ndd -set /dev/tcp tcp\_tstamp\_always 0
&gt;&gt; ndd -set /dev/tcp tcp\_sack\_permitted 2
&gt;&gt; ndd -set /dev/tcp tcp\_wscale\_always 1
&gt;&gt; ndd -set /dev/tcp tcp\_time\_wait\_interval 60000
&gt;&gt; ndd -set /dev/tcp tcp\_keepalive\_interval 120000
&gt;&gt; ndd -set /dev/tcp tcp\_xmit\_hiwat 2097152
&gt;&gt; ndd -set /dev/tcp tcp\_recv\_hiwat 2097152
&gt;&gt; ndd -set /dev/tcp tcp\_max\_buf 8388608
&gt;&gt;
&gt;&gt;
&gt;&gt; Thanks again.
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt; On Tue, Jan 21, 2014 at 9:12 PM, Hector Castro &lt; hec...@basho.com &gt;
&gt;&gt; wrote:
&gt;&gt;
&gt;&gt;
&gt;&gt; Hello,
&gt;&gt;
&gt;&gt; Can you please clarify what type of disk you are using within AWS?
&gt;&gt; EBS, EBS with PIOPS, instance storage? In addition, maybe some details
&gt;&gt; on volume sizes and instance types.
&gt;&gt;
&gt;&gt; These details may help someone attempting to answer your question.
&gt;&gt;
&gt;&gt; --
&gt;&gt; Hector
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt; On Tue, Jan 21, 2014 at 8:11 AM, Hari John Kuriakose &lt; ejh...@gmail.com&gt; 
&gt;&gt; wrote:
&gt;&gt; &gt;
&gt;&gt; &gt; I am running LevelDB on ZFS in Solaris (OmniOS specifically) in Amazon
&gt;&gt; AWS.
&gt;&gt; &gt; The iops is very very low. There is no significant progress with tuning
&gt;&gt; too.
&gt;&gt; &gt;
&gt;&gt; &gt; Why I chose ZFS is that since LevelDB requires the node to be stopped
&gt;&gt; before
&gt;&gt; &gt; taking a backup, I needed a filesystem with snapshot ability. And the
&gt;&gt; most
&gt;&gt; &gt; favourable Amazon community AMI seemed to be using OmniOS (fork of
&gt;&gt; Solaris).
&gt;&gt; &gt; Everything is fine, except the performance.
&gt;&gt; &gt;
&gt;&gt; &gt; I did all the AWS tuning proposed by Basho but still Basho Bench gave
&gt;&gt; twice
&gt;&gt; &gt; iops on Ubuntu as compared to OmniOS, under same conditions. Also, I am
&gt;&gt; &gt; using riak-js client library, and its a 5 node Riak cluster with 8GB ram
&gt;&gt; &gt; each.
&gt;&gt; &gt;
&gt;&gt; &gt; Could not yet figure out what is really causing the congestion in
&gt;&gt; OmniOS.
&gt;&gt; &gt; Any pointers will be really helpful.
&gt;&gt; &gt;
&gt;&gt; &gt; Thanks and regards,
&gt;&gt; &gt; Hari John Kuriakose.
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt; &gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt; &gt; riak-users mailing list
&gt;&gt; &gt; riak-users@lists.basho.com
&gt;&gt; &gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;&gt; &gt;
&gt;&gt;
&gt;&gt;
&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt; riak-users mailing list
&gt;&gt; riak-users@lists.basho.com
&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt; riak-users mailing list
&gt;&gt; riak-users@lists.basho.com
&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;&gt;
&gt;
&gt;
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

