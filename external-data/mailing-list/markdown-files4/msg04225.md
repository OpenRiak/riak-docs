---
title: "Re: In-Memory Performance"
description: ""
project: community
lastmod: 2011-08-04T09:33:10-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg04225"
mailinglist_parent_id: "msg04220"
author_name: "Ryan Zezeski"
project_section: "mailinglistitem"
sent_date: 2011-08-04T09:33:10-07:00
---


On Thu, Aug 4, 2011 at 11:13 AM, Les Mikesell  wrote:
&gt;
&gt;
&gt; I can understand a performance difference in reads, considering the quorum
&gt; strategy, but the timing mentioned involved writes. I don't know the
&gt; membase internals but thought the claim of expanding the cluster by adding
&gt; nodes meant they probably used a distribution strategy similar to riak.
&gt;
&gt; Another recent message mentioned wanting a 'riak wishlist' somewhere. Some
&gt; of mine would be a membase-like client that knows about multiple nodes for
&gt; auto-failover, and the optional addition of some special master-slave nodes
&gt; that could be used for atomic operations without needing to use a completely
&gt; different client library for them.
&gt;
&gt;
Same thing applies for writes in that a coordinator is spun up for each
incoming write, sends async requests, and then waits for W responses. In
the case of membases's master/slave it can just write to master and at some
point the data will get pushed to slave(s). This is why I believe the more
apple/apple comparison would be membase w/ 3 replicas and 1:N replication
strategy.

If you read the links I referenced you'll see that membase does do something
similar with regards to mapping keys to physical servers.

\* vbucket ~ partition on ring
\* hash fun to vbucket ~ hash fun to partition
\* table of vbuckets to node (physical server) ~ ring/preflist of vnodes to
partitions
\* 1 node to many vbuckets ~ 1 node to many vnodes

However, the fact that membase does a similar indirect mapping of keys to
ownership doesn't necessarily mean it replicates by default. Like I said,
reading Dustin's post made me feel like replication is something to have to
configure.

As for your wish list. The first one is a form of smart client which would
essentially participate in gossip of the ring in order to determine which
nodes to contact. Currently we just recommend people put all the Riak nodes
behind a load balancer because any node can service any request. A smart
client is not out of the question but it's not necessarily as easy as it
seems to get right, or so I've been told by people smarter than me.

I'm not sure I follow you on the second thing. Riak strives for every node
to be completely equal. I.e. there is no such thing as a master because
when you introduce special nodes you increase chance for failure. In Riak
all nodes can service all requests; which is in spirit with it's high
availability focus.

-Ryan
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

