---
title: "Re: LevelDB performance (block size question)"
description: ""
project: community
lastmod: 2013-08-13T15:13:18-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg12011"
mailinglist_parent_id: "msg12008"
author_name: "István"
project_section: "mailinglistitem"
sent_date: 2013-08-13T15:13:18-07:00
---


Hi Matthew,

Thank you for the explanation.

I am experimenting with different block size and making sure I have at
least 100G data on disk for the tests.

I.


On Tue, Aug 13, 2013 at 12:11 PM, Matthew Von-Maszewski
wrote:

&gt; Istvan,
&gt;
&gt; "block\_size" is not a "size", it is a threshold. Data is never split
&gt; across blocks. A single block contains one or more key/value pairs.
&gt; leveldb starts a new block only when the total size of all key/values in
&gt; the current block exceed the threshold.
&gt;
&gt; Your must set block\_size to a multiple of your typical key/value size if
&gt; you desire multiple per block.
&gt;
&gt; Plus side: block\_size is computed before compression. So, you might get
&gt; nice reduction in total disk size by having multiple, mutually compressible
&gt; items in a block. leveldb iterators / Riak 2i might give you slightly
&gt; better performance with bigger blocks because there are fewer reads if the
&gt; keys needed are in the same block (or fewer blocks).
&gt;
&gt; Negative side: the entire block, not single key/value pairs, go into the
&gt; block cache uncompressed (cache\_size). You can quickly overwhelm the block
&gt; cache with lots of large blocks. Also random reads / Gets have to read,
&gt; decompress, and CRC check the entire block. Therefore it costs you more
&gt; disk transfer and decompression/CRC CPU time to read random values from
&gt; bigger blocks.
&gt;
&gt;
&gt; I suggest you experiment with your dataset and usage patterns. Be sure to
&gt; build big sample datasets before starting to measure and/or restart Riak
&gt; between building and measuring. These are ways to make sure you see the
&gt; impact of random reads.
&gt;
&gt; Matthew
&gt;
&gt;
&gt; On Aug 13, 2013, at 2:51 PM, István  wrote:
&gt;
&gt; Hi guys,
&gt;
&gt; I am setting up a new Riak cluster and I was wondering if there is any
&gt; drawback of increasing the LevelDB blocksize from 4K to 64K. The reason is
&gt; that we have all of the values way bigger than 4K and I guess from the
&gt; performance point of view it would make sense to increase the block size.
&gt; The tests are still running to confirm this theory but I wanted to clarify
&gt; that there is no big red flag of doing that from the Riak side. I found the
&gt; following discussion about changing block size:
&gt;
&gt; https://groups.google.com/forum/#!msg/leveldb/2JJ4smpSC6Q/1Z7aDSeHiRkJ
&gt;
&gt; Is that a good idea to experiment with this in Riak to achieve better
&gt; performance?
&gt;
&gt; Thank you in advance,
&gt; Istvan
&gt;
&gt;
&gt; --
&gt; the sun shines for all
&gt;
&gt;
&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;
&gt;
&gt;


-- 
the sun shines for all
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

