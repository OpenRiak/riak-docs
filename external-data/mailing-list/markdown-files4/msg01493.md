---
title: "Re: Slow performance using linkwalk, help wanted"
description: ""
project: community
lastmod: 2010-11-08T08:51:13-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg01493"
mailinglist_parent_id: "msg01492"
author_name: "Kevin Smith"
project_section: "mailinglistitem"
sent_date: 2010-11-08T08:51:13-08:00
---


Jan - 

I've run some tests using a 8 GB, 4-core Linux box I had handy along with my 
MBP as a client using riak-java-client over HTTP. For the test I configured a 
user record as you described linked to 250 1KB entries in a separate bucket 
named "documents". I spun up 5 Java threads to simulate 5 concurrent users. 
Each thread performed the link walk from the user to the documents 2500 times. 
From that I was able to observe the follow performance (all times in 
milliseconds):

Average runtime: 124
99th percentile: 220
99.5th percentile: 263
99.9th percentile: 949

The large difference between the 99.5th and 99.9th seems to correlate to the 
beginning of the run so I think those times might reflect the time required for 
Java's server JIT to fully kick in as well as GC times to stabilize.

I was able to reduce performance by triggering "vector clock explosion". 
Setting a bucket's "allow\_mult" value to true and then overwriting existing 
entries with new values while omitting the old entries' vector clock 
information causes the object's vector clock data to bloat which will impact 
read times. Is there any chance this is occurring in your application?

Another possibility is the number of partitions in your cluster is not large 
enough to provide good parallelization for your workload. What's the value of 
ring\_creation\_size in your cluster's app.config? Riak will run with a default 
ring size of 64 partitions if the entry isn't present.

--Kevin

On Nov 8, 2010, at 9:45 AM, Jan Buchholdt wrote:

&gt; Kevin
&gt; 
&gt; We are using HTTP, (have tried PB without any performance gain) and
&gt; using riak-java-client as client lib.
&gt; 
&gt; --
&gt; Jan Buchholdt
&gt; Software Pilot
&gt; Trifork A/S
&gt; Cell +45 50761121
&gt; 
&gt; 
&gt; 
&gt; On 2010-11-08 14:20, Kevin Smith wrote:
&gt;&gt; Jan -
&gt;&gt; 
&gt;&gt; Which protocol (HTTP or protocol buffers) and client lib are you using?
&gt;&gt; 
&gt;&gt; --Kevin
&gt;&gt; On Nov 8, 2010, at 6:36 AM, Jan Buchholdt wrote:
&gt;&gt; 
&gt;&gt;&gt; We are evaluating Riak for a project, but having a hard time making it fast 
&gt;&gt;&gt; enough for our need.
&gt;&gt;&gt; 
&gt;&gt;&gt; Our model is very simple and looks like this:
&gt;&gt;&gt; 
&gt;&gt;&gt; --------------------- \* ---------------------
&gt;&gt;&gt; | Person | ------------------------&gt; | Document |
&gt;&gt;&gt; --------------------- ---------------------
&gt;&gt;&gt; 
&gt;&gt;&gt; We have a set of persons and each person can have many documents.
&gt;&gt;&gt; 
&gt;&gt;&gt; Our typical queries are:
&gt;&gt;&gt; 
&gt;&gt;&gt; Get an overview of all the persons documents. This query returns the person 
&gt;&gt;&gt; along with a subset of data from all the persons documents.
&gt;&gt;&gt; Get document by id.
&gt;&gt;&gt; 
&gt;&gt;&gt; Our requirements are that these quires should be performed under in under 
&gt;&gt;&gt; 100millis when we have 10 requests per second or less load.
&gt;&gt;&gt; 
&gt;&gt;&gt; The size of the data:
&gt;&gt;&gt; A document is approximately 1 kb
&gt;&gt;&gt; No data for a persons except the personidentifier
&gt;&gt;&gt; Around 6 million persons.
&gt;&gt;&gt; Each person has from from 0 to a couple of thousand documents.
&gt;&gt;&gt; All in all we have 120 mio documents.
&gt;&gt;&gt; Most persons don't have more than 1 to 10 documents, but then we have some 
&gt;&gt;&gt; few "heavy" persons having 500 to 1000 documents.
&gt;&gt;&gt; 
&gt;&gt;&gt; Riak setup:
&gt;&gt;&gt; 4 Nodes.
&gt;&gt;&gt; Hardware configuration for each node:
&gt;&gt;&gt; HP ProLiant DL360 G7
&gt;&gt;&gt; 18 gb ram
&gt;&gt;&gt; SAS discs
&gt;&gt;&gt; Intel(R) Xeon(R) CPU E5620 @ 2.40GHz Proc 1
&gt;&gt;&gt; Solaris 10 update 9
&gt;&gt;&gt; 
&gt;&gt;&gt; We use the default bitcask storage engine
&gt;&gt;&gt; We replicate data to 3 machines when it is written.
&gt;&gt;&gt; Reads are read from just one machine
&gt;&gt;&gt; 
&gt;&gt;&gt; We tried implementing our datamodel using Riak links as described below:
&gt;&gt;&gt; 
&gt;&gt;&gt; Persons are stored in a person bucket using their person identifier as key
&gt;&gt;&gt; /person/
&gt;&gt;&gt; {personid}
&gt;&gt;&gt; Documents are saved in another bucket
&gt;&gt;&gt; /document/
&gt;&gt;&gt; {documented}
&gt;&gt;&gt; At each person we store links to the persons documents.
&gt;&gt;&gt; 
&gt;&gt;&gt; We are having problems with the query fetching all the documents for a 
&gt;&gt;&gt; person. Reading all the documents for a person is done using a link walk. 
&gt;&gt;&gt; The linkwalk start reading all the document keys using the personid. It 
&gt;&gt;&gt; then fetches all documents.
&gt;&gt;&gt; For persons with 1 - 5 documents the response times are often over 100 
&gt;&gt;&gt; mills. And for the "heavy" persons with many documents response times are 
&gt;&gt;&gt; several seconds. But we are very new to Riak and are probably using a wrong 
&gt;&gt;&gt; approach.
&gt;&gt;&gt; 
&gt;&gt;&gt; Below are our thoughts (having almost no experience with Riak):
&gt;&gt;&gt; 
&gt;&gt;&gt; The chosen datamodel is good for writes. Writing a new document results in 
&gt;&gt;&gt; 3 operations against Riak. Writing the document using its id as key. 
&gt;&gt;&gt; Reading the Person to get all the persons document links. Append the new 
&gt;&gt;&gt; document's key to the persons links and write back the person.
&gt;&gt;&gt; 
&gt;&gt;&gt; Reading, using linkwalk, is slow because it is expensive to fetch many 
&gt;&gt;&gt; documents even though the linkwalk can read their keys right away by 
&gt;&gt;&gt; reading the links for the person. Even though we have 4 nodes and linkwalks 
&gt;&gt;&gt; are parallelized many documents need to be retrieved from one node. Having 
&gt;&gt;&gt; to fetch for example 100 documents on one node (one disc) is expensive. We 
&gt;&gt;&gt; do not know how data is stored but are afraid Riak is doing a lot of disk 
&gt;&gt;&gt; seeks.
&gt;&gt;&gt; 
&gt;&gt;&gt; We are considering another more denormalized approach where we write all 
&gt;&gt;&gt; the documents for a person in one "blob". But then we are afraid our writes 
&gt;&gt;&gt; become slow, because when adding a new document the blob must be read, the 
&gt;&gt;&gt; new document inserted and the blob written back.
&gt;&gt;&gt; 
&gt;&gt;&gt; We could really need some input. Is our assumptions wrong? (we have not yet 
&gt;&gt;&gt; dug into the problems). Is there a good datamodel for our requirements? 
&gt;&gt;&gt; etc?.
&gt;&gt;&gt; We haven't looked at Riak search at all. Maybe it could solve some of our 
&gt;&gt;&gt; problems.
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; -- --
&gt;&gt;&gt; Jan Buchholdt
&gt;&gt;&gt; Software Pilot
&gt;&gt;&gt; Trifork A/S
&gt;&gt;&gt; Cell +45 50761121
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt; 
&gt; 
&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com


\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

