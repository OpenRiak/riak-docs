---
title: "Re: Weird RIAK behavior"
description: ""
project: community
lastmod: 2014-12-22T19:21:09-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg15405"
mailinglist_parent_id: "msg15403"
author_name: "Claudio Cesar Sanchez Tejeda"
project_section: "mailinglistitem"
sent_date: 2014-12-22T19:21:09-08:00
---


Hi,

On Mon, Dec 22, 2014 at 11:54 PM, Alexander Sicular  wrote:
&gt; Same client code writing to all 5 clusters?

Yes, it is the same code.

&gt;
&gt; How does the config of the 5th cluster differ from the first 4?
&gt;

Two clusters have one more memory backend configured (they are
configured with multibackend). The cluster with issues is one of these
two clusters.

&gt; Quick notes:
&gt; Minimum of 5 nodes for a production deployment to ensure the default 3 
&gt; replicas are all on different physical nodes. Which is a good segue into the 
&gt; fact that you shouldn't run multiple Riak nodes on the same physical 
&gt; hardware. Performance aside, if you lose that physical machine you lose all 
&gt; your data.

Yes, these clusters (that are located in the same physical machine)
are used for the developing team.

Regards.

&gt;
&gt;
&gt; @siculars
&gt; http://siculars.posthaven.com
&gt;
&gt; Sent from my iRotaryPhone
&gt;
&gt;&gt; On Dec 22, 2014, at 18:59, Claudio Cesar Sanchez Tejeda 
&gt;&gt;  wrote:
&gt;&gt;
&gt;&gt; I'm a sysadmin and I managing 5 cluster of RIAK:
&gt;&gt;
&gt;&gt; - two of them are LXC containers on the same physical machine (3 nodes
&gt;&gt; per cluster)
&gt;&gt; - one of them are LXC containers located on different physical
&gt;&gt; machines (6 nodes)
&gt;&gt; - one of them are LXC containers located on different physical
&gt;&gt; machines and XEN VMs (6 nodes)
&gt;&gt; - and the last of them are VMware ESX VMs (3 nodes)
&gt;&gt;
&gt;&gt; Our application works correctly on the first four clusters, but it
&gt;&gt; doesn't work as we expected on the last one.
&gt;&gt;
&gt;&gt; When we update a key and we retrieve this key in order to write it
&gt;&gt; again, it has an old value (it doesn't have the first value that we
&gt;&gt; wrote), for example:
&gt;&gt;
&gt;&gt; The key has: lalala
&gt;&gt; We retrieve the key, and add lololo, so it should be lalala,lololo
&gt;&gt; We retrieve the key again, and try to add lelele, so it should be now:
&gt;&gt; lalala,lololo,lelele, but when we retrieve it again, we only have:
&gt;&gt; lalala,lelele
&gt;&gt;
&gt;&gt; In the second write action, when we retrieve the key, we obtained a
&gt;&gt; key with the old value. We set r, w, pr and rw to 3 to the REST
&gt;&gt; requests, but it doesn't help.
&gt;&gt;
&gt;&gt; All the configuration files are very similiar and we don't have any
&gt;&gt; major differences in the disk I/O and network performance of the nodes
&gt;&gt; of the clusters.
&gt;&gt;
&gt;&gt; Did anyone have a similar issue?
&gt;&gt;
&gt;&gt; Regards.
&gt;&gt;
&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt; riak-users mailing list
&gt;&gt; riak-users@lists.basho.com
&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

