---
title: "Re: Scaling up or out"
description: ""
project: community
lastmod: 2012-12-06T08:49:59-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg09449"
mailinglist_parent_id: "msg09448"
author_name: "Ken Perkins"
project_section: "mailinglistitem"
sent_date: 2012-12-06T08:49:59-08:00
---


We're around ~20% Swapping, IO Wait in the 5-25% range, depending on the
machine.

We're running Lucid, with the deadline scheduler. I'm strongly biasing
towards adding a few more nodes, but I'm not married to it :)


On Wed, Dec 5, 2012 at 9:22 PM, Sean Carey  wrote:

&gt; So Ken,
&gt; Fair amount? &lt; 5% or &gt; 20%
&gt;
&gt; If there's iowait and memory issues, adding nodes could alleviate that. If
&gt; there's almost no iowait or minimal iowait, adding memory will help. Also,
&gt; tuning vm.dirty on linux might get you more memory and less iowait. Or at
&gt; least more consistent iowait.
&gt;
&gt;
&gt; Which linux distro are you on and which scheduler are you using?
&gt;
&gt;
&gt; -Sean
&gt;
&gt;
&gt; On Thursday, December 6, 2012 at 12:15 AM, Ken Perkins wrote:
&gt;
&gt; VMs, not the same host, rackspace has VM affinity to protect against that.
&gt; We do see a fair amount of IO Wait.
&gt;
&gt; Rackspace has a new affinity based SSD block device service that I plan to
&gt; evaluate, but I'm not ready for that in production.
&gt;
&gt;
&gt; On Wed, Dec 5, 2012 at 7:45 PM, Sean Carey  wrote:
&gt;
&gt; Ken,
&gt; Are your vms on different bare metal? Could they potentially be on the
&gt; same bare metal?
&gt;
&gt; Are you seeing any io contention?
&gt;
&gt;
&gt; Sean Carey
&gt; @densone
&gt;
&gt; On Wednesday, December 5, 2012 at 20:41, Ken Perkins wrote:
&gt;
&gt; Yes, we're thrashing on all of the boxes, due to disk access when looking
&gt; through merge\_index. It's not noisy neighbors, given how consistent the
&gt; thrashing is. We had a box with a corrupted index (we had to remove
&gt; merge\_index and rebuild) and that machine instantly went to 0% thrashing.
&gt; So we have a pretty good indication of the source.
&gt;
&gt; The cost for 10 8GB VMs is roughly equivalent to 5 16GB ones.
&gt;
&gt; Thanks for your input Michael!
&gt;
&gt; Ken
&gt;
&gt;
&gt; On Wed, Dec 5, 2012 at 4:47 PM, Michael Johnson wrote:
&gt;
&gt; There are a lot of things that go into this, but I would tend to suggest
&gt; in a hosted VM senario, upping the ram is likely the right solution.
&gt;
&gt; You mention thrashing, but what is that thrashing coming from? I assume
&gt; all the boxes are thrashing and not just one or two of them? Is it due to
&gt; swapping or is it just the raw disk access? Maybe you logging
&gt; too aggressively?
&gt;
&gt; Perhaps your are suffering from a bad neighbor effect. If this is the
&gt; case, increasing the amount of ram will likely put you on a physical host
&gt; with few customers and thus you would be less likely to have a bad neighbor.
&gt;
&gt; Cost-wise in the VM world, you might be better off adding a few nodes
&gt; rather than increasing the ram in your existing vm's.
&gt;
&gt; But then we are talking VMs and thus it should be fairly painless to
&gt; experiment. I would try adding ram first and if that doesn't work, add a
&gt; few nodes. Someone else my have a different opinion, but that is my two
&gt; cents.
&gt;
&gt;
&gt; On Wed, Dec 5, 2012 at 4:33 PM, Ken Perkins  wrote:
&gt;
&gt; Hello all,
&gt;
&gt; We're seeing enough thrashing and low-memory on our production ring that
&gt; we've decided to upgrade our hardware. The real question is should we scale
&gt; up or out.
&gt;
&gt; Currently our ring is 512 partitions. We know that it's a sub-optimal size
&gt; but we can't easily solve that now. We're currently running a search-heavy
&gt; app on 5 8GB VMs. I'm debating between moving the VMs up to 16GB, or adding
&gt; a few more 8GB VMs.
&gt;
&gt; Some of the talk in #riak has pushed me towards adding more machines (thus
&gt; lowering the per node number of partitions) but I wanted to do a quick
&gt; sanity check here with folks that it's better than scaling up my current
&gt; machines.
&gt;
&gt; Thanks!
&gt; Ken Perkins
&gt; clipboard.com
&gt;
&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;
&gt;
&gt;
&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;
&gt;
&gt;
&gt;
&gt;
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

