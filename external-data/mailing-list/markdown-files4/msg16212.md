---
title: "Re: Recommended way to delete keys"
description: ""
project: community
lastmod: 2015-06-04T22:11:02-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg16212"
mailinglist_parent_id: "msg16198"
author_name: "Daniel Abrahamsson"
project_section: "mailinglistitem"
sent_date: 2015-06-04T22:11:02-07:00
---


Multi-backend bitcask with auto-expire for sure sounds like the most
future-proof solution.

For our part, we tend do delete keys in map-reduce jobs, since we have more
complex logic for determining when it is time to delete objects. In our
current setup, it takes about ~3 minutes to go through 1.5M keys, deleting
~10000 of them. Given the amount of keys you specify (50 million being the
amount to be deleted, probably a fraction of the data you store), it seems
to be too expensive for your use case.

//Daniel

On Fri, Jun 5, 2015 at 12:02 AM, Peter Herndon  wrote:

&gt; Well, I’ve been looking to make my theoretical Erlang knowledge less
&gt; theoretical and somewhat more practical, so I wouldn’t say no. And this
&gt; approach is pretty much what we thought we’d use originally.
&gt;
&gt; Since then it has come to light that our product folks have given us
&gt; permission to just delete all the data. But we’re still going to need a
&gt; long-term solution. We may wind up reconfiguring the cluster to use the
&gt; multi-backend solution Sasha proposed.
&gt;
&gt; —Peter
&gt; &gt; On Jun 4, 2015, at 5:54 PM, John O'Brien  wrote:
&gt; &gt;
&gt; &gt; We've got an expiry worker rig I can likely pass over offline. Its not
&gt; overly clever.
&gt; &gt;
&gt; &gt; Basic idea stream a feed of keys into a pool of workers that spin off
&gt; delete calls.
&gt; &gt; We feed this based on continuous search's of an expiry TTL field in all
&gt; keys.
&gt; &gt;
&gt; &gt; It'd likely be better to run this from with the Erlang riak layer... But
&gt; then there's that whole Erlang thing.
&gt; &gt;
&gt; &gt; J
&gt; &gt;
&gt; &gt; On Jun 4, 2015 1:49 PM, "Peter Herndon"  wrote:
&gt; &gt; Mmm, I think we’re looking at deleting about 50 million keys per day.
&gt; That’s a completely back-of-envelope estimate, I haven’t done the actual
&gt; math yet.
&gt; &gt;
&gt; &gt; —Peter
&gt; &gt;
&gt; &gt; &gt; On Jun 4, 2015, at 3:28 AM, Daniel Abrahamsson &lt;
&gt; daniel.abrahams...@klarna.com&gt; wrote:
&gt; &gt; &gt;
&gt; &gt; &gt; Hi Peter,
&gt; &gt; &gt;
&gt; &gt; &gt; What is "large-scale" in your case? How many keys do you need to
&gt; delete, and how often?
&gt; &gt; &gt;
&gt; &gt; &gt; //Daniel
&gt; &gt; &gt;
&gt; &gt; &gt; On Wed, Jun 3, 2015 at 9:54 PM, Peter Herndon 
&gt; wrote:
&gt; &gt; &gt; Interesting thought. It might work for us, it might not, I’ll have to
&gt; check with our CTO to see whether the expense makes sense under our
&gt; circumstances.
&gt; &gt; &gt;
&gt; &gt; &gt; Thanks!
&gt; &gt; &gt;
&gt; &gt; &gt; —Peter
&gt; &gt; &gt; &gt; On Jun 3, 2015, at 2:21 PM, Drew Kerrigan  wrote:
&gt; &gt; &gt; &gt;
&gt; &gt; &gt; &gt; Another idea for a large-scale one-time removal of data, as well as
&gt; an opportunity for a fresh start, would be to:
&gt; &gt; &gt; &gt;
&gt; &gt; &gt; &gt; 1. set up multi-data center replication between 2 clusters
&gt; &gt; &gt; &gt; 2. implement a recv/2 hook on the sink which refuses data from the
&gt; buckets / keys you would like to ignore / delete
&gt; &gt; &gt; &gt; 3. trigger a full sync replication
&gt; &gt; &gt; &gt; 4. start using the sync as your new source of data sans the ignored
&gt; data
&gt; &gt; &gt; &gt;
&gt; &gt; &gt; &gt; Obviously this is costly, but it should have a fairly minimal impact
&gt; to existing production users other than the moment that you switch traffic
&gt; from the old cluster to the new one.
&gt; &gt; &gt; &gt;
&gt; &gt; &gt; &gt; Caveats: Not all Riak features are supported with MDC (search
&gt; indexes and strong consistency in particular).
&gt; &gt; &gt; &gt;
&gt; &gt; &gt; &gt; On Wed, Jun 3, 2015 at 2:11 PM Peter Herndon 
&gt; wrote:
&gt; &gt; &gt; &gt; Sadly, this is a production cluster already using leveldb as the
&gt; backend. With that constraint in mind, and rebuilding the cluster not
&gt; really being an option to enable multi-backends or bitcask, what would our
&gt; best approach be?
&gt; &gt; &gt; &gt;
&gt; &gt; &gt; &gt; Thanks!
&gt; &gt; &gt; &gt;
&gt; &gt; &gt; &gt; —Peter
&gt; &gt; &gt; &gt;
&gt; &gt; &gt; &gt; &gt; On Jun 3, 2015, at 12:09 PM, Alexander Sicular 
&gt; wrote:
&gt; &gt; &gt; &gt; &gt;
&gt; &gt; &gt; &gt; &gt; We are actively investigating better options for deletion of large
&gt; amounts of keys. As Sargun mentioned, deleting the data dir for an entire
&gt; backend via an operationalized rolling restart is probably the best
&gt; approach right now for killing large amounts of keys.
&gt; &gt; &gt; &gt; &gt;
&gt; &gt; &gt; &gt; &gt; But if your key space can fit in memory the best way to kill keys
&gt; is to use bitcask ttl if that's an option. 1. If you can even use bitcask
&gt; in your environment due to the memory overhead and 2. If your use case
&gt; allows for ttls which it may considering you may already be using time
&gt; bound buckets....
&gt; &gt; &gt; &gt; &gt;
&gt; &gt; &gt; &gt; &gt; -Alexander
&gt; &gt; &gt; &gt; &gt;
&gt; &gt; &gt; &gt; &gt; @siculars
&gt; &gt; &gt; &gt; &gt; http://siculars.posthaven.com
&gt; &gt; &gt; &gt; &gt;
&gt; &gt; &gt; &gt; &gt; Sent from my iRotaryPhone
&gt; &gt; &gt; &gt; &gt;
&gt; &gt; &gt; &gt; &gt; On Jun 3, 2015, at 09:54, Sargun Dhillon 
&gt; wrote:
&gt; &gt; &gt; &gt; &gt;
&gt; &gt; &gt; &gt; &gt;&gt; You could map your keys to a given bucket, and that bucket to a
&gt; given backend using multi\_backend. There is some cost to having lots of
&gt; backends (memory overhead, FDs, etc...). When you want to do a mass drop,
&gt; you could down the node, and delete that given backend, and bring it up.
&gt; Caveat: AAE, MDC, nor mutable data play well with this scenario.
&gt; &gt; &gt; &gt; &gt;&gt;
&gt; &gt; &gt; &gt; &gt;&gt; On Wed, Jun 3, 2015 at 10:43 AM, Peter Herndon &lt;
&gt; tphern...@gmail.com&gt; wrote:
&gt; &gt; &gt; &gt; &gt;&gt; Hi list,
&gt; &gt; &gt; &gt; &gt;&gt;
&gt; &gt; &gt; &gt; &gt;&gt; We’re looking for the best way to handle large scale expiration
&gt; of no-longer-useful data stored in Riak. We asked a while back, and the
&gt; recommendation was to store the data in time-segmented buckets (bucket per
&gt; day or per month), query on the current buckets, and use the streaming list
&gt; keys API to handle slowly deleting the buckets that have aged out.
&gt; &gt; &gt; &gt; &gt;&gt;
&gt; &gt; &gt; &gt; &gt;&gt; Is that still the best approach for doing this kind of task? Or
&gt; is there a better approach?
&gt; &gt; &gt; &gt; &gt;&gt;
&gt; &gt; &gt; &gt; &gt;&gt; Thanks!
&gt; &gt; &gt; &gt; &gt;&gt;
&gt; &gt; &gt; &gt; &gt;&gt; —Peter Herndon
&gt; &gt; &gt; &gt; &gt;&gt; Sr. Application Engineer
&gt; &gt; &gt; &gt; &gt;&gt; @Bitly
&gt; &gt; &gt; &gt; &gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt; &gt; &gt; &gt; &gt;&gt; riak-users mailing list
&gt; &gt; &gt; &gt; &gt;&gt; riak-users@lists.basho.com
&gt; &gt; &gt; &gt; &gt;&gt;
&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt; &gt; &gt; &gt; &gt;&gt;
&gt; &gt; &gt; &gt; &gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt; &gt; &gt; &gt; &gt;&gt; riak-users mailing list
&gt; &gt; &gt; &gt; &gt;&gt; riak-users@lists.basho.com
&gt; &gt; &gt; &gt; &gt;&gt;
&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt; &gt; &gt; &gt;
&gt; &gt; &gt; &gt;
&gt; &gt; &gt; &gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt; &gt; &gt; &gt; riak-users mailing list
&gt; &gt; &gt; &gt; riak-users@lists.basho.com
&gt; &gt; &gt; &gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt; &gt; &gt;
&gt; &gt; &gt;
&gt; &gt; &gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt; &gt; &gt; riak-users mailing list
&gt; &gt; &gt; riak-users@lists.basho.com
&gt; &gt; &gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt; &gt; &gt;
&gt; &gt;
&gt; &gt;
&gt; &gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt; &gt; riak-users mailing list
&gt; &gt; riak-users@lists.basho.com
&gt; &gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;
&gt;
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

