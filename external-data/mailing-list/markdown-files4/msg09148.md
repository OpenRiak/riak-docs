---
title: "Re: avg write io wait time regression in 1.2.1"
description: ""
project: community
lastmod: 2012-11-01T17:54:42-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg09148"
mailinglist_parent_id: "msg09147"
author_name: "Dietrich Featherston"
project_section: "mailinglistitem"
sent_date: 2012-11-01T17:54:42-07:00
---


Thanks. The amortized stalls may very well describe what we are seeing. If
I combine leveldb logs from all partitions on one of the upgraded nodes
what should I look for in terms of compaction activity to verify this?


On Thu, Nov 1, 2012 at 5:48 PM, Matthew Von-Maszewski wrote:

&gt; Dietrich,
&gt;
&gt; I can see your concern with the write IOS statistic. Let me comment on
&gt; the easy question first: block\_size.
&gt;
&gt; The block\_size parameter in 1.1 was not getting passed to leveldb from the
&gt; erlang layer. You were using a 4096 byte block parameter no matter what
&gt; you typed in the app.config. The block\_size is used by leveldb as a
&gt; threshold. Once you accumulate enough data above that threshold, the
&gt; current block is written to disk and a new one started. If you have 10k
&gt; data values, your get one data item per block and its size is ~10k. If you
&gt; have 1k data values, you get about four per block and the block is about 4k.
&gt;
&gt; We recommend 4k blocks to help read performance. The entire block has to
&gt; run through decompression and potentially CRC calculation when it comes off
&gt; the disk. That CPU time really kills any disk performance gains by having
&gt; larger blocks. Ok, that might change in 1.3 as we enable hardware CRC â€¦
&gt; but only if you have "verify\_checksums, true" in app.config.
&gt;
&gt;
&gt; Back to performance: I have not seen the change your graph details when
&gt; testing with SAS drives under moderate load. I am only today starting
&gt; qualification tests with SSD drives.
&gt;
&gt; But my 1.2 and 1.3 tests focus on drive / Riak saturation. 1.1 has the
&gt; nasty tendency to stall (intentionally) when we saturate the write side of
&gt; leveldb, . The stall was measured in seconds or even minutes in 1.1.
&gt; 1.2.1 has a write throttle that forecasts leveldb's stall state and
&gt; incrementally slows the individual writes to prevent the stalls. Maybe
&gt; that is what is being seen in the graph. The only way to know for sure is
&gt; to get an dump of your leveldb LOG files, combined them, then compare
&gt; compaction activity to your graph.
&gt;
&gt; Write stalls are detailed here:
&gt; http://basho.com/blog/technical/2012/10/30/leveldb-in-riak-1p2/
&gt;
&gt; How can I better assist you at this point?
&gt;
&gt; Matthew
&gt;
&gt;
&gt; On Nov 1, 2012, at 8:13 PM, Dietrich Featherston wrote:
&gt;
&gt; We've just gone through the process of upgrading two riak clusters from
&gt; 1.1 to 1.2.1. Both are on the leveldb backend backed by RAID0'd SSDs. The
&gt; process has gone smoothly and we see that latencies as measured at the
&gt; gen\_fsm level are largely unaffected.
&gt;
&gt; However, we are seeing some troubling disk statistics and I'm looking for
&gt; an explanation before we upgrade the remainder of our nodes. The source of
&gt; the worry seems to be a huge amplification in the number of writes serviced
&gt; by the disk which may be the cause of rising io wait times.
&gt;
&gt; My first thought was that this could be due to some leveldb tuning in
&gt; 1.2.1 which increases file sizes per the release notes (
&gt; https://github.com/basho/riak/blob/master/RELEASE-NOTES.md). But nodes
&gt; that were upgraded yesterday are still showing this symptom. I would have
&gt; expected any block re-writing to have subsided by now.
&gt;
&gt; Next hypothesis has to do with block size overriding in app.config. In
&gt; 1.1, we had specified custom block sizes of 256k. We removed this prior to
&gt; upgrading to 1.2.1 at the advice of #riak since block size configuration
&gt; was ignored prior to 1.2 ('"block\_size" parameter within app.config for
&gt; leveldb was ignored. This parameter is now properly passed to leveldb.'
&gt; --&gt;
&gt; https://github.com/basho/riak/commit/f12596c221a9d942cc23d8e4fd83c9ca46e02105).
&gt; I'm wondering if the block size parameter really was being passed to
&gt; leveldb, and having removed it, blocks are now being rewritten to a new
&gt; size, perhaps different from what they were being written as before (
&gt; https://github.com/basho/riak\_kv/commit/ad192ee775b2f5a68430d230c0999a2caabd1155
&gt; )
&gt;
&gt; Here is the output of the following script showing the increased writes to
&gt; disk (https://gist.github.com/37319a8ed2679bb8b21d)
&gt;
&gt; --an upgraded 1.2.1 node--
&gt; read ios: 238406742
&gt; write ios: 4814320281
&gt; read/write ratio: .04952033
&gt; avg wait: .10712340
&gt; read wait: .49174364
&gt; write wait: .42695475
&gt;
&gt;
&gt; --a node still running 1.1--
&gt; read ios: 267770032
&gt; write ios: 944170656
&gt; read/write ratio: .28360342
&gt; avg wait: .34237204
&gt; read wait: .47222371
&gt; write wait: 1.83283749
&gt;
&gt; And here's what munin is showing us in terms of avg io wait times.
&gt;
&gt; 
&gt;
&gt;
&gt; Any thoughts on what might explain this?
&gt;
&gt; Thanks,
&gt; D
&gt;
&gt;
&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;
&gt;
&gt;
&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;
&gt;
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

