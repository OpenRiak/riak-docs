---
title: "Re: worker_limit reached for map reduce jobs"
description: ""
project: community
lastmod: 2015-08-28T15:05:42-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg16470"
mailinglist_parent_id: "msg16465"
author_name: "Bryan Fink"
project_section: "mailinglistitem"
sent_date: 2015-08-28T15:05:42-07:00
---


On Thu, 27 Aug 2015 at 18:32:31 +0000 Girish Shankarraman &lt;
gshankarra...@vmware.com&gt; wrote:

&gt;
&gt; Hello,
&gt;
&gt; Currently using Riak 2.1.1. I have 7 nodes in my cluster.
&gt; I am looking for some information on how these worker\_limits are
&gt; configured .
&gt;
&gt;
&gt; I have 50 linux(client) hosts trying to run the map reduce jobs on Riak. I
&gt; am getting the error below where some of the hosts complain about the
&gt; worker\_limit being reached.
&gt;
&gt; Looking for some insights on whether I can tune the system to avoid this
&gt; error? Couldn't find too much documentation around the worker\_limit.
&gt;
&gt; {"phase":0,"error":"[worker\_limit\_reached]","input":"{&lt;&lt;\"provisionentry\"&gt;&gt;,&lt;&lt;\"R89Okhz49SDje0y0qvcnkK7xLH0\"&gt;&gt;}","type":"result","stack":"[]"}
&gt; with query MapReduce(path='/mapred', reply\_headers={'content-length':
&gt; '144', 'access-control-allow-headers': 'Content-Type', 'server':
&gt; 'MochiWeb/1.1 WebMachine/1.10.8 (that head fake, tho)', 'connection':
&gt; 'close', 'date': 'Thu, 27 Aug 2015 00:32:22 GMT',
&gt; 'access-control-allow-origin': '\*', 'access-control-allow-methods': 'POST,
&gt; GET, OPTIONS', 'content-type': 'application/json'}, verb='POST',
&gt; headers={'Content-Type': 'application/json'}
&gt;
&gt; Thanks,
&gt;
&gt; - Girish Shankarraman
&gt;

Hi, Girish. The `worker\_limit` is a riak\_pipe tunable. It's there to keep
you from trying to do "too much" at once. Your MR jobs spin up riak\_pipe
workers, and the number of them per vnode is what this tunable limits.

It has been a while since I last recorded this, but to calculate the number
of workers per vnode that an MR job spins up, use roughly this formula:

 (using key-listing/2i/search ? 1 : 0) +
 (number of map stages) +
 (number of map stages with pre-reduce enabled) +
 (number of reduce stages)

Multiply that sum by the number of MR jobs you want to have in-flight
concurrently. That will give you an overestimate of the worker\_limit to use
(because not all stages use a worker on every vnode). The default is a
somewhat arbitrary 50.

Note that you'll need to re-examine the other limits in the system as well
(like the number of JS VMs if your MR jobs use Javascript).

Back in the day, you'd set this by adding a line like so to riak's
app.config:

 {riak\_pipe, [{worker\_limit, 50}]}

That might be something like this in riak.conf, but I'm not sure:

 riak\_pipe.worker\_limit = 50

-Bryan
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

