---
title: "Re: Corrupted Erlang binary term inside LevelDB"
description: ""
project: community
lastmod: 2013-07-25T17:14:49-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg11761"
mailinglist_parent_id: "msg11759"
author_name: "Vladimir Shabanov"
project_section: "mailinglistitem"
sent_date: 2013-07-25T17:14:49-07:00
---


I prefer second option since it will show are the corrupted blocks related
to race condition. First option needs to be run for a long time to be
completely sure that it really fixes the issue.


2013/7/26 Matthew Von-Maszewski 

&gt; Vladimir,
&gt;
&gt; I apologize for not recognizing your name and previous contribution. I
&gt; just tend to think in terms of code and performance bottlenecks, not people.
&gt;
&gt; Your June contribution resulted in changes that were released in 1.4 and
&gt; 1.3.2. I and the team thank you. However, we have not isolated the source
&gt; of the corruption. We only know today that it does not happen very often.
&gt; We have a second, high transaction site, that has seen the same issue.
&gt;
&gt; I can offer you two non-release options:
&gt;
&gt; - I have a branch to 1.4.0 that fixes a potential, but unproven, race
&gt; condition. Details are here:
&gt;
&gt; https://github.com/basho/leveldb/wiki/mv-sst-fadvise
&gt;
&gt; You would have to build eleveldb locally and copy it into your executable
&gt; tree. The 1.4 leveldb and eleveldb work fine with Riak 1.3.x. should you
&gt; desire to limit changes to your production environment.
&gt;
&gt;
&gt; - I have code, soon to be a branch against 1.3.2, that only adds syslog
&gt; error messages to prove / disprove the race condition. You could take this
&gt; code and see if it reports problems. This route would help the community
&gt; and mostly me know the root cause is within the race condition addressed by
&gt; the mv-sst-fadvise branch.
&gt;
&gt;
&gt; The two options above are what I currently have to offer. I am actively
&gt; working to find the corruption source. The good news is that Riak will
&gt; naturally recover from a "bad CRC" when detected. The bad news is that the
&gt; Google defaults let some bad CRCs become good CRCs. Riak 1.4 and 1.3.2
&gt; cannot identify those bad CRCs that became good CRCs.
&gt;
&gt; Matthew
&gt;
&gt;
&gt;
&gt;
&gt; On Jul 25, 2013, at 4:32 PM, Vladimir Shabanov 
&gt; wrote:
&gt;
&gt; Good. Will wait for doctor.
&gt;
&gt; A month ago I mailed about segmentation fault
&gt;
&gt; http://lists.basho.com/pipermail/riak-users\_lists.basho.com/2013-June/012245.html
&gt; After looking at core dumps you have found this problem with CRC checks
&gt; being skipped. I enabled paranoid\_checks and got my node up an running.
&gt;
&gt; I've also found that lost/BLOCKS.bad sometimes appears in partitions and
&gt; have sent you these blocks for further analysis.
&gt;
&gt; It's very interesting why corrupted data appears in the first place. Nodes
&gt; didn't crashed, hardware didn't failed. As I mentioned previously all my
&gt; machines are with ECC memory and Riak data is kept on ZFS filesystem (which
&gt; also checks CRC for all the data and doesn't report any CRC errors). So it
&gt; looks that data is somehow corrupted by Riak itself.
&gt;
&gt; lost/BLOCKS.bad are usually small 2-8kb and appears very infrequently
&gt; (once a week, once a month or never for many partitions). I found these
&gt; BLOCKS.bad in both data/leveldb and data/anti\_entropy. So I have suspicion
&gt; that there is a bug in LevelDB.
&gt;
&gt; Looking at LOGs they are created during compactions:
&gt; "Moving corrupted block to lost/BLOCKS.bad (size 2393)"
&gt; but there is no more information. What kind of block is it, where it was
&gt; found.
&gt;
&gt; Is it possible to somehow find source of those BLOCKS.bad files? I'm
&gt; building Riak from sources, maybe it's possible to enable some additional
&gt; logging to find what these BLOCKS.bad are?
&gt;
&gt;
&gt; 2013/7/25 Matthew Von-Maszewski 
&gt;
&gt;&gt; Vladimir,
&gt;&gt;
&gt;&gt; I can explain what happened, but not how to correct the problem. The
&gt;&gt; gentleman that can walk you through a repair is tied up on another project,
&gt;&gt; but he intends to respond as soon as he is able.
&gt;&gt;
&gt;&gt; We recently discovered / realized that Google's leveldb code does not
&gt;&gt; check the CRC of each block rewritten during a compaction. This means that
&gt;&gt; blocks with bad CRCs get read without being flagged as bad, then rewritten
&gt;&gt; to a new file with a new, valid CRC. The corruption is now hidden.
&gt;&gt;
&gt;&gt; A more thorough discussion of the problem is found here:
&gt;&gt;
&gt;&gt; https://github.com/basho/leveldb/wiki/mv-verify-compactions
&gt;&gt;
&gt;&gt;
&gt;&gt; We added code to the 1.3.2 and 1.4 Riak releases to have the block CRC
&gt;&gt; checked during both read (Get) requests and compaction rewrites. This
&gt;&gt; prevents future corruption hiding. Unfortunately, it does NOTHING for
&gt;&gt; blocks already corrupted and rewritten with valid CRCs. You are
&gt;&gt; encountering this latter condition. We have a developer advocate / client
&gt;&gt; services person that has walked others through a fix via the Riak data
&gt;&gt; replicas …
&gt;&gt;
&gt;&gt; … please hold and the doctor will be with you shortly.
&gt;&gt;
&gt;&gt; Matthew
&gt;&gt;
&gt;&gt;
&gt;&gt; On Jul 24, 2013, at 9:39 PM, Vladimir Shabanov 
&gt;&gt; wrote:
&gt;&gt;
&gt;&gt; Hello,
&gt;&gt;
&gt;&gt; Recently I've started expanding my Riak cluster and found that handoffs
&gt;&gt; were continuously retried for one partition.
&gt;&gt;
&gt;&gt; Here are logs from two nodes
&gt;&gt; https://gist.github.com/vshabanov/41282e622479fbe81974
&gt;&gt;
&gt;&gt; The most interesting parts of logs are
&gt;&gt; "Handoff receiver for partition ... exited abnormally after processing
&gt;&gt; 2860338 objects: {{badarg,[{erlang,binary\_to\_term,..."
&gt;&gt; and
&gt;&gt; "bad argument in call to erlang:binary\_to\_term(&lt;&lt;131,104,...."
&gt;&gt;
&gt;&gt; Both nodes are running Riak 1.3.2 (old one was running 1.3.1 previously).
&gt;&gt;
&gt;&gt;
&gt;&gt; When I've printed corrupted binary string I found that it corresponds to
&gt;&gt; one value.
&gt;&gt;
&gt;&gt; When I've tried to "get" it, it was read OK but node with corrupted value
&gt;&gt; shown the same binary\_to\_term error.
&gt;&gt;
&gt;&gt; When I've tried to delete corrupted value I've got timeout.
&gt;&gt;
&gt;&gt;
&gt;&gt; I'm running machines with ECC memory and ZFS filesystem (which doesn't
&gt;&gt; report any checksum failures) so I doubt data was silently corrupted on
&gt;&gt; disk.
&gt;&gt;
&gt;&gt; LOG from corresponding LevelDB partition doesn't show any errors. But
&gt;&gt; there is a lost/BLOCKS.bad file in this partition (7kb, created more than a
&gt;&gt; month ago and looks like it doesn't contain corrupted value).
&gt;&gt;
&gt;&gt; At the moment I've stopped handoffs using "risk-admin transfer-limit 0".
&gt;&gt;
&gt;&gt; Why the value was corrupted? It there any way to remove it or fix it?
&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt; riak-users mailing list
&gt;&gt; riak-users@lists.basho.com
&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;
&gt;
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

