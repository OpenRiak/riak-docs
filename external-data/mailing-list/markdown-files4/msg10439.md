---
title: "RE: Bigger data than disk space?"
description: ""
project: community
lastmod: 2013-03-14T14:09:35-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg10439"
mailinglist_parent_id: "msg10438"
author_name: "Kevin Burton"
project_section: "mailinglistitem"
sent_date: 2013-03-14T14:09:35-07:00
---


Then that is not quite as bad but still if I have 10 GB of data and to
support replication that requires 30 GB of disk space, what if I only have
20 GB of disk space per physical node?

-----Original Message-----
From: Mark Phillips [mailto:m...@basho.com] 
Sent: Thursday, March 14, 2013 4:05 PM
To: Kevin Burton
Cc: Alexander Sicular; riak-users@lists.basho.com
Subject: Re: Bigger data than disk space?

Kevin,

On Thu, Mar 14, 2013 at 1:56 PM, Kevin Burton 
wrote:
&gt; So that is what I am missing. If each vnode keeps an entire copy of my 
&gt; data and I have 4 physical node then there are 16 vnodes per physical 
&gt; node. That would mean I have the data replicated 16 times per physical 
&gt; node. 10 GB turns into 160GB etc. Right? So wont I run out of disk space?
&gt;

Your raw data set is replicated 3 times by default. Three different vnodes
of your total (by default 64) will be responsible for each replica. So, 10GB
raw = 30GB replicated.

Mark

&gt;
&gt;
&gt; From: Alexander Sicular [mailto:sicul...@gmail.com]
&gt; Sent: Thursday, March 14, 2013 3:51 PM
&gt;
&gt;
&gt; To: Kevin Burton
&gt; Cc: riak-users@lists.basho.com
&gt; Subject: Re: Bigger data than disk space?
&gt;
&gt;
&gt;
&gt; Each vnode keeps \_an entire copy\_ of your data. There is no striping, 
&gt; which I think you are conflating with RAID. Default replication (also 
&gt; configured in etc/app.config) is set to three. In which case, three 
&gt; entire copies of your data are kept on three different vnodes and if 
&gt; you indeed have five physical nodes in your cluster you are guaranteed 
&gt; to have each of those three vnodes on different physical machines.
&gt;
&gt;
&gt; -Alexander Sicular
&gt;
&gt;
&gt;
&gt; @siculars
&gt;
&gt;
&gt;
&gt; On Mar 14, 2013, at 4:42 PM, "Kevin Burton" 
&gt; wrote:
&gt;
&gt;
&gt;
&gt; Thank you. Let me get it straight. I have a 4 node cluster (4 physical 
&gt; machines). If I have not made any changes to the ring size then I have 
&gt; 16
&gt; (64/4) vnodes. Each physical node stores the actual data (the value) 
&gt; of about ¼ of the data size. So when querying the data with a key 
&gt; given the number of vnodes it can be determined which physical machine the
data is on.
&gt; There must be enough redundancy built in so that if one or more of the 
&gt; physical machines go down the remaining physical machines can 
&gt; reconstruct the values lost by the lost vnodes. Correct so far? Now 
&gt; where does replication some in? The documentation indicates that there 
&gt; are 3 copies of the data (default) made. How is this changed and how 
&gt; can this replication of the data be taken advantage of?
&gt;
&gt;
&gt;
&gt; From: Alexander Sicular [mailto:sicul...@gmail.com]
&gt; Sent: Thursday, March 14, 2013 3:28 PM
&gt; To: Kevin Burton
&gt; Cc: riak-users@lists.basho.com
&gt; Subject: Re: Bigger data than disk space?
&gt;
&gt;
&gt;
&gt; Hi Kevin,
&gt;
&gt;
&gt;
&gt; The Riak distribution model is not based on "buckets" but rather the 
&gt; hash of the bucket/key combination. That hash (and associated data) is 
&gt; then allocated against a "vnode". A vnode, in turn, is one of n where 
&gt; n is the ring\_creation\_size (default is 64, modify in etc/app.config). 
&gt; Each physical machine in a Riak cluster claims an equal share of the 
&gt; ring. For example, a cluster with five machines (the recommended 
&gt; minimum for a production
&gt; cluster) and the default ring\_creation\_size will have 64/5 vnodes per 
&gt; physical machine (not sure if they round down or up but all machines 
&gt; will have about the same number of vnodes). What you would do to make 
&gt; more data available is either add a machine to the cluster whose 
&gt; available disk space is equal or greater than the cluster member with 
&gt; the least amount of total space or increase the space on all machines
already in the cluster.
&gt;
&gt;
&gt;
&gt; tl;dr add a machine to your cluster.
&gt;
&gt;
&gt;
&gt;
&gt; -Alexander Sicular
&gt;
&gt;
&gt;
&gt; @siculars
&gt;
&gt;
&gt;
&gt; On Mar 14, 2013, at 3:41 PM, Kevin Burton 
wrote:
&gt;
&gt;
&gt;
&gt;
&gt; I am relatively new to Riak so forgive me if this has been asked 
&gt; before. I have a very thin understanding of a Riak cluster and 
&gt; understand somewhat about replication. In planning I foresee a time 
&gt; when the amount of data exceeds the disk space that is available to a 
&gt; single node. What facilities are there to essentially split a bucket 
&gt; across several servers? How is this handled?
&gt;
&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;
&gt;
&gt;
&gt;
&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;


\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

