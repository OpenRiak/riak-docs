---
title: "Re: May allow_mult cause DoS?"
description: ""
project: community
lastmod: 2013-12-21T01:26:15-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg13319"
mailinglist_parent_id: "msg13315"
author_name: "Russell Brown"
project_section: "mailinglistitem"
sent_date: 2013-12-21T01:26:15-08:00
---


Hi,

On 20 Dec 2013, at 23:16, Jason Campbell  wrote:

&gt; 
&gt; ----- Original Message -----
&gt; From: "Andrew Stone" 
&gt; To: "Jason Campbell" 
&gt; Cc: "Sean Cribbs" , "riak-users" 
&gt; , "Viable Nisei" 
&gt; Sent: Saturday, 21 December, 2013 10:01:29 AM
&gt; Subject: Re: May allow\_mult cause DoS?
&gt; 
&gt; 
&gt;&gt; Think of an object with thousands of siblings. That's an object that has 1 
&gt;&gt; copy of the data for each sibling. That object could be on the order of 100s 
&gt;&gt; of megabytes. Everytime an object is read off disk and returned to the 
&gt;&gt; client 100mb is being transferred. Furthermore leveldb must rewrite the 
&gt;&gt; entire 100mb to disk everytime a new sibling is added. And it just got 
&gt;&gt; larger with that write. If a merge occurs, the amount of data is a single 
&gt;&gt; copy of the data at that key instead of what amounts to approximately 10000 
&gt;&gt; copies of the same sized data, when all you care about is one of those 
&gt;&gt; 10,000. 
&gt; 
&gt; This makes sense for concurrent writes, but the use case that was being 
&gt; talked about was siblings with no parent object.

What is a "sibling with no parent object”? I think I understand what you’re 
getting at, when each sibling is some fragment of the whole, is that it?

&gt; I understand the original use case being discussed was tens of millions of 
&gt; objects, and the metadata alone would likely exceed recommended object sizes 
&gt; in Riak.
&gt; I've mentioned my use case before, which is trying to get fast writes on 
&gt; large objects. I abuse siblings to some extent, although by the nature of 
&gt; the data, there will never be more than a few thousand small siblings (under 
&gt; a hundred bytes). I merge them on read and write the updated object back. 
&gt; Even with sibling metadata, I doubt the bloated object is over a few MB, 
&gt; especially with snappy compression which handles duplicate content quite 
&gt; well. Even if Riak merges the object on every write, it's still much faster 
&gt; than transferring the whole object over the network every time I want to do a 
&gt; small write. Is there a more efficient way to do this? I thought about 
&gt; writing single objects and using a custom index, but that results in a read 
&gt; and 2 writes, and the index could grow quite large compared to the amount of 
&gt; data I'm writing.

This is similar, i suppose, to Riak 2.0 data types. We send an operation to 
Riak, and apply that inside the database rather then fetching, mutating, 
writing at the client. Think of adding to a Set, you just send the thing to be 
added and Riak merges it for you. For your use case would a user defined merge 
function in the database be a valuable feature? It would be every better if 
Riak stored data differently (incrementally, append-only rather than 
read-merge-write at the vnode.) These are things we’re going to be working on 
soon (I hope!) I had no idea that people used siblings this way. It’s 
interesting.

Cheers

Russell

&gt; 
&gt; Thanks,
&gt; Jason
&gt; 
&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com



signature.asc
Description: Message signed with OpenPGP using GPGMail
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

