---
title: "Re: Node Recovery Questions"
description: ""
project: community
lastmod: 2018-08-09T15:26:52-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg18565"
mailinglist_parent_id: "msg18564"
author_name: "sean mcevoy"
project_section: "mailinglistitem"
sent_date: 2018-08-09T15:26:52-07:00
---


Hi Martin,
Thanks for taking the time.
Yes, by "size of the bitcask directory" I mean I did a "du -h --max-depth=1
bitcask", so I think that would cover all the vnodes. We don't use any
other backends.
Those answers are helpful, will get back to this in a few days and see what
I can determine about where our data physically lies. Might have more
questions then.
Cheers,
//Sean.

On Wed, Aug 8, 2018 at 6:05 PM, Martin Sumner 
wrote:

&gt; Based on a quick read of the code, compaction in bitcask is performed only
&gt; on "readable" files, and the current active file for writing is excluded
&gt; from that list. With default settings, that active file can grow to 2GB.
&gt; So it is possible that if objects had been replaced/deleted many times
&gt; within the active file, that space will not be recovered if all the
&gt; replacements amount to &lt; 2GB per vnode. So at these small data sizes - you
&gt; may get a relatively significant discrepancy between an old and recovered
&gt; node in terms of disk space usage.
&gt;
&gt; On 8 August 2018 at 17:37, Martin Sumner 
&gt; wrote:
&gt;
&gt;&gt; Sean,
&gt;&gt;
&gt;&gt; Some partial answers to your questions.
&gt;&gt;
&gt;&gt; I don't believe force-replace itself will sync anything up - it just
&gt;&gt; reassigns ownership (hence handoff happens very quickly).
&gt;&gt;
&gt;&gt; Read repair would synchronise a portion of the data. So if 10% of you
&gt;&gt; data is read regularly, this might explain some of what you see.
&gt;&gt;
&gt;&gt; AAE should also repair your data. But if nothing has happened for 4
&gt;&gt; days, then that doesn't seem to be the case. It would be worth checking
&gt;&gt; the aae-status page (http://docs.basho.com/riak/kv
&gt;&gt; /2.2.3/using/admin/riak-admin/#aae-status) to confirm things are
&gt;&gt; happening.
&gt;&gt;
&gt;&gt; I don't know if there are any minimum levels of data before bitcask will
&gt;&gt; perform compaction. There's nothing obvious in the code that wouldn't be
&gt;&gt; triggered way before 90%. I don't know if it will merge on the active file
&gt;&gt; (the one currently being written to), but that is 2GB max size (configured
&gt;&gt; through bitcask.max\_file\_size).
&gt;&gt;
&gt;&gt; When you say the size of the bitcask directory - is this the size shared
&gt;&gt; across all vnodes on the node? I guess if each vnode has a single file
&gt;&gt; &lt;2GB, and there are multiple vnodes - something unexpected might happen
&gt;&gt; here? If bitcask does indeed not merge the file active for writing.
&gt;&gt;
&gt;&gt; In terms of distribution around the cluster, if you have an n\_val of 3
&gt;&gt; you should normally expect to see a relatively even distribution of the
&gt;&gt; data on failure (certainly not it all going to one). Worst case scenario
&gt;&gt; is that 3 nodes get all the load from that one failed node.
&gt;&gt;
&gt;&gt; When a vnode is inaccessible, 3 (assuming n=3) fallback vnodes are
&gt;&gt; selected to handle the load for that 1 vnode (as that vnode would normally
&gt;&gt; be in 3 preflists, and commonly a different node will be asked to start a
&gt;&gt; vnode for each preflist).
&gt;&gt;
&gt;&gt;
&gt;&gt; I will try and dig later into bitcask merge/compaction code, to see if I
&gt;&gt; spot anything else.
&gt;&gt;
&gt;&gt; Martin
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

