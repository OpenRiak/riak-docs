---
title: "Re: Solr indexes are dropped after recovering a node"
description: ""
project: community
lastmod: 2015-12-24T08:02:31-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg16888"
mailinglist_parent_id: "msg16886"
author_name: "Fred Dushin"
project_section: "mailinglistitem"
sent_date: 2015-12-24T08:02:31-08:00
---


Hi Istvan,

A couple of questions:

1. Do the buckets (or bucket types) the data is written to still contain the 
search index to be used to index the data?
2. Did the indices get re-created on the new nodes you added? You can verify 
this by looking at /var/lib/riak/yz/ (or wherever your platform 
data directory is on your platform), where  is the name of the index
3. Do you have AAE enabled?

If the indices have been removed on all of the nodes, you will need to recreate 
them, via the riak-admin command [1].

If you have AAE enabled, data should get reindexed when the trees expire. You 
can force them to expire via riak attach:

shell$ bin/riak attach
Remote Shell: Use "Ctrl-C a" to quit. q() or init:stop() will terminate the 
riak node.
Erlang R16B02\_basho9 (erts-5.10.3) [source] [64-bit] [smp:8:8] 
[async-threads:10] [kernel-poll:false] [dtrace]

Eshell V5.10.3 (abort with ^G)
(dev1@127.0.0.1)1&gt; yz\_entropy\_mgr:expire\_trees(). 
ok

It may take some time for the trees to rebuild, but eventually you should see 
entries in the log, such as

2015-12-23 21:18:04.536 [info] &lt;0.4457.0&gt;@yz\_exchange\_fsm:key\_exchange:176 Will 
repair 342 keys of partition 0 for preflist 
{1278813932664540053428224228626747642198940975104,3}
2015-12-23 21:18:19.541 [info] &lt;0.4533.0&gt;@yz\_exchange\_fsm:key\_exchange:176 Will 
repair 368 keys of partition 0 for preflist 
{1370157784997721485815954530671515330927436759040,3}
2015-12-23 21:18:34.560 [info] &lt;0.4609.0&gt;@yz\_exchange\_fsm:key\_exchange:176 Will 
repair 347 keys of partition 0 for preflist {0,3}

If you are not happy with that pace, you can adjust the hash tree build limits 
as described in the "Hash Trees" section of [2]. Those settings are for 
Riak/KV AAE, but YZ will inherit them automatically on restart. If you only 
want to adjust these settings for YZ AAE, you can use the settings listed in 
[3], but you'll need to set those in your advanced.config, as I do not believe 
we have cuttlefish schema for these settings.

Some examples:

# riak.conf
anti\_entropy.tree.build\_limit.per\_timespan = 1m
anti\_entropy.tree.build\_limit.number = 5

%% advanced.config
...
{yokozuna, [{anti\_entropy\_concurrency, 5}, {anti\_entropy\_build\_limit, {5, 
60000}}]}
...

Hope that helps.

-Fred

[1] http://docs.basho.com/riak/latest/dev/using/search/ 

[2] http://docs.basho.com/riak/latest/ops/advanced/aae/#Configuring-AAE 

[3] https://github.com/basho/yokozuna/blob/2.1.1/include/yokozuna.hrl#L192



&gt; On Dec 23, 2015, at 5:10 PM, István  wrote:
&gt; 
&gt; Hi Jason,
&gt; 
&gt; Actually I was moving nodes, one by one. I guess I was missing the riak-admin 
&gt; replace command. Is there an easy way of restoring Solr indexes on disk on a 
&gt; node? The data is fine after the recovery but the index data got deleted in 
&gt; the yz folder. Any advice how to restore it when a cluster is running and the 
&gt; data is there?
&gt; 
&gt; If the fastest way to recover is the reload the entire dataset that is fine 
&gt; too.
&gt; 
&gt; Thank you in advance,
&gt; Istvan
&gt; 
&gt; On Wed, Dec 23, 2015 at 7:51 PM, Jason Voegele  &gt; wrote:
&gt;&gt; On Dec 23, 2015, at 12:54 PM, István &gt; &gt; wrote:
&gt;&gt; 
&gt;&gt; Hi,
&gt;&gt; 
&gt;&gt; I had to move the nodes of a Riak cluster to new ones. Everything is fine 
&gt;&gt; with the data, we have been following the recovery procedures here:
&gt;&gt; 
&gt;&gt; http://docs.basho.com/riak/latest/ops/running/backups/#Restoring-a-Node 
&gt;&gt; 
&gt;&gt; 
&gt;&gt; After the moving all of the nodes I found out that all of the Solr indexes 
&gt;&gt; are gone.
&gt; 
&gt; Hi Istvan,
&gt; 
&gt; It looks like you are restoring an entire cluster, not just a single node 
&gt; within a cluster. If so, the relevant recovery procedures are documented on 
&gt; this page:
&gt; 
&gt; http://docs.basho.com/riak/latest/ops/running/recovery/failure-recovery/#Cluster-Recovery-From-Backups
&gt; 
&gt; 
&gt; 
&gt; Can you try following the full cluster recovery procedure and see if that 
&gt; solves the problem?
&gt; 
&gt; -- 
&gt; Jason Voegele
&gt; Manly's Maxim:
&gt; Logic is a systematic method of coming to the wrong conclusion
&gt; with confidence.
&gt; 
&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com 
&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com 
&gt; 
&gt; 
&gt; 
&gt; 
&gt; 
&gt; -- 
&gt; the sun shines for all
&gt; 
&gt; 
&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

