---
title: "Re: Having N replicas more than nodes in the cluster"
description: ""
project: community
lastmod: 2010-05-11T11:02:37-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg00324"
mailinglist_parent_id: "msg00321"
author_name: "Grant Schofield"
project_section: "mailinglistitem"
sent_date: 2010-05-11T11:02:37-07:00
---


On May 11, 2010, at 12:43 PM, Randall Leeds wrote:

&gt; On Tue, May 11, 2010 at 08:20, Grant Schofield  wrote:
&gt;&gt;&gt; I have a two nodes cluster and a bucket with n\_val = 3.
&gt;&gt;&gt; I put more than 1 600 000 documents in the bucket and it takes less than 
&gt;&gt;&gt; 200 GB with Innostore.
&gt;&gt;&gt; I shut down the second node with "q()." in a "riak console".
&gt;&gt;&gt; Firstly, there was no synchronisation of data on the first node, i assume 
&gt;&gt;&gt; it's better because very similar to a node crash.
&gt;&gt;&gt; Secondly, when i did a read request on a document with :
&gt;&gt;&gt; - ?r=1 -&gt; success
&gt;&gt;&gt; - ?r=2 -&gt; success
&gt;&gt;&gt; - ?r=3 -&gt; fail
&gt;&gt; 
&gt;&gt; This is due to the fact that 2 of the 3 vnodes were able to respond with 
&gt;&gt; your data, but not the third because your took down the node that had 1 of 
&gt;&gt; the 3 partitions.
&gt; 
&gt; After the next round of gossip, will the remaining Riak node attempt
&gt; to reclaim these vnodes or does the ring structure persist until the
&gt; node has explicitly left?

The remaining nodes won't try to take over a partition unless the missing node 
is removed from the cluster explicitly. However, other nodes will handle writes 
so when the node comes back online it will go through the hinted handoff 
process.

&gt; 
&gt; -Randall


\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

