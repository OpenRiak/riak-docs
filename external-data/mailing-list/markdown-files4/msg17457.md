---
title: "Re: How to cold (re)boot a cluster with already existing node data"
description: ""
project: community
lastmod: 2016-06-06T10:33:26-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg17457"
mailinglist_parent_id: "msg17455"
author_name: "Jan-Philip Loos"
project_section: "mailinglistitem"
sent_date: 2016-06-06T10:33:26-07:00
---


&gt; this might be helpful, an Omniti article.
&gt; https://omniti.com/seeds/migrating-riak-do-it-live
&gt;

Thanks for this article, will be valuable for further managing riak in the
cluster.


&gt; As to fixing this specific error. That iirc can be done doing a name
&gt; change in the ring to match your new node name. renaming the node will
&gt; make that orddict lookup succeed.
&gt; Theres a supplied admin utility for that.
&gt;
&gt;
The problem is, there is no cluster anymore just single nodes, because
kubernetes teared all nodes down. And each single node won't start on its
own, because of the orddict error. And IIRC riak-admin cluster replace
works only on a node which is already running, right?

Currently my problem seems to be solved. After 13 hours of fixing our
cluster I missed one node to reintegrate into the cluster but it was
already a load balancing target.

Can I assume, the /var/lib/riak/ring folder is not data-critical? So after
clustering all new nodes with all old data, the data integrity is preserved?

Greeting

Jan
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

