---
title: "Re: Performance Tuning in OmniOS"
description: ""
project: community
lastmod: 2014-01-21T11:40:25-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg13438"
mailinglist_parent_id: "msg13430"
author_name: "Tom Santero"
project_section: "mailinglistitem"
sent_date: 2014-01-21T11:40:25-08:00
---


this message from Hari John Kuriakose was accidentally auto-discarded.
(sorry!)

From: Hari John Kuriakose 
To: Jared Morrow 
Cc: riak-users , Hector Castro 
Date: Wed, 22 Jan 2014 00:35:37 +0530
Subject: Re: Performance Tuning in OmniOS

I am using the default raid itself.

Well, if this is the case, I will run the tests again with a different
setup as you said, and get back as soon as possible. I would just like to
believe that OmniOS is not too hopeless.

Thank you.



On Tue, Jan 21, 2014 at 12:47 PM, Jared Morrow  wrote:

&gt; What type of RAID did you chose for your spool of 5 volumes? If you chose
&gt; the default of raidz, you will not be getting much of a performance boost
&gt; over vanilla EBS, just a big integrity boost. Also, unless you are using
&gt; provisioned IOPS for EBS, you are starting from an extremely slow
&gt; base-case, so adding ZFS on top might not help matters much.
&gt;
&gt; If speed is the concern, as a test I'm willing to bet if you do another
&gt; test run against the two instance storage disks on that m1.large, you will
&gt; probably beat those 5 EBS volumes pretty easily.
&gt;
&gt; -Jared
&gt;
&gt;
&gt; On Tue, Jan 21, 2014 at 9:22 AM, Hari John Kuriakose wrote:
&gt;
&gt;&gt; Hello,
&gt;&gt;
&gt;&gt; I am using standard EBS devices, with a zpool in an instance comprising
&gt;&gt; of five 40GB volumes.
&gt;&gt; Each of the Riak instance is of m1.large type.
&gt;&gt;
&gt;&gt; I have made the following changes in zfs properties:
&gt;&gt;
&gt;&gt; # My reason: the default sst block size for leveldb is 4k.
&gt;&gt; zfs set recordsize=4k tank/riak
&gt;&gt; # My reason: by default, leveldb verifies checksums automatically.
&gt;&gt; zfs set checksum=off tank/riak
&gt;&gt; zfs set atime=off tank/riak
&gt;&gt; zfs set snapdir=visible tank/riak
&gt;&gt;
&gt;&gt; And I did the following with help from Basho AWS tuning docs:
&gt;&gt;
&gt;&gt; projadd -c "riak" -K "process.max-file-descriptor=(basic,65536,deny)"
&gt;&gt; user.riak
&gt;&gt; bash -c "echo 'set rlim\_fd\_max=65536' &gt;&gt; /etc/system"
&gt;&gt; bash -c "echo 'set rlim\_fd\_cur=65536' &gt;&gt; /etc/system"
&gt;&gt; ndd -set /dev/tcp tcp\_conn\_req\_max\_q0 40000
&gt;&gt; ndd -set /dev/tcp tcp\_conn\_req\_max\_q 4000
&gt;&gt; ndd -set /dev/tcp tcp\_tstamp\_always 0
&gt;&gt; ndd -set /dev/tcp tcp\_sack\_permitted 2
&gt;&gt; ndd -set /dev/tcp tcp\_wscale\_always 1
&gt;&gt; ndd -set /dev/tcp tcp\_time\_wait\_interval 60000
&gt;&gt; ndd -set /dev/tcp tcp\_keepalive\_interval 120000
&gt;&gt; ndd -set /dev/tcp tcp\_xmit\_hiwat 2097152
&gt;&gt; ndd -set /dev/tcp tcp\_recv\_hiwat 2097152
&gt;&gt; ndd -set /dev/tcp tcp\_max\_buf 8388608
&gt;&gt;
&gt;&gt; Thanks again.
&gt;&gt;
&gt;&gt;
&gt;&gt; On Tue, Jan 21, 2014 at 9:12 PM, Hector Castro  wrote:
&gt;&gt;
&gt;&gt;&gt; Hello,
&gt;&gt;&gt;
&gt;&gt;&gt; Can you please clarify what type of disk you are using within AWS?
&gt;&gt;&gt; EBS, EBS with PIOPS, instance storage? In addition, maybe some details
&gt;&gt;&gt; on volume sizes and instance types.
&gt;&gt;&gt;
&gt;&gt;&gt; These details may help someone attempting to answer your question.
&gt;&gt;&gt;
&gt;&gt;&gt; --
&gt;&gt;&gt; Hector
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; On Tue, Jan 21, 2014 at 8:11 AM, Hari John Kuriakose 
&gt;&gt;&gt; wrote:
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; I am running LevelDB on ZFS in Solaris (OmniOS specifically) in Amazon
&gt;&gt;&gt; AWS.
&gt;&gt;&gt; &gt; The iops is very very low. There is no significant progress with
&gt;&gt;&gt; tuning too.
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; Why I chose ZFS is that since LevelDB requires the node to be stopped
&gt;&gt;&gt; before
&gt;&gt;&gt; &gt; taking a backup, I needed a filesystem with snapshot ability. And the
&gt;&gt;&gt; most
&gt;&gt;&gt; &gt; favourable Amazon community AMI seemed to be using OmniOS (fork of
&gt;&gt;&gt; Solaris).
&gt;&gt;&gt; &gt; Everything is fine, except the performance.
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; I did all the AWS tuning proposed by Basho but still Basho Bench gave
&gt;&gt;&gt; twice
&gt;&gt;&gt; &gt; iops on Ubuntu as compared to OmniOS, under same conditions. Also, I am
&gt;&gt;&gt; &gt; using riak-js client library, and its a 5 node Riak cluster with 8GB
&gt;&gt;&gt; ram
&gt;&gt;&gt; &gt; each.
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; Could not yet figure out what is really causing the congestion in
&gt;&gt;&gt; OmniOS.
&gt;&gt;&gt; &gt; Any pointers will be really helpful.
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; Thanks and regards,
&gt;&gt;&gt; &gt; Hari John Kuriakose.
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt;&gt; &gt; riak-users mailing list
&gt;&gt;&gt; &gt; riak-users@lists.basho.com
&gt;&gt;&gt; &gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;&gt;&gt; &gt;
&gt;&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt; riak-users mailing list
&gt;&gt; riak-users@lists.basho.com
&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;&gt;
&gt;&gt;
&gt;
&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;
&gt;
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

