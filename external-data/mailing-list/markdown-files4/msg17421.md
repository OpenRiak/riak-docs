---
title: "Re: Very slow acquisition time (99 percentile) while fast median times"
description: ""
project: community
lastmod: 2016-05-27T10:32:12-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg17421"
mailinglist_parent_id: "msg17275"
author_name: "Alexander Sicular"
project_section: "mailinglistitem"
sent_date: 2016-05-27T10:32:12-07:00
---


Awesome! Ya, Solr like resources. If you're on 3 nodes now, consider
adjusting your n\_val from default 3 to 2. With default ring\_size of 64 and
n\_val of 3 and a cluster size less than 5 you are not guaranteed to have
all copies of your data on distinct physical nodes. Some nodes will receive
2 copies of data. Just be aware of that.

On Friday, May 27, 2016, Guillaume Boddaert &lt;
guilla...@lighthouse-analytics.co&gt; wrote:

&gt; A little follow up for you guys since I went offline for quite some times.
&gt;
&gt; As suggested, it was a Solr performance issue, we were able to prove that
&gt; my old 5 hosts were able to handle the load without Solr/Yokozuna.
&gt; Fact was that I lacked CPU for my host, as well as RAM. Since SolR is
&gt; pretty resource consuming, so I switched from :
&gt; - 5 x 16Gb x 2 CPU hosts
&gt; to
&gt; - 3 x 120Gb x 8 CPU hosts
&gt;
&gt; And it now works like a charm,
&gt;
&gt; Thanks for the help (especially to Damien)
&gt;
&gt; Guillaume
&gt;
&gt; On 04/05/2016 15:17, Matthew Von-Maszewski wrote:
&gt;
&gt; Guillaume,
&gt;
&gt; Two points:
&gt;
&gt; 1. You can send the “riak debug” from one server and I will verify that
&gt; 2.0.18 is indicated in the LOG file.
&gt;
&gt; 2. Your previous “riak debug” from server “riak1” indicated that only two
&gt; CPU cores existed. We performance test with eight, twelve, and twenty-four
&gt; core servers, not two. You have two heavy weight applications, Riak and
&gt; Solr, competing for time on those two cores. Actually, you have three
&gt; applications due to leveldb’s background compaction operations.
&gt;
&gt; One leveldb compaction is CPU intensive. The compaction reads a block
&gt; from the disk, computes a CRC32 check of the block, decompresses the block,
&gt; merges the keys of this block with one or more blocks from other files,
&gt; then compresses the new block, computes a new CRC32, and finally writes the
&gt; block to disk. And there can be multiple compactions running
&gt; simultaneously. All of your CPU time could be periodically lost to leveldb
&gt; compactions.
&gt;
&gt; There are some minor tunings we could do, like disabling compression in
&gt; leveldb, that might help. But I seriously doubt you are going to achieve
&gt; your desired results with only two cores. Adding a sixth server with two
&gt; cores is not really going to help.
&gt;
&gt; Matthew
&gt;
&gt;
&gt; On May 4, 2016, at 4:27 AM, Guillaume Boddaert &lt;
&gt; guilla...@lighthouse-analytics.co
&gt; &gt;
&gt; wrote:
&gt;
&gt; Thanks, I've installed the new library as stated in the documentation
&gt; using 2.0.18 files.
&gt;
&gt; I was unable to find the vnode LOG file from the documentation, as my
&gt; vnodes looks like file, not directories. So I can't verify that I run the
&gt; proper version of the library after my riak restart.
&gt;
&gt; Anyway, it has unfortunately no effect:
&gt;
&gt; http://www.awesomescreenshot.com/image/1219821/1b292613c051da86df5696034c114b14
&gt;
&gt; I think i'll try to add a 6th node that don't rely on network disks and
&gt; see what's going on.
&gt;
&gt; G.
&gt;
&gt;
&gt; On 03/05/2016 22:47, Matthew Von-Maszewski wrote:
&gt;
&gt; Guillaume,
&gt;
&gt; A prebuilt eleveldb 2.0.18 for Debian 7 is found here:
&gt;
&gt;
&gt; -
&gt; 
&gt; 
&gt; 
&gt; https://s3.amazonaws.com/downloads.basho.com/patches/eleveldb/2.0.18/eleveldb\_2.0.18\_debian7.tgz
&gt;
&gt;
&gt; There are good instructions for applying an eleveldb patch here:
&gt;
&gt;
&gt; 
&gt; http://docs.basho.com/community/productadvisories/leveldbsegfault/#patch-eleveldb-so
&gt;
&gt; Key points about the above web page:
&gt;
&gt; - use the eleveldb patch file link in this email, NOT links on the web page
&gt;
&gt; - the Debian directory listed on the web page will be slightly different
&gt; than your Riak 2.1.4 installation:
&gt;
&gt; /usr/lib/riak/lib/eleveldb-/priv/
&gt;
&gt;
&gt; Matthew
&gt;
&gt;
&gt; On May 3, 2016, at 1:01 PM, Matthew Von-Maszewski &lt;
&gt; matth...@basho.com
&gt; &gt; wrote:
&gt;
&gt; Guillaume,
&gt;
&gt; I have reviewed the debug package for your riak1 server. There are two
&gt; potential areas of follow-up:
&gt;
&gt; 1. You are running our most recent Riak 2.1.4 which has eleveldb 2.0.17.
&gt; We have seen a case where a recent feature in eleveldb 2.0.17 caused too
&gt; much cache flushing, impacting leveldb’s performance. A discussion is here:
&gt;
&gt; https://github.com/basho/leveldb/wiki/mv-timed-grooming2
&gt;
&gt; 2. Yokozuna search was recently updated for some timeout problems. Those
&gt; updates are not yet in a public build. One of our other engineers is
&gt; likely to respond to you on that topic.
&gt;
&gt;
&gt; An eleveldb 2.0.18 is tagged and available via github if you want to build
&gt; it yourself. Otherwise, Basho may be releasing prebuilt patches of
&gt; eleveldb 2.0.18 in the near future. But no date is currently set.
&gt;
&gt; Matthew
&gt;
&gt; On May 3, 2016, at 10:50 AM, Luke Bakken &lt;
&gt; lbak...@basho.com
&gt; &gt; wrote:
&gt;
&gt; Guillaume -
&gt;
&gt; You said earlier "My data are stored on an openstack volume that
&gt; support up to 3000IOPS". There is a likelihood that your write load is
&gt; exceeding the capacity of your virtual environment, especially if some
&gt; Riak nodes are sharing physical disk or server infrastructure.
&gt;
&gt; Some suggestions:
&gt;
&gt; \* If you're not using Riak Search, set "search = off" in riak.conf
&gt;
&gt; \* Be sure to carefully read and apply all tunings:
&gt; http://docs.basho.com/riak/kv/2.1.4/using/performance/
&gt;
&gt; \* You may wish to increase the memory dedicated to leveldb:
&gt; http://docs.basho.com/riak/kv/2.1.4/configuring/backend/#leveldb
&gt;
&gt; --
&gt; Luke Bakken
&gt; Engineer
&gt; lbak...@basho.com 
&gt;
&gt;
&gt; On Tue, May 3, 2016 at 7:33 AM, Guillaume Boddaert
&gt; 
&gt;  wrote:
&gt;
&gt; Hi,
&gt;
&gt; Sorry for the delay, I've spent a lot of time trying to understand if the
&gt; problem was elsewhere.
&gt; I've simplified my infrastructure and got a simple layout that don't rely
&gt; anymore on loadbalancer and also corrected some minor performance issue on
&gt; my workers.
&gt;
&gt; At the moment, i have up to 32 workers that are calling riak for writes,
&gt; each of them are set to :
&gt; w=1
&gt; dw=0
&gt; timeout=1000
&gt; using protobuf
&gt; a timeouted attempt is rerun 180s later
&gt;
&gt; From my application server perspective, 23% of the calls are rejected by
&gt; timeout (75446 tries, 57564 success, 17578 timeout).
&gt;
&gt; Here is a sample riak-admin stat for one of my 5 hosts:
&gt;
&gt; node\_put\_fsm\_time\_100 : 999331
&gt; node\_put\_fsm\_time\_95 : 773682
&gt; node\_put\_fsm\_time\_99 : 959444
&gt; node\_put\_fsm\_time\_mean : 156242
&gt; node\_put\_fsm\_time\_median : 20235
&gt; vnode\_put\_fsm\_time\_100 : 5267527
&gt; vnode\_put\_fsm\_time\_95 : 2437457
&gt; vnode\_put\_fsm\_time\_99 : 4819538
&gt; vnode\_put\_fsm\_time\_mean : 175567
&gt; vnode\_put\_fsm\_time\_median : 6928
&gt;
&gt; I am using leveldb, so i can't tune bitcask backend as suggested.
&gt;
&gt; I've changed the vmdirty settings and enabled them:
&gt; admin@riak1:~$ sudo sysctl -a | grep dirtyvm.dirty\_background\_ratio = 0
&gt; vm.dirty\_background\_bytes = 209715200
&gt; vm.dirty\_ratio = 40
&gt; vm.dirty\_bytes = 0
&gt; vm.dirty\_writeback\_centisecs = 100
&gt; vm.dirty\_expire\_centisecs = 200
&gt;
&gt; I've seen less idle time between writes, iostat is showing near constant
&gt; writes between 20 and 500 kb/s, with some surges around 4000 kb/s. That's
&gt; better, but not that great.
&gt;
&gt; Here is the current configuration for my "activity\_fr" bucket type and
&gt; "tweet" bucket:
&gt;
&gt;
&gt; admin@riak1:~$ http localhost:8098/types/activity\_fr/props
&gt; HTTP/1.1 200 OK
&gt; Content-Encoding: gzip
&gt; Content-Length: 314
&gt; Content-Type: application/json
&gt; Date: Tue, 03 May 2016 14:30:21 GMT
&gt; Server: MochiWeb/1.1 WebMachine/1.10.8 (that head fake, tho)
&gt; Vary: Accept-Encoding
&gt; {
&gt; "props": {
&gt; "active": true,
&gt; "allow\_mult": false,
&gt; "basic\_quorum": false,
&gt; "big\_vclock": 50,
&gt; "chash\_keyfun": {
&gt; "fun": "chash\_std\_keyfun",
&gt; "mod": "riak\_core\_util"
&gt; },
&gt; "claimant":
&gt; 
&gt; "r...@riak2.lighthouse-analytics.co"
&gt; ,
&gt; "dvv\_enabled": false,
&gt; "dw": "quorum",
&gt; "last\_write\_wins": true,
&gt; "linkfun": {
&gt; "fun": "mapreduce\_linkfun",
&gt; "mod": "riak\_kv\_wm\_link\_walker"
&gt; },
&gt; "n\_val": 3,
&gt; "notfound\_ok": true,
&gt; "old\_vclock": 86400,
&gt; "postcommit": [],
&gt; "pr": 0,
&gt; "precommit": [],
&gt; "pw": 0,
&gt; "r": "quorum",
&gt; "rw": "quorum",
&gt; "search\_index": "activity\_fr.20160422104506",
&gt; "small\_vclock": 50,
&gt; "w": "quorum",
&gt; "young\_vclock": 20
&gt; }
&gt; }
&gt;
&gt; admin@riak1:~$ http localhost:8098/types/activity\_fr/buckets/tweet/props
&gt; HTTP/1.1 200 OK
&gt; Content-Encoding: gzip
&gt; Content-Length: 322
&gt; Content-Type: application/json
&gt; Date: Tue, 03 May 2016 14:30:02 GMT
&gt; Server: MochiWeb/1.1 WebMachine/1.10.8 (that head fake, tho)
&gt; Vary: Accept-Encoding
&gt;
&gt; {
&gt; "props": {
&gt; "active": true,
&gt; "allow\_mult": false,
&gt; "basic\_quorum": false,
&gt; "big\_vclock": 50,
&gt; "chash\_keyfun": {
&gt; "fun": "chash\_std\_keyfun",
&gt; "mod": "riak\_core\_util"
&gt; },
&gt; "claimant":
&gt; 
&gt; "r...@riak2.lighthouse-analytics.co"
&gt; ,
&gt; "dvv\_enabled": false,
&gt; "dw": "quorum",
&gt; "last\_write\_wins": true,
&gt; "linkfun": {
&gt; "fun": "mapreduce\_linkfun",
&gt; "mod": "riak\_kv\_wm\_link\_walker"
&gt; },
&gt; "n\_val": 3,
&gt; "name": "tweet",
&gt; "notfound\_ok": true,
&gt; "old\_vclock": 86400,
&gt; "postcommit": [],
&gt; "pr": 0,
&gt; "precommit": [],
&gt; "pw": 0,
&gt; "r": "quorum",
&gt; "rw": "quorum",
&gt; "search\_index": "activity\_fr.20160422104506",
&gt; "small\_vclock": 50,
&gt; "w": "quorum",
&gt; "young\_vclock": 20
&gt; }
&gt; }
&gt;
&gt; I really don't know what to do. Can you help ?
&gt;
&gt; Guillaume
&gt;
&gt;
&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; 
&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;
&gt;
&gt;
&gt;
&gt;
&gt;
&gt;

-- 


Alexander Sicular
Solutions Architect
Basho Technologies
9175130679
@siculars
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

