---
title: "Re: Understanding Riaks rebalancing and handoff behaviour"
description: ""
project: community
lastmod: 2010-11-11T23:44:04-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg01527"
mailinglist_parent_id: "msg01522"
author_name: "Sven Riedel"
project_section: "mailinglistitem"
sent_date: 2010-11-11T23:44:04-08:00
---



On Nov 11, 2010, at 4:05 PM, Nico Meyer wrote:

&gt;&gt;&gt; 
&gt;&gt;&gt; I discovered another problem while debugging this. I you restart (or
&gt;&gt;&gt; it
&gt;&gt;&gt; crashes) a node that you removed from the cluster which still has
&gt;&gt;&gt; data,
&gt;&gt;&gt; it won't start handing off it's data afterwards. The reason being,
&gt;&gt;&gt; that
&gt;&gt;&gt; is the node watcher also does not get notified that the other nodes
&gt;&gt;&gt; are
&gt;&gt;&gt; up, and so all of them are considered down. This also can only be
&gt;&gt;&gt; worked
&gt;&gt;&gt; around manually via the erlang console.
&gt;&gt;&gt; 
&gt;&gt; 
&gt;&gt; 
&gt;&gt; Why would that have to be worked around at all? My understanding is
&gt;&gt; through the data duplication within the ring having a single node
&gt;&gt; encounter a messy and fatal accident shouldn't destabilize the entire
&gt;&gt; ring. The nodes which contain the duplicate data would just take over
&gt;&gt; until a replacement node gets added, and the newly dead node is
&gt;&gt; removed (ok, via console).
&gt;&gt; 
&gt; 
&gt; It's not a problem right away. But since the replicated data is not
&gt; actively synchronized in the background the keys that were not copied
&gt; until the node dies have one less replica. That is until they are read
&gt; at least once, at which point read repair does replicate the key again.
&gt; So it depends on your setup and requirements, if this is acceptable or
&gt; not.

So if the relevant data isn't read in a while and two more nodes go down (with 
an n\_val of 3), there is a chance that some data is lost.

&gt; 
&gt;&gt; 
&gt;&gt; So this still leaves me with some of my original questions open:
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 1. What would normally trigger a rebalancing of the nodes? 
&gt;&gt;&gt;&gt; 2. Is there a way to manually trigger a rebalancing?
&gt;&gt;&gt;&gt; 3. Did I do anything wrong with the procedure described above to
&gt;&gt;&gt;&gt; be left in the current odd state by riak?
&gt;&gt; 
&gt; 
&gt; Every vnode, which is responsible for one partition, checks after 60
&gt; seconds of inactivity if its is residing one the node where it should
&gt; be, according to the ring state. If not, the data is send to the correct
&gt; node. So the rebalancing of data is triggered by rebalancing the
&gt; partitions among the nodes in the ring.
&gt; The ring is balanced during the gossiping of the ring state, which is
&gt; done by every node with another random node at every 0-60 (also
&gt; randomly) seconds.
&gt; In the worst case it could take some minutes before the ring stabilizes,
&gt; but its statistically likely to converge faster.

Ah, if the partition ownership changes, the data should get relocated straight 
away.
I had gossip put down only for immediate topology changes (which worked fine in 
my case, it just never got around to redistributing the data).

&gt; 
&gt; So there's is nothing to trigger manually really. One problem I see has
&gt; to do again with restarting a node, which still has data that should be
&gt; handed off to another node. Initially only the vnodes that are owned by
&gt; a node started, which by definition don't include the ones to be handed
&gt; off. But if the vnodes are never started, they won't perform the
&gt; handoff.

This sounds less confidence inspiring. On the off-chance that the node (or 
hardware) crashes between telling a node to leave the ring and the data handoff 
having been completed, you'll have more work on your hands than just restarting 
the node, and you have to know about this as well. This isn't a scenario that 
will happen often, but from the point of view of someone who wants the least 
amount of manual intervention when something goes wrong, I'd prefer resilience 
over performance.

&gt; It kind of works anyway, but the vnodes are started and transfered
&gt; sequentially. Normally four partitions are transferred in parallel, so I
&gt; don't know if this is by design or by accident. The details are
&gt; convoluted enough to suspect the latter.
&gt; In any case this would also make also have the effect that those
&gt; partitions won't show up in the output of riak-admin transfers, since
&gt; only running vnodes are considered.

&gt;From what I saw in my case the number of handoffs were displayed correctly in 
&gt;the beginning, however the numbers didn't decrease (or change at all) as data 
&gt;got handed around.

&gt; 
&gt; I also forgot, that I patched another problem in my own version of riak,
&gt; which will prevent any further handoff of data after four of them failed
&gt; with an error or timeout. This probably happened in your case, if your A
&gt; nodes became unresponsive for 30 minutes (did the machine swap by the
&gt; way?).

A was up and responsive the entire time. No swapping; the machines don't have 
any swap space configured. However, they were rather weak in the IO department, 
so the load did rise to 10 and above on bulk inserts, and to around 6 during 
the handoffs IIRC (on a two core machine). One of the reasons I wanted to check 
things out on instances with more IO performance.

&gt; I should probably create a bug report for this, with my patch attached.
&gt; Stupid laziness!
&gt; 
&gt; After reading your original post again, I think almost all of the things
&gt; you saw can be explained by the bug that I mentioned in my first answer
&gt; (the ring status of removed nodes is not synchronized with the remaining
&gt; nodes). The problem obviously becomes worse if you remove several nodes
&gt; at a time.

Which means that I shouldn't just wait for riak-admin ringready to return TRUE, 
but for the data handoffs to have completed as well before changing the network 
topology again?

Thanks for your answers, they have been enlightening.

Regards,
Sven

------------------------------------------
Scoreloop AG, Brecherspitzstrasse 8, 81541 Munich, Germany, www.scoreloop.com
sven.rie...@scoreloop.com

Sitz der Gesellschaft: München, Registergericht: Amtsgericht München, HRB 
174805 
Vorstand: Dr. Marc Gumpinger (Vorsitzender), Dominik Westner, Christian van der 
Leeden, Vorsitzender des Aufsichtsrates: Olaf Jacobi 


\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

