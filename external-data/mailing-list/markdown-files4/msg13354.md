---
title: "Re: anti_entropy_expire"
description: ""
project: community
lastmod: 2014-01-03T06:14:00-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg13354"
mailinglist_parent_id: "msg13348"
author_name: "Edgar Veiga"
project_section: "mailinglistitem"
sent_date: 2014-01-03T06:14:00-08:00
---


By the way,

I think that the number of repaired key is pretty high.

2014-01-03 06:33:42.857 [info]
&lt;0.31440.2586&gt;@riak\_kv\_exchange\_fsm:key\_exchange:206 Repaired 1491787 keys
during active anti-entropy exchange of
{468137243207554840987117797979434404733540892672,3} between
{473846233978378680511350941857232385279071879168,'riak@192.168.20.112'}
and {479555224749202520035584085735030365824602865664,'riak@192.168.20.107'}

I have few but consistent lines like this (every two hours, during this
process).

Best regards.

On 2 January 2014 10:05, Edgar Veiga  wrote:

&gt; This is the only thing related to AAE that exists in my app.config. I
&gt; haven't changed any default values...
&gt;
&gt; %% Enable active anti-entropy subsystem + optional debug
&gt; messages:
&gt; %% {anti\_entropy, {on|off, []}},
&gt; %% {anti\_entropy, {on|off, [debug]}},
&gt; {anti\_entropy, {on, []}},
&gt;
&gt; %% Restrict how fast AAE can build hash trees. Building the
&gt; tree
&gt; %% for a given partition requires a full scan over that
&gt; partition's
&gt; %% data. Once built, trees stay built until they are expired.
&gt; %% Config is of the form:
&gt; %% {num-builds, per-timespan-in-milliseconds}
&gt; %% Default is 1 build per hour.
&gt; {anti\_entropy\_build\_limit, {1, 3600000}},
&gt;
&gt; %% Determine how often hash trees are expired after being
&gt; built.
&gt; %% Periodically expiring a hash tree ensures the on-disk hash
&gt; tree
&gt; %% data stays consistent with the actual k/v backend data. It
&gt; also
&gt; %% helps Riak identify silent disk failures and bit rot.
&gt; However,
&gt; %% expiration is not needed for normal AAE operation and
&gt; should be
&gt; %% infrequent for performance reasons. The time is specified in
&gt; %% milliseconds. The default is 1 week.
&gt; {anti\_entropy\_expire, 604800000},
&gt;
&gt; %% Limit how many AAE exchanges/builds can happen concurrently.
&gt; {anti\_entropy\_concurrency, 2},
&gt;
&gt; %% The tick determines how often the AAE manager looks for work
&gt; %% to do (building/expiring trees, triggering exchanges, etc).
&gt; %% The default is every 15 seconds. Lowering this value will
&gt; %% speedup the rate that all replicas are synced across the
&gt; cluster.
&gt; %% Increasing the value is not recommended.
&gt; {anti\_entropy\_tick, 15000},
&gt;
&gt; %% The directory where AAE hash trees are stored.
&gt; {anti\_entropy\_data\_dir, "/var/lib/riak/anti\_entropy"},
&gt;
&gt; %% The LevelDB options used by AAE to generate the
&gt; LevelDB-backed
&gt; %% on-disk hashtrees.
&gt; {anti\_entropy\_leveldb\_opts, [{write\_buffer\_size, 4194304},
&gt; {max\_open\_files, 20}]},
&gt;
&gt; I'll update the bloom filters value and see what happens...
&gt;
&gt; It's thursday again, and the regeneration process has started again. Since
&gt; I've updated to 1.4.6, I have another thing different. The get/put values
&gt; for each cluster node now have a "random" behaviour. Take a look at the
&gt; next screenshot
&gt;
&gt; https://cloudup.com/cgbu9VNhSo1
&gt;
&gt; Best regards
&gt;
&gt;
&gt; On 31 December 2013 21:16, Charlie Voiselle  wrote:
&gt;
&gt;&gt; Edgar:
&gt;&gt;
&gt;&gt; Could you attach the AAE section of your app.config? Iâ€™d like to look
&gt;&gt; into this issue further for you. Something I think you might be running
&gt;&gt; into is https://github.com/basho/riak\_core/pull/483.
&gt;&gt;
&gt;&gt; The issue of concern is that the LevelDB bloom filter is not enabled
&gt;&gt; properly for the instance into which the AAE data is stored. You can
&gt;&gt; mitigate this particular issue by adding \*{use\_bloomfilter, true}\* as
&gt;&gt; shown below:
&gt;&gt;
&gt;&gt; %% The LevelDB options used by AAE to generate the LevelDB-backed
&gt;&gt;
&gt;&gt; %% on-disk hashtrees.
&gt;&gt; {anti\_entropy\_leveldb\_opts, [{write\_buffer\_size, 4194304},
&gt;&gt; {max\_open\_files, 20}]},
&gt;&gt;
&gt;&gt;
&gt;&gt; Becomes:
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt; %% The LevelDB options used by AAE to generate the LevelDB-backed
&gt;&gt; %% on-disk hashtrees.
&gt;&gt;
&gt;&gt;
&gt;&gt; {anti\_entropy\_leveldb\_opts, [{write\_buffer\_size, 4194304},
&gt;&gt; {use\_bloomfilter, true},
&gt;&gt;
&gt;&gt; {max\_open\_files, 20}]},
&gt;&gt;
&gt;&gt;
&gt;&gt; This might not solve your specific problem, but it will certainly improve
&gt;&gt; your AAE performance.
&gt;&gt;
&gt;&gt; Thanks,
&gt;&gt; Charlie Voiselle
&gt;&gt;
&gt;&gt; On Dec 31, 2013, at 12:04 PM, Edgar Veiga  wrote:
&gt;&gt;
&gt;&gt; Hey guys!
&gt;&gt;
&gt;&gt; Nothing on this one?
&gt;&gt;
&gt;&gt; Btw: Happy new year :)
&gt;&gt;
&gt;&gt;
&gt;&gt; On 27 December 2013 22:35, Edgar Veiga  wrote:
&gt;&gt;
&gt;&gt;&gt; This is a du -hs \* of the riak folder:
&gt;&gt;&gt;
&gt;&gt;&gt; 44G anti\_entropy
&gt;&gt;&gt; 1.1M kv\_vnode
&gt;&gt;&gt; 252G leveldb
&gt;&gt;&gt; 124K ring
&gt;&gt;&gt;
&gt;&gt;&gt; It's a 6 machine cluster, so ~1512G of levelDB.
&gt;&gt;&gt;
&gt;&gt;&gt; Thanks for the tip, I'll upgrade in a near future!
&gt;&gt;&gt;
&gt;&gt;&gt; Best regards
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; On 27 December 2013 21:41, Matthew Von-Maszewski wrote:
&gt;&gt;&gt;
&gt;&gt;&gt;&gt; I have a query out to the developer that can better respond to your
&gt;&gt;&gt;&gt; follow-up questions. It might be Monday before we get a reply due to the
&gt;&gt;&gt;&gt; holidays.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Do you happen to know how much data is in the leveldb dataset and/or
&gt;&gt;&gt;&gt; one vnode? Not sure it will change the response, but might be nice to have
&gt;&gt;&gt;&gt; that info available.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Matthew
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; P.S. Unrelated to your question: Riak 1.4.4 is available for
&gt;&gt;&gt;&gt; download. It has a couple of nice bug fixes for leveldb.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; On Dec 27, 2013, at 2:08 PM, Edgar Veiga  wrote:
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Ok, thanks for confirming!
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Is it normal, that this action affects the overall state of the
&gt;&gt;&gt;&gt; cluster? On the 26th It started the regeneration and the the response times
&gt;&gt;&gt;&gt; of the cluster raised to never seen values. It was a day of heavy traffic
&gt;&gt;&gt;&gt; but everything was going quite ok until it started the regeneration
&gt;&gt;&gt;&gt; process..
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Have you got any advices about changing those app.config values? My
&gt;&gt;&gt;&gt; cluster is running smoothly for the past 6 months and I don't want to start
&gt;&gt;&gt;&gt; all over again :)
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Best Regards
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; On 27 December 2013 18:56, Matthew Von-Maszewski wrote:
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; Yes. Confirmed.
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; There are options available in app.config to control how often this
&gt;&gt;&gt;&gt;&gt; occurs and how many vnodes rehash at once: defaults are every 7 days and
&gt;&gt;&gt;&gt;&gt; two vnodes per server at a time.
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; Matthew Von-Maszewski
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; On Dec 27, 2013, at 13:50, Edgar Veiga  wrote:
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; Hi!
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; I've been trying to find what may be the cause of this.
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; Every once in a week, all the nodes in my riak cluster start to do
&gt;&gt;&gt;&gt;&gt; some kind of operation that lasts at least for two days.
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; You can watch a sample of my munin logs regarding the last week in
&gt;&gt;&gt;&gt;&gt; here:
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; https://cloudup.com/imWiBwaC6fm
&gt;&gt;&gt;&gt;&gt; Take a look at the days 19 and 20, and now it has started again on the
&gt;&gt;&gt;&gt;&gt; 26...
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; I'm suspecting that this may be caused by the aae hash trees being
&gt;&gt;&gt;&gt;&gt; regenerated, as you say in your documentation:
&gt;&gt;&gt;&gt;&gt; For added protection, Riak periodically (default: once a week) clears
&gt;&gt;&gt;&gt;&gt; and regenerates all hash trees from the on-disk K/V data.
&gt;&gt;&gt;&gt;&gt; Can you confirm me that this may be the root of the "problem" and if
&gt;&gt;&gt;&gt;&gt; it's normal for the action to last for two days?
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; I'm using riak 1.4.2 on 6 machines, with centOS. The backend is
&gt;&gt;&gt;&gt;&gt; levelDB.
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; Best Regards,
&gt;&gt;&gt;&gt;&gt; Edgar Veiga
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt;&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt; riak-users mailing list
&gt;&gt; riak-users@lists.basho.com
&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

