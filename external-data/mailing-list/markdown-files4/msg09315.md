---
title: "Re: More Migration Questions"
description: ""
project: community
lastmod: 2012-11-13T14:55:21-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg09315"
mailinglist_parent_id: "msg09314"
author_name: "Matt Black"
project_section: "mailinglistitem"
sent_date: 2012-11-13T14:55:21-08:00
---


I still haven't really gotten to the bottom of the best way to do this
(short of paying for
MDC
):

http://lists.basho.com/pipermail/riak-users\_lists.basho.com/2012-October/009951.html

Previously, I've used backup/restore for situations like this, but our
backup has now grown to around 100GB - so it has become impractical.

Shane, in your maintenance window could you:
\* create your new cluster
\* stop any new data being added to the old cluster
\* run a riak-admin backup
\* run a riak-admin restore into the new one

The maintenance window here saves you a lot of trouble... Unfortunately,
most people won't get one ;)

Cheers
Matt



On 14 November 2012 09:44, Martin Woods  wrote:

&gt; Hi Tom
&gt;
&gt; I'd be very interested to know if Shane's approach should work, or if you
&gt; know of any good reason why that approach would cause issues.
&gt;
&gt; Also, aren't there several very real business use cases here that users of
&gt; Riak will inevitably encounter, and must be able to satisfy? Shane mentions
&gt; two use cases below: creation of a test environment using a copy of data
&gt; from a production cluster; and the migration of data within one cloud
&gt; provider from one set of systems to a distinct, separate set of systems.
&gt;
&gt; To add to this, what about the case where a Riak customer needs to move
&gt; from one cloud provider to another? How does this customer take his data
&gt; with him?
&gt;
&gt; All of the above cases require that a separate cluster be spun up from the
&gt; original cluster, with different names and IP addresses for the Riak nodes
&gt; involved in the cluster.
&gt;
&gt; None of these use cases are satisfied by using the riak-admin cluster
&gt; command.
&gt;
&gt; It seemed that this was the purpose of the reip command, but if Basho is
&gt; poised to deprecate this command, and indeed no longer recommends its use,
&gt; how are the previous cases supported? Surely these are important scenarios
&gt; for users of Riak, and therefore Basho?
&gt;
&gt; At one level, it seems it should be entirely possible to simply copy the
&gt; data directory from each Riak node and tell Riak that the node names and IP
&gt; addresses have changed (reip!). So what's the problem with doing this?
&gt;
&gt; Regards,
&gt; Martin.
&gt;
&gt;
&gt; On 13 November 2012 17:16, Thomas Santero  wrote:
&gt;
&gt;&gt; Hi Shane,
&gt;&gt;
&gt;&gt; I'm sorry for the delay on this. Over the weekend I was working to
&gt;&gt; replicate your setup so I can answer your question from experience. Alas,
&gt;&gt; time got the best of me and I have not yet finished.
&gt;&gt;
&gt;&gt; That said, I'm inclined to suggest upgrading riak on your current cluster
&gt;&gt; first and then using riak-admin replace to move off of the VM's and onto
&gt;&gt; metal.
&gt;&gt;
&gt;&gt; \* In this scenario, do a rolling upgrade (including making backups) of
&gt;&gt; the current cluster.
&gt;&gt; \* Install riak onto the new machines
&gt;&gt; \* join the first machine to the cluster
&gt;&gt; \* use riak-admin replace to replace one of the old nodes with the new node
&gt;&gt; \* wait for ring-ready, then repeat for the other nodes.
&gt;&gt;
&gt;&gt; Tom
&gt;&gt;
&gt;&gt;
&gt;&gt; On Tue, Nov 13, 2012 at 11:59 AM, Shane McEwan wrote:
&gt;&gt;
&gt;&gt;&gt; Anyone? Beuller? :-)
&gt;&gt;&gt;
&gt;&gt;&gt; Installing Riak 1.1.1 on the new nodes, copying the data directories
&gt;&gt;&gt; from the old nodes, issuing a "reip" on all the new nodes, starting up,
&gt;&gt;&gt; waiting for partition handoffs to complete, shutting down, upgrading to
&gt;&gt;&gt; 1.2.1 and starting up again got us to where we want to be. But this is not
&gt;&gt;&gt; very convenient.
&gt;&gt;&gt;
&gt;&gt;&gt; What do I do when I come to creating our test environment where I'll be
&gt;&gt;&gt; wanting to copy production data onto the test nodes on a regular basis? At
&gt;&gt;&gt; that point I won't have the "luxury" of downgrading to 1.1.1 to have a
&gt;&gt;&gt; working "reip" command.
&gt;&gt;&gt;
&gt;&gt;&gt; Surely there's gotta be an easier way to spin up a new cluster with new
&gt;&gt;&gt; names and IPs but with old data?
&gt;&gt;&gt;
&gt;&gt;&gt; Shane.
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; On 08/11/12 21:10, Shane McEwan wrote:
&gt;&gt;&gt;
&gt;&gt;&gt;&gt; G'day!
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Just to add to the list of people asking questions about migrating to
&gt;&gt;&gt;&gt; 1.2.1 . . .
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; We're about to migrate our 4 node production Riak database from 1.1.1 to
&gt;&gt;&gt;&gt; 1.2.1. At the same time we're also migrating from virtual machines to
&gt;&gt;&gt;&gt; physical machines. These machines will have new names and IP addresses.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; The process of doing rolling upgrades is well documented but I'm unsure
&gt;&gt;&gt;&gt; of the correct procedure for moving to an entirely new cluster.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; We have the luxury of a maintenance window so we don't need to keep
&gt;&gt;&gt;&gt; everything running during the migration. Therefore the current plan is
&gt;&gt;&gt;&gt; to stop the current cluster, copy the Riak data directories to the new
&gt;&gt;&gt;&gt; machines and start up the new cluster. The hazy part of the process is
&gt;&gt;&gt;&gt; how we "reip" the database so it will work in the new cluster.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; We've tried using the "riak-admin reip" command but were left with one
&gt;&gt;&gt;&gt; of our nodes in "(legacy)" mode according to "riak-admin member-status".
&gt;&gt;&gt;&gt; From an earlier E-Mail thread[1] it seems like "reip" is deprecated and
&gt;&gt;&gt;&gt; we should be doing a "cluster force replace" instead.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; So, would the new procedure be the following?
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; 1. Shutdown old cluster
&gt;&gt;&gt;&gt; 2. Copy data directory
&gt;&gt;&gt;&gt; 3. Start new cluster (QUESTION: The new nodes don't own any of the
&gt;&gt;&gt;&gt; partitions in the data directory. What does it do?) (QUESTION: The new
&gt;&gt;&gt;&gt; nodes won't be part of a cluster yet. Do I need to "join" them before I
&gt;&gt;&gt;&gt; can do any of the following commands? Or do I just put all the joins and
&gt;&gt;&gt;&gt; force-replace commands into the same plan and commit it all together?)
&gt;&gt;&gt;&gt; 3. Issue "riak-admin cluster force-replace old-node1 new-node1"
&gt;&gt;&gt;&gt; (QUESTION: Do I run this command just on "new-node1" or on all nodes?)
&gt;&gt;&gt;&gt; 4. Issue "force-replace" commands for the remaining three nodes.
&gt;&gt;&gt;&gt; 5. Issue a "cluster plan" and "cluster commit" to commit the changes.
&gt;&gt;&gt;&gt; 6. Cross fingers.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; In my mind the "replace" and/or "force-replace" commands are something
&gt;&gt;&gt;&gt; we would use it we had a failed node and needed to bring a spare online
&gt;&gt;&gt;&gt; to take over. It doesn't feel like something you would do if you don't
&gt;&gt;&gt;&gt; already have a cluster in place and are needing to "replace" ALL nodes.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Of course, we want to test this procedure before doing it for real. What
&gt;&gt;&gt;&gt; are the risks of doing the above procedure while the old cluster is
&gt;&gt;&gt;&gt; still running? While the new nodes are on a segregated network and
&gt;&gt;&gt;&gt; shouldn't be able to contact the old nodes what would happen if we did
&gt;&gt;&gt;&gt; the above and found the network wasn't as segregated as we originally
&gt;&gt;&gt;&gt; thought? Would the new nodes start trying to communicate with the old
&gt;&gt;&gt;&gt; nodes before the "force-replace" can take effect? Or, because all the
&gt;&gt;&gt;&gt; cluster changes are atomic there won't be any risk of that?
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Sorry for all the questions. I'm just trying to get a clear procedure
&gt;&gt;&gt;&gt; for moving an entire cluster to new hardware and hopefully this thread
&gt;&gt;&gt;&gt; will help other people in the future.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Thanks in advance!
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Shane.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; [1] 
&gt;&gt;&gt;&gt; http://comments.gmane.org/\*\*gmane.comp.db.riak.user/8418
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\*\*\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt;&gt; http://lists.basho.com/\*\*mailman/listinfo/riak-users\_\*\*lists.basho.com
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\*\*\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt; http://lists.basho.com/\*\*mailman/listinfo/riak-users\_\*\*lists.basho.com
&gt;&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt; --
&gt;&gt; @tsantero 
&gt;&gt; Technical Evangelist
&gt;&gt; Basho Technologies
&gt;&gt; 347-571-3995
&gt;&gt;
&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt; riak-users mailing list
&gt;&gt; riak-users@lists.basho.com
&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;&gt;
&gt;&gt;
&gt;
&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;
&gt;
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

