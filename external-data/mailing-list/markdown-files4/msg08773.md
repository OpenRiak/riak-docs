---
title: "Re: Riak cluster-f#$%"
description: ""
project: community
lastmod: 2012-10-01T14:08:07-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg08773"
mailinglist_parent_id: "msg08772"
author_name: "Michael Truog"
project_section: "mailinglistitem"
sent_date: 2012-10-01T14:08:07-07:00
---


Still, doesn't that failure show a typical overload of Riak's usage of 
mochiglobal (i.e., the code\_server needing to lock all Erlang schedulers)? I 
understand that running more than one node on a single machine is not realistic 
deployment. However, I don't see why it would cause errors, unless Riak was 
unable to handle the requests incoming.

On 10/01/2012 01:54 PM, Alexander Sicular wrote:
&gt; Any time you overload one box you run into all sorts of i/o dreck, screw with 
&gt; your conf files and mess with your versions you just have too many variables 
&gt; in the mix to get anything meaningful out of what you were trying to do. 
&gt; Since this is a test just tear the whole thing down and start clean. 
&gt;
&gt; If you want to dev test your app just use one node and dial the n val down to 
&gt; one in the app.config, which isn't actually there so you'll have to add it 
&gt; manually to the riak\_core section like so (with some other stuff):
&gt;
&gt; {default\_bucket\_props, [{n\_val,1},
&gt; {allow\_mult,false},
&gt; {last\_write\_wins,false},
&gt; {precommit, []},
&gt; {postcommit, []},
&gt; {chash\_keyfun, {riak\_core\_util, chash\_std\_keyfun}}
&gt; ]} 
&gt;
&gt; (Hey Basho people, that stuff should be in the app.config file by default. 
&gt; Making people go fish for it and figure out how and where to add this stuff 
&gt; is kinda unnecessary. Here is an example of a great conf file with everything 
&gt; you can conf and a whole bunch of docs: 
&gt; https://github.com/antirez/redis/blob/unstable/redis.conf ).
&gt;
&gt; If you want to performance test your app make your dev system as similar to 
&gt; your prod system as possible and knock it out.
&gt;
&gt;
&gt; -Alexander Sicular
&gt;
&gt; @siculars
&gt;
&gt; On Oct 1, 2012, at 4:30 PM, Callixte Cauchois wrote:
&gt;
&gt;&gt; Thank you, but can you explain a bit more?
&gt;&gt; I mean I understand why it is a bad thing with regards to reliability and in 
&gt;&gt; case of hardware issues. But does it have also an impact on the behaviour 
&gt;&gt; when the hardware is performing correctly and the load on the machines are 
&gt;&gt; the same?
&gt;&gt;
&gt;&gt; On Mon, Oct 1, 2012 at 1:25 PM, Alexander Sicular &gt; &gt; wrote:
&gt;&gt;
&gt;&gt; Inline.
&gt;&gt;
&gt;&gt; -Alexander Sicular
&gt;&gt;
&gt;&gt; @siculars
&gt;&gt;
&gt;&gt; On Oct 1, 2012, at 3:23 PM, Callixte Cauchois wrote:
&gt;&gt;
&gt;&gt; &gt; Hi there,
&gt;&gt; &gt;
&gt;&gt; &gt; so, I am currently evaluating Riak to see how it can fit in our 
&gt;&gt; platform. To do so I have set up a cluster of 4 nodes on SmartOS, all of 
&gt;&gt; them on the same physical box.
&gt;&gt;
&gt;&gt; Mistake. Just stop here. Everything else doesn't matter. Do not put all 
&gt;&gt; your virtual machines (riak nodes) on one physical machine. Put em on 
&gt;&gt; different physical machines. Fix the config files and try again.
&gt;&gt;
&gt;&gt; &gt; I then built a simple application in node.js that get log events from 
&gt;&gt; our production system through a RabbitMQ queue and store them in my cluster. 
&gt;&gt; I let Riak generate the ids, but I have added two secondary indices to be 
&gt;&gt; able to retrieve more easily all the log events that belong to a single 
&gt;&gt; session.
&gt;&gt; &gt; Everything was going fine, events come around 130 messages per second 
&gt;&gt; are easily ingested by Riak. When stop it and then restart it, there is a 
&gt;&gt; bit of an issue as the events are read from the queue at 1500 messages per 
&gt;&gt; second and the insertion times go up, so I need some retries to actually 
&gt;&gt; store everything.
&gt;&gt; &gt; I wanted to tweak the LevelDB params to increase the throughput. To do 
&gt;&gt; so, I first upgraded from 1.1.6 to 1.2.0. I chose what I thought was the 
&gt;&gt; safest way: node by node, I have them leave the cluster, then I upgrade, 
&gt;&gt; then join again. During the whole process I kept inserting.
&gt;&gt; &gt; It went quite well. But, when I ran some queries using 2i, it gave me 
&gt;&gt; errors and I realized that for two of my four nodes, I forgot to put back 
&gt;&gt; eLevelDB as the default engine. As soon as I ran this query, everything went 
&gt;&gt; havoc, a lot of inserts failed, some nodes where not reachable using the 
&gt;&gt; ping url.
&gt;&gt; &gt; I changed the default engine and restarted those nodes, nothing 
&gt;&gt; changed. I tried to make them leave the cluster, after two days, they are 
&gt;&gt; still leaving. Riak-admin transfers tells that a lot of transfers need to 
&gt;&gt; occur, but the system is stuck: the numbers there do not change.
&gt;&gt; &gt;
&gt;&gt; &gt; I guess I have done several things wrong. It is test data, so it 
&gt;&gt; doesn't really matter if I loose data or if I have to re-start from scratch, 
&gt;&gt; but I want to understand what have gone wrong how I could have fixed it. Or 
&gt;&gt; if I even can recover from there now.
&gt;&gt; &gt;
&gt;&gt; &gt; Thank you.
&gt;&gt; &gt; C.
&gt;&gt; &gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt; &gt; riak-users mailing list
&gt;&gt; &gt; riak-users@lists.basho.com 
&gt;&gt; &gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;&gt;
&gt;&gt;
&gt;
&gt;
&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

