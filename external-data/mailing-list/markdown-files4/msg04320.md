---
title: "Re: High volume data series storage and queries"
description: ""
project: community
lastmod: 2011-08-10T07:11:09-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg04320"
mailinglist_parent_id: "msg04317"
author_name: "Ciprian Dorin Craciun"
project_section: "mailinglistitem"
sent_date: 2011-08-10T07:11:09-07:00
---


On Wed, Aug 10, 2011 at 16:39, Jeremiah Peschka
 wrote:
&gt;
&gt; ---
&gt; Jeremiah Peschka - Founder, Brent Ozar PLF, LLC
&gt;  Microsoft SQL Server MVP
&gt;
&gt; On Aug 10, 2011, at 6:14 AM, Ciprian Dorin Craciun wrote:
&gt;
&gt;&gt;
&gt;&gt;    Furthermore, to back my claims I've put on Github the old code
&gt;&gt; which I've used to benchmark different databases -- Riak or other
&gt;&gt; key-value stores are not included except BerkeleyDB, but I've included
&gt;&gt; others like Hypertable which could be a good choice -- at the
&gt;&gt; following address (you can easily extend the "framework" to include
&gt;&gt; new backends):
&gt;&gt;        https://github.com/cipriancraciun/sds-benchmark
&gt;&gt;
&gt;&gt;    Also there is a "so-called" report I've put up at that time in the
&gt;&gt; same place at -- the second link is the content of the wkipage and
&gt;&gt; there are also images which GitHub doesn't display in the page:
&gt;&gt;        https://github.com/cipriancraciun/sds-benchmark/tree/master/results
&gt;&gt;        
&gt;&gt; https://github.com/cipriancraciun/sds-benchmark/blob/master/results/results.mediawiki
&gt;&gt;
&gt;&gt;    For the record we've tested up to 100m records on some
&gt;&gt; data-stores, but on some other (like PostgreSQL and MySQL we've
&gt;&gt; stopped at 10 million as the insertion rate dropped tremendously).
&gt;&gt;
&gt;&gt;    My conclusion from this experiment was: any database which is
&gt;&gt; backed by a tree-like data structure (almost all use B trees or
&gt;&gt; derivate) will get to a grinding halt in insert speed if the
&gt;&gt; clustering keys (clientid + timpstamp in this case) don't exhibit any
&gt;&gt; kind of locality (as is your case). See the quote from my report:
&gt;&gt; ~~~~
&gt;&gt; \* this section applies to Postgres and SQLite, as MonetDB behaved Ok;
&gt;&gt; \* it seems that the initial insert speed is good for the first couple
&gt;&gt; million records;
&gt;&gt; \* as the data accumulates and new inserts are done, the indices start
&gt;&gt; to be rewritten, and consume the entire disk bandwidth; (the initial
&gt;&gt; good speed is due the fact that the indices fit into the RAM memory;)
&gt;&gt; \* if the inserts are done without any indices defined, the insert
&gt;&gt; speed is incredible (600k in the case of Postgres), but the scan speed
&gt;&gt; is under 100;
&gt;&gt; \* maybe, it is possible, that this behavior is specific to any
&gt;&gt; database which uses trees (like BerkeleyDB?);
&gt;&gt; ~~~~
&gt;
&gt; There are a lot of items in your benchmark that lead to the performance 
&gt; problems that you saw with RDBMSes. Notably turning fsync off. An RDBMS will 
&gt; constantly stream writes to a write ahead log before committing them to disk. 
&gt; Most RDBMSes do this every minute or so. You can adjust your RDBMS so that 
&gt; when it checkpoints it can either dump all rows to disk as fast as possible 
&gt; or else attempt to determine an optimal rate to write at so that other read 
&gt; operations can continue and only experience minimal I/O blocking from writes.

 I don't remember if I've disabled fsync also for PostreSQL, or
only on SQLite. But the setup I've arrived to isn't at the first
iteration. I've tried the benchmark a few times and kept the setup
that shown the best performance. (For example for BerkeleyDB I've
almost obtained double the performance after correctly setting up the
number of entries per Btree page.) But I admit I didn't had the input
of a professionist DBA.


&gt; With a clustering key of clientid + timestamp, you'll see tremendous B-tree 
&gt; fragmentation on the unique index supporting the clustering key without using 
&gt; some kind of table level partitioning - either as an RDBMS feature in MSSQL 
&gt; Server or using something like partitioned views in PostgreSQL.

 Exactly this is what I've guessed that lead to the performance degradation.


&gt; With any sufficiently complex system (an RDBMS is arguably more complex than 
&gt; most operating systems), you do need to apply some level of domain specific 
&gt; knowledge to the problem and then tune appropriately.

 I tend to agree with the need of in-depth domain specific
knowledge. On the other hand if you apply to much of it it will be
also very hard to change the requirements after the initial system was
completed.

 On the other hand for example Hypertable didn't show the
weaknesses of RDBMS as it internally does something similar to
partitioning (but unattended).

 Ciprian.

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

