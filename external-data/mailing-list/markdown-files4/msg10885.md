---
title: "Re: Using Riak to perform aggregate queries"
description: ""
project: community
lastmod: 2013-04-14T21:48:48-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg10885"
mailinglist_parent_id: "msg10884"
author_name: "Alexander Sicular"
project_section: "mailinglistitem"
sent_date: 2013-04-14T21:48:48-07:00
---


As with most things MR I find that you need to do more work at the application 
level. Meaning either in the MR itself or in the application calling the MR. 
SQL has all sorts of magic that just doesn't directly translate to MR world. 
What's worse, all MR is not created equal. Things are vastly different 
depending on things like your environment and data structure. 

In your case I would limit the number of keys that go into any given MR by date 
via a secondary index query or via riak search. Oh, and precompute everything. 
Pick whichever time slice has less keys than the number of keys that make your 
queries go boom. If a month is too big do a week or even a day. Persist all 
computation in materialized keys like "201301" or "20130101", the former being 
a monthly rollup and the later being a day's rollup. I like to use iso8601 date 
formatting[0]. It may sound like a lot of work up front but you basically will 
get a system that can grow as your application does.

-Alexander Sicular

@siculars


[0] http://en.wikipedia.org/wiki/ISO\_8601

On Apr 14, 2013, at 11:09 PM, Chris Corbyn  wrote:

&gt;&gt; MapReduce works well when:
&gt;&gt; 
&gt;&gt; a) you know the set of objects you're going to MapReduce over
&gt;&gt; b) you need to return the entire object
&gt;&gt; c) when you plan to manipulate a lot of data inside Riak - read all sales 
&gt;&gt; records and write them out as shipping invoices
&gt; 
&gt; Right, thanks. I do know the set of objects… that set is the full bucket, 
&gt; because our finance department want to be able to request report on the sales 
&gt; data over many years. I actually tried getting all the keys in a separate 
&gt; place, then requesting my data using a list of bucket-key pairs. This caused 
&gt; my app to run out of memory on the client side.
&gt; 
&gt;&gt; 
&gt;&gt; 
&gt;&gt; Everywhere I read, people say you shouldn't use Riak's MapReduce over an 
&gt;&gt; entire bucket and that there are other ways of achieving your goals. I'm not 
&gt;&gt; sure how, though. I'm also not clear on why using an entire bucket is slow, 
&gt;&gt; if you only have one bucket in the entire system, so either way, you need to 
&gt;&gt; go over all the entries. Passing in a list of key-bucket pairs has the same 
&gt;&gt; effect. Maybe the rule should be "don't use MapReduce with more than a 
&gt;&gt; handful of keys". Which makes me wonder (apart from link traversal) what use 
&gt;&gt; it really has in the real world.
&gt;&gt; 
&gt;&gt; From my limited understanding of things, MR across an entire bucket is slow 
&gt;&gt; because you need to scan the entire key space. Much like a SELECT with no 
&gt;&gt; WHERE clause, a MapReduce without a key list is going to sweep across the 
&gt;&gt; entire bucket pulling all data off of disk in the process. Someone better 
&gt;&gt; versed in Riak internals would have to chime in on why this is so slow.
&gt;&gt; 
&gt;&gt; I do know, however, that having on master bucket o' things may not be the 
&gt;&gt; best idea. If you're storing sales data, a better approach may be to segment 
&gt;&gt; buckets out by primary querying criteria.
&gt; 
&gt; The thing is, the \*only\* querying criteria for this app is to aggregate the 
&gt; data for the purpose of reporting. Our finance guys want to be able to check 
&gt; a few boxes to specify how they want the data grouped, then generate the 
&gt; report. We actually do this data warehousing style in MySQL at the moment, 
&gt; where the data is renormalized about as far as we can, but still allowing for 
&gt; some run time use of GROUP BY and ordering.
&gt; 
&gt; I could use separate buckets for each product, perhaps, but we'd still be 
&gt; looking at a lot of data and I'd have to repeatedly query for each product, 
&gt; then merge all those aggregates together.
&gt; 
&gt;&gt; 
&gt;&gt; 
&gt;&gt; I have a list of 500K+ documents that represent sales data. I need to view 
&gt;&gt; this data in different ways: for example, how much revenue was made in each 
&gt;&gt; month the business was operating? How much revenue did each product raise? 
&gt;&gt; How many of each product were sold in a given month? I always thought 
&gt;&gt; MapReduce was supposed to be good at solving these types of aggregate 
&gt;&gt; problems. That sounds like a myth now though, unless we're just looking at 
&gt;&gt; Hadoop.
&gt;&gt; 
&gt;&gt; MapReduce can aggregate well. RDBMSes can aggregate well, too. But there are 
&gt;&gt; practical limits to each paradigm that you have to be aware of. From 
&gt;&gt; watching the list and working with people in the real world there are some 
&gt;&gt; paradigms that you want to use in Riak that don't immediately seem obvious 
&gt;&gt; coming from the RDBMS world.
&gt;&gt; 
&gt;&gt; - Know your queries. Examine your application and make a list of common 
&gt;&gt; queries that you're going to be running. You'll want to be familiar with 
&gt;&gt; this list because you're going to try to write your data in a way that 
&gt;&gt; answers these questions.
&gt; 
&gt; As I say, the only query we'll be running is to aggregate the data for the 
&gt; purpose of reporting. I was exploring Riak because its MapReduce attracted me 
&gt; and reading various articles they show off FaceBook-like networks (which are 
&gt; naturally huge) and Twitter stream analysis etc. But for aggregating data 
&gt; from huge record sets, it just seems like it can't do it. People say to use 
&gt; secondary indexes, but that only helps to you to narrow down your recordset… 
&gt; if you \*know\* your recordset is going to contain hundreds of thousands of 
&gt; records, then it seems like Riak is going to give you hell. In other systems, 
&gt; the place where MapReduce shines is for big data processing; but in Riak's 
&gt; case its the inverse.
&gt; 
&gt;&gt; - Know your query pattern. Understand how these queries are working with the 
&gt;&gt; data itself. If many queries retrieve data aggregated for a day, week, 
&gt;&gt; month, store, sales region, or other logical division then you'll want to 
&gt;&gt; make sure that you can answer them quickly. (See Choosing the Right Tool [3])
&gt; 
&gt; We're never requesting just a view on a single day, or month etc… on the UI 
&gt; we present a table with a row for each period that has been grouped by (e.g. 
&gt; a row for each month that the business has been operating). I'm actually 
&gt; ripping the data out of MySQL and into Riak for this, but keeping it in MySQL 
&gt; in the first place seems way more efficient. Especially if I have to employ 
&gt; the same data warehousing tricks to pre-aggregate the data as much as I can 
&gt; anyway. I could probably iterate through each month and perform repeated 
&gt; MapReduce queries across those months, but I suspect that would be extremely 
&gt; slow too, due to the repeat querying.
&gt; 
&gt; Is this just a current limitation in Riak, or a fundamental design conflict 
&gt; that means Riak can never be used to solve these kinds of problems?
&gt; 
&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

