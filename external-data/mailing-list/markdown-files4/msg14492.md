---
title: "Re: riak handoffs stalled"
description: ""
project: community
lastmod: 2014-07-14T06:04:27-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg14492"
mailinglist_parent_id: "msg14491"
author_name: "Ciprian Manea"
project_section: "mailinglistitem"
sent_date: 2014-07-14T06:04:27-07:00
---


Hi Leonid,

Lets try to increase the handoff\_timeout and see if it can solve your
problem.

Could you please paste the below code in a $ riak attach

riak\_core\_util:rpc\_every\_member\_ann(application,set\_env,[riak\_core,
handoff\_timeout, 5400000],infinity).
riak\_core\_util:rpc\_every\_member\_ann(application,set\_env,[riak\_core,
handoff\_receive\_timeout, 5400000],infinity).

You should be able to exit back at the shell prompt by pressing ^D

Could you please archive/compress and send me directly by email:

+ the ring directory (including its content) from one of your riak nodes
+ recent log files (console.log, error.log, crash.log if any), same node


Thanks,
Ciprian


On Mon, Jul 14, 2014 at 3:33 PM, Леонид Рябоштан &lt;
leonid.riabosh...@twiket.com&gt; wrote:

&gt; Hello,
&gt;
&gt; riak version is 1.1.4-1. We set transfer limit in config made it equal to
&gt; 4.
&gt;
&gt; I don't think we have riak-admin transfer-limit or riak-admin cluster plan.
&gt;
&gt; The problem is that damn nodes can't pass partition between each other,
&gt; probably because they're too big. Each 5k files(leveldb backend) and
&gt; weights 10GB each. There're no problems with smaller partitions. We can't
&gt; find anything usefull on handoff fail in riak or system logs. Seems like
&gt; ulimit and erlang ports are way higher, we increased it 4 times today.
&gt;
&gt; It begins like:
&gt; 2014-07-14 12:22:45.518 UTC [info]
&gt; &lt;0.10544.0&gt;@riak\_core\_handoff\_sender:start\_fold:83 Starting handoff of
&gt; partition riak\_kv\_vnode 68507889249886074290797726533575766546371837952
&gt; from 'riak@192.168.153.182' to 'riak@192.168.164.133'
&gt;
&gt; And ends like:
&gt; 2014-07-14 08:43:28.829 UTC [error]
&gt; &lt;0.2264.0&gt;@riak\_core\_handoff\_sender:start\_fold:152 Handoff of partition
&gt; riak\_kv\_vnode 68507889249886074290797726533575766546371837952 from '
&gt; riak@192.168.153.182' to 'riak@192.168.164.133' FAILED after sending
&gt; 1318000 objects in 1455.15 seconds: closed
&gt; 2014-07-14 10:40:18.294 UTC [error]
&gt; &lt;0.11555.0&gt;@riak\_core\_handoff\_sender:start\_fold:152 Handoff of partition
&gt; riak\_kv\_vnode 68507889249886074290797726533575766546371837952 from '
&gt; riak@192.168.153.182' to 'riak@192.168.164.133' FAILED after sending
&gt; 911000 objects in 2734.48 seconds: closed
&gt; 2014-07-14 09:43:43.197 UTC [error]
&gt; &lt;0.26922.2&gt;@riak\_core\_handoff\_sender:start\_fold:152 Handoff of partition
&gt; riak\_kv\_vnode 68507889249886074290797726533575766546371837952 from '
&gt; riak@192.168.153.182' to 'riak@192.168.164.133' FAILED after sending
&gt; 32000 objects in 963.06 seconds: timeout
&gt;
&gt; Maybe we need to check something else on target node? Actually it always
&gt; runs in GC problems:
&gt; 2014-07-14 12:30:03.579 UTC [info]
&gt; &lt;0.99.0&gt;@riak\_core\_sysmon\_handler:handle\_event:85 monitor long\_gc &lt;0.468.0&gt;
&gt; [{initial\_call,{riak\_kv\_js\_vm,init,1}},{almost\_current\_function,{xmerl\_ucs,expand\_utf8\_1,3}},{message\_queue\_len,0}]
&gt; [{timeout,118},{old\_heap\_block\_size,0},{heap\_block\_size,196418},{mbuf\_size,0},{stack\_size,45},{old\_heap\_size,0},{heap\_size,136165}]
&gt; 2014-07-14 12:30:44.386 UTC [info]
&gt; &lt;0.99.0&gt;@riak\_core\_sysmon\_handler:handle\_event:85 monitor long\_gc &lt;0.713.0&gt;
&gt; [{initial\_call,{riak\_core\_vnode,init,1}},{almost\_current\_function,{gen\_fsm,loop,7}},{message\_queue\_len,0}]
&gt; [{timeout,126},{old\_heap\_block\_size,0},{heap\_block\_size,1597},{mbuf\_size,0},{stack\_size,38},{old\_heap\_size,0},{heap\_size,658}]
&gt;
&gt; Probably we have some CPU issues here, but node is not under load
&gt; currently.
&gt;
&gt; Thank you,
&gt; Leonid
&gt;
&gt;
&gt; 2014-07-14 16:11 GMT+04:00 Ciprian Manea :
&gt;
&gt; Hi Leonid,
&gt;&gt;
&gt;&gt; Which Riak version are you running?
&gt;&gt;
&gt;&gt; Have you committed\* the cluster plan after issuing the cluster
&gt;&gt; force-remove  commands?
&gt;&gt;
&gt;&gt; What is the output of $ riak-admin transfer-limit, ran from one of your
&gt;&gt; riak nodes?
&gt;&gt;
&gt;&gt;
&gt;&gt; \*Do not run this command yet if you have not done it already.
&gt;&gt; Please run a riak-admin cluster plan and attach its output here.
&gt;&gt;
&gt;&gt;
&gt;&gt; Thanks,
&gt;&gt; Ciprian
&gt;&gt;
&gt;&gt;
&gt;&gt; On Mon, Jul 14, 2014 at 2:41 PM, Леонид Рябоштан &lt;
&gt;&gt; leonid.riabosh...@twiket.com&gt; wrote:
&gt;&gt;
&gt;&gt;&gt; Hello, guys,
&gt;&gt;&gt;
&gt;&gt;&gt; It seems like we ran into emergency. I wonder if there can be any help
&gt;&gt;&gt; on that.
&gt;&gt;&gt;
&gt;&gt;&gt; Everything that happened below was because we were trying to rebalace
&gt;&gt;&gt; space used by nodes that we running out of space.
&gt;&gt;&gt;
&gt;&gt;&gt; Cluster is 7 machines now, member\_status looks like:
&gt;&gt;&gt; Attempting to restart script through sudo -u riak
&gt;&gt;&gt; ================================= Membership
&gt;&gt;&gt; ==================================
&gt;&gt;&gt; Status Ring Pending Node
&gt;&gt;&gt;
&gt;&gt;&gt; -------------------------------------------------------------------------------
&gt;&gt;&gt; valid 15.6% 20.3% 'riak@192.168.135.180'
&gt;&gt;&gt; valid 0.0% 0.0% 'riak@192.168.152.90'
&gt;&gt;&gt; valid 0.0% 0.0% 'riak@192.168.153.182'
&gt;&gt;&gt; valid 26.6% 23.4% 'riak@192.168.164.133'
&gt;&gt;&gt; valid 27.3% 21.1% 'riak@192.168.177.36'
&gt;&gt;&gt; valid 8.6% 15.6% 'riak@192.168.194.138'
&gt;&gt;&gt; valid 21.9% 19.5% 'riak@192.168.194.149'
&gt;&gt;&gt;
&gt;&gt;&gt; -------------------------------------------------------------------------------
&gt;&gt;&gt; Valid:7 / Leaving:0 / Exiting:0 / Joining:0 / Down:0
&gt;&gt;&gt;
&gt;&gt;&gt; 2 nodes with 0 Ring was made to force leave the cluster, they have
&gt;&gt;&gt; plenty of data on them which is now seems to be not accessible. Handoffs
&gt;&gt;&gt; are stuck it seems. Node 'riak@192.168.152.90'(is in same situation as '
&gt;&gt;&gt; riak@192.168.153.182') tries to handoff partitions to '
&gt;&gt;&gt; riak@192.168.164.133' but fails for unknown reason after huge
&gt;&gt;&gt; timeouts(from 5 to 40 minutes). Partition it's trying to move is about 10Gb
&gt;&gt;&gt; in size. It grows slowly on target node, but probably it's just usual
&gt;&gt;&gt; writes from normal operation. It doesn't get any smaller on source node.
&gt;&gt;&gt;
&gt;&gt;&gt; I wonder is there any way to let cluster know that we want those nodes
&gt;&gt;&gt; to be actually members of source node and there's no actual need to
&gt;&gt;&gt; transfer them? How to redo cluster ownership balance? Revert this
&gt;&gt;&gt; force-leave stuff.
&gt;&gt;&gt;
&gt;&gt;&gt; Thank you,
&gt;&gt;&gt; Leonid
&gt;&gt;&gt;
&gt;&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;
&gt;
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

