---
title: "Re: RiakCS poor s3 upload speeds 2MB/s"
description: ""
project: community
lastmod: 2015-01-21T18:40:27-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg15566"
mailinglist_parent_id: "msg15536"
author_name: "Kota Uenishi"
project_section: "mailinglistitem"
sent_date: 2015-01-21T18:40:27-08:00
---


Toby,

Glad to hear you gained some.

For disk usage gaps, I'd recommend smaller leeway\_seconds, like an
hour or even less, depending on the average object size and read
concurrency. leeway seconds is just for assuring long-running download
not being interrupted by concurrent overwrite or deletion and GC. And
increasing gc\_max\_workers in app.config might help the speed of
reclaiming disk usage, eating some disk IOPS for more parallel
deletion. The default concurrency is 5 for 1.5 series.
delete\_concurrency also might help - by multiplying the concurrency
whose default is 1.

Another possibility is large number of siblings. If you have frequent
and concurrent upload against same key, it might create fair number of
siblings (and parallel manifests in single sibling), which means
rather large Riak object in leveldb. This might also affect the
performance either when is number is large as logged in CS, or the
object size is small. When objects are small, the overhead of fetching
whole related data (buckets, users, metadata in the manifest) cannot
be ignored. That'd affect byte through put, maybe.

As you may see in github, we're working on CS 2.0 which works with
Riak 2.0, so please stay tuned!



On Wed, Jan 21, 2015 at 10:07 AM, Toby Corkindale  wrote:
&gt; Hi Kota,
&gt; I had a bit of an off-list chat about this a while ago, plus continued
&gt; to investigate locally, and eventually achieved some faster speeds,
&gt; around 15MByte/sec writes.
&gt; Things that were changed:
&gt; \* Adjusted Riak CS GC to be spread out over the cluster much more.
&gt; \* Tweaked up the put buffers and concurrency further
&gt; \* Moved most of the files out of CS and into Amazon S3+Glacier
&gt; \* Switched from nginx to haproxy
&gt; \* simplified firewalling for internal clients
&gt;
&gt; Each one of those changes made a small to modest improvement, but
&gt; overall combined to make a quite noticeable improvement.
&gt;
&gt; I did notice something odd though -- despite moving most of the data
&gt; out of the cluster, the disk-space-in-use by Riak is still very large
&gt; compared to the amount stored. I mean, we moved more than 90% of the
&gt; data out of the cluster, yet the actual disk space used only halved.
&gt; For every gigabyte of file stored in CS, dozens of gigabytes are
&gt; actually on disk!
&gt;
&gt; Either the garbage collection algorithm is very, very lazy, or somehow
&gt; something has gone a bit wrong in the past, which might have explained
&gt; part of the performance problems.
&gt;
&gt; We're going to look at redeploying a new, fresh cluster based on Riak
&gt; 2 in the not too distant future, once Riak CS looks like it's approved
&gt; for use there, and maybe that'll clear all of this up.
&gt;
&gt; Toby
&gt;
&gt; On 21 January 2015 at 11:07, Kota Uenishi  wrote:
&gt;&gt; Toby and David,
&gt;&gt;
&gt;&gt; Thank you for working on Riak CS and I apologize for being late responder.
&gt;&gt;
&gt;&gt; I believe the reason of being slow down is different between Toby's
&gt;&gt; and David's cluster.
&gt;&gt;
&gt;&gt; Toby's reason looks like that's just because of the data increasing.
&gt;&gt; How much data per vnode do you have in your cluster, Toby? Do you have
&gt;&gt; deletion in your workload?
&gt;&gt; Riak CS's garbage collection, deleting block keys in Riak and merging
&gt;&gt; Bitcask files make some load more than the exact amount of data
&gt;&gt; visible via CS (even taking the replication factor into account).
&gt;&gt; Also, if you turn on AAE, building AAE trees scans all the data stored
&gt;&gt; in Riak to fix unexpected bit rot or partial replication. I'd like you
&gt;&gt; to check the background load to underlying storage. If the performance
&gt;&gt; decrease is \*not\* due to such background load, maybe there's the same
&gt;&gt; dragon lurking under the water as David's cluster.
&gt;&gt;
&gt;&gt; One thing I can suggest from David's app.config, SSL is turned on -
&gt;&gt; Riak CS uses Erlang's built-in SSL library for https scheme which is
&gt;&gt; said to have not so good performance. I wonder those benchmarks were
&gt;&gt; done over https or just http.
&gt;&gt;
&gt;&gt; As far as I test Riak CS, local or cluster-wide, I haven't met such
&gt;&gt; bad performance less than 10MB/s in such a fresh state cluster. There
&gt;&gt; should be something wrong, either the setup or the software. Would you
&gt;&gt; mind sending us the result riak-debug and riak-cs-debug commands, if
&gt;&gt; you still can reproduce such situation. Those packs up the environment
&gt;&gt; info as much as it can.
&gt;&gt;
&gt;&gt; Thanks,
&gt;&gt; Kota
&gt;&gt;
&gt;&gt; On Thu, Nov 27, 2014 at 10:36 AM, Toby Corkindale  wrote:
&gt;&gt;&gt; Thanks, that's interesting to hear.
&gt;&gt;&gt; How have you been finding the stability and reliability to be with
&gt;&gt;&gt; leofs, over time?
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; I still wish I could just get our Riak CS cluster performing better;
&gt;&gt;&gt; it just seems so unreasonably slow at the moment, that I suspect
&gt;&gt;&gt; there's \*something\* holding it back. I can build a test cluster on my
&gt;&gt;&gt; desktop, and even with five virtual riak nodes on the one machine, I
&gt;&gt;&gt; still see 20-40x the performance, so it seems bizarre that dedicated
&gt;&gt;&gt; bare-metal servers would be so slow. (Although obviously there's much
&gt;&gt;&gt; more network latency between real machines, than a virtual cluster on
&gt;&gt;&gt; one desktop; and they have a lot more data in their bitcask databases)
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; However I've tried fiddling with all the Riak and Riak CS options..
&gt;&gt;&gt; ethtool offloads.. mount options.. sysctls.. MTU sizes.. even dropping
&gt;&gt;&gt; single nodes out of the cluster one at a time in case they were
&gt;&gt;&gt; somehow at fault.. seems like the only performance changes I can make
&gt;&gt;&gt; are negative.
&gt;&gt;&gt;
&gt;&gt;&gt; We're still double the speed of the original poster in this thread,
&gt;&gt;&gt; but.. that isn't saying much.
&gt;&gt;&gt;
&gt;&gt;&gt; Toby
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; On 26 November 2014 at 06:44, Heinz Nikolaus Gies  
&gt;&gt;&gt; wrote:
&gt;&gt;&gt;&gt; If you’re evaluating RiakCS vs. Ceph you might want to toss LeoFS[1] in the
&gt;&gt;&gt;&gt; mix and give it a run. Just as RiakCS it is a dynamo inspired system build
&gt;&gt;&gt;&gt; in Erlang and comes with the same advantages and disadvantages. But unlike
&gt;&gt;&gt;&gt; RiakCS it is pretty much exclusive a Object Store so can take a few
&gt;&gt;&gt;&gt; different optimizations for this kind of work that might not be possible in
&gt;&gt;&gt;&gt; a general purpose database as Riak (this is my personal guess not a 
&gt;&gt;&gt;&gt; research
&gt;&gt;&gt;&gt; founded conclusion). The team is (much) smaller then bash (obviously) but
&gt;&gt;&gt;&gt; they’re a very nice and responsive bunch. I ended up using it as a s3
&gt;&gt;&gt;&gt; backend for Project-FiFo due to it’s performance characteristics. With
&gt;&gt;&gt;&gt; current releases I manage to get a sigle file upload speed of ~1.2GB/s 
&gt;&gt;&gt;&gt; using
&gt;&gt;&gt;&gt; gof3r[2] (this might be a client limitation but I haven’t had time to
&gt;&gt;&gt;&gt; investigate the details).
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; [1] http://leo-project.net/leofs/
&gt;&gt;&gt;&gt; [2] https://github.com/rlmcpherson/s3gof3r/tree/master/gof3r
&gt;&gt;&gt;&gt; ---
&gt;&gt;&gt;&gt; Cheers,
&gt;&gt;&gt;&gt; Heinz Nikolaus Gies
&gt;&gt;&gt;&gt; he...@licenser.net
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; On Nov 25, 2014, at 6:08, Toby Corkindale  wrote:
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Hi,
&gt;&gt;&gt;&gt; I wondered if you managed to significantly improve your Riak CS
&gt;&gt;&gt;&gt; performance, or not?
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; I just ask as we've been getting not-dissimilar performance out of
&gt;&gt;&gt;&gt; Riak CS too (4-5 mbyte/sec max per client, on bare metal hardware),
&gt;&gt;&gt;&gt; for quite a long time. (I swear it was faster originally, when there
&gt;&gt;&gt;&gt; was a lot less data in the whole system.)
&gt;&gt;&gt;&gt; This is after applying all the tweaks available -- networking stack,
&gt;&gt;&gt;&gt; filesystem mount options, assorted Erlang vm.args, and increased put
&gt;&gt;&gt;&gt; concurrency/buffer options.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; We put up with it because it's been just-about sufficient enough for
&gt;&gt;&gt;&gt; our needs and Riak CS has been reliable and easy to administer -- but
&gt;&gt;&gt;&gt; it's becoming more of an issue, and so I'm curious to know if other
&gt;&gt;&gt;&gt; people \*do\* manage to achieve \*good\* per-client speeds out of Riak CS
&gt;&gt;&gt;&gt; or if this is just how things always are?
&gt;&gt;&gt;&gt; And we're way off the mark, maybe we can find out why..
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Details of our setup:
&gt;&gt;&gt;&gt; 6 node cluster. RIng size of 64.
&gt;&gt;&gt;&gt; Riak 1.4.10
&gt;&gt;&gt;&gt; Riak CS 1.5.2
&gt;&gt;&gt;&gt; (installed from official Basho repos)
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Tests conducted using both multi-part and non-multi-part upload mode;
&gt;&gt;&gt;&gt; performance is similar with both. Tested against cluster when very
&gt;&gt;&gt;&gt; lightly loaded.
&gt;&gt;&gt;&gt; For the sake of testing, a 100M file is being used, that contains
&gt;&gt;&gt;&gt; random (hard to compress) data.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Cheers,
&gt;&gt;&gt;&gt; Toby
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; On 8 November 2014 at 01:41, David Meekin 
&gt;&gt;&gt;&gt; wrote:
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Hi,
&gt;&gt;&gt;&gt; I’ve setup a test 4 node RiakCS cluster on HP BL460c hardware and I can’t
&gt;&gt;&gt;&gt; seem to get S3 upload speeds above 2MB/s
&gt;&gt;&gt;&gt; I’m connecting direct to RiackCS on one of the nodes so there is no load
&gt;&gt;&gt;&gt; balancing software in place.
&gt;&gt;&gt;&gt; I have also installed s3cmd locally onto one of the nodes and the speeds
&gt;&gt;&gt;&gt; locally are the same.
&gt;&gt;&gt;&gt; These 4 nodes also run a test CEPH cluster with RadosGW and s3 uploads to
&gt;&gt;&gt;&gt; CEPH achieve 125MB/s
&gt;&gt;&gt;&gt; Any help would be appreciated as I’m currently evaluating both CEPH and
&gt;&gt;&gt;&gt; RiakCS.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; --
&gt;&gt;&gt; Turning and turning in the widening gyre
&gt;&gt;&gt; The falcon cannot hear the falconer
&gt;&gt;&gt; Things fall apart; the center cannot hold
&gt;&gt;&gt; Mere anarchy is loosed upon the world
&gt;&gt;&gt;
&gt;&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt; --
&gt;&gt; Kota UENISHI / @kuenishi
&gt;&gt; Basho Japan KK
&gt;
&gt;
&gt;
&gt; --
&gt; Turning and turning in the widening gyre
&gt; The falcon cannot hear the falconer
&gt; Things fall apart; the center cannot hold
&gt; Mere anarchy is loosed upon the world



-- 
Kota UENISHI / @kuenishi
Basho Japan KK

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

