---
title: "Re: Riak-CS , S3cmd and A single Node outage on a 3 node cluster"
description: ""
project: community
lastmod: 2013-08-23T10:36:00-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg12124"
mailinglist_parent_id: "msg12120"
author_name: "Brady Wetherington"
project_section: "mailinglistitem"
sent_date: 2013-08-23T10:36:00-07:00
---


Oh, I thought the rule was "if you have one more node than your n\_val, then
you can be sure each replica will be on a distinct node" - is that not
correct?

Also with n of 2 I would wonder how you would handle r and w - my guess
would be you \*could\* set them to 1 each, and just deal with possible
consistency issues if you lost a node. But it would be 'weird' in that you
wouldn't really have a quorum to make - it'd be "whoever is up right now
wins". And then - when the downed node comes back - you might spit back
inconsistent answers until AAE fixes up your data. Read-repair would \*not\*
end up fixing your data up, because the r value of 1 would be satisfied.
Does that sound right?

I'm asking not only for the original poster; but to make sure I understand
what I've got here for myself too!

-B.




On Thu, Aug 22, 2013 at 1:41 PM, Kelly McLaughlin  wrote:

&gt; Idan,
&gt;
&gt; Actually in the case you described of using a 3 node Riak cluster with
&gt; n\_val of 2 the behavior you see makes perfect sense. When using only three
&gt; nodes Riak does not guarantee that all replicas of an object will be on
&gt; distinct physical nodes. So if you have one node down you can hit a case
&gt; where both of the replicas of an object stored in Riak live on the downed
&gt; node. This explains the reason you see occasional failure to retrieve an
&gt; object with s3cmd, but most of the time it works just fine. It is for this
&gt; reason we strongly recommend people to use at least 5 nodes in their
&gt; production deployments. For testing, using at least use 4 nodes in the
&gt; cluster will likely reduce the chances of this situation occurring versus
&gt; using only 3.
&gt;
&gt; Also just be very cautious about reducing the n\_val to 2. You will save on
&gt; storage space doing that, but there is a trade-off to be made in
&gt; availability and performance in doing so.
&gt;
&gt; Hope that helps shed some light.
&gt;
&gt; Kelly
&gt;
&gt; On August 22, 2013 at 10:12:06 AM, Idan Shinberg (idan.shinb...@idomoo.com)
&gt; wrote:
&gt;
&gt; Thanks for the aid
&gt;
&gt; This is a testing environemt . no-one accesses it aside of me
&gt;
&gt; Prior to creating the objects via riak-cs , I set the cluster to n\_val 2
&gt; for the default bucket props .
&gt; On that empty cluster , i started creating objects . ( around 500) .
&gt; Then I stopped and killed one of the nodes .
&gt; Then the issues mentioned above were seen.
&gt;
&gt; still doesn't make any sense...
&gt;
&gt; Regards,
&gt;
&gt; Idan Shinberg
&gt;
&gt;
&gt; System Architect
&gt;
&gt; Idomoo Ltd.
&gt;
&gt;
&gt;
&gt; Mob +972.54.562.2072
&gt;
&gt; email idan.shinb...@idomoo.com
&gt;
&gt; web www.idomoo.com
&gt;
&gt; [image: Description:
&gt; file:///Users/kelly/Library/Containers/it.bloop.airmail/Data/Library/Application
&gt; Support/Airmail/General/Local/1377192732334581760/Attachments/fc66336e-e750-4c2d-b6e3-985d5a06b...@idomoo.co.il]
&gt;
&gt;
&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;
&gt;
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

