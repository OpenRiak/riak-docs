---
title: "Re: \"consistent\" map/reduce"
description: ""
project: community
lastmod: 2011-11-28T09:38:59-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg05755"
mailinglist_parent_id: "msg05750"
author_name: "Kresten Krab Thorup"
project_section: "mailinglistitem"
sent_date: 2011-11-28T09:38:59-08:00
---


Super. Thanks. I'll have to play around with it a bit and get back to you. 

Kresten 

On 28/11/2011, at 16.17, "Bryan Fink"  wrote:

&gt; On Mon, Nov 21, 2011 at 2:46 PM, Kresten Krab Thorup  wrote:
&gt;&gt; I'd like to be able to do a "consistent map/reduce" job i.e., with "R=2 
&gt;&gt; semantics" for an "N=3 bucket". Maybe other people have the same need, but 
&gt;&gt; I can't see if this is possible ... perhaps with the new riak\_pipe 
&gt;&gt; infrastructure?
&gt; 
&gt; Hi, Kresten. Indeed, this same topic came up in an independent
&gt; conversation last week. I think there are a few ways to attack it.
&gt; Let's start with yours:
&gt; 
&gt;&gt; The map function yields {Key, [{VectorClock,1,Hash}]} for each replica, but 
&gt;&gt; needs to run on \*all\* replicas of objects in a given Bucket. Hash is the 
&gt;&gt; real value I'm interested in i.e., the content-hash for the object; but it 
&gt;&gt; could be some other "map" function output.
&gt;&gt; 
&gt;&gt; Then, the reduce phase needs to "merge" a list of {VectorClock,N,Hash} 
&gt;&gt; tuples, by considering the VectorClocks to determine if results are in 
&gt;&gt; "conflict", or if one is before/after the other. N is reduced to the sum of 
&gt;&gt; all elements with equal Hash value.
&gt; 
&gt; I like many of the ideas in this approach. It has a nice distributed
&gt; data-provenance feel to it. I think the danger lies in the work that
&gt; the reduce phase would have to do. With distributed parallel
&gt; keylisting, there's no way to guarantee the order that the map results
&gt; arrive at the reduce processor. This means that the reduce state may
&gt; become quite large tracking all of the key/version pairs
&gt; produced-but-not-yet-satisfying-R. Maybe this is manageable, though,
&gt; so I'll also try to answer your questions:
&gt; 
&gt;&gt; - How can I have a M/R job run on \*all\* vnodes? Not just for objects that 
&gt;&gt; are owned by a primary?
&gt; 
&gt; The only way to do this right now is to use Riak Pipe directly. Setup
&gt; a pipe with your "map" and "reduce" fittings, then send inputs to it
&gt; such that one input hashes to each vnode. Using riak\_pipe\_qcover\_fsm
&gt; with N=1 might ease this process.
&gt; 
&gt;&gt; - The M/R "input" is essentially listkeys(Bucket) ... can this be done 
&gt;&gt; using "async keylisting", so that the operation does not hold up the vnode 
&gt;&gt; while listing?
&gt; 
&gt; Yes, absolutely. The riak\_kv\_pipe\_listkeys module does just this, and
&gt; is also an example of using riak\_pipe\_qcover\_fsm.
&gt; 
&gt; These two questions and answers lead to the basic pipe layout of:
&gt; 
&gt; [
&gt; {module= riak\_kv\_pipe\_listkeys}, %% setup with qcover N=1
&gt; 
&gt; {module= riak\_kv\_pipe\_get,
&gt; chashfun= follow}, %% each vnode processes keys it produces
&gt; 
&gt; {module= riak\_kv\_mrc\_map,
&gt; chashfun= follow},
&gt; 
&gt; {module= riak\_kv\_w\_reduce,
&gt; chashfun= ContstantOrCustom} %% see below
&gt; ]
&gt; 
&gt; Riak KV reduce fittings are normally set up with a constant chashfun,
&gt; such that \*all\* results are processed in one place. To help alleviate
&gt; the reduce-state-size problem, I might suggest using a chashfun that
&gt; spreads results, yet makes sure all results for each key end up at the
&gt; same reducer (most likely, hash the result's key, just as you would
&gt; for determining its KV preflist).
&gt; 
&gt; Note also that the "get" fitting has a 'follow' chashfun. This is
&gt; also different from normal MR usage, since we would normally set N=3
&gt; (or whatever the bucket has set) for the qcover-ing listkeys fitting.
&gt; The normal setting ensures that each key is produced only once, but
&gt; N=1 will produce the same key multiple times for buckets where N&gt;1.
&gt; You want each result processed locally ('follow') to get the
&gt; possibly-different vclock/hash stored at that vnode.
&gt; 
&gt; It may also be possible to take a completely different approach. A
&gt; simple modification of riak\_kv\_pipe\_get could allow it to attempt to
&gt; read all N replicas, perhaps even by simply starting a
&gt; riak\_kv\_get\_fsm. In this case, all of the merging of vclocks would
&gt; happen before the mapping instead of after. But, it would also miss
&gt; keys that were not fully replicated, since you'd likely want to
&gt; maintain N=3 for the keylisting qcover operation. I also haven't put
&gt; as much thought into this path, so there may be other demons lurking.
&gt; 
&gt;&gt; If someone can sketch a solution, I'd be happy to go hacking on it ...
&gt; 
&gt; Hopefully that's enough sketching to at least generate a second round
&gt; of questions. ;) I'd be very interested in hearing how it goes.
&gt; Please fire back with anything that needs more explanation.
&gt; 
&gt; -Bryan

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

