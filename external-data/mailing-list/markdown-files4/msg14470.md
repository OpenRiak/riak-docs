---
title: "Re: leveldb Hot Threads in 1.4.9?"
description: ""
project: community
lastmod: 2014-07-08T06:30:19-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg14470"
mailinglist_parent_id: "msg14468"
author_name: "Matthew Von-Maszewski"
project_section: "mailinglistitem"
sent_date: 2014-07-08T06:30:19-07:00
---


Responses inline.


On Jul 7, 2014, at 10:54 PM, Tom Lanyon  wrote:

&gt; Hi Matthew,
&gt; 
&gt; On Sunday, 6 July 2014 at 3:04, Matthew Von-Maszewski wrote: 
&gt;&gt; Tom,
&gt;&gt; 
&gt;&gt; Basho prides itself on quickly responding to all user queries. I have failed 
&gt;&gt; that tradition in this case. Please accept my apologies.
&gt; No problem; I appreciate you taking the time to look into our LOG.
&gt; 
&gt;&gt; 
&gt;&gt; The LOG data suggests leveldb is not stalling, especially not for 4 hours. 
&gt;&gt; Therefore the problem is related to disk utilization.
&gt; 
&gt; That matches our experience - leveldb itself is working hard on disk 
&gt; operations whilst Riak fails to respond to... anything, causing an apparent 
&gt; 'stall' from the client application's perspective.
&gt; 
&gt;&gt; You appear to have large values. I see .sst files where the average value is 
&gt;&gt; 100K to 1Mbyte in size. Is this intentional, or might you have a sibling 
&gt;&gt; problem?
&gt; Yes, we have a split between very small (headers only, no body) items and 1MB 
&gt; binary chunks. If we had our time again we'd probably use multi-backend to 
&gt; store these 1MB chunks in bitcask and keep leveldb for the small body-less 
&gt; items which require 2i.
&gt; 
&gt;&gt; My assessment is that your lower levels are full and therefore cascading 
&gt;&gt; regularly. "cascading" is like the typical champagne glass pyramid you see 
&gt;&gt; at weddings. Once all the glasses are full, new champagne at the top causes 
&gt;&gt; each subsequent layer to overflow into the one below that. You have the same 
&gt;&gt; problem, but with data. 
&gt;&gt; 
&gt;&gt; Your large values have filled each of the lower levels and regularly cause 
&gt;&gt; cascading data between multiple levels. The cascading is causing each 100K 
&gt;&gt; value write to become the equivalent of a 300K or 500K value as levels 
&gt;&gt; overflow. This cascading is chewing up your hard disk performance (by 
&gt;&gt; reducing the amount of time the hard drive has available for read requests).
&gt; By increasing the size of the lower levels (as you show below), does this 
&gt; mean there's more capacity for writes to occur in those levels before 
&gt; compaction is triggered and hence compacting them less frequently?

Exactly.

&gt; 
&gt; I guess this turns your champagne fountain analogy into more of a 'tipping 
&gt; bucket' where the data is no longer 'flowing' through the levels but is 
&gt; instead building up in each level before tipping into the next when it's at 
&gt; capacity? (pictorial representation: 
&gt; http://4.bp.blogspot.com/\_DUDhlpPD8X8/SIcN8D66j9I/AAAAAAAAASs/2Va3\_n3vamk/s400/23157087\_261a5da413.jpg)

Very good photo. May have to save it for some future presentation. Though I 
was visualizing champaign glasses versus large Octoberfest beer mugs.

&gt; 
&gt;&gt; The leveldb code for Riak 2.0 has increased the size of all the levels. The 
&gt;&gt; table of sizes is found at the top of leveldb's db/version\_set.cc 
&gt;&gt; (http://version\_set.cc). You could patch your current code if desired with 
&gt;&gt; this table from 2.0:
&gt;&gt; 
&gt;&gt; { 
&gt;&gt; {10485760, 262144000, 57671680, 209715200, 0, 420000000, true}, 
&gt;&gt; {10485760, 82914560, 57671680, 419430400, 0, 209715200, true}, 
&gt;&gt; {10485760, 314572800, 57671680, 3082813440, 200000000, 314572800, false}, 
&gt;&gt; {10485760, 419430400, 57671680, 6442450944ULL, 4294967296ULL, 419430400, 
&gt;&gt; false}, 
&gt;&gt; {10485760, 524288000, 57671680, 128849018880ULL, 85899345920ULL, 524288000, 
&gt;&gt; false}, 
&gt;&gt; {10485760, 629145600, 57671680, 2576980377600ULL, 1717986918400ULL, 
&gt;&gt; 629145600, false}, 
&gt;&gt; {10485760, 734003200, 57671680, 51539607552000ULL, 34359738368000ULL, 
&gt;&gt; 734003200, false} 
&gt;&gt; }; 
&gt;&gt; 
&gt;&gt; 
&gt;&gt; You cannot take the entire 2.0 leveldb into your 1.4 code base due to 
&gt;&gt; various option changes.
&gt; I assume leveldb will just 'handle' making the levels larger once nodes are 
&gt; restarted with this updated configuration? I also assume that it would not 
&gt; be wise to then rollback the change to smaller levels after this has been 
&gt; done?

Yes, it "just works". Rollback to smaller size would cause leveldb to churn 
for a long time as you assumed.

&gt;&gt; Let me know if this helps. I have previously hypothesized that "grooming" 
&gt;&gt; compactions should be limited to one thread total. However my test datasets 
&gt;&gt; never demonstrated a benefit. Your dataset might be the case that proves the 
&gt;&gt; benefit. I will go find the grooming patch to hot\_threads for you if the 
&gt;&gt; above table proves insufficient.
&gt; 
&gt; Do I understand correctly that this would mean compactions would continue, 
&gt; but limited to one thread, so that the rest of the application can still 
&gt; respond to client requests? If so, that sounds like it may help a situation 
&gt; like ours - although I'd wonder whether the rate-limited compaction would 
&gt; ever "keep up" with the inflowing data.
&gt; 

The fourth and fifth columns of the table above represent compaction 
thresholds. The fourth column is the size where leveldb should start 
"grooming" a level (compact data in this level up to next level). The fifth 
column is the size where leveldb decides the compactions are not keeping pace 
with incoming writes. This is the point where the write throttle gets applied 
to incoming user Write calls. My theory is that the buffer zone between the 
fourth and fifth column sizes would be limited to one thread of compaction to 
keep I/O bandwidth available for your read operations. If compactions get too 
far behind, all threads would activate again once level size exceeds the fifth 
column amount.

And while we are talking tuning, I have data from different loads that suggests 
the fourth and fifth numbers of row four above should be doubled (6442450944, 
4294967296 becoming 12884901888, 8589934492). This makes the two values 10x 
smaller than those of row five. Currently the row four thresholds are 20x 
smaller than row five. No other rows change.

&gt; Thanks,
&gt; Tom
&gt; 
&gt; 


\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

