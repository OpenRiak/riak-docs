---
title: "Re: Schema Architecture, Map Reduce & Key Lists"
description: ""
project: community
lastmod: 2011-02-10T16:13:34-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg02277"
mailinglist_parent_id: "msg02276"
author_name: "Mat Ellis"
project_section: "mailinglistitem"
sent_date: 2011-02-10T16:13:34-08:00
---


Good idea, thanks.

M.

On Feb 10, 2011, at 4:10 PM, Alexander Sicular wrote:

&gt; i would change the model and have another stream for "converted" clicks.
&gt; 
&gt; -Alexander Sicular
&gt; 
&gt; @siculars
&gt; 
&gt; On Feb 10, 2011, at 5:58 PM, Mat Ellis wrote:
&gt; 
&gt;&gt; Thanks Bryan, that certainly looks interesting. The clicks are amended but 
&gt;&gt; just once and only a tiny percentage (when they convert). We're basically 
&gt;&gt; doing what you describe: taking a click stream and processing it once into a 
&gt;&gt; set of summary tables for reporting & decision making. We'll take a look at 
&gt;&gt; it as soon as we've finished getting our head around the Ripple goodness.
&gt;&gt; 
&gt;&gt; Cheers
&gt;&gt; 
&gt;&gt; M.
&gt;&gt; 
&gt;&gt; On Feb 10, 2011, at 11:54 AM, Bryan Fink wrote:
&gt;&gt; 
&gt;&gt;&gt; On Thu, Feb 10, 2011 at 12:35 PM, Mat Ellis  wrote:
&gt;&gt;&gt;&gt; We are converting a mysql based schema to Riak using Ripple. We're tracking
&gt;&gt;&gt;&gt; a lot of clicks, and each click belongs to a cascade of other objects:
&gt;&gt;&gt;&gt; click -&gt; placement -&gt; campaign -&gt; customer
&gt;&gt;&gt;&gt; i.e. we do a lot of operations on these clicks grouped by placement or sets
&gt;&gt;&gt;&gt; of placements.
&gt;&gt;&gt; … snip …
&gt;&gt;&gt;&gt; On a related noob-note, what would be the best way of creating a set of the
&gt;&gt;&gt;&gt; clicks for a given placement? Map Reduce or Riak Search or some other
&gt;&gt;&gt;&gt; method?
&gt;&gt;&gt; 
&gt;&gt;&gt; Hi, Mat. I have an alternative strategy I think you could try if
&gt;&gt;&gt; you're up for stepping outside of the Ripple interface. Your incoming
&gt;&gt;&gt; clicks reminded me of other stream data I've processed before, so the
&gt;&gt;&gt; basic idea is to store clicks as a stream, and then process that
&gt;&gt;&gt; stream later. The tools I'd use to do this are Luwak[1] and
&gt;&gt;&gt; luwak\_mr[2].
&gt;&gt;&gt; 
&gt;&gt;&gt; First, store all clicks, as they arrive, in one Luwak file (or maybe
&gt;&gt;&gt; one Luwak file per host accepting clicks, depending on your service's
&gt;&gt;&gt; arrangement). Luwak has a streaming interface that's available
&gt;&gt;&gt; natively in distributed Erlang, or over HTTP by exploiting the
&gt;&gt;&gt; "chunked" encoding type. Roll over to a new file on whatever
&gt;&gt;&gt; convenient trigger you like (time period, timeout, manual
&gt;&gt;&gt; intervention, etc.).
&gt;&gt;&gt; 
&gt;&gt;&gt; Next, use map/reduce to process the stream. The luwak\_mr utility will
&gt;&gt;&gt; allow you to specify a Luwak file by name, and it will handle toss
&gt;&gt;&gt; each of the chunks of that file to various cluster nodes for
&gt;&gt;&gt; processing. The first stage of your map/reduce query just needs to be
&gt;&gt;&gt; able to handle any single chunk of the file.
&gt;&gt;&gt; 
&gt;&gt;&gt; I've posted a few examples about how to use the luwak\_mr
&gt;&gt;&gt; utility.[3][4][5] They deal with analyzing events in baseball games
&gt;&gt;&gt; (another sort of stream of events).
&gt;&gt;&gt; 
&gt;&gt;&gt; Pros:
&gt;&gt;&gt; - No need to list keys.
&gt;&gt;&gt; - The time to process a day's data should be proportional to the
&gt;&gt;&gt; number of clicks on that day (i.e. proportional to the size of the
&gt;&gt;&gt; file).
&gt;&gt;&gt; 
&gt;&gt;&gt; Caveats:
&gt;&gt;&gt; - Luwak works best with write-once data. Modifying a block of a
&gt;&gt;&gt; Luwak file after it has been written causes the block to be copied,
&gt;&gt;&gt; and the old version of the block is not deleted. (Even if some of
&gt;&gt;&gt; your data is modification-heavy, this might work for the non-modified
&gt;&gt;&gt; parts … like the key list for a day's clicks?)
&gt;&gt;&gt; - I don't have good numbers for Luwak's speed/efficiency.
&gt;&gt;&gt; - I've only recently started experimenting with Luwak in this
&gt;&gt;&gt; map/reducing manner, so I'm not sure if there are other pitfalls.
&gt;&gt;&gt; 
&gt;&gt;&gt; [1] http://wiki.basho.com/Luwak.html
&gt;&gt;&gt; [2] http://contrib.basho.com/luwak\_mr.html
&gt;&gt;&gt; [3] http://blog.beerriot.com/2011/01/16/mapreducing-luwak/
&gt;&gt;&gt; [4] 
&gt;&gt;&gt; http://blog.basho.com/2011/01/20/baseball-batting-average%2c-using-riak-map/reduce/
&gt;&gt;&gt; [5] http://blog.basho.com/2011/01/26/fixing-the-count/
&gt;&gt; 
&gt;&gt; 
&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt; riak-users mailing list
&gt;&gt; riak-users@lists.basho.com
&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt; 


\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

