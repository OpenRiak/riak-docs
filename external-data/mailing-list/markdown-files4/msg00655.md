---
title: "Re: Riak and Amazon EC2"
description: ""
project: community
lastmod: 2010-06-27T03:36:56-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg00655"
mailinglist_parent_id: "msg00653"
author_name: "Dmitry Demeshchuk"
project_section: "mailinglistitem"
sent_date: 2010-06-27T03:36:56-07:00
---


On Fri, Jun 25, 2010 at 8:25 PM, Ryan Tilder  wrote:
&gt; Hi, Dmitry.  There are some gaps in the information you included here that
&gt; might help clarify what's going on so I'm going to just rattle off some
&gt; questions for clarification.
&gt; Is your test driver only making requests of a single EC2 instance?  Or are
&gt; you querying all 7 nodes directly in so sort of load distribution?   If you
&gt; aren't querying all 7 nodes directly, then you will likely see performance
&gt; on par with a cluster with only a single "physical" node.

I tried both ways: querying only one node and querying all the nodes.
The results were approximately the same. But as far as I understand,
for map-reduce queries it's an expected result, isn't it?

&gt; Are you certain that the 7 nodes are communicating with each other?  The
&gt; output of the "riak-admin status" command should list the nodes in the
&gt; "ring\_members" field.

Yes, sure.

&gt; Are the "documents" a separate key with Riak's built in links to the
&gt; "entities" or are they keys with a data blob that refer to the entities?[1]
&gt;  If the latter, have you
&gt; read http://blog.basho.com/2010/02/24/link-walking-by-example/ ?

To simplify, document data (I mean, value in Riak database) had
structure like this:

[{entities, [123, 456, 745, 2352, 235 | ...]}].

I actually used timestamps in microseconds for Ids but that doesn't
really matter.
And, regarding your last question, documents and entities were stored
in different buckets.

What about links? Should they give better speed in that case? Also,
neither Erlang native API (I mean riak\_client module) nor Erlang PBC
seem to have link-walking functions like REST API.


&gt; It's also important for me to note that EC2 instances do not necessarily
&gt; have the same characteristics of actual physical hardware when it comes to
&gt; preventing resource contention.  Since EC2 instances are virtualized, you
&gt; have no idea what other load the physical host of a given instance may be
&gt; under.  As a result it is possible to have a Riak instance running on the
&gt; same hardware as another IO and CPU intensive instance without your
&gt; knowledge, impeding each other to a certain degree.  We've had a number of
&gt; users complain of performance problems with Riak clusters running on EC2 at
&gt; various times.  From my personal and anecdotal experience, EC2 seems to be
&gt; pretty heavily oversubscribed much of the time which leads to intermittent
&gt; performance issues for all kinds of applications.
&gt; All of that is just a long winded way of saying: don't expect shared
&gt; virtualized resources to provide the same performance as dedicated physical
&gt; hardware.  But you should still see at least somewhat better performance
&gt; that you're seeing now if your testing harness is testing properly.

Sure, I understand that. But I expected at least a bit better performance.

Anyway, the day before yesterday I ran some tests using basho\_bench.
These tests cheered me up a bit :)
Here's the link to the results:

http://demmonoid.livejournal.com/4098.html

Please let me know if you want me to add or correct any links to your
resources or add any more information about the tests.

&gt; --Ryan
&gt; 1. I'm not certain if you're saying that the documents are stored in a
&gt; separate bucket from the entities in the same Riak cluster or a separate
&gt; Riak cluster entirely.
&gt; On Fri, Jun 25, 2010 at 12:02 AM, Dmitry Demeshchuk 
&gt; wrote:
&gt;&gt;
&gt;&gt; Greetings.
&gt;&gt;
&gt;&gt; I tried running Riak with bitcask backend on 7 Amazon EC2 standard
&gt;&gt; large instances (7.5 GB RAM, 4 EC2 CPU units) and performed some
&gt;&gt; tests.
&gt;&gt; For comparison, I built up the following Riak clusters:
&gt;&gt;
&gt;&gt; 7 physical nodes ring
&gt;&gt; 1 physical node ring (on one of the 7 instances, but I ran the tests
&gt;&gt; separately so the rings won't mess with each other)
&gt;&gt; 1 physical node ring on an extra large instance (15 GB RAM, 8EC2 CPU
&gt;&gt; units)
&gt;&gt;
&gt;&gt; and ran a couple of tests with putting and getting data using Riak
&gt;&gt; native Erlang API (not PBC).
&gt;&gt;
&gt;&gt; I had 2 buckets, the first one having small (averagely about 1KB)
&gt;&gt; values, but a lot of them (about several millions) called "entities",
&gt;&gt; and the second one having lists of keys from the first database,
&gt;&gt; called "documents". So, every document consists of a lot of entities
&gt;&gt; (I used 100 and 1000 for my tests). So, the approximate size of every
&gt;&gt; document was either 100KB or 1MB.
&gt;&gt;
&gt;&gt; So, I performed tests of putting documents and entities to database
&gt;&gt; and then obtaining them. I tried to perform reads and writes using 10
&gt;&gt; and 100 concurrent Erlang processes (well, 100 was generally too much
&gt;&gt; as I ran out of CPU), first from only one machine and then from 2 and
&gt;&gt; 3 machines at the same time (for the 7-nodes ring). Of course, the
&gt;&gt; entities were obtained using map-reduce.
&gt;&gt;
&gt;&gt; The first weird thing was that even with 10 concurrent reads and
&gt;&gt; writes the performance didn't differ for all three clusters. Okay, 1
&gt;&gt; large and 1 extra large nodes don't differ so much but the 7 nodes
&gt;&gt; should have given me some performance, shouldn't they?
&gt;&gt;
&gt;&gt; The second thing was that the average read time for one document with
&gt;&gt; 1000 entities was about 5 seconds, and again, the number of machines
&gt;&gt; in the cluster didn't affect the result. I guess I just stumbled upon
&gt;&gt; the performance of the instance that sent all the map-reduce requests
&gt;&gt; and then collected the replies because when I ran tests on the other 2
&gt;&gt; instances, all three had the same performance.
&gt;&gt;
&gt;&gt; The other strange thing was that during data writes most of the time
&gt;&gt; nodes were not io-loaded. If it was a one-stream write, it would be
&gt;&gt; obvious. But it were 10 and then 20 and 30 simultaneous writing
&gt;&gt; processes!
&gt;&gt;
&gt;&gt;
&gt;&gt; Unfortunately I cannot provide the detailed results now, they are
&gt;&gt; pretty messed up. I'm going to use basho\_bench to make good graphs and
&gt;&gt; tables of these tests.
&gt;&gt;
&gt;&gt; Any advises for the future tests or any explanations for such strange
&gt;&gt; performance?
&gt;&gt;
&gt;&gt; Thank you in advance and sorry for a little messed up e-mail.
&gt;&gt;
&gt;&gt; --
&gt;&gt; Best regards,
&gt;&gt; Dmitry Demeshchuk
&gt;&gt;
&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt; riak-users mailing list
&gt;&gt; riak-users@lists.basho.com
&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;
&gt;



-- 
Best regards,
Dmitry Demeshchuk

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

