---
title: "Re: Two nodes cluster, 2i queries impossible when one node down?"
description: ""
project: community
lastmod: 2012-03-30T11:42:50-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg07092"
mailinglist_parent_id: "msg07091"
author_name: "Cedric Maion"
project_section: "mailinglistitem"
sent_date: 2012-03-30T11:42:50-07:00
---


Hi

Thanks for your reply. Was expecting something along those lines.
My production setup don't justify more than two physical nodes for that
caching feature.

So I guess that my options with riak are:
1. keep N=2 but add two more physical nodes
2. keep two physical nodes, but set N=3 and hope that all 3 copies of
each partition don't end up on the same physical node
3. ?


Thanks again,

 Cedric


On 30/03/2012 20:15, Jared Morrow wrote:
&gt; Cedric,
&gt;
&gt; The problem you are seeing is that the list of vnodes does not spread
&gt; perfectly between physical nodes. So with N=2 on only a 2 node
&gt; cluster, you aren't given assurances that those two copies of data
&gt; will end up on two vnodes owned by different physical nodes. So if
&gt; you have data X on vnode 15 and 18 pretend, those two vnodes might
&gt; both be owned by 192.168.0.21.
&gt;
&gt; If you are running less (or equal) nodes than your N value, all bets
&gt; are off when it comes to testing riak functionality. We suggest that
&gt; you have a physical node count &gt; N +1 to see Riak's true
&gt; capabilities. If you just want to test functionality on a small
&gt; scale, you can use the 'make devrel' setup to run 4 nodes on a single
&gt; machine for testing [1]. Please do not test speed or throughput in
&gt; that setup, but if you are trying to see how 2i works, that's a better
&gt; option than two nodes with N=2. 
&gt;
&gt;
&gt; [1] http://wiki.basho.com/Building-a-Development-Environment.html
&gt;
&gt; Hope that helps,
&gt; Jared
&gt;
&gt;
&gt;
&gt;
&gt;
&gt;
&gt; On Mar 30, 2012, at 11:49 AM, Cedric Maion wrote:
&gt;
&gt;&gt; Hey there,
&gt;&gt;
&gt;&gt; I'm playing with a two node riak 1.1.1 cluster, but can't figure how to
&gt;&gt; make riak happy with only one node (and the other one failed).
&gt;&gt;
&gt;&gt; My use case is the following:
&gt;&gt; - HTML pages gets stored in the riak cluster (it's used as a page cache)
&gt;&gt; - I'm using secondary indexes to tag those documents
&gt;&gt; - When I need to invalidate some HTML pages, I make a query on those
&gt;&gt; secondary indexes to retrieve the list of keys that needs to be deleted
&gt;&gt; (documents matching a specific tag value)
&gt;&gt;
&gt;&gt; The bucket is configured with
&gt;&gt; {"props":{"n\_val":2,"allow\_mult":false,"r":1,"w":1,"dw":0,"rw":1}}.
&gt;&gt;
&gt;&gt; Everything is working fine when both node are up and running.
&gt;&gt; However, if I turn one node off, secondary indexes queries returns
&gt;&gt; {error,{error,insufficient\_vnodes\_available}} errors.
&gt;&gt;
&gt;&gt; I can't find a way to have the cluster converge to a stable state on only 
&gt;&gt; one node.
&gt;&gt;
&gt;&gt;
&gt;&gt; (192.168.0.21 is the node that has been turned off, by just stopping riak 
&gt;&gt; with /etc/init.d/riak stop)
&gt;&gt;
&gt;&gt;
&gt;&gt; root@192.168.0.51:~# riak-admin member\_status
&gt;&gt; Attempting to restart script through sudo -u riak
&gt;&gt; ================================= 
&gt;&gt; Membership==================================
&gt;&gt; Status Ring Pending Node
&gt;&gt; -------------------------------------------------------------------------------
&gt;&gt; valid 50.0% -- 'riak@192.168.0.21'
&gt;&gt; valid 50.0% -- 'riak@192.168.0.51'
&gt;&gt; -------------------------------------------------------------------------------
&gt;&gt; Valid:2 / Leaving:0 / Exiting:0 / Joining:0 / Down:0
&gt;&gt;
&gt;&gt;
&gt;&gt; =&gt; 192.168.0.21 is not seen as being down, not sure if it's an issue or not
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt; root@192.168.0.51:~# riak-admin transfers
&gt;&gt; Attempting to restart script through sudo -u riak
&gt;&gt; Nodes ['riak@192.168.0.21'] are currently down.
&gt;&gt; 'riak@192.168.0.51' waiting to handoff 5 partitions
&gt;&gt;
&gt;&gt;
&gt;&gt; =&gt; by creating/reading many keys, I finally get "waiting to handoff 32 
&gt;&gt; partitions", which seems OK to me (ring size is 64, so each node should 
&gt;&gt; normally own 32).
&gt;&gt; =&gt; however, secondary indexes queries always fails, until I turn the failed 
&gt;&gt; node ON again.
&gt;&gt;
&gt;&gt;
&gt;&gt; I tried to force "riak-admin down riak@192.168.0.21" from the valid node, 
&gt;&gt; but no luck either.
&gt;&gt;
&gt;&gt; Not being able to use secondary indexes while a node is down is a real 
&gt;&gt; problem.
&gt;&gt; Is this expected behavior, or what am I missing?
&gt;&gt;
&gt;&gt;
&gt;&gt; Thanks in advance!
&gt;&gt; Kind regards,
&gt;&gt;
&gt;&gt; Cedric
&gt;&gt;
&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt; riak-users mailing list
&gt;&gt; riak-users@lists.basho.com 
&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

