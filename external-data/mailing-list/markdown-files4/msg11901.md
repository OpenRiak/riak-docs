---
title: "Re: erlang go boom"
description: ""
project: community
lastmod: 2013-08-05T15:54:25-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg11901"
mailinglist_parent_id: "msg11900"
author_name: "Paul Ingalls"
project_section: "mailinglistitem"
sent_date: 2013-08-05T15:54:25-07:00
---


I watched top on all the instances when things started to fall apart. This is 
what I sawâ€¦

Everything was jamming along just fine. CPU usage was about 25%, ram usage was 
about 25% (3 of the 7 were at about 15%).

Suddenly, CPU usage spikes to over 50% and ram usage spikes to 80-90% (and I'm 
guessing on those nodes that crash, it hit over 100%). This happens within 
seconds, its not a gradual growth in RAM or CPU but a spike. On those nodes 
that survived, they stayed at this higher water mark for RAM (and CPU keeps 
running at the incremental difference since I killed the client). Is there any 
easy way to figure out what the process is working on?

Could compaction cause this? Basically I have hit a place where it needs to 
merge SSTables to a higher level, and there isn't enough RAM to pull the data 
set into memory?

Paul



Paul Ingalls
Founder & CEO Fanzo
p...@fanzo.me
@paulingalls
http://www.linkedin.com/in/paulingalls



On Aug 5, 2013, at 2:58 PM, Paul Ingalls  wrote:

&gt; Hey Kresten,
&gt; 
&gt; Thanks for the response!
&gt; 
&gt; I learned my lesson on setting bucket properties. So all buckets currently 
&gt; use the defaults.
&gt; 
&gt; here is the output from one of our nodes:
&gt; 
&gt; total 40
&gt; drwxr-xr-x 2 root root 4096 Aug 5 21:10 ./
&gt; drwxr-xr-x 6 root root 4096 Aug 4 17:26 ../
&gt; -rw-r--r-- 1 root root 14187 Aug 4 17:37 
&gt; riak\_core\_ring.default.20130804173753
&gt; -rw-r--r-- 1 root root 14586 Aug 5 21:10 
&gt; riak\_core\_ring.default.20130805211043
&gt; 
&gt; another node:
&gt; 
&gt; total 72
&gt; drwxr-xr-x 2 root root 4096 Aug 5 21:10 ./
&gt; drwxr-xr-x 6 root root 4096 Aug 4 17:26 ../
&gt; -rw-r--r-- 1 root root 14187 Aug 4 17:37 
&gt; riak\_core\_ring.default.20130804173753
&gt; -rw-r--r-- 1 root root 14358 Aug 5 20:56 
&gt; riak\_core\_ring.default.20130805205650
&gt; -rw-r--r-- 1 root root 14529 Aug 5 21:09 
&gt; riak\_core\_ring.default.20130805210924
&gt; -rw-r--r-- 1 root root 14586 Aug 5 21:10 
&gt; riak\_core\_ring.default.20130805211043
&gt; 
&gt; looks like the largest number of files in that directory on any node is 5
&gt; 
&gt; Paul
&gt; 
&gt; Paul Ingalls
&gt; Founder & CEO Fanzo
&gt; p...@fanzo.me
&gt; @paulingalls
&gt; http://www.linkedin.com/in/paulingalls
&gt; 
&gt; 
&gt; 
&gt; On Aug 5, 2013, at 2:52 PM, Kresten Krab Thorup  wrote:
&gt; 
&gt;&gt; I'd think the large #buckets could be the issue; especially if there is any 
&gt;&gt; bucket properties being set, because that would cause the ring data 
&gt;&gt; structure to be enormous.
&gt;&gt; 
&gt;&gt; Could you provide an "ls -l" output of the riak data/ring directory?
&gt;&gt; 
&gt;&gt; Sent from my iPhone
&gt;&gt; 
&gt;&gt; On 05/08/2013, at 21.52, "Paul Ingalls" 
&gt;&gt; &gt; wrote:
&gt;&gt; 
&gt;&gt; As promised in previous email, I hit a fairly big problem over the weekend 
&gt;&gt; and then reproduced it this morning and I was wondering if I could get some 
&gt;&gt; help.
&gt;&gt; 
&gt;&gt; Basically, I was running my code against our risk cluster and everything was 
&gt;&gt; moving along just fine. However, at some point Riak just seems to hit a 
&gt;&gt; wall. I get to a certain scale of content, about 9-10 GB per node, and 
&gt;&gt; about half the cluster gives up the ghost.
&gt;&gt; 
&gt;&gt; I've done this twice from scratch now, the first time I thought maybe I was 
&gt;&gt; trying to push too many transactions per second. But the second time I 
&gt;&gt; reduced the speed and changed some settings in the app.config that I thought 
&gt;&gt; would reduce memory usage. Ran it again, and 24 hrs later I still hit the 
&gt;&gt; wall (almost the same place).
&gt;&gt; 
&gt;&gt; So, I figure I'm still doing something wrong, I'm just not sure what. I'm 
&gt;&gt; obviously hitting some kind of heap issue, but I'm not sure what else I can 
&gt;&gt; do about it. I'm hoping there is something obvious I'm missing in my 
&gt;&gt; ignorance, cuz at the moment I'm stuck...
&gt;&gt; 
&gt;&gt; Some details:
&gt;&gt; 
&gt;&gt; 7 node cluster running 1.4
&gt;&gt; each VM has 7GB RAM and 4 CPUs
&gt;&gt; the data directory is on a RAID0 with 750GB of space
&gt;&gt; 128 partitions, levelDB backend
&gt;&gt; using links and secondary indexes
&gt;&gt; lots of buckets (over 10 million), a couple buckets have lots of keys (one 
&gt;&gt; was around 6.6 million keys when it crashed, the other around 3.7 million).
&gt;&gt; Values are pretty small, almost all are just a few bytes. There is one 
&gt;&gt; bucket, the largest (6.6 million keys), with value sizes between 1-2k.
&gt;&gt; 
&gt;&gt; primary custom app config settings:
&gt;&gt; {kernel,
&gt;&gt; [
&gt;&gt; {inet\_dist\_listen\_min, 6000},
&gt;&gt; {inet\_dist\_listen\_max, 7999}
&gt;&gt; ]},
&gt;&gt; 
&gt;&gt; {riak\_core, [
&gt;&gt; {default\_bucket\_props, [
&gt;&gt; {allow\_mult, true},
&gt;&gt; {r, 1},
&gt;&gt; {w, 1},
&gt;&gt; {dw, 1},
&gt;&gt; {dw, 1}
&gt;&gt; ]},
&gt;&gt; {ring\_creation\_size, 128},
&gt;&gt; ]},
&gt;&gt; 
&gt;&gt; {riak\_kv, [
&gt;&gt; {storage\_backend, riak\_kv\_eleveldb\_backend},
&gt;&gt; ]}
&gt;&gt; 
&gt;&gt; {eleveldb, [
&gt;&gt; {max\_open\_files, 32}
&gt;&gt; ]},
&gt;&gt; 
&gt;&gt; custom vm.args settings
&gt;&gt; +A 16
&gt;&gt; 
&gt;&gt; Here is some of the error information I am seeing when it all goes boom:
&gt;&gt; 
&gt;&gt; \*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*
&gt;&gt; \* On one of the nodes that crashes:
&gt;&gt; \*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*
&gt;&gt; last lines of console.log
&gt;&gt; -------------------
&gt;&gt; 2013-08-05 17:59:08.071 [info] 
&gt;&gt; &lt;0.29251.556&gt;@riak\_kv\_exchange\_fsm:key\_exchange:206 Repaired 1 keys during 
&gt;&gt; active anti-entropy exchange of 
&gt;&gt; {468137243207554840987117797979434404733540892672,3} between 
&gt;&gt; {479555224749202520035584085735030365824602865664,riak@riak001} and 
&gt;&gt; {490973206290850199084050373490626326915664838656,riak@riak002}
&gt;&gt; 2013-08-05 18:01:10.234 [info] 
&gt;&gt; &lt;0.83.0&gt;@riak\_core\_sysmon\_handler:handle\_event:92 monitor large\_heap 
&gt;&gt; &lt;0.14832.557&gt; 
&gt;&gt; [{initial\_call,{riak\_kv\_get\_fsm,init,1}},{almost\_current\_function,{riak\_object,encode\_maybe\_binary,1}},{message\_queue\_len,1}]
&gt;&gt; 
&gt;&gt; [{old\_heap\_block\_size,0},{heap\_block\_size,47828850},{mbuf\_size,0},{stack\_size,10},{old\_heap\_size,0},{heap\_size,40978448}]
&gt;&gt; 2013-08-05 18:01:10.672 [info] 
&gt;&gt; &lt;0.83.0&gt;@riak\_core\_sysmon\_handler:handle\_event:92 monitor large\_heap 
&gt;&gt; &lt;0.12133.557&gt; 
&gt;&gt; [{initial\_call,{riak\_api\_pb\_server,init,1}},{almost\_current\_function,{riak\_pb\_kv\_codec,'-encode\_content\_meta/3-lc$^0/1-1-',1}},{message\_queue\_len,0}]
&gt;&gt; 
&gt;&gt; [{old\_heap\_block\_size,0},{heap\_block\_size,47828850},{mbuf\_size,0},{stack\_size,45},{old\_heap\_size,0},{heap\_size,40978360}]
&gt;&gt; 2013-08-05 18:01:12.993 [info] 
&gt;&gt; &lt;0.83.0&gt;@riak\_core\_sysmon\_handler:handle\_event:92 monitor large\_heap 
&gt;&gt; &lt;0.14832.557&gt; 
&gt;&gt; [{initial\_call,{riak\_kv\_get\_fsm,init,1}},{almost\_current\_function,{dict,fold\_bucket,3}},{message\_queue\_len,1}]
&gt;&gt; 
&gt;&gt; [{old\_heap\_block\_size,0},{heap\_block\_size,59786060},{mbuf\_size,0},{stack\_size,50},{old\_heap\_size,0},{heap\_size,40978626}]
&gt;&gt; 2013-08-05 18:01:13.816 [info] 
&gt;&gt; &lt;0.83.0&gt;@riak\_core\_sysmon\_handler:handle\_event:92 monitor large\_heap 
&gt;&gt; &lt;0.14832.557&gt; 
&gt;&gt; [{initial\_call,{riak\_kv\_get\_fsm,init,1}},{almost\_current\_function,{dict,fold\_bucket,3}},{message\_queue\_len,1}]
&gt;&gt; 
&gt;&gt; [{old\_heap\_block\_size,0},{heap\_block\_size,59786060},{mbuf\_size,0},{stack\_size,50},{old\_heap\_size,0},{heap\_size,40978626}]
&gt;&gt; 2013-08-05 18:01:13.819 [info] 
&gt;&gt; &lt;0.83.0&gt;@riak\_core\_sysmon\_handler:handle\_event:92 monitor large\_heap 
&gt;&gt; &lt;0.12133.557&gt; 
&gt;&gt; [{initial\_call,{riak\_api\_pb\_server,init,1}},{almost\_current\_function,{riak\_kv\_pb,pack,5}},{message\_queue\_len,0}]
&gt;&gt; 
&gt;&gt; [{old\_heap\_block\_size,0},{heap\_block\_size,59786060},{mbuf\_size,0},{stack\_size,200971},{old\_heap\_size,0},{heap\_size,47627275}]
&gt;&gt; 2013-08-05 18:01:14.594 [info] 
&gt;&gt; &lt;0.83.0&gt;@riak\_core\_sysmon\_handler:handle\_event:92 monitor large\_heap 
&gt;&gt; &lt;0.14832.557&gt; 
&gt;&gt; [{initial\_call,{riak\_kv\_get\_fsm,init,1}},{almost\_current\_function,{riak\_object,encode\_maybe\_binary,1}},{message\_queue\_len,1}]
&gt;&gt; 
&gt;&gt; [{old\_heap\_block\_size,0},{heap\_block\_size,116769640},{mbuf\_size,0},{stack\_size,49},{old\_heap\_size,0},{heap\_size,81956796}]
&gt;&gt; 2013-08-05 18:01:15.729 [info] 
&gt;&gt; &lt;0.83.0&gt;@riak\_core\_sysmon\_handler:handle\_event:92 monitor large\_heap 
&gt;&gt; &lt;0.12133.557&gt; 
&gt;&gt; [{initial\_call,{riak\_api\_pb\_server,init,1}},{almost\_current\_function,{riak\_kv\_pb,encode,2}},{message\_queue\_len,0}]
&gt;&gt; 
&gt;&gt; [{old\_heap\_block\_size,0},{heap\_block\_size,74732575},{mbuf\_size,0},{stack\_size,81},{old\_heap\_size,0},{heap\_size,42778355}]
&gt;&gt; 2013-08-05 18:01:15.878 [info] 
&gt;&gt; &lt;0.83.0&gt;@riak\_core\_sysmon\_handler:handle\_event:92 monitor large\_heap 
&gt;&gt; &lt;0.14832.557&gt; 
&gt;&gt; [{initial\_call,{riak\_kv\_get\_fsm,init,1}},{almost\_current\_function,{riak\_object,encode\_maybe\_binary,1}},{message\_queue\_len,1}]
&gt;&gt; 
&gt;&gt; [{old\_heap\_block\_size,0},{heap\_block\_size,116769640},{mbuf\_size,0},{stack\_size,49},{old\_heap\_size,0},{heap\_size,81956794}]
&gt;&gt; 2013-08-05 18:01:15.878 [info] 
&gt;&gt; &lt;0.83.0&gt;@riak\_core\_sysmon\_handler:handle\_event:92 monitor large\_heap 
&gt;&gt; &lt;0.14832.557&gt; 
&gt;&gt; [{initial\_call,{riak\_kv\_get\_fsm,init,1}},{almost\_current\_function,{riak\_object,encode\_maybe\_binary,1}},{message\_queue\_len,1}]
&gt;&gt; 
&gt;&gt; [{old\_heap\_block\_size,0},{heap\_block\_size,116769640},{mbuf\_size,0},{stack\_size,41},{old\_heap\_size,0},{heap\_size,81956788}]
&gt;&gt; 2013-08-05 18:01:15.878 [info] 
&gt;&gt; &lt;0.83.0&gt;@riak\_core\_sysmon\_handler:handle\_event:92 monitor large\_heap 
&gt;&gt; &lt;0.14832.557&gt; 
&gt;&gt; [{initial\_call,{riak\_kv\_get\_fsm,init,1}},{almost\_current\_function,{riak\_object,encode\_maybe\_binary,1}},{message\_queue\_len,1}]
&gt;&gt; 
&gt;&gt; [{old\_heap\_block\_size,0},{heap\_block\_size,116769640},{mbuf\_size,0},{stack\_size,52},{old\_heap\_size,0},{heap\_size,81956774}]
&gt;&gt; 2013-08-05 18:01:15.878 [info] 
&gt;&gt; &lt;0.83.0&gt;@riak\_core\_sysmon\_handler:handle\_event:92 monitor large\_heap 
&gt;&gt; &lt;0.14832.557&gt; 
&gt;&gt; [{initial\_call,{riak\_kv\_get\_fsm,init,1}},{almost\_current\_function,{riak\_object,encode\_maybe\_binary,1}},{message\_queue\_len,1}]
&gt;&gt; 
&gt;&gt; [{old\_heap\_block\_size,0},{heap\_block\_size,116769640},{mbuf\_size,0},{stack\_size,52},{old\_heap\_size,0},{heap\_size,81956791}]
&gt;&gt; 2013-08-05 18:01:15.880 [info] 
&gt;&gt; &lt;0.83.0&gt;@riak\_core\_sysmon\_handler:handle\_event:92 monitor large\_heap 
&gt;&gt; &lt;0.12133.557&gt; 
&gt;&gt; [{initial\_call,{riak\_api\_pb\_server,init,1}},{almost\_current\_function,{lists,reverse,1}},{message\_queue\_len,0}]
&gt;&gt; 
&gt;&gt; [{old\_heap\_block\_size,0},{heap\_block\_size,74732575},{mbuf\_size,0},{stack\_size,71},{old\_heap\_size,0},{heap\_size,69315695}]
&gt;&gt; 
&gt;&gt; 
&gt;&gt; in erlang.log:
&gt;&gt; ----------------------
&gt;&gt; ===== ALIVE Mon Aug 5 17:56:26 UTC 2013
&gt;&gt; Erlang has closed
&gt;&gt; /home/fanzo/riak/rel/riak/bin/../lib/os\_mon-2.2.9/priv/bin/memsup: Erlang 
&gt;&gt; has closed.
&gt;&gt; 
&gt;&gt; ===== Mon Aug 5 18:13:22 UTC 2013
&gt;&gt; ^M
&gt;&gt; Crash dump was written to: ./log/erl\_crash.dump^M
&gt;&gt; eheap\_alloc: Cannot allocate 934157120 bytes of memory (of type "heap").^M
&gt;&gt; 
&gt;&gt; with this initial crash, nothing is written to crash.log or error.log. 
&gt;&gt; There is a big honking erl\_crash.dump file though...
&gt;&gt; 
&gt;&gt; \*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*
&gt;&gt; \* on the next node:
&gt;&gt; \*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*
&gt;&gt; last lines of console.log
&gt;&gt; ----------------------
&gt;&gt; 2013-08-05 17:49:02.789 [info] 
&gt;&gt; &lt;0.3014.550&gt;@riak\_kv\_exchange\_fsm:key\_exchange:206 Repaired 1 keys during 
&gt;&gt; active anti-entropy exchange of 
&gt;&gt; {91343852333181432387730302044767688728495783936,3} between 
&gt;&gt; {91343852333181432387730302044767688728495783936,riak@riak002} and 
&gt;&gt; {114179815416476790484662877555959610910619729920,riak@riak004}
&gt;&gt; 2013-08-05 18:00:19.830 [info] &lt;0.61.0&gt; alarm\_handler: 
&gt;&gt; {set,{system\_memory\_high\_watermark,[]}}
&gt;&gt; 2013-08-05 18:00:56.272 [info] 
&gt;&gt; &lt;0.85.0&gt;@riak\_core\_sysmon\_handler:handle\_event:92 monitor large\_heap 
&gt;&gt; &lt;0.7460.0&gt; 
&gt;&gt; [{initial\_call,{riak\_core\_vnode,init,1}},{almost\_current\_function,{eleveldb,get,3}},{message\_queue\_len,6}]
&gt;&gt; 
&gt;&gt; [{old\_heap\_block\_size,0},{heap\_block\_size,59786060},{mbuf\_size,0},{stack\_size,43},{old\_heap\_size,0},{heap\_size,40978885}]
&gt;&gt; 
&gt;&gt; in erlang.log:
&gt;&gt; ----------------------
&gt;&gt; ===== ALIVE Mon Aug 5 17:56:55 UTC 2013
&gt;&gt; /home/fanzo/riak/rel/riak/bin/../lib/os\_mon-2.2.9/priv/bin/memsup: Erlang 
&gt;&gt; has closed.Erlang has closed
&gt;&gt; 
&gt;&gt; 
&gt;&gt; ===== Mon Aug 5 18:13:33 UTC 2013
&gt;&gt; ^M
&gt;&gt; Crash dump was written to: ./log/erl\_crash.dump^M
&gt;&gt; eheap\_alloc: Cannot allocate 934157120 bytes of memory (of type "heap").^M
&gt;&gt; 
&gt;&gt; \*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*
&gt;&gt; \* on the next node:
&gt;&gt; \*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*
&gt;&gt; console.log looks similar to the first
&gt;&gt; 
&gt;&gt; nothing was logged in erlang.log and there was no crash dump. Looks like 
&gt;&gt; the beam proc just crashed hard.
&gt;&gt; 
&gt;&gt; 
&gt;&gt; \*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*
&gt;&gt; \* on the next node:
&gt;&gt; \*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*
&gt;&gt; last lines of console.log
&gt;&gt; ----------------------
&gt;&gt; console.log looks similar to the first
&gt;&gt; 
&gt;&gt; in erlang.log:
&gt;&gt; ----------------------
&gt;&gt; ===== Mon Aug 5 18:01:43 UTC 2013
&gt;&gt; Erlang has closed
&gt;&gt; /home/fanzo/riak/rel/riak/bin/../lib/os\_mon-2.2.9/priv/bin/memsup: Erlang 
&gt;&gt; has closed.
&gt;&gt; ^M
&gt;&gt; Crash dump was written to: ./log/erl\_crash.dump^M
&gt;&gt; eheap\_alloc: Cannot allocate 2280657000 bytes of memory (of type "heap").^M
&gt;&gt; 
&gt;&gt; 
&gt;&gt; If I try to start things back up, the nodes keep periodically crashing, 
&gt;&gt; usually within a few minutes. And I can't put any more data in without it 
&gt;&gt; blowing up...
&gt;&gt; 
&gt;&gt; Any help would be appreciated.
&gt;&gt; 
&gt;&gt; Paul Ingalls
&gt;&gt; Founder & CEO Fanzo
&gt;&gt; p...@fanzo.me
&gt;&gt; @paulingalls
&gt;&gt; http://www.linkedin.com/in/paulingalls
&gt;&gt; 
&gt;&gt; 
&gt;&gt; 
&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt; riak-users mailing list
&gt;&gt; riak-users@lists.basho.com
&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt; 

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

