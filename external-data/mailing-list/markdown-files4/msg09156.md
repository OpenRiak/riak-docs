---
title: "Re: avg write io wait time regression in 1.2.1"
description: ""
project: community
lastmod: 2012-11-02T06:19:51-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg09156"
mailinglist_parent_id: "msg09150"
author_name: "Matthew Von-Maszewski"
project_section: "mailinglistitem"
sent_date: 2012-11-02T06:19:51-07:00
---


Dietrich,

I can make two guesses into the increased disk writes. But I am also willing 
to review your actual LOG files to isolate root cause. If you could run the 
following and post the resulting file from one server, I will review it over 
the weekend or early next week:

sort /var/lib/riak/leveldb/\*/LOG &gt;LOG.all

The file will compress well. And no need to stop the server, just gather the 
LOG data live.

Guess 1: your data is in a transition phase. 1.1 used 2 Megabyte files 
exclusively. 1.2 is resizing the files to much larger sizes during a 
compaction. You could be seeing a larger number of files than usual 
participating in each compaction as the file sizes change. While this is 
possible, I have doubts … hence this is a guess.

Guess 2: I increased the various leveldb file sizes to reduce the number of 
open and closes, both for writes and random reads. This helped latencies in 
both the compactions and random reads. Any compaction in 1.2 is likely to 
reread and write larger total number of bytes. While this is possible, I again 
have doubts … the number of read operations should also go up if this guess is 
correct. Your read operations have not increased. This guess might still be 
valid if the read operations were satisfied by the Linux memory data cache. I 
do not how those would be counted or not counted.


Matthew


On Nov 1, 2012, at 10:01 PM, Dietrich Featherston wrote:

&gt; Will check on that.
&gt; 
&gt; Can you think of anything that would explain the 5x increase in disk writes 
&gt; we are seeing with the same workload?
&gt; 
&gt; 
&gt; On Thu, Nov 1, 2012 at 6:03 PM, Matthew Von-Maszewski  
&gt; wrote:
&gt; Look for any activity in the LOG. Level-0 "creations" are fast and not 
&gt; typically relevant. You would be most interested in LOG lines containing 
&gt; "Compacting" (start) and "Compacted" (end). The time in between will 
&gt; throttle. The idea is that these compaction events can pile up, one after 
&gt; another and multiple overlapping. It is these heavy times where the throttle 
&gt; saves the user experience.
&gt; 
&gt; Matthew
&gt; 
&gt; 
&gt; On Nov 1, 2012, at 8:54 PM, Dietrich Featherston wrote:
&gt; 
&gt;&gt; Thanks. The amortized stalls may very well describe what we are seeing. If I 
&gt;&gt; combine leveldb logs from all partitions on one of the upgraded nodes what 
&gt;&gt; should I look for in terms of compaction activity to verify this?
&gt;&gt; 
&gt;&gt; 
&gt;&gt; On Thu, Nov 1, 2012 at 5:48 PM, Matthew Von-Maszewski  
&gt;&gt; wrote:
&gt;&gt; Dietrich,
&gt;&gt; 
&gt;&gt; I can see your concern with the write IOS statistic. Let me comment on the 
&gt;&gt; easy question first: block\_size.
&gt;&gt; 
&gt;&gt; The block\_size parameter in 1.1 was not getting passed to leveldb from the 
&gt;&gt; erlang layer. You were using a 4096 byte block parameter no matter what you 
&gt;&gt; typed in the app.config. The block\_size is used by leveldb as a threshold. 
&gt;&gt; Once you accumulate enough data above that threshold, the current block is 
&gt;&gt; written to disk and a new one started. If you have 10k data values, your 
&gt;&gt; get one data item per block and its size is ~10k. If you have 1k data 
&gt;&gt; values, you get about four per block and the block is about 4k.
&gt;&gt; 
&gt;&gt; We recommend 4k blocks to help read performance. The entire block has to 
&gt;&gt; run through decompression and potentially CRC calculation when it comes off 
&gt;&gt; the disk. That CPU time really kills any disk performance gains by having 
&gt;&gt; larger blocks. Ok, that might change in 1.3 as we enable hardware CRC … but 
&gt;&gt; only if you have "verify\_checksums, true" in app.config.
&gt;&gt; 
&gt;&gt; 
&gt;&gt; Back to performance: I have not seen the change your graph details when 
&gt;&gt; testing with SAS drives under moderate load. I am only today starting 
&gt;&gt; qualification tests with SSD drives.
&gt;&gt; 
&gt;&gt; But my 1.2 and 1.3 tests focus on drive / Riak saturation. 1.1 has the 
&gt;&gt; nasty tendency to stall (intentionally) when we saturate the write side of 
&gt;&gt; leveldb, . The stall was measured in seconds or even minutes in 1.1. 1.2.1 
&gt;&gt; has a write throttle that forecasts leveldb's stall state and incrementally 
&gt;&gt; slows the individual writes to prevent the stalls. Maybe that is what is 
&gt;&gt; being seen in the graph. The only way to know for sure is to get an dump of 
&gt;&gt; your leveldb LOG files, combined them, then compare compaction activity to 
&gt;&gt; your graph.
&gt;&gt; 
&gt;&gt; Write stalls are detailed here: 
&gt;&gt; http://basho.com/blog/technical/2012/10/30/leveldb-in-riak-1p2/
&gt;&gt; 
&gt;&gt; How can I better assist you at this point?
&gt;&gt; 
&gt;&gt; Matthew
&gt;&gt; 
&gt;&gt; 
&gt;&gt; On Nov 1, 2012, at 8:13 PM, Dietrich Featherston wrote:
&gt;&gt; 
&gt;&gt;&gt; We've just gone through the process of upgrading two riak clusters from 1.1 
&gt;&gt;&gt; to 1.2.1. Both are on the leveldb backend backed by RAID0'd SSDs. The 
&gt;&gt;&gt; process has gone smoothly and we see that latencies as measured at the 
&gt;&gt;&gt; gen\_fsm level are largely unaffected.
&gt;&gt;&gt; 
&gt;&gt;&gt; However, we are seeing some troubling disk statistics and I'm looking for 
&gt;&gt;&gt; an explanation before we upgrade the remainder of our nodes. The source of 
&gt;&gt;&gt; the worry seems to be a huge amplification in the number of writes serviced 
&gt;&gt;&gt; by the disk which may be the cause of rising io wait times.
&gt;&gt;&gt; 
&gt;&gt;&gt; My first thought was that this could be due to some leveldb tuning in 1.2.1 
&gt;&gt;&gt; which increases file sizes per the release notes 
&gt;&gt;&gt; (https://github.com/basho/riak/blob/master/RELEASE-NOTES.md). But nodes 
&gt;&gt;&gt; that were upgraded yesterday are still showing this symptom. I would have 
&gt;&gt;&gt; expected any block re-writing to have subsided by now.
&gt;&gt;&gt; 
&gt;&gt;&gt; Next hypothesis has to do with block size overriding in app.config. In 1.1, 
&gt;&gt;&gt; we had specified custom block sizes of 256k. We removed this prior to 
&gt;&gt;&gt; upgrading to 1.2.1 at the advice of #riak since block size configuration 
&gt;&gt;&gt; was ignored prior to 1.2 ('"block\_size" parameter within app.config for 
&gt;&gt;&gt; leveldb was ignored. This parameter is now properly passed to leveldb.' 
&gt;&gt;&gt; --&gt; 
&gt;&gt;&gt; https://github.com/basho/riak/commit/f12596c221a9d942cc23d8e4fd83c9ca46e02105).
&gt;&gt;&gt; I'm wondering if the block size parameter really was being passed to 
&gt;&gt;&gt; leveldb, and having removed it, blocks are now being rewritten to a new 
&gt;&gt;&gt; size, perhaps different from what they were being written as before 
&gt;&gt;&gt; (https://github.com/basho/riak\_kv/commit/ad192ee775b2f5a68430d230c0999a2caabd1155)
&gt;&gt;&gt; 
&gt;&gt;&gt; Here is the output of the following script showing the increased writes to 
&gt;&gt;&gt; disk (https://gist.github.com/37319a8ed2679bb8b21d)
&gt;&gt;&gt; 
&gt;&gt;&gt; --an upgraded 1.2.1 node--
&gt;&gt;&gt; read ios: 238406742
&gt;&gt;&gt; write ios: 4814320281
&gt;&gt;&gt; read/write ratio: .04952033
&gt;&gt;&gt; avg wait: .10712340
&gt;&gt;&gt; read wait: .49174364
&gt;&gt;&gt; write wait: .42695475
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; --a node still running 1.1--
&gt;&gt;&gt; read ios: 267770032
&gt;&gt;&gt; write ios: 944170656
&gt;&gt;&gt; read/write ratio: .28360342
&gt;&gt;&gt; avg wait: .34237204
&gt;&gt;&gt; read wait: .47222371
&gt;&gt;&gt; write wait: 1.83283749
&gt;&gt;&gt; 
&gt;&gt;&gt; And here's what munin is showing us in terms of avg io wait times.
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; Any thoughts on what might explain this?
&gt;&gt;&gt; 
&gt;&gt;&gt; Thanks,
&gt;&gt;&gt; D
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;&gt; 
&gt;&gt; 
&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt; riak-users mailing list
&gt;&gt; riak-users@lists.basho.com
&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;&gt; 
&gt;&gt; 
&gt; 
&gt; 

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

