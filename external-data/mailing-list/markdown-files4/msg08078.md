---
title: "Re: How to store data"
description: ""
project: community
lastmod: 2012-07-26T01:23:47-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg08078"
mailinglist_parent_id: "msg08075"
author_name: "Andrew Kondratovich"
project_section: "mailinglistitem"
sent_date: 2012-07-26T01:23:47-07:00
---


1) Identifiers is not random. They are collected in groups, but number of
these groups is large and the same identifier can occur in different groups.
2) It's a fixed amount of time - it can be changed, but usually it's one
day. But request can be "get items from day before", not only for today.

On Thu, Jul 26, 2012 at 10:48 AM, Erik Søe Sørensen  wrote:

&gt; Never mind the number of requests (well, almost) - what you certainly want
&gt; to keep down is the number of disk seeks.
&gt;
&gt; To that end...:
&gt; 1) those 500 identifiers present in a request - are they totally
&gt; unrelated, or do they occur in some pattern - e.g. the same together always?
&gt; 2) the cut-off time - may it be anything, or is it something like a fixed
&gt; amount back in time?
&gt;
&gt;
&gt; ----- Reply message -----
&gt; Fra: "Andrew Kondratovich" 
&gt; Dato: ons., jul. 25, 2012 18:23
&gt; Emne: How to store data
&gt; Til: "Andres Jaan Tack" 
&gt; Cc: "riak-users@lists.basho.com" 
&gt;
&gt;
&gt; Yeap.. half a thousand requests to riak isn't cool =( I'm looking some
&gt; strategy of storing data so that i could fetch all items by 1 request.
&gt;
&gt; I could use index MR at time and filter results at map phase. I could use
&gt; special keys with from data and use key filters (with time filtering at map
&gt; phase)... I wish I could use several 2i at MR or combine 2i with
&gt; keyfilters, or perform MR on buckets... I wish... =)
&gt;
&gt; On Wed, Jul 25, 2012 at 5:35 PM, Andres Jaan Tack &lt;
&gt; andres.jaan.t...@eesti.ee&gt; wrote:
&gt; Is that a realistic strategy for low latency requirements? Imagine this
&gt; were some web service, and people generate this query at some reasonable
&gt; frequency.
&gt;
&gt; (not that I know what Andrew is looking for, exactly)
&gt;
&gt;
&gt; 2012/7/25 Yousuf Fauzan  yousuffau...@gmail.com&gt;&gt;
&gt; Since 500 is not that big a number, I think you can run that many M/Rs
&gt; with each emitting only records having "time" greater than specified. Input
&gt; would be {index, &lt;&lt;"bucket"&gt;&gt;, &lt;&lt;"from\_bin"&gt;&gt;, &lt;&lt;"from\_field\_value"&gt;&gt;}
&gt;
&gt; If you decide to split the data into separate buckets based on "from"
&gt; field, input would be {index, &lt;&lt;"from\_field\_value"&gt;&gt;, &lt;&lt;"time\_bin"&gt;&gt;,
&gt; &lt;&lt;"time\_low"&gt;&gt;, &lt;&lt;"time\_high"&gt;&gt;}
&gt;
&gt;
&gt; --
&gt; Yousuf
&gt;
&gt; On Wed, Jul 25, 2012 at 6:35 PM, Andrew Kondratovich &lt;
&gt; andrew.kondratov...@gmail.com&gt;
&gt; wrote:
&gt; Hello, Yousuf.
&gt;
&gt; Thanks for your reply.
&gt;
&gt; We have several millions of items. It's about 10 000 of unique 'from'
&gt; fields (about 1000 items for each). Usually, we need to get items for about
&gt; 500 'from' identifiers with 'time' limit (about 5% of items is
&gt; corresponding).
&gt;
&gt; On Wed, Jul 25, 2012 at 1:02 PM, Yousuf Fauzan  &gt; wrote:
&gt; Hi Andrew,
&gt;
&gt; First of all, the correct answer to your question is the proverbial "it
&gt; depends". Having said that, here is what I could do in your case
&gt;
&gt; 1. If there are enough data points with the same "from" field, I will make
&gt; it a bucket and then index on time.
&gt; 2. If the above is not true, I will index on "from" and "time" field.
&gt; a. If number of records where "time" is greater than the one your
&gt; require is small, I will run a map/reduce with the initial input as those
&gt; records.
&gt; b. If number of records having a particular "from" is small, I will do
&gt; the above with the initial input as records having that "from" field. This
&gt; could be a problem as Riak only supports range and exact queries so if you
&gt; want to query multiple identifiers, you will have to run multiple queries.
&gt; In both the above cases, I will use secondary indexes to get the
&gt; initial records.
&gt; Note that we are using M/R as Riak does not support querying by
&gt; multiple indexes.
&gt;
&gt; What I would also suggest is to partition your data into different
&gt; buckets. You will need to understand the queries that you will be
&gt; supporting and partition it accordingly.
&gt;
&gt; --
&gt; Yousuf
&gt;
&gt; On Wed, Jul 25, 2012 at 2:50 PM, Andrew Kondratovich &lt;
&gt; andrew.kondratov...@gmail.com&gt;
&gt; wrote:
&gt; Good afternoon.
&gt;
&gt; I am considering several storage solutions for my project, and now I look
&gt; at Riak.
&gt; We work with the following pattern of data:
&gt; {
&gt; time: unixtime
&gt; from: int
&gt; data: binary
&gt; ...
&gt; }
&gt;



-- 
Andrew Kondratovich
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

