---
title: "Re: Leveldb segfault during Riak startup"
description: ""
project: community
lastmod: 2015-12-18T06:47:46-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg16872"
mailinglist_parent_id: "msg16871"
author_name: "Luke Bakken"
project_section: "mailinglistitem"
sent_date: 2015-12-18T06:47:46-08:00
---


Hi Antti,

Riak is not tested on btrfs and the file system is not officially
supported. We recommend ext4 or xfs for Linux. ZFS is an option on
Solaris derivatives and FreeBSD.

--
Luke Bakken
Engineer
lbak...@basho.com


On Fri, Dec 18, 2015 at 6:14 AM, Antti Kuusela
 wrote:
&gt; Hi,
&gt;
&gt; I have been testing Riak and Riak CS as a possible solution for our future
&gt; storage needs. I have a five server cluster running Centos 7. Riak version
&gt; is 2.1.3 (first installed as 2.1.1, updated twice via Basho repo) and Riak
&gt; CS version is 2.1.0. The servers each have 64GB RAM and six 4TB disks in
&gt; raid 6 using btrfs.
&gt;
&gt; I have been pushing random data into Riak-CS via s3cmd to see how the system
&gt; behaves. Smallest objects have been 2000 bytes, largest 100MB. I have also
&gt; been making btrfs snapshots of the entire platform data dir nightly for
&gt; backup purposes. Stop Riak CS, wait 10 seconds, stop Riak, wait 10, make
&gt; snapshot, start Riak, wait 180 seconds, start Riak CS. This is performed on
&gt; each of the servers in turn with a five minute wait in between. I have added
&gt; the waits to try spread the startup load and allow the system time to get
&gt; things running. New data is constantly pushed to the S3 API but restarting
&gt; the nodes in rotation causes by far the highest stress on the system.
&gt;
&gt; I have encountered one problem in particular. Quite often one of the Riak
&gt; nodes starts up but after a couple of minutes it just drops, all processes
&gt; exited except for epmd.
&gt;
&gt; Following is from /var/log/riak/console, most of the lines skipped for sake
&gt; of brevity. Normal startup stuff, as far as I can see:
&gt;
&gt; 2015-12-16 00:26:04.446 [info] &lt;0.7.0&gt; Application lager started on node
&gt; 'riak@192.168.50.32'
&gt; ...
&gt; 2015-12-16 00:26:04.490 [info] &lt;0.72.0&gt; alarm\_handler:
&gt; {set,{system\_memory\_high\_watermark,[]}}
&gt; ...
&gt; 2015-12-16 00:26:04.781 [info]
&gt; &lt;0.206.0&gt;@riak\_core\_capability:process\_capability\_changes:555 New
&gt; capability: {riak\_core,vnode\_routing} = proxy
&gt; ...
&gt; 2015-12-16 00:26:04.869 [info] &lt;0.7.0&gt; Application riak\_core started on node
&gt; 'riak@192.168.50.32'
&gt; ...
&gt; 2015-12-16 00:26:04.969 [info] &lt;0.407.0&gt;@riak\_kv\_env:doc\_env:46 Environment
&gt; and OS variables:
&gt; 2015-12-16 00:26:05.124 [warning] &lt;0.6.0&gt; lager\_error\_logger\_h dropped 9
&gt; messages in the last second that exceeded the limit of 100 messages/sec
&gt; 2015-12-16 00:26:05.124 [info] &lt;0.407.0&gt; riak\_kv\_env: Open file limit: 65536
&gt; 2015-12-16 00:26:05.124 [warning] &lt;0.407.0&gt; riak\_kv\_env: Cores are disabled,
&gt; this may hinder debugging
&gt; 2015-12-16 00:26:05.124 [info] &lt;0.407.0&gt; riak\_kv\_env: Erlang process limit:
&gt; 262144
&gt; 2015-12-16 00:26:05.125 [info] &lt;0.407.0&gt; riak\_kv\_env: Erlang ports limit:
&gt; 65536
&gt; 2015-12-16 00:26:05.125 [info] &lt;0.407.0&gt; riak\_kv\_env: ETS table count limit:
&gt; 256000
&gt; 2015-12-16 00:26:05.125 [info] &lt;0.407.0&gt; riak\_kv\_env: Thread pool size: 64
&gt; 2015-12-16 00:26:05.125 [info] &lt;0.407.0&gt; riak\_kv\_env: Generations before
&gt; full sweep: 0
&gt; 2015-12-16 00:26:05.125 [info] &lt;0.407.0&gt; riak\_kv\_env: Schedulers: 12 for 12
&gt; cores
&gt; 2015-12-16 00:26:05.125 [info] &lt;0.407.0&gt; riak\_kv\_env: sysctl vm.swappiness
&gt; is 0 greater than or equal to 0)
&gt; 2015-12-16 00:26:05.125 [info] &lt;0.407.0&gt; riak\_kv\_env: sysctl
&gt; net.core.wmem\_default is 8388608 lesser than or equal to 8388608)
&gt; ...
&gt; 2015-12-16 00:26:05.139 [info] &lt;0.478.0&gt;@riak\_core:wait\_for\_service:504
&gt; Waiting for service riak\_kv to start (0 seconds)
&gt; 2015-12-16 00:26:05.158 [info]
&gt; &lt;0.495.0&gt;@riak\_kv\_entropy\_manager:set\_aae\_throttle\_limits:790 Setting AAE
&gt; throttle limits: [{-1,0},{200,10},{500,50},{750,250},{900,1000},{1100,5000}]
&gt; ...
&gt; 2015-12-16 00:26:30.160 [info]
&gt; &lt;0.495.0&gt;@riak\_kv\_entropy\_manager:perhaps\_log\_throttle\_change:853 Changing
&gt; AAE throttle from undefined -&gt; 5000 msec/key, based on maximum vnode mailbox
&gt; size {unknown\_mailbox\_sizes,node\_list,['riak@192.168.50.32']} from
&gt; ['riak@192.168.50.32']
&gt; 2015-12-16 00:27:12.053 [info] &lt;0.478.0&gt;@riak\_core:wait\_for\_service:504
&gt; Waiting for service riak\_kv to start (60 seconds)
&gt; 2015-12-16 00:28:25.057 [info] &lt;0.478.0&gt;@riak\_core:wait\_for\_service:504
&gt; Waiting for service riak\_kv to start (120 seconds)
&gt;
&gt; And then nothing
&gt;
&gt; From /var/log/messages:
&gt;
&gt; Dec 16 00:26:02 storage2 su: (to riak) root on none
&gt; Dec 16 00:26:04 storage2 riak[48174]: Starting up
&gt; Dec 16 00:28:59 storage2 kernel: traps: beam.smp[48492] general protection
&gt; ip:7fcaf9402f16 sp:7fca6affcdd0 error:0 in eleveldb.so[7fcaf93b5000+93000]
&gt; Dec 16 00:28:59 storage2 run\_erl[48172]: Erlang closed the connection.
&gt;
&gt; On another node at a different time /var/log/riak/console.log had similar
&gt; messages, and also some warnings about invalid hint files, such as:
&gt;
&gt; 2015-12-13 00:15:41.232 [warning] &lt;0.815.0&gt; Hintfile
&gt; '/data/riak/bitcask/570899077082383952423314387779798054553098649600/56.bitcask.hint'
&gt; invalid
&gt;
&gt; In this latter example riak was started with "systemctl start riak" rather
&gt; than "riak start". From /var/log/messages:
&gt;
&gt; Dec 13 00:15:29 storage1 riak: Starting riak: [ OK ]
&gt; Dec 13 00:15:29 storage1 systemd: Started SYSV: Riak is a distributed data
&gt; store.
&gt; Dec 13 00:15:56 storage1 kernel: beam.smp[131820]: segfault at 160 ip
&gt; 00007f24c0902ce6 sp 00007f24337fddd0 error 4 in
&gt; eleveldb.so[7f24c08b5000+93000]
&gt; Dec 13 00:15:56 storage1 run\_erl[131501]: Erlang closed the connection.
&gt;
&gt; Of reported Riak bugs, this is similar to
&gt; https://github.com/basho/riak/issues/790 . However, the poster of that issue
&gt; reported that his problem was fixed by repairing leveldb partitions. I
&gt; looked at this following
&gt; http://docs.basho.com/riak/latest/ops/running/recovery/repairing-leveldb/
&gt; but didn't find any errors.
&gt;
&gt; Incidentally, I started having problems with btrfs as well. On one node
&gt; btrfs caused a kernel crash and on another kernel killed beam.smp process
&gt; after it stopped responding for over 120 seconds while syncing to btrfs. The
&gt; kernel in Centos 7 probably isn't best suited for working with btrfs.
&gt; Advertised version is 3.10.0.
&gt;
&gt; So, my question is what is your take on this? Is this a bug in the leveldb
&gt; library? The same one already reported? What log data would help debug or
&gt; reproduce it? Or is there potentially some problem with my setup? Or could
&gt; this be caused by a bug in btrfs? What is your take on using Riak with
&gt; btrfs?
&gt;
&gt; --
&gt; Antti Kuusela, M.Sc
&gt; Senior Software Developer
&gt; Firstbeat Technologies Ltd.
&gt;
&gt;
&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

