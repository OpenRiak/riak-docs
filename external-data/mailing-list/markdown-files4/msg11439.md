---
title: "Re: Migration from memcachedb to riak"
description: ""
project: community
lastmod: 2013-07-10T02:39:25-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg11439"
mailinglist_parent_id: "msg11438"
author_name: "Edgar Veiga"
project_section: "mailinglistitem"
sent_date: 2013-07-10T02:39:25-07:00
---


Guido, we'r not using Java and that won't be an option.

The technology stack is php and/or node.js

Thanks anyway :)

Best regards


On 10 July 2013 10:35, Edgar Veiga  wrote:

&gt; Hi Damien,
&gt;
&gt; We have ~1100000000 keys and we are using ~2TB of disk space.
&gt; (The average object length will be ~2000 bytes).
&gt;
&gt; This is a lot to fit in memory (We have bad past experiencies with
&gt; couchDB...).
&gt;
&gt; Thanks for the rest of the tips!
&gt;
&gt;
&gt; On 10 July 2013 10:13, damien krotkine  wrote:
&gt;
&gt;&gt;
&gt;&gt; ( first post here, hi everybody... )
&gt;&gt;
&gt;&gt; If you don't need MR, 2i, etc, then BitCask will be faster. You just need
&gt;&gt; to make sure all your keys fit in memory, which should not be a problem.
&gt;&gt; How many keys do you have and what's their average length ?
&gt;&gt;
&gt;&gt; About the values,you can save a lot of space by choosing an appropriate
&gt;&gt; serialization. We use Sereal[1] to serialize our data, and it's small
&gt;&gt; enough that we don't need to compress it further (it can automatically use
&gt;&gt; snappy to compress further). There is a php client [2]
&gt;&gt;
&gt;&gt; If you use leveldb, it can compress using snappy, but I've been a bit
&gt;&gt; disappointed by snappy, because it didn't work well with our data. If you
&gt;&gt; serialize your php object as verbose string (I don't know what's the usual
&gt;&gt; way to serialize php objects), then you should probably benchmark different
&gt;&gt; compressions algorithms on the application side.
&gt;&gt;
&gt;&gt;
&gt;&gt; [1]: https://github.com/Sereal/Sereal/wiki/Sereal-Comparison-Graphs
&gt;&gt; [2]: https://github.com/tobyink/php-sereal/tree/master/PHP
&gt;&gt;
&gt;&gt; On 10 July 2013 10:49, Edgar Veiga  wrote:
&gt;&gt;
&gt;&gt;&gt; Hello all!
&gt;&gt;&gt;
&gt;&gt;&gt; I have a couple of questions that I would like to address all of you
&gt;&gt;&gt; guys, in order to start this migration the best as possible.
&gt;&gt;&gt;
&gt;&gt;&gt; Context:
&gt;&gt;&gt; - I'm responsible for the migration of a pure key/value store that for
&gt;&gt;&gt; now is being stored on memcacheDB.
&gt;&gt;&gt; - We're serializing php objects and storing them.
&gt;&gt;&gt; - The total size occupied it's ~2TB.
&gt;&gt;&gt;
&gt;&gt;&gt; - The idea it's to migrate this data to a riak cluster with elevelDB
&gt;&gt;&gt; backend (starting with 6 nodes, 256 partitions. This thing is scaling very
&gt;&gt;&gt; fast).
&gt;&gt;&gt; - We only need to access the information by key. \*We won't need neither
&gt;&gt;&gt; map/reduces, searches or secondary indexes\*. It's a pure key/value
&gt;&gt;&gt; store!
&gt;&gt;&gt;
&gt;&gt;&gt; My questions are:
&gt;&gt;&gt; - Do you have any riak fine tunning tip regarding this use case (due to
&gt;&gt;&gt; the fact that we will only use the key/value capabilities of riak)?
&gt;&gt;&gt; - It's expected that those 2TB would be reduced due to the levelDB
&gt;&gt;&gt; compression. Do you think we should compress our objects to on the client?
&gt;&gt;&gt;
&gt;&gt;&gt; Best regards,
&gt;&gt;&gt; Edgar Veiga
&gt;&gt;&gt;
&gt;&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;
&gt;
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

