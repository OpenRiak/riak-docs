---
title: "Re: Riak Recap for Dec. 13 - 14"
description: ""
project: community
lastmod: 2010-12-16T20:24:17-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg01813"
mailinglist_parent_id: "msg01811"
author_name: "Dan Reverri"
project_section: "mailinglistitem"
sent_date: 2010-12-16T20:24:17-08:00
---


Yes.

Daniel Reverri
Developer Advocate
Basho Technologies, Inc.
d...@basho.com


On Thu, Dec 16, 2010 at 7:56 PM, Daniel Woo  wrote:

&gt; Hi Daniel,
&gt;
&gt; So, the hashing algorithm is still consistent, but partitions (vnodes) are
&gt; re-distributed to nodes when new nodes are added, and the nodes gossip and
&gt; share the knowledge of the partition distribution, right? So the client can
&gt; query any of the nodes, if the node doesn't know where the partition located
&gt; it will query or gossip with adjacent nodes, right?
&gt;
&gt; Thanks,
&gt; Daniel
&gt;
&gt;
&gt;
&gt; On Fri, Dec 17, 2010 at 10:42 AM, Dan Reverri  wrote:
&gt;
&gt;&gt; Hi Daniel,
&gt;&gt;
&gt;&gt; Clients do not specify the partition when making a request. A client can
&gt;&gt; request any key from any node in the cluster and Riak will return the
&gt;&gt; associated value.
&gt;&gt;
&gt;&gt; Thanks,
&gt;&gt; Dan
&gt;&gt;
&gt;&gt; Daniel Reverri
&gt;&gt; Developer Advocate
&gt;&gt; Basho Technologies, Inc.
&gt;&gt; d...@basho.com
&gt;&gt;
&gt;&gt;
&gt;&gt; On Thu, Dec 16, 2010 at 6:01 PM, Daniel Woo wrote:
&gt;&gt;
&gt;&gt;&gt; Hi Mark,
&gt;&gt;&gt;
&gt;&gt;&gt; Thanks for your explanation, so in this case the partitions would be
&gt;&gt;&gt; re-distributed
&gt;&gt;&gt;
&gt;&gt;&gt; \*from\*
&gt;&gt;&gt;
&gt;&gt;&gt; Node1: p1 ~ p16
&gt;&gt;&gt; Node2: p17 ~ p32
&gt;&gt;&gt; Node3: p33 ~ p48
&gt;&gt;&gt; Node4: p49 ~ p64
&gt;&gt;&gt;
&gt;&gt;&gt; \*to \*
&gt;&gt;&gt; Node1: p1 ~ p13 (remove 3 partitions)
&gt;&gt;&gt; Node2: p17 ~ p29 (remove 3 partitions)
&gt;&gt;&gt; Node3: p33 ~ p45 (remove 3 partitions)
&gt;&gt;&gt; Node4: p49 ~ p61 (remove 3 partitions)
&gt;&gt;&gt; Node5: p14, 15, 16, 30, 31, 32, 46, 47, 48, 62, 63, 64 (aprox 1/5
&gt;&gt;&gt; partitions will be transferred to this new node)
&gt;&gt;&gt;
&gt;&gt;&gt; Since there is no centralized node in Riak, how do we know the partition
&gt;&gt;&gt; 32 is moved to node 5 from the client caller? Cassandra seems break half of
&gt;&gt;&gt; the adjacent node's data into the new node, that will be easy for the client
&gt;&gt;&gt; to search for the datum just around the node-circle clockwise, although it
&gt;&gt;&gt; causes unbalanced data distribution and you have to move them by command
&gt;&gt;&gt; lines. Riak seems to have this solved by moving partitions into new nodes
&gt;&gt;&gt; equally, that's very interesting, how do you guys make it? If the client
&gt;&gt;&gt; caller queries for partition 32 which was originally on node 2, how do the
&gt;&gt;&gt; client know it's on a new node now?
&gt;&gt;&gt;
&gt;&gt;&gt; Thanks,
&gt;&gt;&gt; Daniel
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; On Fri, Dec 17, 2010 at 6:56 AM, Mark Phillips  wrote:
&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Hey Daniel,
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; [snip]
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; &gt; So, I guess Riak would have to re-hash the whole partitions into all
&gt;&gt;&gt;&gt; the 5
&gt;&gt;&gt;&gt; &gt; nodes, right? Is this done lazily when the node finds the requested
&gt;&gt;&gt;&gt; data is
&gt;&gt;&gt;&gt; &gt; missing?
&gt;&gt;&gt;&gt; &gt; Or is there a way to handle this with consistent re-hashing so we can
&gt;&gt;&gt;&gt; avoid
&gt;&gt;&gt;&gt; &gt; moving data around when new nodes added?
&gt;&gt;&gt;&gt; &gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Riak won't rehash all the partitions in the ring when new nodes are
&gt;&gt;&gt;&gt; added. When you go from 4 -&gt; 5 nodes, for example, approx. 1/5 of the
&gt;&gt;&gt;&gt; existing partitions are transferred to the new node. The other 4/5s of
&gt;&gt;&gt;&gt; the partitions will remain unchanged. As far as moving data around
&gt;&gt;&gt;&gt; when new nodes are added, this is impossible to avoid. Data needs to
&gt;&gt;&gt;&gt; be handed off to be spread around the ring.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Hope that helps.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Mark
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; --
&gt;&gt;&gt; Thanks & Regards,
&gt;&gt;&gt; Daniel
&gt;&gt;&gt;
&gt;&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;
&gt;
&gt;
&gt; --
&gt; Thanks & Regards,
&gt; Daniel
&gt;
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

