---
title: "Re: How to cold (re)boot a cluster with already existing node data"
description: ""
project: community
lastmod: 2016-06-06T13:06:00-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg17459"
mailinglist_parent_id: "msg17458"
author_name: "Sargun Dhillon"
project_section: "mailinglistitem"
sent_date: 2016-06-06T13:06:00-07:00
---


Two suggestions:
1. Use Riak-EE, and have two rings. When you do an update, copy over one
ring to the other side after you do a "cold reboot"
2. Use the Riak Mesos Framework. Mesos is like K8s, but it has stateful
storage primitives. (Link: https://github.com/basho-labs/riak-mesos)

On Mon, Jun 6, 2016 at 10:37 AM, Jan-Philip Loos  wrote:

&gt;
&gt;
&gt; On Mon, 6 Jun 2016 at 16:52 Alex Moore  wrote:
&gt;
&gt;&gt; Hi Jan,
&gt;&gt;
&gt;&gt; When you update the Kubernates nodes, do you have to do them all at once
&gt;&gt; or can they be done in a rolling fashion (one after another)?
&gt;&gt;
&gt;
&gt; Thnaks for your reply,
&gt;
&gt; sadly this is not possible. Kubernetes with GKE just tears all nodes down,
&gt; creating new nodes with new kubernets version and reschedule all services
&gt; on these nodes. So after an upgrade, all riak nodes are stand-alone (when
&gt; starting after deleting /var/lib/riak/ring)
&gt;
&gt; Greetings
&gt;
&gt; Jan
&gt;
&gt;
&gt;&gt; If you can do them rolling-wise, you should be able to:
&gt;&gt;
&gt;&gt; For each node, one at a time:
&gt;&gt; 1. Shut down Riak
&gt;&gt; 2. Shutdown/restart/upgrade Kubernates
&gt;&gt; 3. Start Riak
&gt;&gt; 4. Use `riak-admin force-replace` to rename the old node name to the new
&gt;&gt; node name
&gt;&gt; 5. Repeat on remaining nodes.
&gt;&gt;
&gt;&gt; This is covered in "Renaming Multi-node clusters
&gt;&gt; "
&gt;&gt; doc.
&gt;&gt;
&gt;&gt; As for your current predicament, have you created any new
&gt;&gt; buckets/changed bucket props in the default namespace since you restarted?
&gt;&gt; Or have you only done regular operations since?
&gt;&gt;
&gt;&gt; Thanks,
&gt;&gt; Alex
&gt;&gt;
&gt;&gt;
&gt;&gt; On Mon, Jun 6, 2016 at 5:25 AM Jan-Philip Loos 
&gt;&gt; wrote:
&gt;&gt;
&gt;&gt;&gt; Hi,
&gt;&gt;&gt;
&gt;&gt;&gt; we are using riak in a kuberentes cluster (on GKE). Sometimes it's
&gt;&gt;&gt; necessary to reboot the complete cluster to update the kubernetes-nodes.
&gt;&gt;&gt; This results in a complete shutdown of the riak cluster and the riak-nodes
&gt;&gt;&gt; are rescheduled with a new IP. So how can I handle this situation? How can
&gt;&gt;&gt; I form a new riak cluster out of the old nodes with new names?
&gt;&gt;&gt;
&gt;&gt;&gt; The /var/lib/riak directory is persisted. I had to delete the
&gt;&gt;&gt; /var/lib/riak/ring folder otherwise "riak start" crashed with this message
&gt;&gt;&gt; (but saved the old ring state in a tar):
&gt;&gt;&gt;
&gt;&gt;&gt; {"Kernel pid
&gt;&gt;&gt;&gt; terminated",application\_controller,"{application\_start\_failure,riak\_core,{{shutdown,{failed\_to\_start\_child,riak\_core\_broadcast,{'EXIT',{function\_clause,[{orddict,fetch,['
&gt;&gt;&gt;&gt; riak@10.44.2.8
&gt;&gt;&gt;&gt; ',[]],[{file,\"orddict.erl\"},{line,72}]},{riak\_core\_broadcast,init\_peers,1,[{file,\"src/riak\_core\_broadcast.erl\"},{line,616}]},{riak\_core\_broadcast,start\_link,0,[{file,\"src/riak\_core\_broadcast.erl\"},{line,116}]},{supervisor,do\_start\_child,2,[{file,\"supervisor.erl\"},{line,310}]},{supervisor,start\_children,3,[{file,\"supervisor.erl\"},{line,293}]},{supervisor,init\_children,2,[{file,\"supervisor.erl\"},{line,259}]},{gen\_server,init\_it,6,[{file,\"gen\_server.erl\"},{line,304}]},{proc\_lib,init\_p\_do\_apply,3,[{file,\"proc\_lib.erl\"},{line,239}]}]}}}},{riak\_core\_app,start,[normal,[]]}}}"}
&gt;&gt;&gt;&gt; Crash dump was written to: /var/log/riak/erl\_crash.dump
&gt;&gt;&gt;&gt; Kernel pid terminated (application\_controller)
&gt;&gt;&gt;&gt; ({application\_start\_failure,riak\_core,{{shutdown,{failed\_to\_start\_child,riak\_core\_broadcast,{'EXIT',{function\_clause,[{orddict,fetch,['
&gt;&gt;&gt;&gt; riak@10.44.2.8',
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; The I formed a new cluster via join & plan & commit.
&gt;&gt;&gt;
&gt;&gt;&gt; But now, I discovered a problems with incomplete and inconsistent
&gt;&gt;&gt; partitions:
&gt;&gt;&gt;
&gt;&gt;&gt; \*$ \*curl -Ss "
&gt;&gt;&gt; http://riak.default.svc.cluster.local:8098/buckets/users/keys?keys=true"
&gt;&gt;&gt; | jq '.[] | length'
&gt;&gt;&gt;
&gt;&gt;&gt; 3064
&gt;&gt;&gt;
&gt;&gt;&gt; \*$\* curl -Ss "
&gt;&gt;&gt; http://riak.default.svc.cluster.local:8098/buckets/users/keys?keys=true"
&gt;&gt;&gt; | jq '.[] | length'
&gt;&gt;&gt;
&gt;&gt;&gt; 2987
&gt;&gt;&gt;
&gt;&gt;&gt; \*$\* curl -Ss "
&gt;&gt;&gt; http://riak.default.svc.cluster.local:8098/buckets/users/keys?keys=true"
&gt;&gt;&gt; | jq '.[] | length'
&gt;&gt;&gt;
&gt;&gt;&gt; 705
&gt;&gt;&gt;
&gt;&gt;&gt; \*$\* curl -Ss "
&gt;&gt;&gt; http://riak.default.svc.cluster.local:8098/buckets/users/keys?keys=true"
&gt;&gt;&gt; | jq '.[] | length'
&gt;&gt;&gt; 3064
&gt;&gt;&gt;
&gt;&gt;&gt; Is there a way to fix this? I guess this is caused by the missing old
&gt;&gt;&gt; ring-state?
&gt;&gt;&gt;
&gt;&gt;&gt; Greetings
&gt;&gt;&gt;
&gt;&gt;&gt; Jan
&gt;&gt;&gt;
&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;&gt;&gt;
&gt;&gt;
&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;
&gt;
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

