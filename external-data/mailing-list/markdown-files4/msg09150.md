---
title: "Re: avg write io wait time regression in 1.2.1"
description: ""
project: community
lastmod: 2012-11-01T19:01:50-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg09150"
mailinglist_parent_id: "msg09149"
author_name: "Dietrich Featherston"
project_section: "mailinglistitem"
sent_date: 2012-11-01T19:01:50-07:00
---


Will check on that.

Can you think of anything that would explain the 5x increase in disk writes
we are seeing with the same workload?


On Thu, Nov 1, 2012 at 6:03 PM, Matthew Von-Maszewski wrote:

&gt; Look for any activity in the LOG. Level-0 "creations" are fast and not
&gt; typically relevant. You would be most interested in LOG lines containing
&gt; "Compacting" (start) and "Compacted" (end). The time in between will
&gt; throttle. The idea is that these compaction events can pile up, one after
&gt; another and multiple overlapping. It is these heavy times where the
&gt; throttle saves the user experience.
&gt;
&gt; Matthew
&gt;
&gt;
&gt; On Nov 1, 2012, at 8:54 PM, Dietrich Featherston wrote:
&gt;
&gt; Thanks. The amortized stalls may very well describe what we are seeing. If
&gt; I combine leveldb logs from all partitions on one of the upgraded nodes
&gt; what should I look for in terms of compaction activity to verify this?
&gt;
&gt;
&gt; On Thu, Nov 1, 2012 at 5:48 PM, Matthew Von-Maszewski 
&gt; wrote:
&gt;
&gt;&gt; Dietrich,
&gt;&gt;
&gt;&gt; I can see your concern with the write IOS statistic. Let me comment on
&gt;&gt; the easy question first: block\_size.
&gt;&gt;
&gt;&gt; The block\_size parameter in 1.1 was not getting passed to leveldb from
&gt;&gt; the erlang layer. You were using a 4096 byte block parameter no matter
&gt;&gt; what you typed in the app.config. The block\_size is used by leveldb as a
&gt;&gt; threshold. Once you accumulate enough data above that threshold, the
&gt;&gt; current block is written to disk and a new one started. If you have 10k
&gt;&gt; data values, your get one data item per block and its size is ~10k. If you
&gt;&gt; have 1k data values, you get about four per block and the block is about 4k.
&gt;&gt;
&gt;&gt; We recommend 4k blocks to help read performance. The entire block has to
&gt;&gt; run through decompression and potentially CRC calculation when it comes off
&gt;&gt; the disk. That CPU time really kills any disk performance gains by having
&gt;&gt; larger blocks. Ok, that might change in 1.3 as we enable hardware CRC â€¦
&gt;&gt; but only if you have "verify\_checksums, true" in app.config.
&gt;&gt;
&gt;&gt;
&gt;&gt; Back to performance: I have not seen the change your graph details when
&gt;&gt; testing with SAS drives under moderate load. I am only today starting
&gt;&gt; qualification tests with SSD drives.
&gt;&gt;
&gt;&gt; But my 1.2 and 1.3 tests focus on drive / Riak saturation. 1.1 has the
&gt;&gt; nasty tendency to stall (intentionally) when we saturate the write side of
&gt;&gt; leveldb, . The stall was measured in seconds or even minutes in 1.1.
&gt;&gt; 1.2.1 has a write throttle that forecasts leveldb's stall state and
&gt;&gt; incrementally slows the individual writes to prevent the stalls. Maybe
&gt;&gt; that is what is being seen in the graph. The only way to know for sure is
&gt;&gt; to get an dump of your leveldb LOG files, combined them, then compare
&gt;&gt; compaction activity to your graph.
&gt;&gt;
&gt;&gt; Write stalls are detailed here:
&gt;&gt; http://basho.com/blog/technical/2012/10/30/leveldb-in-riak-1p2/
&gt;&gt;
&gt;&gt; How can I better assist you at this point?
&gt;&gt;
&gt;&gt; Matthew
&gt;&gt;
&gt;&gt;
&gt;&gt; On Nov 1, 2012, at 8:13 PM, Dietrich Featherston wrote:
&gt;&gt;
&gt;&gt; We've just gone through the process of upgrading two riak clusters from
&gt;&gt; 1.1 to 1.2.1. Both are on the leveldb backend backed by RAID0'd SSDs. The
&gt;&gt; process has gone smoothly and we see that latencies as measured at the
&gt;&gt; gen\_fsm level are largely unaffected.
&gt;&gt;
&gt;&gt; However, we are seeing some troubling disk statistics and I'm looking for
&gt;&gt; an explanation before we upgrade the remainder of our nodes. The source of
&gt;&gt; the worry seems to be a huge amplification in the number of writes serviced
&gt;&gt; by the disk which may be the cause of rising io wait times.
&gt;&gt;
&gt;&gt; My first thought was that this could be due to some leveldb tuning in
&gt;&gt; 1.2.1 which increases file sizes per the release notes (
&gt;&gt; https://github.com/basho/riak/blob/master/RELEASE-NOTES.md). But nodes
&gt;&gt; that were upgraded yesterday are still showing this symptom. I would have
&gt;&gt; expected any block re-writing to have subsided by now.
&gt;&gt;
&gt;&gt; Next hypothesis has to do with block size overriding in app.config. In
&gt;&gt; 1.1, we had specified custom block sizes of 256k. We removed this prior to
&gt;&gt; upgrading to 1.2.1 at the advice of #riak since block size configuration
&gt;&gt; was ignored prior to 1.2 ('"block\_size" parameter within app.config for
&gt;&gt; leveldb was ignored. This parameter is now properly passed to leveldb.'
&gt;&gt; --&gt;
&gt;&gt; https://github.com/basho/riak/commit/f12596c221a9d942cc23d8e4fd83c9ca46e02105).
&gt;&gt; I'm wondering if the block size parameter really was being passed to
&gt;&gt; leveldb, and having removed it, blocks are now being rewritten to a new
&gt;&gt; size, perhaps different from what they were being written as before (
&gt;&gt; https://github.com/basho/riak\_kv/commit/ad192ee775b2f5a68430d230c0999a2caabd1155
&gt;&gt; )
&gt;&gt;
&gt;&gt; Here is the output of the following script showing the increased writes
&gt;&gt; to disk (https://gist.github.com/37319a8ed2679bb8b21d)
&gt;&gt;
&gt;&gt; --an upgraded 1.2.1 node--
&gt;&gt; read ios: 238406742
&gt;&gt; write ios: 4814320281
&gt;&gt; read/write ratio: .04952033
&gt;&gt; avg wait: .10712340
&gt;&gt; read wait: .49174364
&gt;&gt; write wait: .42695475
&gt;&gt;
&gt;&gt;
&gt;&gt; --a node still running 1.1--
&gt;&gt; read ios: 267770032
&gt;&gt; write ios: 944170656
&gt;&gt; read/write ratio: .28360342
&gt;&gt; avg wait: .34237204
&gt;&gt; read wait: .47222371
&gt;&gt; write wait: 1.83283749
&gt;&gt;
&gt;&gt; And here's what munin is showing us in terms of avg io wait times.
&gt;&gt;
&gt;&gt; 
&gt;&gt;
&gt;&gt;
&gt;&gt; Any thoughts on what might explain this?
&gt;&gt;
&gt;&gt; Thanks,
&gt;&gt; D
&gt;&gt;
&gt;&gt;
&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt; riak-users mailing list
&gt;&gt; riak-users@lists.basho.com
&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt; riak-users mailing list
&gt;&gt; riak-users@lists.basho.com
&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;&gt;
&gt;&gt;
&gt;
&gt;
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

