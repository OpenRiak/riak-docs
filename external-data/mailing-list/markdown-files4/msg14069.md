---
title: "Re: oddness when using java client within storm"
description: ""
project: community
lastmod: 2014-04-14T08:42:05-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg14069"
mailinglist_parent_id: "msg14068"
author_name: "Alexander Sicular"
project_section: "mailinglistitem"
sent_date: 2014-04-14T08:42:05-07:00
---


I'm not sure what "looking up entries... in batches of 100 from Riak" devolves 
into in the java client but riak doesn't have a native multiget. It either does 
100 get ops or a [search&gt;]mapreduce. That might inform some of your performance 
issues. 

-Alexander 

@siculars
http://siculars.posthaven.com

Sent from my iRotaryPhone

&gt; On Apr 14, 2014, at 8:26, Sean Allen  wrote:
&gt; 
&gt; I'm seeing something very odd trying to scale out part of code I'm working on.
&gt; 
&gt; It runs inside of Storm and lookups up entries from 10 node riak cluster.
&gt; I've hit a wall that we can't get past. We are looking up entries (json 
&gt; representation of a job)
&gt; in batches of 100 from Riak, each batch gets handled by a bolt in Storm, 
&gt; adding more
&gt; bolts (an instance of the bolt class with a dedicated thread) results in no 
&gt; increase
&gt; in performance. I instrumted the code and saw that waiting for all riak 
&gt; futures to finish
&gt; increases as more bolts are added. Thinking that perhaps there was contention 
&gt; around the
&gt; RiakCluster object that we were sharing per jvm, I tried giving each bolt 
&gt; instance its own
&gt; cluster object and there wasn't any change.
&gt; 
&gt; Note that changing Thread spool size given to withExecutor not 
&gt; withExecutionAttempts value
&gt; has any impact.
&gt; 
&gt; We're working off of the develop branch for the java client. We've been using 
&gt; d3cc30d but I also tried with cef7570 and had the same issue.
&gt; 
&gt; A simplied version of the scala code running this:
&gt; 
&gt; // called once upon bolt initialization.
&gt; def prepare(config: JMap[\_, \_],
&gt; context: TopologyContext,
&gt; collector: OutputCollector): Unit = {
&gt; ...
&gt; 
&gt; val nodes = RiakNode.Builder.buildNodes(new RiakNode.Builder, (1 to 
&gt; 10).map(n =&gt; s"riak-beavis-$n").toList.asJava)
&gt; riak = new RiakCluster.Builder(nodes)
&gt; // varying this has made no difference
&gt; .withExecutionAttempts(1)
&gt; // nor has varying this
&gt; .withExecutor(new ScheduledThreadPoolExecutor(200))
&gt; .build()
&gt; riak.start
&gt; 
&gt; ...
&gt; }
&gt; 
&gt; private def get(jobLocationId: String): RiakFuture[FetchOperation.Response] 
&gt; = {
&gt; val location = new 
&gt; Location("jobseeker-job-view").setBucketType("no-siblings").setKey(jobLocationId)
&gt; val fop = new 
&gt; FetchOperation.Builder(location).withTimeout(75).withR(1).build
&gt; 
&gt; riak.execute(fop)
&gt; }
&gt; 
&gt; def execute(tuple: Tuple): Unit = {
&gt; val indexType = tuple.getStringByField("index\_type")
&gt; val indexName = tuple.getStringByField("index\_name")
&gt; val batch = tuple.getValueByField("batch").asInstanceOf[Set[Payload]]
&gt; 
&gt; var lookups: Set[(Payload, RiakFuture[FetchOperation.Response])] = 
&gt; Set.empty
&gt; 
&gt; // this always returns in a standard time based on batch size
&gt; time("dispatch-calls") {
&gt; lookups = batch.filter(\_.key.isDefined).map {
&gt; payload =&gt; {(payload, get(payload.key.get))}
&gt; }
&gt; }
&gt; 
&gt; val futures = lookups.map(\_.\_2)
&gt; 
&gt; // this is what takes longer and longer when more bolts are added.
&gt; // it doesnt matter what the sleep time is.
&gt; time("waiting-on-futures") {
&gt; while (futures.count(!\_.isDone) &gt; 0) {
&gt; Thread.sleep(25L)
&gt; }
&gt; }
&gt; 
&gt; 
&gt; // everything from here to the end returns in a fixed amount of time
&gt; // and doesn't change with the number of bolts
&gt; ...
&gt; 
&gt; }
&gt; 
&gt; 
&gt; It seems like we are running into contention somewhere in the riak java 
&gt; client.
&gt; My first thought was the LinkedBlockingQueue that serves as the retry queue 
&gt; in RiakCluster
&gt; but, I've tried running with only a single execution attempt as well as a 
&gt; custom client
&gt; version where I removed all retries from the codebase and still experience 
&gt; the same problem.
&gt; 
&gt; I'm still digging through the code looking for possible points of contention.
&gt; 
&gt; Any thoughts?
&gt; 
&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

