---
title: "Re: missing writes / inconsistent number of keys read after a bulk	write"
description: ""
project: community
lastmod: 2012-01-09T14:39:48-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg06189"
author_name: "Karthik K"
project_section: "mailinglistitem"
sent_date: 2012-01-09T14:39:48-08:00
---


(Changing subject to reflect the problem better and reposting ).

Any idea , based on the configuration inline as to what explains the
inconsistency number of keys read after a bulk write ( say 1M of payload
1000 bytes ).

Any appropriate write flush setting , that is missed on the client / server
side ?


--
 Karthik.


On Fri, Jan 6, 2012 at 5:54 PM, Karthik K  wrote:

&gt; Further,
&gt; ulimit -n is 10K on the box.
&gt;
&gt;
&gt; # tail -100 /var//log/riak/erlang.log.1
&gt;
&gt; ....
&gt; 17:26:26.960 [info] alarm\_handler: {set,{system\_memory\_high\_watermark,[]}}
&gt; /usr/lib/riak/lib/os\_mon-2.2.6/priv/bin/memsup: Erlang has closed.
&gt; Erlang has closed
&gt; 17:26:34.443 [info] alarm\_handler: {clear,system\_memory\_high\_watermark}
&gt;
&gt;
&gt; So - is there a commit / flush setting that is missed when it comes to
&gt; high volume writes ?
&gt;
&gt;
&gt; On Fri, Jan 6, 2012 at 5:37 PM, Karthik K  wrote:
&gt;
&gt;&gt; I am using Riak with LevelDB as the storage engine.
&gt;&gt;
&gt;&gt; app.config:
&gt;&gt;
&gt;&gt; {storage\_backend, riak\_kv\_eleveldb\_backend},
&gt;&gt;
&gt;&gt;
&gt;&gt; {eleveldb, [
&gt;&gt; {data\_root, "/var/lib/riak/leveldb"},
&gt;&gt; {write\_buffer\_size, 4194304}, %% 4MB in bytes
&gt;&gt; {max\_open\_files, 50}, %% Maximum number of files open at once
&gt;&gt; per partition
&gt;&gt; {block\_size, 65536}, %% 4K blocks
&gt;&gt; {cache\_size, 33554432}, %% 32 MB default cache size
&gt;&gt; per-partition
&gt;&gt; {verify\_checksums, true} %% make sure data is what we
&gt;&gt; expected it to be
&gt;&gt; ]},
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt; I want to insert a million keys into the store ( into a given bucket ) .
&gt;&gt;
&gt;&gt; pseudo-code:
&gt;&gt; riakClient = RiakFactory.pbcClient();
&gt;&gt; myBucket =
&gt;&gt; riakClient.createBucket("myBucket").nVal(1).execute();
&gt;&gt; for (int i = 1; i &lt;= 1000000; ++i) {
&gt;&gt; final String key = String.valueOf(i);
&gt;&gt; myBucket.store(key, new
&gt;&gt; String(payload)).returnBody(false);
&gt;&gt; }
&gt;&gt;
&gt;&gt;
&gt;&gt; after this operation, when I do:
&gt;&gt;
&gt;&gt; int count = 0;
&gt;&gt; for (String key : myBucket.keys() ) {
&gt;&gt; ++count;
&gt;&gt; }
&gt;&gt; return count;
&gt;&gt;
&gt;&gt; This returns a total of 14K keys, while I was expecting close to 1
&gt;&gt; million or so.
&gt;&gt;
&gt;&gt; I am using riak-java-client (pbc).
&gt;&gt;
&gt;&gt; Which setting / missing client code can explain the discrepancy ?
&gt;&gt; Thanks.
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

