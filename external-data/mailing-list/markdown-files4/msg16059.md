---
title: "Re: Yokozuna queries slow"
description: ""
project: community
lastmod: 2015-04-21T19:37:53-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg16059"
mailinglist_parent_id: "msg16047"
author_name: "Jason Campbell"
project_section: "mailinglistitem"
sent_date: 2015-04-21T19:37:53-07:00
---


Sorry, still running into issues.

So I've disabled cache on all solr nodes, but the yokozuna query times are 
still high.

Oddly, running queries against any one node completes in a few ms. So I 
injected nginx between riak and solr so I could log queries, and now I'm even 
more confused.

There is 5 requests to each solr node for every request to yokozuna.

These are the nginx logs for one Solr node. The log format is apache combined, 
plus the total query time, and a dump of the POST body.

10.0.1.238 - - [22/Apr/2015:02:18:01 +0000] "POST 
/internal\_solr/snapshots\_index/select HTTP/1.1" 200 774 "-" 
"Solr[org.apache.solr.client.solrj.impl.HttpSolrServer] 1.0" 0.007 
"rows=0&10.0.1.237%3A8093=\_yz\_pn%3A54+OR+\_yz\_pn%3A44+OR+\_yz\_pn%3A34+OR+\_yz\_pn%3A24+OR+\_yz\_pn%3A14+OR+\_yz\_pn%3A4&q=\*%3A\*&10.0.1.234%3A8093=%28\_yz\_pn%3A51+AND+%28\_yz\_fpn%3A51%29%29+OR+%28\_yz\_pn%3A41+AND+%28\_yz\_fpn%3A41%29%29+OR+%28\_yz\_pn%3A31+AND+%28\_yz\_fpn%3A31%29%29+OR+%28\_yz\_pn%3A21+AND+%28\_yz\_fpn%3A21%29%29+OR+%28\_yz\_pn%3A11+AND+%28\_yz\_fpn%3A11%29%29+OR+\_yz\_pn%3A1&10.0.1.235%3A8093=%28\_yz\_pn%3A62+AND+%28\_yz\_fpn%3A62+OR+\_yz\_fpn%3A61%29%29+OR+\_yz\_pn%3A57+OR+\_yz\_pn%3A47+OR+\_yz\_pn%3A37+OR+\_yz\_pn%3A27+OR+\_yz\_pn%3A17+OR+\_yz\_pn%3A7&10.0.1.238%3A8093=\_yz\_pn%3A60+OR+\_yz\_pn%3A50+OR+\_yz\_pn%3A40+OR+\_yz\_pn%3A30+OR+\_yz\_pn%3A20+OR+\_yz\_pn%3A10&start=0&fsv=true&fl=\_yz\_id%2Cscore&distrib=false&isShard=true&shard.url=10.0.1.235%3A8093%2Finternal\_solr%2Fsnapshots\_index&NOW=1429669081777&wt=javabin&version=2"
10.0.1.238 - - [22/Apr/2015:02:18:01 +0000] "POST 
/internal\_solr/snapshots\_index/select HTTP/1.1" 200 774 "-" 
"Solr[org.apache.solr.client.solrj.impl.HttpSolrServer] 1.0" 0.008 
"rows=0&10.0.1.237%3A8093=\_yz\_pn%3A54+OR+\_yz\_pn%3A44+OR+\_yz\_pn%3A34+OR+\_yz\_pn%3A24+OR+\_yz\_pn%3A14+OR+\_yz\_pn%3A4&q=\*%3A\*&10.0.1.234%3A8093=%28\_yz\_pn%3A51+AND+%28\_yz\_fpn%3A51%29%29+OR+%28\_yz\_pn%3A41+AND+%28\_yz\_fpn%3A41%29%29+OR+%28\_yz\_pn%3A31+AND+%28\_yz\_fpn%3A31%29%29+OR+%28\_yz\_pn%3A21+AND+%28\_yz\_fpn%3A21%29%29+OR+%28\_yz\_pn%3A11+AND+%28\_yz\_fpn%3A11%29%29+OR+\_yz\_pn%3A1&10.0.1.235%3A8093=%28\_yz\_pn%3A62+AND+%28\_yz\_fpn%3A62+OR+\_yz\_fpn%3A61%29%29+OR+\_yz\_pn%3A57+OR+\_yz\_pn%3A47+OR+\_yz\_pn%3A37+OR+\_yz\_pn%3A27+OR+\_yz\_pn%3A17+OR+\_yz\_pn%3A7&10.0.1.238%3A8093=\_yz\_pn%3A60+OR+\_yz\_pn%3A50+OR+\_yz\_pn%3A40+OR+\_yz\_pn%3A30+OR+\_yz\_pn%3A20+OR+\_yz\_pn%3A10&start=0&fsv=true&fl=\_yz\_id%2Cscore&distrib=false&isShard=true&shard.url=10.0.1.234%3A8093%2Finternal\_solr%2Fsnapshots\_index&NOW=1429669081777&wt=javabin&version=2"
10.0.1.238 - - [22/Apr/2015:02:18:01 +0000] "POST 
/internal\_solr/snapshots\_index/select HTTP/1.1" 200 774 "-" 
"Solr[org.apache.solr.client.solrj.impl.HttpSolrServer] 1.0" 0.001 
"rows=0&10.0.1.237%3A8093=\_yz\_pn%3A54+OR+\_yz\_pn%3A44+OR+\_yz\_pn%3A34+OR+\_yz\_pn%3A24+OR+\_yz\_pn%3A14+OR+\_yz\_pn%3A4&q=\*%3A\*&10.0.1.234%3A8093=%28\_yz\_pn%3A51+AND+%28\_yz\_fpn%3A51%29%29+OR+%28\_yz\_pn%3A41+AND+%28\_yz\_fpn%3A41%29%29+OR+%28\_yz\_pn%3A31+AND+%28\_yz\_fpn%3A31%29%29+OR+%28\_yz\_pn%3A21+AND+%28\_yz\_fpn%3A21%29%29+OR+%28\_yz\_pn%3A11+AND+%28\_yz\_fpn%3A11%29%29+OR+\_yz\_pn%3A1&10.0.1.235%3A8093=%28\_yz\_pn%3A62+AND+%28\_yz\_fpn%3A62+OR+\_yz\_fpn%3A61%29%29+OR+\_yz\_pn%3A57+OR+\_yz\_pn%3A47+OR+\_yz\_pn%3A37+OR+\_yz\_pn%3A27+OR+\_yz\_pn%3A17+OR+\_yz\_pn%3A7&10.0.1.238%3A8093=\_yz\_pn%3A60+OR+\_yz\_pn%3A50+OR+\_yz\_pn%3A40+OR+\_yz\_pn%3A30+OR+\_yz\_pn%3A20+OR+\_yz\_pn%3A10&start=0&fsv=true&fl=\_yz\_id%2Cscore&distrib=false&isShard=true&shard.url=10.0.1.237%3A8093%2Finternal\_solr%2Fsnapshots\_index&NOW=1429669081777&wt=javabin&version=2"

These queries look fine, they are from localhost, but over the internal IP 
address, and they all complete in under 10ms. The return the exact same body 
length, so I'm confused why it's run 3 times, but not much of a performance 
concern.

10.0.1.238 - - [22/Apr/2015:02:18:03 +0000] "POST 
/internal\_solr/snapshots\_index/select HTTP/1.1" 200 778 "-" 
"Solr[org.apache.solr.client.solrj.impl.HttpSolrServer] 1.0" 1.844 
"rows=0&10.0.1.237%3A8093=\_yz\_pn%3A54+OR+\_yz\_pn%3A44+OR+\_yz\_pn%3A34+OR+\_yz\_pn%3A24+OR+\_yz\_pn%3A14+OR+\_yz\_pn%3A4&q=\*%3A\*&10.0.1.234%3A8093=%28\_yz\_pn%3A51+AND+%28\_yz\_fpn%3A51%29%29+OR+%28\_yz\_pn%3A41+AND+%28\_yz\_fpn%3A41%29%29+OR+%28\_yz\_pn%3A31+AND+%28\_yz\_fpn%3A31%29%29+OR+%28\_yz\_pn%3A21+AND+%28\_yz\_fpn%3A21%29%29+OR+%28\_yz\_pn%3A11+AND+%28\_yz\_fpn%3A11%29%29+OR+\_yz\_pn%3A1&10.0.1.235%3A8093=%28\_yz\_pn%3A62+AND+%28\_yz\_fpn%3A62+OR+\_yz\_fpn%3A61%29%29+OR+\_yz\_pn%3A57+OR+\_yz\_pn%3A47+OR+\_yz\_pn%3A37+OR+\_yz\_pn%3A27+OR+\_yz\_pn%3A17+OR+\_yz\_pn%3A7&10.0.1.238%3A8093=\_yz\_pn%3A60+OR+\_yz\_pn%3A50+OR+\_yz\_pn%3A40+OR+\_yz\_pn%3A30+OR+\_yz\_pn%3A20+OR+\_yz\_pn%3A10&start=0&fsv=true&fl=\_yz\_id%2Cscore&distrib=false&isShard=true&shard.url=10.0.1.238%3A8093%2Finternal\_solr%2Fsnapshots\_index&NOW=1429669081777&wt=javabin&version=2"

This is where I start getting worried, the query is now changed to \*:\* instead 
of the one I passed to yokozuna, and rows=0. Not surprisingly, this takes 
almost 2 seconds since it has to count a massive amount of documents.

127.0.0.1 - - [22/Apr/2015:02:18:03 +0000] "POST 
/internal\_solr/snapshots\_index/select HTTP/1.1" 200 882 "-" "-" 1.856 
"q=%2A%3A%2A&rows=0&wt=json&shards=10.0.1.234%3A8093%2Finternal\_solr%2Fsnapshots\_index%2C10.0.1.235%3A8093%2Finternal\_solr%2Fsnapshots\_index%2C10.0.1.237%3A8093%2Finternal\_solr%2Fsnapshots\_index%2C10.0.1.238%3A8093%2Finternal\_solr%2Fsnapshots\_index&10.0.1.234%3A8093=%28\_yz\_pn%3A51+AND+%28\_yz\_fpn%3A51%29%29+OR+%28\_yz\_pn%3A41+AND+%28\_yz\_fpn%3A41%29%29+OR+%28\_yz\_pn%3A31+AND+%28\_yz\_fpn%3A31%29%29+OR+%28\_yz\_pn%3A21+AND+%28\_yz\_fpn%3A21%29%29+OR+%28\_yz\_pn%3A11+AND+%28\_yz\_fpn%3A11%29%29+OR+\_yz\_pn%3A1&10.0.1.235%3A8093=%28\_yz\_pn%3A62+AND+%28\_yz\_fpn%3A62+OR+\_yz\_fpn%3A61%29%29+OR+\_yz\_pn%3A57+OR+\_yz\_pn%3A47+OR+\_yz\_pn%3A37+OR+\_yz\_pn%3A27+OR+\_yz\_pn%3A17+OR+\_yz\_pn%3A7&10.0.1.237%3A8093=\_yz\_pn%3A54+OR+\_yz\_pn%3A44+OR+\_yz\_pn%3A34+OR+\_yz\_pn%3A24+OR+\_yz\_pn%3A14+OR+\_yz\_pn%3A4&10.0.1.238%3A8093=\_yz\_pn%3A60+OR+\_yz\_pn%3A50+OR+\_yz\_pn%3A40+OR+\_yz\_pn%3A30+OR+\_yz\_pn%3A20+OR+\_yz\_pn%3A10"

This is a similar query as the previous one, but this one is from localhost 
(not the private IP) and the output format is json, not javabin. It's also 
missing the usual Solr user agent. This is still a \*:\* query requesting 0 rows 
though, and also takes almost 2 seconds.

Now I'm not sure these last 2 queries are actually required to complete the 
yokozuna request, but they are run every time I make a new request. Although 
each of these queries takes over 1.8 seconds, the yokozuna request completed in 
1.2 seconds. However, if it was only waiting on the shorter requests, even 
1.2s seconds is much too long.

Can anyone give insight into what yokozuna is doing here and hopefully a way to 
make it as fast as the underlying solr queries (&gt;10ms)?

Still confused,
Jason

&gt; On 22 Apr 2015, at 08:27, Jason Campbell  wrote:
&gt; 
&gt; That is really weird, I tried that before posting, yet it seems to fix things 
&gt; now.
&gt; 
&gt; Oh well, I'll try rolling it out across the cluster, hopefully it works.
&gt; 
&gt; Maybe someone with more Solr knowledge can enlighten me though, does 
&gt; disabling the filter cache change behaviour as well?
&gt; 
&gt; For example, running q=\_yz\_pn:55 OR \_yz\_pn:40 OR \_yz\_pn:25 OR \_yz\_pn:10 still 
&gt; takes ~1500ms, yet q=timestamp:[1429579919010 TO 1429579921010]&qf=\_yz\_pn:55 
&gt; OR \_yz\_pn:40 OR \_yz\_pn:25 OR \_yz\_pn:10 takes 0-5ms.
&gt; 
&gt; So clearly the query filter isn't being executed in the same way it was 
&gt; before, it's not just that it doesn't have to write the result to a cache.
&gt; 
&gt; Thanks for the help guys.
&gt; 
&gt;&gt; On 22 Apr 2015, at 08:02, Zeeshan Lakhani  wrote:
&gt;&gt; 
&gt;&gt; Hey Steve,
&gt;&gt; 
&gt;&gt; Yep, that’s the specific reference point I was referring to in the link I 
&gt;&gt; shared. 
&gt;&gt; 
&gt;&gt; We’re working on cleaning-up some stuff to help perf for large indexes 
&gt;&gt; within the yz code itself. We’ve also discussed how to improve configuring 
&gt;&gt; specific solrconfig tunable items as a property of the index (not bucket); 
&gt;&gt; so, it’s something we’re looking into.
&gt;&gt; 
&gt;&gt; Thanks for helping out :).
&gt;&gt; 
&gt;&gt; Zeeshan Lakhani
&gt;&gt; programmer | 
&gt;&gt; software engineer at @basho | 
&gt;&gt; org. member/founder of @papers\_we\_love | paperswelove.org
&gt;&gt; twitter =&gt; @zeeshanlakhani
&gt;&gt; 
&gt;&gt;&gt; On Apr 21, 2015, at 5:41 PM, Steve Garon  wrote:
&gt;&gt;&gt; 
&gt;&gt;&gt; Zeeshan, 
&gt;&gt;&gt; 
&gt;&gt;&gt; For that specific case, you guys should add {!cache=false} in front on your 
&gt;&gt;&gt; query plan. Therefore, queries on large index won't be slowed down. I'd 
&gt;&gt;&gt; really like to see some of the solrconfig.xml config to be exported to the 
&gt;&gt;&gt; riak bucket properties. The caching flag could be a property on the bucket. 
&gt;&gt;&gt; Same for soft commit timeouts. We had to increase soft commit timeouts to 
&gt;&gt;&gt; 10sec instead of the 1sec default. 
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; Steve
&gt;&gt;&gt; 
&gt;&gt;&gt; On 21 April 2015 at 16:02, Zeeshan Lakhani  wrote:
&gt;&gt;&gt; Nice Steve.
&gt;&gt;&gt; 
&gt;&gt;&gt; Zeeshan Lakhani
&gt;&gt;&gt; programmer | 
&gt;&gt;&gt; software engineer at @basho | 
&gt;&gt;&gt; org. member/founder of @papers\_we\_love | paperswelove.org
&gt;&gt;&gt; twitter =&gt; @zeeshanlakhani
&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; On Apr 21, 2015, at 3:57 PM, Steve Garon  wrote:
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Jason, 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Comment the  section in the bucket's solrconfig.xml and 
&gt;&gt;&gt;&gt; restart riak. Now your queries will be fast again :-)
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Steve
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; On 21 April 2015 at 04:24, Zeeshan Lakhani  wrote:
&gt;&gt;&gt;&gt; No real workaround other than what you described or looking into 
&gt;&gt;&gt;&gt; config/fq-no-cache settings as mentioned in 
&gt;&gt;&gt;&gt; http://lucidworks.com/blog/advanced-filter-caching-in-solr/ and playing 
&gt;&gt;&gt;&gt; around with those.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Riak is now at 2.1.0. I hope that one of the next few point releases will 
&gt;&gt;&gt;&gt; see the fix. 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Zeeshan Lakhani
&gt;&gt;&gt;&gt; programmer | 
&gt;&gt;&gt;&gt; software engineer at @basho | 
&gt;&gt;&gt;&gt; org. member/founder of @papers\_we\_love | paperswelove.org
&gt;&gt;&gt;&gt; twitter =&gt; @zeeshanlakhani
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; On Apr 21, 2015, at 4:11 AM, Jason Campbell  wrote:
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; Thanks Zeeshan for the info.
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; Is there a workaround in the mean time, or is the only option to handle 
&gt;&gt;&gt;&gt;&gt; queries to the individual nodes ourselves?
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; Is there a planned timeframe for the 2.0.1 release?
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; Thanks,
&gt;&gt;&gt;&gt;&gt; Jason
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; On 21 Apr 2015, at 16:13, Zeeshan Lakhani  wrote:
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; Hey Jason,
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; We’re working on performance issues with YZ filter queries, e.g. 
&gt;&gt;&gt;&gt;&gt;&gt; https://github.com/basho/yokozuna/issues/392, and coverage plan 
&gt;&gt;&gt;&gt;&gt;&gt; generation/caching, and our CliServ team has started doing a ton of 
&gt;&gt;&gt;&gt;&gt;&gt; benchmarks as well.
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; You can bypass YZ, but then you’d have to create a way to generate your 
&gt;&gt;&gt;&gt;&gt;&gt; own coverage plans and other things involving distributed solr that YZ 
&gt;&gt;&gt;&gt;&gt;&gt; gives you. Nonetheless, we’re actively working on improving these issues 
&gt;&gt;&gt;&gt;&gt;&gt; you’ve encountered. 
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; Zeeshan Lakhani
&gt;&gt;&gt;&gt;&gt;&gt; programmer | 
&gt;&gt;&gt;&gt;&gt;&gt; software engineer at @basho | 
&gt;&gt;&gt;&gt;&gt;&gt; org. member/founder of @papers\_we\_love | paperswelove.org
&gt;&gt;&gt;&gt;&gt;&gt; twitter =&gt; @zeeshanlakhani
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; On Apr 21, 2015, at 1:06 AM, Jason Campbell  wrote:
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Hello,
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; I'm currently trying to debug slow YZ queries, and I've narrowed down 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; the issue, but not sure how to solve it.
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; First off, we have about 80 million records in Riak (and YZ), but the 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; queries return relatively few (a thousand or so at most). Our query 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; times are anywhere from 800ms to 1.5s.
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; I have been experimenting with queries directly on the Solr node, and 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; it seems to be a problem with YZ and the way it does vnode filters.
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Here is the same query, emulating YZ first:
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; {
&gt;&gt;&gt;&gt;&gt;&gt;&gt; "responseHeader":{
&gt;&gt;&gt;&gt;&gt;&gt;&gt; "status":0,
&gt;&gt;&gt;&gt;&gt;&gt;&gt; "QTime":958,
&gt;&gt;&gt;&gt;&gt;&gt;&gt; "params":{
&gt;&gt;&gt;&gt;&gt;&gt;&gt; "q":"timestamp:[1429579919010 TO 1429579921010]",
&gt;&gt;&gt;&gt;&gt;&gt;&gt; "indent":"true",
&gt;&gt;&gt;&gt;&gt;&gt;&gt; "fq":"\_yz\_pn:55 OR \_yz\_pn:40 OR \_yz\_pn:25 OR \_yz\_pn:10",
&gt;&gt;&gt;&gt;&gt;&gt;&gt; "rows":"0",
&gt;&gt;&gt;&gt;&gt;&gt;&gt; "wt":"json"}},
&gt;&gt;&gt;&gt;&gt;&gt;&gt; "response":{"numFound":80,"start":0,"docs":[]
&gt;&gt;&gt;&gt;&gt;&gt;&gt; }}
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; And the same query, but including the vnode filter in the main body 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; instead of using a filter query:
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; {
&gt;&gt;&gt;&gt;&gt;&gt;&gt; "responseHeader":{
&gt;&gt;&gt;&gt;&gt;&gt;&gt; "status":0,
&gt;&gt;&gt;&gt;&gt;&gt;&gt; "QTime":1,
&gt;&gt;&gt;&gt;&gt;&gt;&gt; "params":{
&gt;&gt;&gt;&gt;&gt;&gt;&gt; "q":"timestamp:[1429579919010 TO 1429579921010] AND (\_yz\_pn:55 OR 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; \_yz\_pn:40 OR \_yz\_pn:25 OR \_yz\_pn:10)",
&gt;&gt;&gt;&gt;&gt;&gt;&gt; "indent":"true",
&gt;&gt;&gt;&gt;&gt;&gt;&gt; "rows":"0",
&gt;&gt;&gt;&gt;&gt;&gt;&gt; "wt":"json"}},
&gt;&gt;&gt;&gt;&gt;&gt;&gt; "response":{"numFound":80,"start":0,"docs":[]
&gt;&gt;&gt;&gt;&gt;&gt;&gt; }}
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; I understand there is a caching benefit to using filter queries, but a 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; performance difference of 100x or greater doesn't seem worth it, 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; especially with a constant data stream.
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Is there a way to make YZ do this, or is the only way to query Solr 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; directly, bypassing YZ? Does anyone have any other suggestions of how 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; to make this faster?
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; The timestamp field is a SolrTrieLongField with default settings if 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; anyone is curious.
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Thanks,
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Jason
&gt;&gt;&gt;&gt;&gt;&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt;&gt;&gt;&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt;&gt;&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt;&gt;&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt;&gt;&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt;&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt;&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt; 
&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt; riak-users mailing list
&gt;&gt; riak-users@lists.basho.com
&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt; 
&gt; 
&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com


\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

