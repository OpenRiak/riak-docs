---
title: "simulating physical node crash"
description: ""
project: community
lastmod: 2011-09-26T09:17:36-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg04871"
author_name: "francisco treacy"
project_section: "mailinglistitem"
sent_date: 2011-09-26T09:17:36-07:00
---


Hi all,

I have a 3-node Riak cluster, and I am simulating the scenario of physical
nodes crashing.

When 2 nodes go down, and I query the remaining one, it fails with:

{error,
 {exit,
 {{{error,
 {no\_candidate\_nodes,exhausted\_prefist,
 [{riak\_kv\_mapred\_planner,claim\_keys,3},
 {riak\_kv\_map\_phase,schedule\_input,5},
 {riak\_kv\_map\_phase,handle\_input,3},
 {luke\_phase,executing,3},
 {gen\_fsm,handle\_msg,7},
 {proc\_lib,init\_p\_do\_apply,3}],
 []}},
 {gen\_fsm,sync\_send\_event,
 [&lt;0.31566.2330&gt;,
 {inputs,

(...)

Here I'm doing a M/R, inputs being fed by Search.

(1) All of the involved buckets have N=3, and all involved requests R=1 (I
don't really need quorum for this usecase)

Why is it failing? I'm sure i'm missing something basic here

(2) Probably worth noting, those 3 nodes are spread across \*two\* physical
servers (1 on small one, 2 on beefier one). I've heard it is "not a good
idea", not sure why though. These two servers are definitely enough still
for our current load; should I consider adding a third one?

(3) To overcome the aforementioned error, I added a new node to the cluster
(installed on the small server). Now the setup looks like: 4 nodes = 2 on
small server, 2 on beefier one.

When 2 nodes go down, this works. Which brings me to another topic... could
you point me to good strategies to "pre-" invoke read-repair? Is it up to
clients to scan the keyspace forcing reads? It's a disaster usability-wise
when first users start getting 404s all over the place.

Francisco
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

