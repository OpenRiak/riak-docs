---
title: "Re: Large ring_creation_size"
description: ""
project: community
lastmod: 2011-04-14T13:48:04-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg03050"
mailinglist_parent_id: "msg03024"
author_name: "Dave Barnes"
project_section: "mailinglistitem"
sent_date: 2011-04-14T13:48:04-07:00
---


Greg,

What is the size of the HW or VM you plan to deploy as 1000 nodes (memory
and disk space)?
I'm very interested in the trade-off between hardware and software...

Dave

On Thu, Apr 14, 2011 at 2:00 PM, Jon Meredith  wrote:

&gt; Hi Greg,
&gt;
&gt; I played with this a little last night and this morning and I can reproduce
&gt; the behavior you are seeing - my two nodes ate more than a combined 15gig of
&gt; memory with 16384 partitions and were promptly killed by the O/S.
&gt;
&gt; I haven't had a chance to analyze yet, so this is pure speculation. I
&gt; suspect that implementations of functions for calculating partition owners,
&gt; preference lists and which partitions to take when nodes join perform
&gt; acceptably for the &lt;= 1024 partition case but blow up in some way beyond
&gt; that. I was hoping that 4096 would work for you, but it sounds like that has
&gt; problems too.
&gt;
&gt; I'm finishing up some high priority work at the moment but will investigate
&gt; as time permits.
&gt;
&gt; Jon
&gt;
&gt; On Thu, Apr 14, 2011 at 11:48 AM, Greg Nelson  wrote:
&gt;
&gt;&gt; We have a exact idea of the amount of data we'll be storing, and the
&gt;&gt; kinds of machines we'll be storing them on. The simple math of (total data
&gt;&gt; we'll be storing 6 months from now) / (total capacity of a single node) \*
&gt;&gt; (number of duplicates of each datum we'd like to store for redundancy) gives
&gt;&gt; us a number a concrete number of machines we'll need, whether we're using
&gt;&gt; Riak or something else...
&gt;&gt;
&gt;&gt; However, that's a little off track from the question I'm trying to answer.
&gt;&gt; Which is:
&gt;&gt;
&gt;&gt; Why is it when I start two nodes with a large-ish ring\_creation\_size -- as
&gt;&gt; soon as I join the second node to the first, CPU and memory usage of both
&gt;&gt; nodes goes through the roof? No data stored yet. This even happens with a
&gt;&gt; ring\_creation\_size I wouldn't consider huge, like 4096.
&gt;&gt;
&gt;&gt; So that is the small configuration I'm starting with and that is the limit
&gt;&gt; I'm hitting.
&gt;&gt;
&gt;&gt; I realize that the feasibility of building out a single 1000 node cluster
&gt;&gt; is a larger question. But I can tell you we'll get there; having to shard
&gt;&gt; across 10 100-node clusters is an option. Regardless, I'd like to have an
&gt;&gt; understanding of what the resource overhead of each vnode is...
&gt;&gt;
&gt;&gt; On Thursday, April 14, 2011 at 6:38 AM, Sean Cribbs wrote:
&gt;&gt;
&gt;&gt; Good points, Dave.
&gt;&gt;
&gt;&gt; Also, it's worth mentioning that we've seen that many customers and
&gt;&gt; open-source users think they will need many more nodes than they actually
&gt;&gt; do. Many are able to start with 5 nodes and are happy for quite a while.
&gt;&gt; The only way to tell what you actually need is to start with a baseline
&gt;&gt; configuration and simulate some percentage above your current load. Once
&gt;&gt; you've figured out what size that initial cluster is, start with (number of
&gt;&gt; nodes) \* 50 as the ring\_creation\_size (rounded to the nearest power of 2 of
&gt;&gt; course). This gives you a growth factor of about 5 before you need to
&gt;&gt; consider changing.
&gt;&gt;
&gt;&gt; As well, there's some ops "common sense" that says the lifetime of any
&gt;&gt; single architecture is 18 months or less. That doesn't necessarily mean
&gt;&gt; that you'll be building a new cluster with a larger ring size in 18 months,
&gt;&gt; but just that your needs will be different at that time and are hard to
&gt;&gt; predict. Plan for now, worry about the 1000 node cluster when you actually
&gt;&gt; need it.
&gt;&gt;
&gt;&gt; Sean Cribbs 
&gt;&gt; Developer Advocate
&gt;&gt; Basho Technologies, Inc.
&gt;&gt; http://basho.com/
&gt;&gt;
&gt;&gt; On Apr 14, 2011, at 9:09 AM, Dave Barnes wrote:
&gt;&gt;
&gt;&gt; Sorry I feel compelled to chime in.
&gt;&gt;
&gt;&gt; Maybe you could assess your physical node limits and start with a small
&gt;&gt; configuration, then increase it and increase it until you hit a limit.
&gt;&gt;
&gt;&gt; Work small to large.
&gt;&gt;
&gt;&gt; Once you find the pain point, lets us know what resource ran out.
&gt;&gt;
&gt;&gt; You will learn a lot along the way on how your servers behave and we'll
&gt;&gt; discover a lot when you share the results.
&gt;&gt;
&gt;&gt; Thanks for digging in,
&gt;&gt;
&gt;&gt; Dave
&gt;&gt;
&gt;&gt; On Wed, Apr 13, 2011 at 5:11 PM, Greg Nelson  wrote:
&gt;&gt;
&gt;&gt; Ok, how about in this case I described? It runs out of memory with a
&gt;&gt; single pair of nodes...
&gt;&gt;
&gt;&gt; (Or did you mean there's a connection between each pair of vnodes?)
&gt;&gt;
&gt;&gt; On Wednesday, April 13, 2011 at 1:56 PM, Jon Meredith wrote:
&gt;&gt;
&gt;&gt; Hi Greg et al,
&gt;&gt;
&gt;&gt; As you say largest known is not largest possible. Internally within
&gt;&gt; Basho, the largest cluster we've experimented with so far had 50 nodes.
&gt;&gt;
&gt;&gt; Going beyond that it's speculation from me about pain points.
&gt;&gt;
&gt;&gt; 1) It is true that you need enough file descriptors to start up all
&gt;&gt; partitions when a node restarts - Riak checks if there is any handoff data
&gt;&gt; pending for each partition. We have work scheduled to address that in the
&gt;&gt; medium term. The plan is to only spin up partitions the node owns and any
&gt;&gt; that have been started as fallbacks that handoff has not completed for.
&gt;&gt; Until that work is done you will need a high ulimit with large ring sizes.
&gt;&gt;
&gt;&gt; 2) It is also true that Erlang runs a fully connected network, so there
&gt;&gt; will be connections between each node pair in the cluster. We haven't
&gt;&gt; determined the point at which it becomes a problem.
&gt;&gt;
&gt;&gt; So it looks like you'll be pushing the known limits. Basho will do our
&gt;&gt; very best to help overcome any obstacles as you encounter them.
&gt;&gt;
&gt;&gt; Jon Meredith
&gt;&gt; Basho Technologies.
&gt;&gt;
&gt;&gt; On Wed, Apr 13, 2011 at 1:41 PM, Greg Nelson  wrote:
&gt;&gt;
&gt;&gt; The largest known riak cluster != the largest possible riak cluster.
&gt;&gt; ;-)
&gt;&gt;
&gt;&gt; The inter node communication of the cluster depends on the data set and
&gt;&gt; usage pattern, doesn't it? Or is there some constant overhead that tops out
&gt;&gt; at a few hundred nodes? I should point out that we'll have big data, but
&gt;&gt; not a huge number of keys.
&gt;&gt;
&gt;&gt; The number of vnodes in the cluster should be equal to the
&gt;&gt; ring\_creation\_size under normal circumstances, shouldn't it? So when I have
&gt;&gt; a one node cluster, that node is running ring\_creation\_size vnodes... File
&gt;&gt; descriptors probably isn't a problem -- these machines won't be doing
&gt;&gt; anything else, and the limits are set to 65536.
&gt;&gt;
&gt;&gt; Thinking about the internode communication you mentioned, that's probably
&gt;&gt; where the resource hog is.. socket buffers, etc.
&gt;&gt;
&gt;&gt; Anyway, I'd also love to hear more from basho. :)
&gt;&gt;
&gt;&gt; On Wednesday, April 13, 2011 at 12:33 PM, sicul...@gmail.com wrote:
&gt;&gt;
&gt;&gt; Ill just chime in and say that this is not practical for a few reasons.
&gt;&gt; The largest known riak cluster has like 50 or 60 nodes. Afaik, inter node
&gt;&gt; communication of erlang clusters top out at a few hundred nodes. I'm also
&gt;&gt; under the impression that each physical node has to have enough file
&gt;&gt; descriptors to accommodate every virtual node in the cluster.
&gt;&gt;
&gt;&gt; I'd love to hear more from basho.
&gt;&gt;
&gt;&gt; -alexander
&gt;&gt;
&gt;&gt;
&gt;&gt; Sent from my Verizon Wireless BlackBerry
&gt;&gt;
&gt;&gt; -----Original Message-----
&gt;&gt; From: Greg Nelson 
&gt;&gt; Sender: riak-users-boun...@lists.basho.com
&gt;&gt; Date: Wed, 13 Apr 2011 12:13:34
&gt;&gt; To: 
&gt;&gt; Subject: Large ring\_creation\_size
&gt;&gt;
&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt; riak-users mailing list
&gt;&gt; riak-users@lists.basho.com
&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt; riak-users mailing list
&gt;&gt; riak-users@lists.basho.com
&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt; riak-users mailing list
&gt;&gt; riak-users@lists.basho.com
&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;&gt;
&gt;&gt;
&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt; riak-users mailing list
&gt;&gt; riak-users@lists.basho.com
&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt; riak-users mailing list
&gt;&gt; riak-users@lists.basho.com
&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;&gt;
&gt;&gt;
&gt;
&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;
&gt;
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

