---
title: "Re: riak_core question when a node dies"
description: ""
project: community
lastmod: 2012-03-28T14:19:58-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg07075"
mailinglist_parent_id: "msg07069"
author_name: "Samuel Elliott"
project_section: "mailinglistitem"
sent_date: 2012-03-28T14:19:58-07:00
---


How idempotent are the requests? You could do all three in parallel,
and then use the results of the one that returns first (and hence
doesn't blow up).

Sam

On Wed, Mar 28, 2012 at 7:09 PM, Jon Brisbin  wrote:
&gt; I'm using get\_primary\_apl to get my preflist but the problem is how to
&gt; handle a failure of trying to dispatch to a node that is just now going down
&gt; and hasn't had time to notify the caller yet. I don't want to loose the web
&gt; request currently in progress. Maybe I need to get a list of indexes to
&gt; possibly dispatch to and iterate over them, stopping at the first one that
&gt; doesn't blow up.
&gt;
&gt; Sent from my iPhone
&gt;
&gt; On Mar 28, 2012, at 12:00 PM, Sean Cribbs  wrote:
&gt;
&gt; Jon,
&gt;
&gt; Generally I would use the riak\_core\_apl module to calculate the preflist for
&gt; your request. It takes into account node visibility and service
&gt; availability. Use riak\_core\_node\_watcher:service\_up to announce that your
&gt; app is available after registering with riak\_core.
&gt;
&gt; When doing some "split brain" testing/simulation for gen\_leader we would do
&gt; something like the following on a node we wanted to partition:
&gt;
&gt; 1&gt; erlang:set\_cookie(node(), riak2).
&gt; 2&gt; erlang:disconnect\_node('dev3@127.0.0.1'),
&gt; erlang:disconnect\_node('dev4@127.0.0.1').
&gt;
&gt; Basically, set the cookie so it can't connect to the other nodes, then
&gt; manually disconnect. That might help you simulate node-outage.
&gt;
&gt; On Wed, Mar 28, 2012 at 12:49 PM, Jon Brisbin  wrote:
&gt;&gt;
&gt;&gt; I'm testing the example code that dispatches a web request from misultin
&gt;&gt; into a riak\_core ring of vnodes. It works fantastic when all nodes are up!
&gt;&gt; :)
&gt;&gt;
&gt;&gt; Doing "ab -k -c 200 -n 10000 http://localhost:3000/" yields a
&gt;&gt; none-to-shabby performance (dispatching at random into all available vnodes
&gt;&gt; on two separate riak\_core processes):
&gt;&gt;
&gt;&gt; Concurrency Level:      200
&gt;&gt; Time taken for tests:   1.446 seconds
&gt;&gt; Complete requests:      10000
&gt;&gt; Failed requests:        0
&gt;&gt; Write errors:           0
&gt;&gt; Keep-Alive requests:    10000
&gt;&gt; Total transferred:      1600480 bytes
&gt;&gt; HTML transferred:       120036 bytes
&gt;&gt; Requests per second:    6914.04 [#/sec] (mean)
&gt;&gt; Time per request:       28.927 [ms] (mean)
&gt;&gt; Time per request:       0.145 [ms] (mean, across all concurrent requests)
&gt;&gt; Transfer rate:          1080.64 [Kbytes/sec] received
&gt;&gt;
&gt;&gt; Connection Times (ms)
&gt;&gt;               min  mean[+/-sd] median   max
&gt;&gt; Connect:        0    0   1.0      0      12
&gt;&gt; Processing:     4   28   9.8     27      78
&gt;&gt; Waiting:        4   28   9.8     27      78
&gt;&gt; Total:          4   28  10.1     27      83
&gt;&gt;
&gt;&gt; Percentage of the requests served within a certain time (ms)
&gt;&gt;   50%     27
&gt;&gt;   66%     31
&gt;&gt;   75%     34
&gt;&gt;   80%     36
&gt;&gt;   90%     41
&gt;&gt;   95%     47
&gt;&gt;   98%     53
&gt;&gt;   99%     58
&gt;&gt;  100%     83 (longest request)
&gt;&gt;
&gt;&gt; If I were really zealous, I'd set up haproxy to load balance between these
&gt;&gt; two misultin servers and get double failover.
&gt;&gt;
&gt;&gt; I'm trying to catch the situation of going into the console of one of my
&gt;&gt; nodes and hitting "CTL-C" to kill that process. I'm not sure what the best
&gt;&gt; way is to handle this. Check before I dispatch to make sure the node is up?
&gt;&gt; Keep a watch of some other kind that, when it sees that node go down and if
&gt;&gt; it's trying to dispatch to that node, it tries to find another one?
&gt;&gt;
&gt;&gt; Essentially, I'm trying to prevent misultin from completely bailing on the
&gt;&gt; request because the sync\_spawn\_command blows up trying to do a
&gt;&gt; gen\_server:call to a non-existent node. I'd like to retry to dispatch to a
&gt;&gt; different node if one happens to have crashed while I'm serving requests (I
&gt;&gt; don't want to loose a request, essentially).
&gt;&gt;
&gt;&gt; Thanks!
&gt;&gt;
&gt;&gt; Jon Brisbin
&gt;&gt; http://about.me/jonbrisbin
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt; riak-users mailing list
&gt;&gt; riak-users@lists.basho.com
&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;&gt;
&gt;
&gt;
&gt;
&gt; --
&gt; Sean Cribbs 
&gt; Software Engineer
&gt; Basho Technologies, Inc.
&gt; http://basho.com/
&gt;
&gt;
&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;



-- 
Samuel Elliott
s...@lenary.co.uk
http://lenary.co.uk/
+44 (0)7891 993 664

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

