---
title: "Re: anti_entropy_expire"
description: ""
project: community
lastmod: 2014-01-02T02:07:04-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg13349"
mailinglist_parent_id: "msg13348"
author_name: "Edgar Veiga"
project_section: "mailinglistitem"
sent_date: 2014-01-02T02:07:04-08:00
---


This is the only thing related to AAE that exists in my app.config. I
haven't changed any default values...

 %% Enable active anti-entropy subsystem + optional debug
messages:
 %% {anti\_entropy, {on|off, []}},
 %% {anti\_entropy, {on|off, [debug]}},
 {anti\_entropy, {on, []}},

 %% Restrict how fast AAE can build hash trees. Building the tree
 %% for a given partition requires a full scan over that
partition's
 %% data. Once built, trees stay built until they are expired.
 %% Config is of the form:
 %% {num-builds, per-timespan-in-milliseconds}
 %% Default is 1 build per hour.
 {anti\_entropy\_build\_limit, {1, 3600000}},

 %% Determine how often hash trees are expired after being built.
 %% Periodically expiring a hash tree ensures the on-disk hash
tree
 %% data stays consistent with the actual k/v backend data. It
also
 %% helps Riak identify silent disk failures and bit rot.
However,
 %% expiration is not needed for normal AAE operation and should
be
 %% infrequent for performance reasons. The time is specified in
 %% milliseconds. The default is 1 week.
 {anti\_entropy\_expire, 604800000},

 %% Limit how many AAE exchanges/builds can happen concurrently.
 {anti\_entropy\_concurrency, 2},

 %% The tick determines how often the AAE manager looks for work
 %% to do (building/expiring trees, triggering exchanges, etc).
 %% The default is every 15 seconds. Lowering this value will
 %% speedup the rate that all replicas are synced across the
cluster.
 %% Increasing the value is not recommended.
 {anti\_entropy\_tick, 15000},

 %% The directory where AAE hash trees are stored.
 {anti\_entropy\_data\_dir, "/var/lib/riak/anti\_entropy"},

 %% The LevelDB options used by AAE to generate the
LevelDB-backed
 %% on-disk hashtrees.
 {anti\_entropy\_leveldb\_opts, [{write\_buffer\_size, 4194304},
 {max\_open\_files, 20}]},

I'll update the bloom filters value and see what happens...

It's thursday again, and the regeneration process has started again. Since
I've updated to 1.4.6, I have another thing different. The get/put values
for each cluster node now have a "random" behaviour. Take a look at the
next screenshot

https://cloudup.com/cgbu9VNhSo1

Best regards


On 31 December 2013 21:16, Charlie Voiselle  wrote:

&gt; Edgar:
&gt;
&gt; Could you attach the AAE section of your app.config? Iâ€™d like to look
&gt; into this issue further for you. Something I think you might be running
&gt; into is https://github.com/basho/riak\_core/pull/483.
&gt;
&gt; The issue of concern is that the LevelDB bloom filter is not enabled
&gt; properly for the instance into which the AAE data is stored. You can
&gt; mitigate this particular issue by adding \*{use\_bloomfilter, true}\* as
&gt; shown below:
&gt;
&gt; %% The LevelDB options used by AAE to generate the LevelDB-backed
&gt; %% on-disk hashtrees.
&gt; {anti\_entropy\_leveldb\_opts, [{write\_buffer\_size, 4194304},
&gt; {max\_open\_files, 20}]},
&gt;
&gt;
&gt; Becomes:
&gt;
&gt;
&gt; %% The LevelDB options used by AAE to generate the LevelDB-backed
&gt; %% on-disk hashtrees.
&gt;
&gt; {anti\_entropy\_leveldb\_opts, [{write\_buffer\_size, 4194304},
&gt; {use\_bloomfilter, true},
&gt; {max\_open\_files, 20}]},
&gt;
&gt;
&gt; This might not solve your specific problem, but it will certainly improve
&gt; your AAE performance.
&gt;
&gt; Thanks,
&gt; Charlie Voiselle
&gt;
&gt; On Dec 31, 2013, at 12:04 PM, Edgar Veiga  wrote:
&gt;
&gt; Hey guys!
&gt;
&gt; Nothing on this one?
&gt;
&gt; Btw: Happy new year :)
&gt;
&gt;
&gt; On 27 December 2013 22:35, Edgar Veiga  wrote:
&gt;
&gt;&gt; This is a du -hs \* of the riak folder:
&gt;&gt;
&gt;&gt; 44G anti\_entropy
&gt;&gt; 1.1M kv\_vnode
&gt;&gt; 252G leveldb
&gt;&gt; 124K ring
&gt;&gt;
&gt;&gt; It's a 6 machine cluster, so ~1512G of levelDB.
&gt;&gt;
&gt;&gt; Thanks for the tip, I'll upgrade in a near future!
&gt;&gt;
&gt;&gt; Best regards
&gt;&gt;
&gt;&gt;
&gt;&gt; On 27 December 2013 21:41, Matthew Von-Maszewski wrote:
&gt;&gt;
&gt;&gt;&gt; I have a query out to the developer that can better respond to your
&gt;&gt;&gt; follow-up questions. It might be Monday before we get a reply due to the
&gt;&gt;&gt; holidays.
&gt;&gt;&gt;
&gt;&gt;&gt; Do you happen to know how much data is in the leveldb dataset and/or one
&gt;&gt;&gt; vnode? Not sure it will change the response, but might be nice to have
&gt;&gt;&gt; that info available.
&gt;&gt;&gt;
&gt;&gt;&gt; Matthew
&gt;&gt;&gt;
&gt;&gt;&gt; P.S. Unrelated to your question: Riak 1.4.4 is available for download.
&gt;&gt;&gt; It has a couple of nice bug fixes for leveldb.
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; On Dec 27, 2013, at 2:08 PM, Edgar Veiga  wrote:
&gt;&gt;&gt;
&gt;&gt;&gt; Ok, thanks for confirming!
&gt;&gt;&gt;
&gt;&gt;&gt; Is it normal, that this action affects the overall state of the cluster?
&gt;&gt;&gt; On the 26th It started the regeneration and the the response times of the
&gt;&gt;&gt; cluster raised to never seen values. It was a day of heavy traffic but
&gt;&gt;&gt; everything was going quite ok until it started the regeneration process..
&gt;&gt;&gt;
&gt;&gt;&gt; Have you got any advices about changing those app.config values? My
&gt;&gt;&gt; cluster is running smoothly for the past 6 months and I don't want to start
&gt;&gt;&gt; all over again :)
&gt;&gt;&gt;
&gt;&gt;&gt; Best Regards
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; On 27 December 2013 18:56, Matthew Von-Maszewski wrote:
&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Yes. Confirmed.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; There are options available in app.config to control how often this
&gt;&gt;&gt;&gt; occurs and how many vnodes rehash at once: defaults are every 7 days and
&gt;&gt;&gt;&gt; two vnodes per server at a time.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Matthew Von-Maszewski
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; On Dec 27, 2013, at 13:50, Edgar Veiga  wrote:
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Hi!
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; I've been trying to find what may be the cause of this.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Every once in a week, all the nodes in my riak cluster start to do some
&gt;&gt;&gt;&gt; kind of operation that lasts at least for two days.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; You can watch a sample of my munin logs regarding the last week in here:
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; https://cloudup.com/imWiBwaC6fm
&gt;&gt;&gt;&gt; Take a look at the days 19 and 20, and now it has started again on the
&gt;&gt;&gt;&gt; 26...
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; I'm suspecting that this may be caused by the aae hash trees being
&gt;&gt;&gt;&gt; regenerated, as you say in your documentation:
&gt;&gt;&gt;&gt; For added protection, Riak periodically (default: once a week) clears
&gt;&gt;&gt;&gt; and regenerates all hash trees from the on-disk K/V data.
&gt;&gt;&gt;&gt; Can you confirm me that this may be the root of the "problem" and if
&gt;&gt;&gt;&gt; it's normal for the action to last for two days?
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; I'm using riak 1.4.2 on 6 machines, with centOS. The backend is levelDB.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Best Regards,
&gt;&gt;&gt;&gt; Edgar Veiga
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;
&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;
&gt;
&gt;
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

