---
title: "Riak YZ/Solr creating invalid segments"
description: ""
project: community
lastmod: 2016-01-10T23:09:07-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg16935"
author_name: "Josh Yudaken"
project_section: "mailinglistitem"
sent_date: 2016-01-10T23:09:07-08:00
---


Hi,

We've been hitting random issues with corrupted solr indices on single
machines within our riak cluster. This seems to happen fairly randomly
(possibly handoff related) but has also been triggered by clean
shutdown/startup of nodes.

The error in question is the following in our solr.log:
2016-01-11 05:08:20,283 [ERROR]
@SolrException.java:120
null:org.apache.solr.common.SolrException: So
lrCore 'entity\_search20151228\_2' is not available due to init failure:
Error opening new searcher
&lt;...&gt;
Caused by: java.io.FileNotFoundException:
&lt;...&gt;
/data/riak/yz/entity\_search20151228\_2/data/index/\_c1hp.si
 at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:340)

Unfortunately the standard Lucene CheckIndex is unable to recover this
error, but there is a patch available at:
https://issues.apache.org/jira/browse/LUCENE-6762

After modifying the patch to run on Lucene 4.7 [any plans on
upgrading?] we managed to bring our nodes back up and they seem to be
functioning fine.

Have you seen these issues anywhere else? Any advice on how to try
solve them besides continually running the updated CheckIndex script
after each failure?

Regards,
Josh

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

