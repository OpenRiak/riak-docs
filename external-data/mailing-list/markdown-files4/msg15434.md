---
title: "Re: How you are dealing with spikes?"
description: ""
project: community
lastmod: 2014-12-31T03:39:07-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg15434"
mailinglist_parent_id: "msg15398"
author_name: "Alexander Popov"
project_section: "mailinglistitem"
sent_date: 2014-12-31T03:39:07-08:00
---


Was replaced most 2i and MapReduce calls to SOLR requests,
seems not helps to much. Now Solr request has peaks sometimes


looking on /stats :
what is difference between search\_index\_latency search\_query\_latency


 - search\_query\_throughput\_count: 364711,
 - search\_query\_throughput\_one: 0,
 - search\_query\_fail\_count: 6,
 - search\_query\_fail\_one: 0,
 - search\_query\_latency\_min: 0,
 - search\_query\_latency\_max: 0,
 - search\_query\_latency\_median: 0,
 - search\_query\_latency\_95: 0,
 - search\_query\_latency\_99: 0,
 - search\_query\_latency\_999: 0,
 - search\_index\_throughput\_count: 300612,
 - search\_index\_throughtput\_one: 2585,
 - search\_index\_fail\_count: 367,
 - search\_index\_fail\_one: 12,
 - search\_index\_latency\_min: 765,
 - search\_index\_latency\_max: 49859,
 - search\_index\_latency\_median: 1097,
 - search\_index\_latency\_95: 2801,
 - search\_index\_latency\_99: 18763,
 - search\_index\_latency\_999: 37138,



On Mon, Dec 22, 2014 at 1:39 AM, Alexander Popov 
wrote:

&gt; Left graph show counts, Right graph show times, graphs are synchronized by
&gt; time
&gt; What about SOLR requests instead 2i? should it be faster?
&gt; Or what you recommend to use for populating lists of users data? for
&gt; example now we have files,
&gt; that have 2i like owner, so when user request his files, we populating
&gt; buckets/files/owner\_bin/user\_id, If we change this query to SOLR analog -&gt;
&gt; can we gave some boost?
&gt;
&gt; Also, does key length matter for 2i performance? Does number of 2i
&gt; indexes per object matter for 2i?
&gt;
&gt; On Tue, Dec 9, 2014 at 7:54 PM, Alexander Popov 
&gt; wrote:
&gt;
&gt;&gt; Stats when recent spike happens for 15 minutes around it
&gt;&gt; get (826)
&gt;&gt; save (341)
&gt;&gt; listByIndex (1161)
&gt;&gt; mapReduce (621) //Input is IDs list
&gt;&gt; SOLR (4294)
&gt;&gt;
&gt;&gt; 6 Solr requests was longer than 9sec ( all returns 0 rows )
&gt;&gt; 4 Solr requests was longer within 4-5s ( both returns 0 rows )
&gt;&gt; 11 listByIndex requests was longer than within 4-5s ( both returns 0 rows
&gt;&gt; )
&gt;&gt; all another requests was less than 300ms
&gt;&gt;
&gt;&gt;
&gt;&gt; Sometimes more load do not make such spikes
&gt;&gt; Some graphs from maintanance tasks:
&gt;&gt; 1. http://i.imgur.com/xAE6B06.png
&gt;&gt; 3 simple tasks, first 2 of them reads all keys, decide to do
&gt;&gt; nothing and continue so just read happens, third task resave all data
&gt;&gt; in bucket.
&gt;&gt; since rate is pretty good, some peaks happens
&gt;&gt;
&gt;&gt; 2. More complex task
&gt;&gt; http://i.imgur.com/7nwHb3Q.png, it have more serious computing, and
&gt;&gt; updating typed bucked( map ), but no peaks to 9s
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt; sysctl -a | fgrep vm.dirty\_:
&gt;&gt;
&gt;&gt; vm.dirty\_background\_bytes = 0
&gt;&gt; vm.dirty\_background\_ratio = 10
&gt;&gt; vm.dirty\_bytes = 0
&gt;&gt; vm.dirty\_expire\_centisecs = 3000
&gt;&gt; vm.dirty\_ratio = 20
&gt;&gt; vm.dirty\_writeback\_centisecs = 500
&gt;&gt;
&gt;&gt; On Tue, Dec 9, 2014 at 5:46 PM, Luke Bakken  wrote:
&gt;&gt; &gt; Hi Alexander,
&gt;&gt; &gt;
&gt;&gt; &gt; Can you comment on the read vs. write load of this cluster/
&gt;&gt; &gt;
&gt;&gt; &gt; Could you please run the following command and reply with the output?
&gt;&gt; &gt;
&gt;&gt; &gt; sysctl -a | fgrep vm.dirty\_
&gt;&gt; &gt;
&gt;&gt; &gt; We've seen cases where dirty pages get written in a synchronous manner
&gt;&gt; &gt; all at once, causing latency spikes due to I/O blocking.
&gt;&gt; &gt; --
&gt;&gt; &gt; Luke Bakken
&gt;&gt; &gt; Engineer / CSE
&gt;&gt; &gt; lbak...@basho.com
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt; &gt; On Tue, Dec 9, 2014 at 4:58 AM, Alexander Popov 
&gt;&gt; wrote:
&gt;&gt; &gt;&gt; I have Riak 2.0.1 cluster with 5 nodes ( ec2 m3-large ) with elnm in
&gt;&gt; front
&gt;&gt; &gt;&gt; sometimes I got spikes up to 10 seconds
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; I can't say that I have huge load at this time, max 200 requests per
&gt;&gt; &gt;&gt; second for all 5 nodes.
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; Most expensive queries is
&gt;&gt; &gt;&gt; \* list by secondary index ( usually returns from 0 to 100 records )
&gt;&gt; &gt;&gt; \* and solr queries( max 10 records )
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; save operations is slowdown sometimes but not so much ( up to 1 sec )
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; It's slowdown not for specific requests, same one work pretty fast
&gt;&gt; later.
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; Does it any possibilities to profile|log somehow to determine reason
&gt;&gt; &gt;&gt; why this happen?
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt; &gt;&gt; riak-users mailing list
&gt;&gt; &gt;&gt; riak-users@lists.basho.com
&gt;&gt; &gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;&gt;
&gt;
&gt;
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

