---
title: "Re: performance"
description: ""
project: community
lastmod: 2013-08-01T19:51:55-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg11859"
mailinglist_parent_id: "msg11858"
author_name: "Paul Ingalls"
project_section: "mailinglistitem"
sent_date: 2013-08-01T19:51:55-07:00
---


I should say that I build riak from the master branch on the git repository. 
Perhaps that was a bad idea?

Paul Ingalls
Founder & CEO Fanzo
p...@fanzo.me
@paulingalls
http://www.linkedin.com/in/paulingalls



On Aug 1, 2013, at 7:47 PM, Paul Ingalls  wrote:

&gt; Thanks for the quick response Matthew!
&gt; 
&gt; I gave that a shot, and if anything the performance was worse. When I picked 
&gt; 128 I ran through the calculations on this page:
&gt; 
&gt; http://docs.basho.com/riak/latest/ops/advanced/backends/leveldb/#Parameter-Planning
&gt; 
&gt; and thought that would work, but it sounds like I was quite a bit off from 
&gt; what you have below.
&gt; 
&gt; Looking at risk control, the memory was staying pretty low, and watching top 
&gt; the CPU was well in hand. iostat had very little of the CPU in iowait, 
&gt; although it was writing a lot. I imagine, however, that this is missing a 
&gt; lot of the details.
&gt; 
&gt; Any other ideas? I can't imagine one get/update/put cycle per second is the 
&gt; best I can doâ€¦
&gt; 
&gt; Thanks!
&gt; 
&gt; Paul Ingalls
&gt; Founder & CEO Fanzo
&gt; p...@fanzo.me
&gt; @paulingalls
&gt; http://www.linkedin.com/in/paulingalls
&gt; 
&gt; 
&gt; 
&gt; On Aug 1, 2013, at 7:12 PM, Matthew Von-Maszewski  wrote:
&gt; 
&gt;&gt; Try cutting your max open files in half. I am working from my iPad not my 
&gt;&gt; workstation so my numbers are rough. Will get better ones to you in the 
&gt;&gt; morning.
&gt;&gt; 
&gt;&gt; The math goes like this: 
&gt;&gt; 
&gt;&gt; - vnode/partition heap usage is (4Mbytes \* (max\_open\_files -10)) + 8Mbyte
&gt;&gt; - you have 18 vnodes per server (multiply the above times 18)
&gt;&gt; - AAE (active anti-entropy is"on") so that adds (4Mbyte\* 10 + 8Mbyte) times 
&gt;&gt; 18 vnodes 
&gt;&gt; 
&gt;&gt; The three lines above give the total memory leveldb will attempt to use per 
&gt;&gt; server if your dataset is large enough to fill it.
&gt;&gt; 
&gt;&gt; Matthew
&gt;&gt; 
&gt;&gt; 
&gt;&gt; On Aug 1, 2013, at 21:33, Paul Ingalls  wrote:
&gt;&gt; 
&gt;&gt;&gt; I should add more details about the nodes that crashed. I ran this for the 
&gt;&gt;&gt; first time for all of 10 minutes.
&gt;&gt;&gt; 
&gt;&gt;&gt; Here is the log from the first one:
&gt;&gt;&gt; 
&gt;&gt;&gt; 2013-08-02 00:09:44 =ERROR REPORT====
&gt;&gt;&gt; \*\* State machine &lt;0.2368.0&gt; terminating
&gt;&gt;&gt; \*\* Last event in was unregistered
&gt;&gt;&gt; \*\* When State == active
&gt;&gt;&gt; \*\* Data == 
&gt;&gt;&gt; {state,114179815416476790484662877555959610910619729920,riak\_kv\_vnode,{deleted,{state,114179815416476790484662877555959610910619729920,riak\_kv\_eleveldb\_backend,{state,&lt;&lt;&gt;&gt;,"/mnt/datadrive/riak/data/leveldb/114179815416476790484662877555959610910619729920",[{create\_if\_missing,true},{max\_open\_files,128},{use\_bloomfilter,true},{write\_buffer\_size,58858594}],[{add\_paths,[]},{allow\_strfun,false},{anti\_entropy,{on,[]}},{anti\_entropy\_build\_limit,{1,3600000}},{anti\_entropy\_concurrency,2},{anti\_entropy\_data\_dir,"/mnt/datadrive/riak/data/anti\_entropy"},{anti\_entropy\_expire,604800000},{anti\_entropy\_leveldb\_opts,[{write\_buffer\_size,4194304},{max\_open\_files,20}]},{anti\_entropy\_tick,15000},{create\_if\_missing,true},{data\_root,"/mnt/datadrive/riak/data/leveldb"},{fsm\_limit,50000},{hook\_js\_vm\_count,2},{http\_url\_encoding,on},{included\_applications,[]},{js\_max\_vm\_mem,8},{js\_thread\_stack,16},{legacy\_stats,true},{listkeys\_backpressure,true},{map\_js\_vm\_count,8},{mapred\_2i\_pipe,true},{mapred\_name,"mapred"},{max\_open\_files,128},{object\_format,v1},{reduce\_js\_vm\_count,6},{stats\_urlpath,"stats"},{storage\_backend,riak\_kv\_eleveldb\_backend},{use\_bloomfilter,true},{vnode\_vclocks,true},{write\_buffer\_size,58858594}],[],[],[{fill\_cache,false}],true,false},{dict,0,16,16,8,80,48,{[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]},{{[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]}}},undefined,3000,1000,100,100,true,true,undefined}},riak@riak003,none,undefined,undefined,undefined,{pool,riak\_kv\_worker,10,[]},undefined,107615}
&gt;&gt;&gt; \*\* Reason for termination =
&gt;&gt;&gt; \*\* 
&gt;&gt;&gt; {badarg,[{eleveldb,close,[&lt;&lt;&gt;&gt;],[]},{riak\_kv\_eleveldb\_backend,stop,1,[{file,"src/riak\_kv\_eleveldb\_backend.erl"},{line,149}]},{riak\_kv\_vnode,terminate,2,[{file,"src/riak\_kv\_vnode.erl"},{line,836}]},{riak\_core\_vnode,terminate,3,[{file,"src/riak\_core\_vnode.erl"},{line,847}]},{gen\_fsm,terminate,7,[{file,"gen\_fsm.erl"},{line,586}]},{proc\_lib,init\_p\_do\_apply,3,[{file,"proc\_lib.erl"},{line,227}]}]}
&gt;&gt;&gt; 2013-08-02 00:09:44 =CRASH REPORT====
&gt;&gt;&gt; crasher:
&gt;&gt;&gt; initial call: riak\_core\_vnode:init/1
&gt;&gt;&gt; pid: &lt;0.2368.0&gt;
&gt;&gt;&gt; registered\_name: []
&gt;&gt;&gt; exception exit: 
&gt;&gt;&gt; {{badarg,[{eleveldb,close,[&lt;&lt;&gt;&gt;],[]},{riak\_kv\_eleveldb\_backend,stop,1,[{file,"src/riak\_kv\_eleveldb\_backend.erl"},{line,149}]},{riak\_kv\_vnode,terminate,2,[{file,"src/riak\_kv\_vnode.erl"},{line,836}]},{riak\_core\_vnode,terminate,3,[{file,"src/riak\_core\_vnode.erl"},{line,847}]},{gen\_fsm,terminate,7,[{file,"gen\_fsm.erl"},{line,586}]},{proc\_lib,init\_p\_do\_apply,3,[{file,"proc\_lib.erl"},{line,227}]}]},[{gen\_fsm,terminate,7,[{file,"gen\_fsm.erl"},{line,589}]},{proc\_lib,init\_p\_do\_apply,3,[{file,"proc\_lib.erl"},{line,227}]}]}
&gt;&gt;&gt; ancestors: [riak\_core\_vnode\_sup,riak\_core\_sup,&lt;0.139.0&gt;]
&gt;&gt;&gt; messages: []
&gt;&gt;&gt; links: [&lt;0.142.0&gt;]
&gt;&gt;&gt; dictionary: [{random\_seed,{8115,23258,22987}}]
&gt;&gt;&gt; trap\_exit: true
&gt;&gt;&gt; status: running
&gt;&gt;&gt; heap\_size: 196418
&gt;&gt;&gt; stack\_size: 24
&gt;&gt;&gt; reductions: 12124
&gt;&gt;&gt; neighbours:
&gt;&gt;&gt; 2013-08-02 00:09:44 =SUPERVISOR REPORT====
&gt;&gt;&gt; Supervisor: {local,riak\_core\_vnode\_sup}
&gt;&gt;&gt; Context: child\_terminated
&gt;&gt;&gt; Reason: 
&gt;&gt;&gt; {badarg,[{eleveldb,close,[&lt;&lt;&gt;&gt;],[]},{riak\_kv\_eleveldb\_backend,stop,1,[{file,"src/riak\_kv\_eleveldb\_backend.erl"},{line,149}]},{riak\_kv\_vnode,terminate,2,[{file,"src/riak\_kv\_vnode.erl"},{line,836}]},{riak\_core\_vnode,terminate,3,[{file,"src/riak\_core\_vnode.erl"},{line,847}]},{gen\_fsm,terminate,7,[{file,"gen\_fsm.erl"},{line,586}]},{proc\_lib,init\_p\_do\_apply,3,[{file,"proc\_lib.erl"},{line,227}]}]}
&gt;&gt;&gt; Offender: 
&gt;&gt;&gt; [{pid,&lt;0.2368.0&gt;},{name,undefined},{mfargs,{riak\_core\_vnode,start\_link,undefined}},{restart\_type,temporary},{shutdown,300000},{child\_type,worker}]
&gt;&gt;&gt; 
&gt;&gt;&gt; The second one looks like it ran out of heap, I assume I have something 
&gt;&gt;&gt; miss configured here...
&gt;&gt;&gt; 
&gt;&gt;&gt; ===== Fri Aug 2 00:51:28 UTC 2013
&gt;&gt;&gt; Erlang has closed
&gt;&gt;&gt; /home/fanzo/riak/rel/riak/bin/../lib/os\_mon-2.2.9/priv/bin/memsup: Erlang 
&gt;&gt;&gt; has closed.
&gt;&gt;&gt; ^M
&gt;&gt;&gt; Crash dump was written to: ./log/erl\_crash.dump^M
&gt;&gt;&gt; eheap\_alloc: Cannot allocate 5568010120 bytes of memory (of type "heap").^M
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; Paul Ingalls
&gt;&gt;&gt; Founder & CEO Fanzo
&gt;&gt;&gt; p...@fanzo.me
&gt;&gt;&gt; @paulingalls
&gt;&gt;&gt; http://www.linkedin.com/in/paulingalls
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; On Aug 1, 2013, at 6:28 PM, Paul Ingalls  wrote:
&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Couple of questions.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; I have migrated my system to use Riak on the back end. I have setup a 1.4 
&gt;&gt;&gt;&gt; cluster with 128 partitions on 7 nodes with LevelDB as the store. Each 
&gt;&gt;&gt;&gt; node looks like:
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Azure Large instance (4CPU 7GB RAM)
&gt;&gt;&gt;&gt; data directory is on a RAID 0
&gt;&gt;&gt;&gt; max files is set to 128
&gt;&gt;&gt;&gt; async thread on the VM is 16
&gt;&gt;&gt;&gt; everything else is defaults
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; I'm using the 1.4.1 java client, connecting via the protocol buffer 
&gt;&gt;&gt;&gt; cluster.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; With this setup, I'm seeing poor throughput on my service load. I ran a 
&gt;&gt;&gt;&gt; test for a bit and was seeing only a few gets/puts per second. And then 
&gt;&gt;&gt;&gt; when I stopped the client two of the nodes crashed.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; I'm very new with Riak, so I figure I'm doing something wrong. I saw a 
&gt;&gt;&gt;&gt; note on the list earlier of someone getting well over 1000 puts per 
&gt;&gt;&gt;&gt; second, so I know it can move pretty fast. 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; What is a good strategy for troubleshooting?
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; How many fetch/update/store loops per second should I expect to see on a 
&gt;&gt;&gt;&gt; cluster of this size?
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Thanks!
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Paul
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Paul Ingalls
&gt;&gt;&gt;&gt; Founder & CEO Fanzo
&gt;&gt;&gt;&gt; p...@fanzo.me
&gt;&gt;&gt;&gt; @paulingalls
&gt;&gt;&gt;&gt; http://www.linkedin.com/in/paulingalls
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt; 

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

