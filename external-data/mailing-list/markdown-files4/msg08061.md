---
title: "Re: How to store data"
description: ""
project: community
lastmod: 2012-07-25T09:23:06-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg08061"
mailinglist_parent_id: "msg08058"
author_name: "Andrew Kondratovich"
project_section: "mailinglistitem"
sent_date: 2012-07-25T09:23:06-07:00
---


Yeap.. half a thousand requests to riak isn't cool =( I'm looking some
strategy of storing data so that i could fetch all items by 1 request.

I could use index MR at time and filter results at map phase. I could use
special keys with from data and use key filters (with time filtering at map
phase)... I wish I could use several 2i at MR or combine 2i with
keyfilters, or perform MR on buckets... I wish... =)

On Wed, Jul 25, 2012 at 5:35 PM, Andres Jaan Tack  wrote:

&gt; Is that a realistic strategy for low latency requirements? Imagine this
&gt; were some web service, and people generate this query at some reasonable
&gt; frequency.
&gt;
&gt; (not that I know what Andrew is looking for, exactly)
&gt;
&gt;
&gt; 2012/7/25 Yousuf Fauzan 
&gt;
&gt;&gt; Since 500 is not that big a number, I think you can run that many M/Rs
&gt;&gt; with each emitting only records having "time" greater than specified. Input
&gt;&gt; would be {index, &lt;&lt;"bucket"&gt;&gt;, &lt;&lt;"from\_bin"&gt;&gt;, &lt;&lt;"from\_field\_value"&gt;&gt;}
&gt;&gt;
&gt;&gt; If you decide to split the data into separate buckets based on "from"
&gt;&gt; field, input would be {index, &lt;&lt;"from\_field\_value"&gt;&gt;, &lt;&lt;"time\_bin"&gt;&gt;,
&gt;&gt; &lt;&lt;"time\_low"&gt;&gt;, &lt;&lt;"time\_high"&gt;&gt;}
&gt;&gt;
&gt;&gt;
&gt;&gt; --
&gt;&gt; Yousuf
&gt;&gt;
&gt;&gt; On Wed, Jul 25, 2012 at 6:35 PM, Andrew Kondratovich &lt;
&gt;&gt; andrew.kondratov...@gmail.com&gt; wrote:
&gt;&gt;
&gt;&gt;&gt; Hello, Yousuf.
&gt;&gt;&gt;
&gt;&gt;&gt; Thanks for your reply.
&gt;&gt;&gt;
&gt;&gt;&gt; We have several millions of items. It's about 10 000 of unique 'from'
&gt;&gt;&gt; fields (about 1000 items for each). Usually, we need to get items for about
&gt;&gt;&gt; 500 'from' identifiers with 'time' limit (about 5% of items is
&gt;&gt;&gt; corresponding).
&gt;&gt;&gt;
&gt;&gt;&gt; On Wed, Jul 25, 2012 at 1:02 PM, Yousuf Fauzan 
&gt;&gt;&gt; wrote:
&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Hi Andrew,
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; First of all, the correct answer to your question is the proverbial "it
&gt;&gt;&gt;&gt; depends". Having said that, here is what I could do in your case
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; 1. If there are enough data points with the same "from" field, I will
&gt;&gt;&gt;&gt; make it a bucket and then index on time.
&gt;&gt;&gt;&gt; 2. If the above is not true, I will index on "from" and "time" field.
&gt;&gt;&gt;&gt; a. If number of records where "time" is greater than the one your
&gt;&gt;&gt;&gt; require is small, I will run a map/reduce with the initial input as those
&gt;&gt;&gt;&gt; records.
&gt;&gt;&gt;&gt; b. If number of records having a particular "from" is small, I will
&gt;&gt;&gt;&gt; do the above with the initial input as records having that "from" field.
&gt;&gt;&gt;&gt; This could be a problem as Riak only supports range and exact queries so if
&gt;&gt;&gt;&gt; you want to query multiple identifiers, you will have to run multiple
&gt;&gt;&gt;&gt; queries.
&gt;&gt;&gt;&gt; In both the above cases, I will use secondary indexes to get the
&gt;&gt;&gt;&gt; initial records.
&gt;&gt;&gt;&gt; Note that we are using M/R as Riak does not support querying by
&gt;&gt;&gt;&gt; multiple indexes.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; What I would also suggest is to partition your data into different
&gt;&gt;&gt;&gt; buckets. You will need to understand the queries that you will be
&gt;&gt;&gt;&gt; supporting and partition it accordingly.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; --
&gt;&gt;&gt;&gt; Yousuf
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; On Wed, Jul 25, 2012 at 2:50 PM, Andrew Kondratovich &lt;
&gt;&gt;&gt;&gt; andrew.kondratov...@gmail.com&gt; wrote:
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; Good afternoon.
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; I am considering several storage solutions for my project, and now I
&gt;&gt;&gt;&gt;&gt; look at Riak.
&gt;&gt;&gt;&gt;&gt; We work with the following pattern of data:
&gt;&gt;&gt;&gt;&gt; {
&gt;&gt;&gt;&gt;&gt; time: unixtime
&gt;&gt;&gt;&gt;&gt; from: int
&gt;&gt;&gt;&gt;&gt; data: binary
&gt;&gt;&gt;&gt;&gt; ...
&gt;&gt;&gt;&gt;&gt; }
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; The amount of data is about several millions items for now, but it's
&gt;&gt;&gt;&gt;&gt; growing. It is necessary to handle the folloring requests: for a list of
&gt;&gt;&gt;&gt;&gt; identifiers (about 500 items) return all records where id = from and time
&gt;&gt;&gt;&gt;&gt; greater than a certain value.
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; How to store such data and to effectively handle such requests with
&gt;&gt;&gt;&gt;&gt; the Riak?
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; Thanks.
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; --
&gt;&gt;&gt;&gt;&gt; Andrew Kondratovich
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt;&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; --
&gt;&gt;&gt; Andrew Kondratovich
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;
&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt; riak-users mailing list
&gt;&gt; riak-users@lists.basho.com
&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;&gt;
&gt;&gt;
&gt;


-- 
Andrew Kondratovich
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

