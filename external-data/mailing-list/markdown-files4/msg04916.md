---
title: "Re: simulating physical node crash"
description: ""
project: community
lastmod: 2011-09-28T12:39:48-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg04916"
mailinglist_parent_id: "msg04871"
author_name: "francisco treacy"
project_section: "mailinglistitem"
sent_date: 2011-09-28T12:39:48-07:00
---


Regarding (3) I found a Forcing Read Repair contrib function (
http://contrib.basho.com/bucket\_inspector.html) which should help.

Otherwise for the m/r error, all of my buckets use default n\_val and write
quorum. Could it be that some data never reached that particular node in the
cluster? That is, should've I used W=3? During the failure, many assets
were returning 404s which triggered read-repair (and were ok upon subsequent
request), but no luck with the Map/Reduce function (it kept on failing).
 Could it have something to do with Riak Search?

Thanks,

Francisco


2011/9/26 francisco treacy 

&gt; Hi all,
&gt;
&gt; I have a 3-node Riak cluster, and I am simulating the scenario of physical
&gt; nodes crashing.
&gt;
&gt; When 2 nodes go down, and I query the remaining one, it fails with:
&gt;
&gt; {error,
&gt; {exit,
&gt; {{{error,
&gt; {no\_candidate\_nodes,exhausted\_prefist,
&gt; [{riak\_kv\_mapred\_planner,claim\_keys,3},
&gt; {riak\_kv\_map\_phase,schedule\_input,5},
&gt; {riak\_kv\_map\_phase,handle\_input,3},
&gt; {luke\_phase,executing,3},
&gt; {gen\_fsm,handle\_msg,7},
&gt; {proc\_lib,init\_p\_do\_apply,3}],
&gt; []}},
&gt; {gen\_fsm,sync\_send\_event,
&gt; [&lt;0.31566.2330&gt;,
&gt; {inputs,
&gt;
&gt; (...)
&gt;
&gt; Here I'm doing a M/R, inputs being fed by Search.
&gt;
&gt; (1) All of the involved buckets have N=3, and all involved requests R=1 (I
&gt; don't really need quorum for this usecase)
&gt;
&gt; Why is it failing? I'm sure i'm missing something basic here
&gt;
&gt; (2) Probably worth noting, those 3 nodes are spread across \*two\* physical
&gt; servers (1 on small one, 2 on beefier one). I've heard it is "not a good
&gt; idea", not sure why though. These two servers are definitely enough still
&gt; for our current load; should I consider adding a third one?
&gt;
&gt; (3) To overcome the aforementioned error, I added a new node to the cluster
&gt; (installed on the small server). Now the setup looks like: 4 nodes = 2 on
&gt; small server, 2 on beefier one.
&gt;
&gt; When 2 nodes go down, this works. Which brings me to another topic...
&gt; could you point me to good strategies to "pre-" invoke read-repair? Is it up
&gt; to clients to scan the keyspace forcing reads? It's a disaster
&gt; usability-wise when first users start getting 404s all over the place.
&gt;
&gt; Francisco
&gt;
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

