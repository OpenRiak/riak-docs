---
title: "Re: Yokozuna inconsistent search results"
description: ""
project: community
lastmod: 2016-04-05T12:56:13-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg17198"
author_name: "Oleksiy Krivoshey"
project_section: "mailinglistitem"
sent_date: 2016-04-05T12:56:13-07:00
---


Hi Fred,

Thanks for internal call tips, I'll dig deeper!

I've attached recent results of `riak-admin search aae-status` from all
nodes.


On 5 April 2016 at 22:41, Fred Dushin  wrote:

&gt; Hi Oleksiy,
&gt;
&gt; I assume you are getting this information through riak-admin. Can you
&gt; post the results here?
&gt;
&gt; If you want to dig deeper, you can probe the individual hash trees for
&gt; their build time. I will paste a few snippets of erlang here, which I am
&gt; hoping you can extend to use with list comprehensions and rpc:multicalls.
&gt; If that's too much to ask, let us know and I can try to put something
&gt; together that is more "big easy button".
&gt;
&gt; First, on any individual node, you can get the Riak partitions on that
&gt; node, via
&gt;
&gt; (dev1@127.0.0.1)1&gt; Partitons = [P || {\_, P, \_} &lt;-
&gt; riak\_core\_vnode\_manager:all\_vnodes(riak\_kv\_vnode)].
&gt; [913438523331814323877303020447676887284957839360,
&gt; 182687704666362864775460604089535377456991567872,
&gt; 1187470080331358621040493926581979953470445191168,
&gt; 730750818665451459101842416358141509827966271488,
&gt; 1370157784997721485815954530671515330927436759040,
&gt; 1004782375664995756265033322492444576013453623296,
&gt; 822094670998632891489572718402909198556462055424,
&gt; 456719261665907161938651510223838443642478919680,
&gt; 274031556999544297163190906134303066185487351808,
&gt; 1096126227998177188652763624537212264741949407232,
&gt; 365375409332725729550921208179070754913983135744,
&gt; 91343852333181432387730302044767688728495783936,
&gt; 639406966332270026714112114313373821099470487552,0,
&gt; 1278813932664540053428224228626747642198940975104,
&gt; 548063113999088594326381812268606132370974703616]
&gt;
&gt; For any one partition, you can get to the Pid associated with the
&gt; yz\_index\_hashtree associated with that partition, e.g.,
&gt;
&gt; (dev1@127.0.0.1)2&gt; {ok, Pid} =
&gt; yz\_entropy\_mgr:get\_tree(913438523331814323877303020447676887284957839360).
&gt; {ok,&lt;0.2872.0&gt;}
&gt;
&gt; and from there you can get the state information about the hahstree, which
&gt; includes its build time. You can read the record definitions associated
&gt; with the yz\_index\_hashtree state by calling rr() on the yz\_index\_hashtree
&gt; module first, if you want to make the state slightly more readable:
&gt;
&gt; (dev1@127.0.0.1)3&gt; rr(yz\_index\_hashtree).
&gt; [entropy\_data,state,xmerl\_event,xmerl\_fun\_states,
&gt; xmerl\_scanner,xmlAttribute,xmlComment,xmlContext,xmlDecl,
&gt; xmlDocument,xmlElement,xmlNamespace,xmlNode,xmlNsNode,
&gt; xmlObj,xmlPI,xmlText]
&gt; (dev1@127.0.0.1)5&gt; sys:get\_state(Pid).
&gt; #state{index = 913438523331814323877303020447676887284957839360,
&gt; built = true,expired = false,lock = undefined,
&gt; path =
&gt; "./data/yz\_anti\_entropy/913438523331814323877303020447676887284957839360",
&gt; build\_time = {1459,801655,506719},
&gt; trees = [{{867766597165223607683437869425293042920709947392,
&gt; 3},
&gt; {state,&lt;&lt;152,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...&gt;&gt;,
&gt;
&gt; 913438523331814323877303020447676887284957839360,3,1048576,
&gt; 1024,0,
&gt; {dict,0,16,16,8,80,48,{[],[],...},{{...}}},
&gt; &lt;&lt;&gt;&gt;,
&gt;
&gt; "./data/yz\_anti\_entropy/913438523331814323877303020447676887284957839360",
&gt; &lt;&lt;&gt;&gt;,incremental,[],0,
&gt; {array,38837,0,...}}},
&gt; {{890602560248518965780370444936484965102833893376,3},
&gt; {state,&lt;&lt;156,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...&gt;&gt;,
&gt;
&gt; 913438523331814323877303020447676887284957839360,3,1048576,
&gt; 1024,0,
&gt; {dict,0,16,16,8,80,48,{[],...},{...}},
&gt; &lt;&lt;&gt;&gt;,
&gt;
&gt; "./data/yz\_anti\_entropy/913438523331814323877303020447676887284957839360",
&gt; &lt;&lt;&gt;&gt;,incremental,[],0,
&gt; {array,38837,...}}},
&gt; {{913438523331814323877303020447676887284957839360,3},
&gt; {state,&lt;&lt;160,0,0,0,0,0,0,0,0,0,0,0,0,0,...&gt;&gt;,
&gt;
&gt; 913438523331814323877303020447676887284957839360,3,1048576,
&gt; 1024,0,
&gt; {dict,0,16,16,8,80,48,{...},...},
&gt; &lt;&lt;&gt;&gt;,
&gt;
&gt; "./data/yz\_anti\_entropy/913438523331814323877303020447676887284957839360",
&gt; &lt;&lt;&gt;&gt;,incremental,[],0,
&gt; {array,...}}}],
&gt; closed = false}
&gt;
&gt; You can convert the timestamp to local time via:
&gt;
&gt; (dev1@127.0.0.1)8&gt; calendar:now\_to\_local\_time({1459,801655,506719}).
&gt; {{2016,4,4},{16,27,35}}
&gt;
&gt;
&gt; Again, this is just an example, but with the right erlang incantations,
&gt; you should be able to iterate over all the timestamps across all the nodes.
&gt;
&gt; Let us know if that is helpful, or if you need more examples so you can do
&gt; it in one swipe.
&gt;
&gt; -Fred
&gt;
&gt; On Apr 5, 2016, at 9:29 AM, Oleksiy Krivoshey  wrote:
&gt;
&gt; How can I check that AAE trees have expired? Yesterday I ran "
&gt; riak\_core\_util:rpc\_every\_member\_ann(yz\_entropy\_mgr, expire\_trees, [],
&gt; 5000)." on each node (just to be sure). Still today I see that on 3 nodes
&gt; (of 5) all entropy tress and all last AAE exchanges are older than 20 days.
&gt;
&gt; On 4 April 2016 at 17:15, Oleksiy Krivoshey  wrote:
&gt;
&gt;&gt; Continuation...
&gt;&gt;
&gt;&gt; The new index has the same inconsistent search results problem.
&gt;&gt; I was making a snapshot of `search aae-status` command almost each day.
&gt;&gt; There were absolutely no Yokozuna errors in logs.
&gt;&gt;
&gt;&gt; I can see that some AAE trees were not expired (built &gt; 20 days ago). I
&gt;&gt; can also see that on two nodes (of 5) last AAE exchanges happened &gt; 20 days
&gt;&gt; ago.
&gt;&gt;
&gt;&gt; For now I have issued
&gt;&gt; ` riak\_core\_util:rpc\_every\_member\_ann(yz\_entropy\_mgr, expire\_trees, [],
&gt;&gt; 5000).` on each node again. I will wait 10 days more but I don't think that
&gt;&gt; will fix anything.
&gt;&gt;
&gt;&gt;
&gt;&gt; On 25 March 2016 at 09:28, Oleksiy Krivoshey  wrote:
&gt;&gt;
&gt;&gt;&gt; One interesting moment happened when I tried removing the index:
&gt;&gt;&gt;
&gt;&gt;&gt; - this index was associated with a bucket type, called fs\_chunks
&gt;&gt;&gt; - so I first called RpbSetBucketTypeReq to set search\_index: \_dont\_index\_
&gt;&gt;&gt; - i then tried to remove the index with RpbYokozunaIndexDeleteReq which
&gt;&gt;&gt; failed with "index is in use" and list of all buckets of the fs\_chunks type
&gt;&gt;&gt; - for some reason all these buckets had their own search\_index property
&gt;&gt;&gt; set to that same index
&gt;&gt;&gt;
&gt;&gt;&gt; How can this happen if I definitely never set the search\_index property
&gt;&gt;&gt; per bucket?
&gt;&gt;&gt;
&gt;&gt;&gt; On 24 March 2016 at 22:41, Oleksiy Krivoshey  wrote:
&gt;&gt;&gt;
&gt;&gt;&gt;&gt; OK!
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; On 24 March 2016 at 21:11, Magnus Kessler  wrote:
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; Hi Oleksiy,
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; On 24 March 2016 at 14:55, Oleksiy Krivoshey 
&gt;&gt;&gt;&gt;&gt; wrote:
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt; Hi Magnus,
&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt; Thanks! I guess I will go with index deletion because I've already
&gt;&gt;&gt;&gt;&gt;&gt; tried expiring the trees before.
&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt; Do I need to delete AAE data somehow or removing the index is enough?
&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; If you expire the AAE trees with the commands I posted earlier, there
&gt;&gt;&gt;&gt;&gt; should be no need to remove the AAE data directories manually.
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; I hope this works for you. Please monitor the tree rebuild and
&gt;&gt;&gt;&gt;&gt; exchanges with `riak-admin search aae-status` for the next few days. In
&gt;&gt;&gt;&gt;&gt; particular the exchanges should be ongoing on a continuous basis once all
&gt;&gt;&gt;&gt;&gt; trees have been rebuilt. If they don't, please let me know. At that point
&gt;&gt;&gt;&gt;&gt; you should also gather `riak-debug` output from all nodes before it gets
&gt;&gt;&gt;&gt;&gt; rotated out after 5 days by default.
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; Kind Regards,
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; Magnus
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt; On 24 March 2016 at 13:28, Magnus Kessler  wrote:
&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Hi Oleksiy,
&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt; As a first step, I suggest to simply expire the Yokozuna AAE trees
&gt;&gt;&gt;&gt;&gt;&gt;&gt; again if the output of `riak-admin search aae-status` still suggests 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; that
&gt;&gt;&gt;&gt;&gt;&gt;&gt; no recent exchanges have taken place. To do this, run `riak attach` on 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; one
&gt;&gt;&gt;&gt;&gt;&gt;&gt; node and then
&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt; riak\_core\_util:rpc\_every\_member\_ann(yz\_entropy\_mgr, expire\_trees, [], 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 5000).
&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Exit from the riak console with `Ctrl+G q`.
&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Depending on your settings and amount of data the full index should
&gt;&gt;&gt;&gt;&gt;&gt;&gt; be rebuilt within the next 2.5 days (for a cluster with ring size 128 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; and
&gt;&gt;&gt;&gt;&gt;&gt;&gt; default settings). You can monitor the progress with `riak-admin search
&gt;&gt;&gt;&gt;&gt;&gt;&gt; aae-status` and also in the logs, which should have messages along the
&gt;&gt;&gt;&gt;&gt;&gt;&gt; lines of
&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 2016-03-24 10:28:25.372 [info]
&gt;&gt;&gt;&gt;&gt;&gt;&gt; &lt;0.4647.6477&gt;@yz\_exchange\_fsm:key\_exchange:179 Repaired 83055 keys 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; during
&gt;&gt;&gt;&gt;&gt;&gt;&gt; active anti-entropy exchange of partition
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 1210306043414653979137426502093171875652569137152 for preflist
&gt;&gt;&gt;&gt;&gt;&gt;&gt; {1164634117248063262943561351070788031288321245184,3}
&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Re-indexing can put additional strain on the cluster and may cause
&gt;&gt;&gt;&gt;&gt;&gt;&gt; elevated latency on a cluster already under heavy load. Please monitor 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; the
&gt;&gt;&gt;&gt;&gt;&gt;&gt; response times while the cluster is re-indexing data.
&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt; If the cluster load allows it, you can force more rapid re-indexing
&gt;&gt;&gt;&gt;&gt;&gt;&gt; by changing a few parameters. Again at the `riak attach` console, run
&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt; riak\_core\_util:rpc\_every\_member\_ann(application, set\_env, [yokozuna, 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; anti\_entropy\_build\_limit, {4, 60000}], 5000).
&gt;&gt;&gt;&gt;&gt;&gt;&gt; riak\_core\_util:rpc\_every\_member\_ann(application, set\_env, [yokozuna, 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; anti\_entropy\_concurrency, 5], 5000).
&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt; This will allow up to 4 trees per node to be built/exchanged per
&gt;&gt;&gt;&gt;&gt;&gt;&gt; hour, with up to 5 concurrent exchanges throughout the cluster. To 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; return
&gt;&gt;&gt;&gt;&gt;&gt;&gt; back to the default settings, use
&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt; riak\_core\_util:rpc\_every\_member\_ann(application, set\_env, [yokozuna, 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; anti\_entropy\_build\_limit, {1, 360000}], 5000).
&gt;&gt;&gt;&gt;&gt;&gt;&gt; riak\_core\_util:rpc\_every\_member\_ann(application, set\_env, [yokozuna, 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; anti\_entropy\_concurrency, 2], 5000).
&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt; If the cluster still doesn't make any progress with automatically
&gt;&gt;&gt;&gt;&gt;&gt;&gt; re-indexing data, the next steps are pretty much what you already
&gt;&gt;&gt;&gt;&gt;&gt;&gt; suggested, to drop the existing index and re-index from scratch. I'm
&gt;&gt;&gt;&gt;&gt;&gt;&gt; assuming that losing the indexes temporarily is acceptable to you at 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; this
&gt;&gt;&gt;&gt;&gt;&gt;&gt; point.
&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Using any client API that supports RpbYokozunaIndexDeleteReq, you
&gt;&gt;&gt;&gt;&gt;&gt;&gt; can drop the index from all Solr instances, losing any data stored there
&gt;&gt;&gt;&gt;&gt;&gt;&gt; immediately. Next, you'll have to re-create the index. I have tried this
&gt;&gt;&gt;&gt;&gt;&gt;&gt; with the python API, where I deleted the index and re-created it with 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; the
&gt;&gt;&gt;&gt;&gt;&gt;&gt; same already uploaded schema:
&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt; from riak import RiakClient
&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt; c = RiakClient()
&gt;&gt;&gt;&gt;&gt;&gt;&gt; c.delete\_search\_index('my\_index')
&gt;&gt;&gt;&gt;&gt;&gt;&gt; c.create\_search\_index('my\_index', 'my\_schema')
&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Note that simply deleting the index does not remove it's existing
&gt;&gt;&gt;&gt;&gt;&gt;&gt; association with any bucket or bucket type. Any PUT operations on these
&gt;&gt;&gt;&gt;&gt;&gt;&gt; buckets will lead to indexing failures being logged until the index has
&gt;&gt;&gt;&gt;&gt;&gt;&gt; been recreated. However, this also means that no separate operation in
&gt;&gt;&gt;&gt;&gt;&gt;&gt; `riak-admin` is required to associate the newly recreated index with the
&gt;&gt;&gt;&gt;&gt;&gt;&gt; buckets again.
&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt; After recreating the index expire the trees as explained previously.
&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Let us know if this solves your issue.
&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Kind Regards,
&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Magnus
&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt; On 24 March 2016 at 08:44, Oleksiy Krivoshey 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; wrote:
&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; This is how things are looking after two weeks:
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; - there are no solr indexing issues for a long period (2 weeks)
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; - there are no yokozuna errors at all for 2 weeks
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; - there is an index with all empty schema, just \_yz\_\* fields,
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; objects stored in a bucket(s) are binary and so are not analysed by 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; yokozuna
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; - same yokozuna query repeated gives different number
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; for num\_found, typically the difference between real number of keys in 
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; a
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; bucket and num\_found is about 25%
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; - number of keys repaired by AAE (according to logs) is about 1-2
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; per few hours (number of keys "missing" in index is close to 1,000,000)
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Should I now try to delete the index and yokozuna AAE data and wait
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; another 2 weeks? If yes - how should I delete the index and AAE data?
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Will RpbYokozunaIndexDeleteReq be enough?
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt; --
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Magnus Kessler
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Client Services Engineer
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Basho Technologies Limited
&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Registered Office - 8 Lincoln’s Inn Fields London WC2A 3BP Reg
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 07970431
&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; --
&gt;&gt;&gt;&gt;&gt; Magnus Kessler
&gt;&gt;&gt;&gt;&gt; Client Services Engineer
&gt;&gt;&gt;&gt;&gt; Basho Technologies Limited
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; Registered Office - 8 Lincoln’s Inn Fields London WC2A 3BP Reg 07970431
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;
&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;
&gt;
&gt;
&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;
&gt;


search-aae.tar.gz
Description: GNU Zip compressed data
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

