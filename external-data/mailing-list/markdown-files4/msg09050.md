---
title: "Re: riak cluster mitosis"
description: ""
project: community
lastmod: 2012-10-24T16:37:14-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg09050"
mailinglist_parent_id: "msg09024"
author_name: "Matt Black"
project_section: "mailinglistitem"
sent_date: 2012-10-24T16:37:14-07:00
---


Hi Simon,

My intention for this is to create a pre-prod test environment - so having
an exact replica of the data isn't an issue. Previously I have used the
riak-admin backup/restore, but the file has grown to about 70GB, and so
takes around a day to backup, copy and restore into a new cluster.

Multi-data centre replication looks interesting - and probably we'll end up
using that for redundancy reasons. In the short term I'm looking for an
as-quick-as-possible spawn of a new cluster, with as-accurate-as-possible
data in there..

Asking too much?! Don't want to sound ungrateful ;)


On 24 October 2012 01:07, Simon Vans-Colina  wrote:

&gt; Hi David, Matt,
&gt;
&gt; We've just discussed this and its a bit tricky. For 4 servers with N=3 it
&gt; is possible to stop 2 nodes, (say 3 and 4) and then send a "force-remove"
&gt; command to the remaining servers (1 and 2).
&gt;
&gt; Then you could start (3 and 4), and send \*them\* a force-remove of (1 and
&gt; 2).
&gt;
&gt; This is risky because you're only guaranteed one replica of the data on
&gt; each of the new clusters.
&gt;
&gt; The other risk is that new data written to (1 and 2) will not be
&gt; replicated over to (3 and 4). Are you planning on re-joining the cluster
&gt; back up afterwards?
&gt;
&gt; There's better ways than doing "mitotis" (although i love the name). Look
&gt; into Multi Data Center Replication, or just add new nodes and do a
&gt; force-replace. If the data is small enough, you could use the backup commn
&gt;
&gt; Hope i got this right, i'm quite new to this myself.
&gt;
&gt; Cheers
&gt;
&gt; On Thu, Oct 18, 2012 at 10:35 PM, David Lowell  wrote:
&gt;
&gt;&gt; If I have a cluster of say, 4 nodes, is it possible to split that cluster
&gt;&gt; into two clusters of 2 nodes, each with a full complement of the original
&gt;&gt; cluster's data, while the data is continuously being served? Obviously, we
&gt;&gt; would require that the data be small enough to fit on 2 nodes.
&gt;&gt;
&gt;&gt; Thanks for your help,
&gt;&gt;
&gt;&gt; Dave
&gt;&gt;
&gt;&gt; --
&gt;&gt; Dave Lowell
&gt;&gt; d...@connectv.com
&gt;&gt;
&gt;&gt;
&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt; riak-users mailing list
&gt;&gt; riak-users@lists.basho.com
&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;&gt;
&gt;&gt;
&gt;
&gt;
&gt; --
&gt; Simon Vans-Colina - Client Services Engineer - Basho
&gt;
&gt; Tel:+44 744 791 4640
&gt; Twitter: @simonvc
&gt;
&gt;
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

