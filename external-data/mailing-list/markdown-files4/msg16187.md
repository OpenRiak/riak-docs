---
title: "Re: Recommended way to delete keys"
description: ""
project: community
lastmod: 2015-06-03T11:12:06-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg16187"
mailinglist_parent_id: "msg16186"
author_name: "Peter Herndon"
project_section: "mailinglistitem"
sent_date: 2015-06-03T11:12:06-07:00
---


Sadly, this is a production cluster already using leveldb as the backend. With 
that constraint in mind, and rebuilding the cluster not really being an option 
to enable multi-backends or bitcask, what would our best approach be? 

Thanks!

—Peter

&gt; On Jun 3, 2015, at 12:09 PM, Alexander Sicular  wrote:
&gt; 
&gt; We are actively investigating better options for deletion of large amounts of 
&gt; keys. As Sargun mentioned, deleting the data dir for an entire backend via an 
&gt; operationalized rolling restart is probably the best approach right now for 
&gt; killing large amounts of keys. 
&gt; 
&gt; But if your key space can fit in memory the best way to kill keys is to use 
&gt; bitcask ttl if that's an option. 1. If you can even use bitcask in your 
&gt; environment due to the memory overhead and 2. If your use case allows for 
&gt; ttls which it may considering you may already be using time bound buckets....
&gt; 
&gt; -Alexander 
&gt; 
&gt; @siculars
&gt; http://siculars.posthaven.com
&gt; 
&gt; Sent from my iRotaryPhone
&gt; 
&gt; On Jun 3, 2015, at 09:54, Sargun Dhillon  wrote:
&gt; 
&gt;&gt; You could map your keys to a given bucket, and that bucket to a given 
&gt;&gt; backend using multi\_backend. There is some cost to having lots of backends 
&gt;&gt; (memory overhead, FDs, etc...). When you want to do a mass drop, you could 
&gt;&gt; down the node, and delete that given backend, and bring it up. Caveat: AAE, 
&gt;&gt; MDC, nor mutable data play well with this scenario. 
&gt;&gt; 
&gt;&gt; On Wed, Jun 3, 2015 at 10:43 AM, Peter Herndon  wrote:
&gt;&gt; Hi list,
&gt;&gt; 
&gt;&gt; We’re looking for the best way to handle large scale expiration of 
&gt;&gt; no-longer-useful data stored in Riak. We asked a while back, and the 
&gt;&gt; recommendation was to store the data in time-segmented buckets (bucket per 
&gt;&gt; day or per month), query on the current buckets, and use the streaming list 
&gt;&gt; keys API to handle slowly deleting the buckets that have aged out.
&gt;&gt; 
&gt;&gt; Is that still the best approach for doing this kind of task? Or is there a 
&gt;&gt; better approach?
&gt;&gt; 
&gt;&gt; Thanks!
&gt;&gt; 
&gt;&gt; —Peter Herndon
&gt;&gt; Sr. Application Engineer
&gt;&gt; @Bitly
&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt; riak-users mailing list
&gt;&gt; riak-users@lists.basho.com
&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;&gt; 
&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt; riak-users mailing list
&gt;&gt; riak-users@lists.basho.com
&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com


\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

