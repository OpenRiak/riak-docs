---
title: "Re: Riak cluster-f#$%"
description: ""
project: community
lastmod: 2012-10-02T08:01:34-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg08786"
mailinglist_parent_id: "msg08773"
author_name: "Callixte Cauchois"
project_section: "mailinglistitem"
sent_date: 2012-10-02T08:01:34-07:00
---


I understand that having everything on one box have side effect and that I
may be limited by disk IOs. But, still, I do not understand why the same
bucket can use different engine on different node and why Riak Control was
reporting everything as OK whereas some nodes were not responding.

On Mon, Oct 1, 2012 at 2:07 PM, Michael Truog  wrote:

&gt; \*\*
&gt; Still, doesn't that failure show a typical overload of Riak's usage of
&gt; mochiglobal (i.e., the code\_server needing to lock all Erlang schedulers)?
&gt; I understand that running more than one node on a single machine is not
&gt; realistic deployment. However, I don't see why it would cause errors,
&gt; unless Riak was unable to handle the requests incoming.
&gt;
&gt;
&gt; On 10/01/2012 01:54 PM, Alexander Sicular wrote:
&gt;
&gt; Any time you overload one box you run into all sorts of i/o dreck, screw
&gt; with your conf files and mess with your versions you just have too many
&gt; variables in the mix to get anything meaningful out of what you were trying
&gt; to do. Since this is a test just tear the whole thing down and start
&gt; clean.
&gt;
&gt; If you want to dev test your app just use one node and dial the n val
&gt; down to one in the app.config, which isn't actually there so you'll have to
&gt; add it manually to the riak\_core section like so (with some other stuff):
&gt;
&gt; {default\_bucket\_props, [{n\_val,1},
&gt; {allow\_mult,false},
&gt; {last\_write\_wins,false},
&gt; {precommit, []},
&gt; {postcommit, []},
&gt; {chash\_keyfun, {riak\_core\_util, chash\_std\_keyfun}}
&gt; ]}
&gt;
&gt; (Hey Basho people, that stuff should be in the app.config file by
&gt; default. Making people go fish for it and figure out how and where to add
&gt; this stuff is kinda unnecessary. Here is an example of a great conf file
&gt; with everything you can conf and a whole bunch of docs:
&gt; https://github.com/antirez/redis/blob/unstable/redis.conf ).
&gt;
&gt; If you want to performance test your app make your dev system as similar
&gt; to your prod system as possible and knock it out.
&gt;
&gt;
&gt; -Alexander Sicular
&gt;
&gt; @siculars
&gt;
&gt; On Oct 1, 2012, at 4:30 PM, Callixte Cauchois wrote:
&gt;
&gt; Thank you, but can you explain a bit more?
&gt; I mean I understand why it is a bad thing with regards to reliability and
&gt; in case of hardware issues. But does it have also an impact on the
&gt; behaviour when the hardware is performing correctly and the load on the
&gt; machines are the same?
&gt;
&gt; On Mon, Oct 1, 2012 at 1:25 PM, Alexander Sicular wrote:
&gt;
&gt; Inline.
&gt;
&gt; -Alexander Sicular
&gt;
&gt; @siculars
&gt;
&gt; On Oct 1, 2012, at 3:23 PM, Callixte Cauchois wrote:
&gt;
&gt; &gt; Hi there,
&gt; &gt;
&gt; &gt; so, I am currently evaluating Riak to see how it can fit in our
&gt; platform. To do so I have set up a cluster of 4 nodes on SmartOS, all of
&gt; them on the same physical box.
&gt;
&gt; Mistake. Just stop here. Everything else doesn't matter. Do not put all
&gt; your virtual machines (riak nodes) on one physical machine. Put em on
&gt; different physical machines. Fix the config files and try again.
&gt;
&gt; &gt; I then built a simple application in node.js that get log events from
&gt; our production system through a RabbitMQ queue and store them in my
&gt; cluster. I let Riak generate the ids, but I have added two secondary
&gt; indices to be able to retrieve more easily all the log events that belong
&gt; to a single session.
&gt; &gt; Everything was going fine, events come around 130 messages per second
&gt; are easily ingested by Riak. When stop it and then restart it, there is a
&gt; bit of an issue as the events are read from the queue at 1500 messages per
&gt; second and the insertion times go up, so I need some retries to actually
&gt; store everything.
&gt; &gt; I wanted to tweak the LevelDB params to increase the throughput. To do
&gt; so, I first upgraded from 1.1.6 to 1.2.0. I chose what I thought was the
&gt; safest way: node by node, I have them leave the cluster, then I upgrade,
&gt; then join again. During the whole process I kept inserting.
&gt; &gt; It went quite well. But, when I ran some queries using 2i, it gave me
&gt; errors and I realized that for two of my four nodes, I forgot to put back
&gt; eLevelDB as the default engine. As soon as I ran this query, everything
&gt; went havoc, a lot of inserts failed, some nodes where not reachable using
&gt; the ping url.
&gt; &gt; I changed the default engine and restarted those nodes, nothing changed.
&gt; I tried to make them leave the cluster, after two days, they are still
&gt; leaving. Riak-admin transfers tells that a lot of transfers need to occur,
&gt; but the system is stuck: the numbers there do not change.
&gt; &gt;
&gt; &gt; I guess I have done several things wrong. It is test data, so it doesn't
&gt; really matter if I loose data or if I have to re-start from scratch, but I
&gt; want to understand what have gone wrong how I could have fixed it. Or if I
&gt; even can recover from there now.
&gt; &gt;
&gt; &gt; Thank you.
&gt; &gt; C.
&gt; &gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt; &gt; riak-users mailing list
&gt; &gt; riak-users@lists.basho.com
&gt; &gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;
&gt;
&gt;
&gt;
&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt; riak-users mailing 
&gt; listriak-users@lists.basho.comhttp://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;
&gt;
&gt;
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

