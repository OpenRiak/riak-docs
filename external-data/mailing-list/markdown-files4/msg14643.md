---
title: "Re: Issues with search (2.0)"
description: ""
project: community
lastmod: 2014-08-11T08:15:40-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg14643"
mailinglist_parent_id: "msg14642"
author_name: "Eric Redmond"
project_section: "mailinglistitem"
sent_date: 2014-08-11T08:15:40-07:00
---


If Solr is stumbling over bad data, your node's solr.log should be filled up. 
If Yokozuna is stumbling over bad data that it's trying to send Solr in a loop, 
the console.log should be full. If yokozuna is going ahead and indexing bad 
values (such as unparsable json), it will go ahead and index a blank object 
with \_yz\_err (just search for existence). If you have a case of sibling 
explosion, you'll have many duplicates of the same object with different 
\_yz\_vtag fields (again search for existence).

You said it's not a resource issue, but just to rule that out, how much RAM 
does each node have? Also, how much is made available to Solr? You can adjust 
the max heap size given to Solr in riak.conf, by changing 
search.solr.jvm\_options max heap size values from -Xmx1g to -Xmx2g or more.

Eric


On Aug 11, 2014, at 8:03 AM, Chaim Solomon  wrote:

&gt; Hi,
&gt; 
&gt; I don't think that it is a resource issue now.
&gt; 
&gt; After removing the data, the other nodes had low load and are handling the 
&gt; workload just fine.
&gt; And the Java process - when it crashed - was really dead, on shutting down 
&gt; Riak it stayed around and needed a -9 to go away.
&gt; 
&gt; I don't think the disks are a problem but rather suspect that a crash may 
&gt; have caused Solr to stumble over bad data and then crash.
&gt; 
&gt; Chaim Solomon
&gt; 
&gt; 
&gt; 
&gt; On Mon, Aug 11, 2014 at 5:47 PM, Jordan West  wrote:
&gt; Chaim,
&gt; 
&gt; Some comments inline:
&gt; 
&gt; On Mon, Aug 11, 2014 at 4:14 AM, Chaim Solomon  
&gt; wrote:
&gt; Hi,
&gt; 
&gt; I've been running into an issue with the yz search acting up.
&gt; 
&gt; I've been getting a lot of these: 
&gt; 
&gt; 2014-08-11 06:45:22.005 [error] &lt;0.913.0&gt;@yz\_kv:index:206 failed to index 
&gt; object {&lt;&lt;"bucketname"&gt;&gt;,&lt;&lt;"123"&gt;&gt;} with error {"Failed to index 
&gt; docs",{error,req\_timedout}} because [{yz\_solr,index,3,[{file,"s
&gt; rc/yz\_solr.erl"},{line,192}]},{yz\_kv,index,7,[{file,"src/yz\_kv.erl"},{line,258}]},{yz\_kv,index,3,[{file,
&gt; "src/yz\_kv.erl"},{line,193}]},{riak\_kv\_vnode,actual\_put,6,[{file,"src/riak\_kv\_vnode.erl"},{line,1416}]},
&gt; {riak\_kv\_vnode,perform\_put,3,[{file,"src/riak\_kv\_vnode.erl"},{line,1404}]},{riak\_kv\_vnode,do\_put,7,[{fil
&gt; e,"src/riak\_kv\_vnode.erl"},{line,1199}]},{riak\_kv\_vnode,handle\_command,3,[{file,"src/riak\_kv\_vnode.erl"}
&gt; ,{line,485}]},{riak\_core\_vnode,vnode\_command,3,[{file,"src/riak\_core\_vnode.erl"},{line,345}]}]
&gt; 
&gt; and the Java process uses a lot of CPU and eventually runs out of memory or 
&gt; something like that and gets stuck. Killing the process gets the cluster back 
&gt; up and running.
&gt; 
&gt; I am guessing that it may be data corruption on the yz data on one node. 
&gt; 
&gt; Clearing away the yz data on that node and restarting riak makes the system 
&gt; work again - and I guess AAE will rebuild the index.
&gt; 
&gt; 
&gt; This sounds very similar to the issue last week. I would certainly like to 
&gt; rule out any sort of data corruption (are you thinking your disks are 
&gt; corrupting the data or are you assuming Solr is?).
&gt; 
&gt; However, it is also possible, like the last issue, that the node/cluster 
&gt; simply does not have enough memory. When you delete the data Solr no longer 
&gt; has anything to cache in-memory thus using significantly less. As discussed, 
&gt; the recommended minimum 
&gt; 
&gt; But I'm wondering why a crashing Java on one node practically takes down the 
&gt; search on the cluster. Shouldn't Riak be more resilient than that?
&gt; 
&gt; The hard part here is, at least initially, the Java process doesn't crash, it 
&gt; just starts to timeout. In distributed systems a slow-node is often worse 
&gt; than a down node. Riak, prior to 1.4 had something called "health check" that 
&gt; would mark a node down in this situation. Unfortunately in some workloads, 
&gt; and I believe given your cluster's limited resources it would happen here, 
&gt; this often results in excessive work being offloaded to another node, which 
&gt; also does not have sufficient resources and around we go until the entire 
&gt; cluster falls over. A capacity problem, typically, can only be solved by 
&gt; adding more capacity. 
&gt; 
&gt; 
&gt; Is there a explicit reindex command for the full text search subsystem?
&gt; 
&gt; Could Riak keep an eye on the java process and restart it if it crashes or 
&gt; runs away?
&gt; 
&gt; 
&gt; Riak does manage the JVM process (starting/stopping/restarting) .I agree that 
&gt; if we could include run-away process, like in your case, that would be even 
&gt; better. I would have to think a bit more about how this would work (to 
&gt; prevent the same problems mentioned above with the old-style health check)
&gt; 
&gt; Jordan
&gt; 
&gt; 
&gt; Chaim Solomon
&gt; 
&gt; 
&gt; 
&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt; 
&gt; 
&gt; 
&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

