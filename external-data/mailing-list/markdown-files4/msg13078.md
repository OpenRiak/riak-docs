---
title: "Re: Runaway \"Failed to compact\" errors"
description: ""
project: community
lastmod: 2013-11-24T13:01:07-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg13078"
mailinglist_parent_id: "msg13076"
author_name: "Justin Long"
project_section: "mailinglistitem"
sent_date: 2013-11-24T13:01:07-08:00
---


Hi Richard,

Result turned up empty on the failed node. Hereâ€™s what is in vm.args:

------------------------------------------------------

# Name of the riak node
-name riak@192.168.3.3

## Cookie for distributed erlang. All nodes in the same cluster
## should use the same cookie or they will not be able to communicate.
-setcookie riak

## Heartbeat management; auto-restarts VM if it dies or becomes unresponsive
## (Disabled by default..use with caution!)
##-heart

## Enable kernel poll and a few async threads
+K true
+A 64

## Treat error\_logger warnings as warnings
+W w

## Increase number of concurrent ports/sockets
-env ERL\_MAX\_PORTS 4096

## Tweak GC to run more often
-env ERL\_FULLSWEEP\_AFTER 0

## Set the location of crash dumps
-env ERL\_CRASH\_DUMP /var/log/riak/erl\_crash.dump

## Raise the ETS table limit
-env ERL\_MAX\_ETS\_TABLES 22000

------------------------------------------------------


Before I received your email, I have since isolated the node and force-removed 
it from the cluster. In the meantime, I brought up a new fresh node and joined 
it to the cluster. When Riak went to handoff some of the RiakSearch indexes 
here is what was popping up in console.log:

------------------------------------------------------

&lt;0.4262.0&gt;@merge\_index\_backend:async\_fold\_fun:116 failed to iterate the index 
with reason 
{badarg,[{erlang,binary\_to\_term,[&lt;&lt;131,104,3,109,0,0,0,25,99,111,108,108,101,99,116,111,114,45,99,111,108,108,101,99,116,45,116,119,105,116,116,101,114,109,0,0,0,14,100,97,116,97,95,102,111,108,108,111,119,101,114,115,109,0,0,128,199,123,34,105,100,115,34,58,91,49,52,52,55,51,54,54,57,53,48,44,53,48,48,55,53,57,48,55,44,52,51,56,49,55,53,52,56,53,44,49,51,54,53,49,50,49,52,50,44,52,54,50,52,52,54,56,51,44,49,48,55,57,56,55,49,50,48,48,44,55,55,48,56,51,54,55,57,44,50,56,51,56,51,57,55,56,44,49,57,50,48,55,50,55,51,48,44,51,57,54,57,56,56,57,56,55,44,50,56,48,50,54,51,56,48,52,44,53,57,50,56,56,53,50,51,48,44,49,50,52,55,53,56,57,53,55,56,44,49,55,51,56,56,51,53,52,50,44,49,53,56,57,54,51,50,50,50,48,44,53,53,49,51,57,57,51,48,49,44,50,50,48,53,52,55,52,55,55,54,44,49,51,51,52,57,56,57,56,50,53,44,51,49,50,51,57,53,55,54,50&gt;&gt;],[]},{mi\_segment,iterate\_all\_bytes,2,[{file,"src/mi\_segment.erl"},{line,167}]},{mi\_server,'-group\_iterator/2-fun-1-',2,[{file,"src/mi\_server.erl"},{line,725}]},{mi\_server,'-group\_iterator/2-fun-0-',2,[{file,"src/mi\_server.erl"},{line,722}]},{mi\_server,iterate2,5,[{file,"src/mi\_server.erl"},{line,693}]}]}
 and partial acc 
{{ho\_acc,226,ok,#Fun,riak\_search\_vnode,&lt;0.4042.0&gt;,#Port&lt;0.754041&gt;,{274031556999544297163190906134303066185487351808,274031556999544297163190906134303066185487351808},{ho\_stats,{1385,326398,498434},undefined,14225,2426123},gen\_tcp,50226},{{&lt;&lt;"collector-collect-instagram-cache"&gt;&gt;,{&lt;&lt;"data\_follows"&gt;&gt;,&lt;&lt;"who"&gt;&gt;}},[{&lt;&lt;"3700758"&gt;&gt;,[{p,[561]}],1383759429413536},{&lt;&lt;"368835984"&gt;&gt;,[{p,[297,303]}],1383611963556763},{&lt;&lt;"368835984"&gt;&gt;,[{p,[298,304]}],1383756713657753},{&lt;&lt;"31715058"&gt;&gt;,[{p,[325]}],1383611996352193}]},4}
2013-11-24 20:53:17.468 [error] 
&lt;0.16054.14&gt;@riak\_core\_handoff\_sender:start\_fold:215 ownership\_handoff transfer 
of riak\_search\_vnode from 'riak@192.168.3.2' 
274031556999544297163190906134303066185487351808 to 'riak@192.168.3.13' 
274031556999544297163190906134303066185487351808 failed because of 
error:{badmatch,{error,{badarg,[{erlang,binary\_to\_term,[&lt;&lt;131,104,3,109,0,0,0,25,99,111,108,108,101,99,116,111,114,45,99,111,108,108,101,99,116,45,116,119,105,116,116,101,114,109,0,0,0,14,100,97,116,97,95,102,111,108,108,111,119,101,114,115,109,0,0,128,199,123,34,105,100,115,34,58,91,49,52,52,55,51,54,54,57,53,48,44,53,48,48,55,53,57,48,55,44,52,51,56,49,55,53,52,56,53,44,49,51,54,53,49,50,49,52,50,44,52,54,50,52,52,54,56,51,44,49,48,55,57,56,55,49,50,48,48,44,55,55,48,56,51,54,55,57,44,50,56,51,56,51,57,55,56,44,49,57,50,48,55,50,55,51,48,44,51,57,54,57,56,56,57,56,55,44,50,56,48,50,54,51,56,48,52,44,53,57,50,56,56,53,50,51,48,44,49,50,52,55,53,56,57,53,55,56,44,49,55,51,56,56,51,53,52,50,44,49,53,56,57,54,51,50,50,50,48,44,53,53,49,51,57,57,51,48,49,44,50,50,48,53,52,55,52,55,55,54,44,49,51,51,52,57,56,57,56,50,53,44,51,49,50,51,57,53,55,54,50&gt;&gt;],[]},{mi\_segment,iterate\_all\_bytes,2,[{file,"src/mi\_segment.erl"},{line,167}]},{mi\_server,'-group\_iterator/2-fun-1-',2,[{file,"src/mi\_server.erl"},{line,725}]},{mi\_server,'-group\_iterator/2-fun-0-',2,[{file,"src/mi\_server.erl"},{line,722}]},{mi\_server,iterate2,5,[{file,"src/mi\_server.erl"},{line,693}]}]},{{ho\_acc,226,ok,#Fun,riak\_search\_vnode,&lt;0.4042.0&gt;,#Port&lt;0.754041&gt;,{274031556999544297163190906134303066185487351808,274031556999544297163190906134303066185487351808},{ho\_stats,{1385,326398,498434},undefined,14225,2426123},gen\_tcp,50226},{{&lt;&lt;"collector-collect-instagram-cache"&gt;&gt;,{&lt;&lt;"data\_follows"&gt;&gt;,&lt;&lt;"who"&gt;&gt;}},[{&lt;&lt;"3700758"&gt;&gt;,[{p,[561]}],1383759429413536},{&lt;&lt;"368835984"&gt;&gt;,[{p,[297,303]}],1383611963556763},{&lt;&lt;"368835984"&gt;&gt;,[{p,[298,304]}],1383756713657753},{&lt;&lt;"31715058"&gt;&gt;,[{p,[325]}],1383611996352193}]},4}}}
 
[{riak\_core\_handoff\_sender,start\_fold,5,[{file,"src/riak\_core\_handoff\_sender.erl"},{line,161}]}]

------------------------------------------------------

I am aware that values that bucket might be larger than most of our other 
objects. Not sure if that would cause the issues, though. Thanks for your help!

J



On Nov 24, 2013, at 12:51 PM, Richard Shaw  wrote:

&gt; Hi Justin,
&gt; 
&gt; Please can you run this command to look for compaction errors in the leveldb 
&gt; logs on the node with the crash log entries
&gt; 
&gt; grep -R "Compaction error" /var/lib/riak/leveldb/\*/LOG
&gt; 
&gt; Where the path matches your path to the leveldb dir
&gt; 
&gt; Thanks
&gt; 
&gt; Richard
&gt; 
&gt; 
&gt; 
&gt; On 24 November 2013 10:45, Justin Long  wrote:
&gt; Hello everyone,
&gt; 
&gt; Our Riak cluster has failed after what seems to be an issue in LevelDB. 
&gt; Noticed that a process running a segment compact has started to throw errors 
&gt; non-stop. I opened a Stack Overflow question here where you will find a lot 
&gt; of log data: 
&gt; http://stackoverflow.com/questions/20172878/riak-is-throwing-failed-to-compact-like-crazy
&gt; 
&gt; Here is exactly what we're getting in console.log:
&gt; 
&gt; 2013-11-24 10:38:46.803 [info] 
&gt; &lt;0.19760.0&gt;@riak\_core\_handoff\_receiver:process\_message:99 Receiving handoff 
&gt; data for partition 
&gt; riak\_search\_vnode:1050454301831586472458898473514828420377701515264
&gt; 2013-11-24 10:38:47.239 [info] 
&gt; &lt;0.19760.0&gt;@riak\_core\_handoff\_receiver:handle\_info:69 Handoff receiver for 
&gt; partition 1050454301831586472458898473514828420377701515264 exited after 
&gt; processing 5409 objects
&gt; 2013-11-24 10:38:49.743 [error] emulator Error in process &lt;0.19767.0&gt; on node 
&gt; 'riak@192.168.3.3' with exit value: {badarg,[{erlang,binary\_to\_term,[&lt;&lt;260 
&gt; bytes&gt;&gt;],[]},{mi\_segment,iterate\_all\_bytes,2,[{file,"src/mi\_segment.erl"},{line,167}]},{mi\_server,'-group\_iterator/2-fun-0-',2,[{file,"src/mi\_server.erl"},{line,722}]},{mi\_server,'-group\_iterator/2-fun-1-'...
&gt; 
&gt; 
&gt; 2013-11-24 10:38:49.743 [error] &lt;0.580.0&gt;@mi\_scheduler:worker\_loop:141 Failed 
&gt; to compact &lt;0.11868.0&gt;: 
&gt; {badarg,[{erlang,binary\_to\_term,[&lt;&lt;131,104,3,109,0,0,0,25,99,111,108,108,101,99,116,111,114,45,99,111,108,108,101,99,116,45,116,119,105,116,116,101,114,109,0,0,0,14,100,97,116,97,95,102,111,108,108,111,119,101,114,115,109,0,0,128,203,123,34,105,100,115,34,58,91,49,54,50,51,53,50,50,50,50,51,44,49,55,51,55,51,52,52,50,44,49,50,56,51,52,52,56,55,51,57,44,51,57,56,56,57,56,50,51,52,44,49,52,52,55,51,54,54,57,53,48,44,53,48,48,55,53,57,48,55,44,52,51,56,49,55,53,52,56,53,44,49,51,54,53,49,50,49,52,50,44,52,54,50,52,52,54,56,51,44,49,48,55,57,56,55,49,50,48,48,44,55,55,48,56,51,54,55,57,44,50,56,51,56,51,57,55,56,44,49,57,50,48,55,50,55,51,48,44,51,57,54,57,56,56,57,56,55,44,50,56,48,50,54,51,56,48,52,44,53,57,50,56,56,53,50,51,48,44,49,50,52,55,53,56,57,53,55,56,44,49,55,51,56,56,51,53,52,50,44,49,53,56,57,54,51,50,50,50,48,44,53,53,49,51&gt;&gt;],[]},{mi\_segment,iterate\_all\_bytes,2,[{file,"src/mi\_segment.erl"},{line,167}]},{mi\_server,'-group\_iterator/2-fun-0-',2,[{file,"src/mi\_server.erl"},{line,722}]},{mi\_server,'-group\_iterator/2-fun-1-',2,[{file,"src/mi\_server.erl"},{line,725}]},{mi\_server,'-group\_iterator/2-fun-0-',2,[{file,"src/mi\_server.erl"},{line,722}]},{mi\_server,'-group\_iterator/2-fun-1-',2,[{file,"src/mi\_server.erl"},{line,725}]},{mi\_server,'-group\_iterator/2-fun-0-',2,[{file,"src/mi\_server.erl"},{line,722}]},{mi\_segment\_writer,from\_iterator,4,[{file,"src/mi\_segment\_writer.erl"},{line,110}]}]}
&gt; 
&gt; 
&gt; 
&gt; 
&gt; 
&gt; 
&gt; The log is just full of them. Thanks for your help! We need to get this 
&gt; cluster back up ASAP, appreciated!
&gt; 
&gt; - Justin
&gt; 
&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt; 
&gt; 

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

