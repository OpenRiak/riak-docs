---
title: "Re: Single node causing cluster to be extremely slow (leveldb)"
description: ""
project: community
lastmod: 2014-01-10T09:00:25-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg13379"
mailinglist_parent_id: "msg13374"
author_name: "Sean McKibben"
project_section: "mailinglistitem"
sent_date: 2014-01-10T09:00:25-08:00
---


Excellent and informative explanation, thank you very much. We’re very happy 
that our adjustments have returned the cluster to its normal operating 
parameters. Also glad that Riak 2 will be handling this stuff programmatically, 
as prior to your spreadsheet and explanation it was pure voodoo for us. I think 
the automation will significantly decrease the number of animal sacrifices 
needed to appease the levelDB gods! :)

Sean McKibben


On Jan 10, 2014, at 9:18 AM, Matthew Von-Maszewski  wrote:

&gt; Attached is the spreadsheet I used for deriving the cache\_size and 
&gt; max\_open\_files. The general guidelines of the spreadsheet are:
&gt; 
&gt; vnode count: ring size divided by (number of nodes minus one)
&gt; write\_buf\_min/max: don't touch … you will screw up my leveldb tuning
&gt; cache\_size: 8Mbytes is hard minimum
&gt; max\_open\_files: this is NOT a file count in 1.4. It is 4Mbytes times the 
&gt; value. File cache is meta-data size based, not file count.
&gt; 
&gt; lower cache\_size and raise max\_open\_files as necessary to keep "remaining" 
&gt; close to zero AND cover your total file metadata size
&gt; 
&gt; What is file metadata size? I looked at one vnode's LOG file for rough 
&gt; estimates:
&gt; 
&gt; - Your total file count was 1,479 in one vnode
&gt; - You typically hit the 75,000 key limit
&gt; - Key count (75,000) divided into a typical file size is 496 bytes … used 496 
&gt; as average value size
&gt; - Block\_size is 4096. 496 value size goes into block size about 10 times (no 
&gt; need for fractions since block\_size is a threshold, not fixed value)
&gt; - 75,000 total keys in file, 10 keys per block … that means 7,500 keys in 
&gt; file's index … 100 bytes per key is 750,000 bytes of keys in index.
&gt; - bloom filter is 2 bytes per key (all 75,000 keys) or 150,00 bytes
&gt; - metadata loaded into file cache is therefore 750,000 + 150,000 bytes per 
&gt; file or 900,000 bytes.
&gt; - 900,000 bytes per file times 1,479 files is 1,331,100,000 bytes of file 
&gt; cache needed …
&gt; 
&gt; Your original 315 max\_open\_files is 1,279,262,720 in size (315 \* 4Mbytes) … 
&gt; file cache is thrashing since 1,279,262,720 is less than 1,331,100,000.
&gt; 
&gt; I told you 425 as a max\_open\_files setting, spreadsheet has 400 as more 
&gt; conservative number.
&gt; 
&gt; Matthew
&gt; 
&gt; 
&gt; 
&gt; On Jan 10, 2014, at 9:41 AM, Martin May  wrote:
&gt; 
&gt;&gt; Hi Matthew,
&gt;&gt; 
&gt;&gt; We applied this change to node 4, started it up, and it seems much happier 
&gt;&gt; (no crazy CPU). We’re going to keep an eye on it for a little while, and 
&gt;&gt; then apply this setting to all the other nodes as well.
&gt;&gt; 
&gt;&gt; Is there anything we can do to prevent this scenario in the future, or 
&gt;&gt; should the settings you suggested take care of that?
&gt;&gt; 
&gt;&gt; Thanks,
&gt;&gt; Martin
&gt;&gt; 
&gt;&gt; On Jan 10, 2014, at 6:42 AM, Matthew Von-Maszewski  
&gt;&gt; wrote:
&gt;&gt; 
&gt;&gt;&gt; Sean,
&gt;&gt;&gt; 
&gt;&gt;&gt; I did some math based upon the app.config and LOG files. I am guessing 
&gt;&gt;&gt; that you are starting to thrash your file cache.
&gt;&gt;&gt; 
&gt;&gt;&gt; This theory should be easy to prove / disprove. On that one node, change 
&gt;&gt;&gt; the cache\_size and max\_open\_files to:
&gt;&gt;&gt; 
&gt;&gt;&gt; cache\_size 68435456
&gt;&gt;&gt; max\_open\_files 425
&gt;&gt;&gt; 
&gt;&gt;&gt; If I am correct, the node should come up and not cause problems. We are 
&gt;&gt;&gt; trading block cache space for file cache space. A miss in the file cache 
&gt;&gt;&gt; is far more costly than a miss in the block cache.
&gt;&gt;&gt; 
&gt;&gt;&gt; Let me know how this works for you. It is possible that we might want to 
&gt;&gt;&gt; talk about raising your block size slightly to reduce file cache overhead.
&gt;&gt;&gt; 
&gt;&gt;&gt; Matthew
&gt;&gt;&gt; 
&gt;&gt;&gt; On Jan 9, 2014, at 9:33 PM, Sean McKibben  wrote:
&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; We have a 5 node cluster using elevelDB (1.4.2) and 2i, and this afternoon 
&gt;&gt;&gt;&gt; it started responding extremely slowly. CPU on member 4 was extremely high 
&gt;&gt;&gt;&gt; and we restarted that process, but it didn’t help. We temporarily shut 
&gt;&gt;&gt;&gt; down member 4 and cluster speed returned to normal, but as soon as we boot 
&gt;&gt;&gt;&gt; member 4 back up, the cluster performance goes to shit.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; We’ve run in to this before but were able to just start with a fresh set 
&gt;&gt;&gt;&gt; of data after wiping machines as it was before we migrated to this 
&gt;&gt;&gt;&gt; bare-metal cluster. Now it is causing some pretty significant issues and 
&gt;&gt;&gt;&gt; we’re not sure what we can do to get it back to normal, many of our queues 
&gt;&gt;&gt;&gt; are filling up and we’ll probably have to take node 4 off again just so we 
&gt;&gt;&gt;&gt; can provide a regular quality of service.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; We’ve turned off AAE on node 4 but it hasn’t helped. We have some 
&gt;&gt;&gt;&gt; transfers that need to happen but they are going very slowly.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 'riak-admin top’ on node 4 reports this:
&gt;&gt;&gt;&gt; Load: cpu 610 Memory: total 503852 binary 
&gt;&gt;&gt;&gt; 231544
&gt;&gt;&gt;&gt; procs 804 processes 179850 code 
&gt;&gt;&gt;&gt; 11588
&gt;&gt;&gt;&gt; runq 134 atom 533 ets 
&gt;&gt;&gt;&gt; 4581
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Pid Name or Initial Func Time Reds 
&gt;&gt;&gt;&gt; Memory MsgQ Current Function
&gt;&gt;&gt;&gt; -------------------------------------------------------------------------------------------------------------------------------
&gt;&gt;&gt;&gt; &lt;6175.29048.3&gt; proc\_lib:init\_p/5 '-' 462231 
&gt;&gt;&gt;&gt; 51356760 0 mochijson2:json\_bin\_is\_safe/1
&gt;&gt;&gt;&gt; &lt;6175.12281.6&gt; proc\_lib:init\_p/5 '-' 307183 
&gt;&gt;&gt;&gt; 64195856 1 gen\_fsm:loop/7
&gt;&gt;&gt;&gt; &lt;6175.1581.5&gt; proc\_lib:init\_p/5 '-' 286143 
&gt;&gt;&gt;&gt; 41085600 0 mochijson2:json\_bin\_is\_safe/1
&gt;&gt;&gt;&gt; &lt;6175.6659.0&gt; proc\_lib:init\_p/5 '-' 281845 
&gt;&gt;&gt;&gt; 13752 0 sext:decode\_binary/3
&gt;&gt;&gt;&gt; &lt;6175.6666.0&gt; proc\_lib:init\_p/5 '-' 209113 
&gt;&gt;&gt;&gt; 21648 0 sext:decode\_binary/3
&gt;&gt;&gt;&gt; &lt;6175.12219.6&gt; proc\_lib:init\_p/5 '-' 168832 
&gt;&gt;&gt;&gt; 16829200 0 riak\_client:wait\_for\_query\_results/4
&gt;&gt;&gt;&gt; &lt;6175.8403.0&gt; proc\_lib:init\_p/5 '-' 133333 
&gt;&gt;&gt;&gt; 13880 1 eleveldb:iterator\_move/2
&gt;&gt;&gt;&gt; &lt;6175.8813.0&gt; proc\_lib:init\_p/5 '-' 119548 
&gt;&gt;&gt;&gt; 9000 1 eleveldb:iterator/3
&gt;&gt;&gt;&gt; &lt;6175.8411.0&gt; proc\_lib:init\_p/5 '-' 115759 
&gt;&gt;&gt;&gt; 34472 0 riak\_kv\_vnode:'-result\_fun\_ack/2-fun-0-'
&gt;&gt;&gt;&gt; &lt;6175.5679.0&gt; proc\_lib:init\_p/5 '-' 109577 
&gt;&gt;&gt;&gt; 8952 0 riak\_kv\_vnode:'-result\_fun\_ack/2-fun-0-'
&gt;&gt;&gt;&gt; Output server crashed: connection\_lost
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Based on that, is there anything anyone can think to do to try to bring 
&gt;&gt;&gt;&gt; performance back in to the land of usability? Does this thing appear to be 
&gt;&gt;&gt;&gt; something that may have been resolved in 1.4.6 or 1.4.7?
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Only thing we can think of at this point might be to remove or force 
&gt;&gt;&gt;&gt; remove the member and join in a new freshly built one, but last time we 
&gt;&gt;&gt;&gt; attempted that (on a different cluster) our secondary indexes got 
&gt;&gt;&gt;&gt; irreparably damaged and only regained consistency when we copied every 
&gt;&gt;&gt;&gt; individual key to (this) new cluster! Not a good experience :( but i’m 
&gt;&gt;&gt;&gt; hopeful that 1.4.6 may have addressed some of our issues.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Any help is appreciated.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Thank you,
&gt;&gt;&gt;&gt; Sean McKibben
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;&gt; 
&gt; 


\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

