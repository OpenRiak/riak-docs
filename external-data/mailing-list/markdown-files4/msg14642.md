---
title: "Re: Issues with search (2.0)"
description: ""
project: community
lastmod: 2014-08-11T08:04:55-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg14642"
mailinglist_parent_id: "msg14641"
author_name: "Chaim Solomon"
project_section: "mailinglistitem"
sent_date: 2014-08-11T08:04:55-07:00
---


Hi,

I don't think that it is a resource issue now.

After removing the data, the other nodes had low load and are handling the
workload just fine.
And the Java process - when it crashed - was really dead, on shutting down
Riak it stayed around and needed a -9 to go away.

I don't think the disks are a problem but rather suspect that a crash may
have caused Solr to stumble over bad data and then crash.

Chaim Solomon



On Mon, Aug 11, 2014 at 5:47 PM, Jordan West  wrote:

&gt; Chaim,
&gt;
&gt; Some comments inline:
&gt;
&gt; On Mon, Aug 11, 2014 at 4:14 AM, Chaim Solomon  &gt; wrote:
&gt;
&gt;&gt; Hi,
&gt;&gt;
&gt;&gt; I've been running into an issue with the yz search acting up.
&gt;&gt;
&gt;&gt; I've been getting a lot of these:
&gt;&gt;
&gt;&gt; 2014-08-11 06:45:22.005 [error] &lt;0.913.0&gt;@yz\_kv:index:206 failed to index
&gt;&gt; object {&lt;&lt;"bucketname"&gt;&gt;,&lt;&lt;"123"&gt;&gt;} with error {"Failed to index
&gt;&gt; docs",{error,req\_timedout}} because [{yz\_solr,index,3,[{file,"s
&gt;&gt;
&gt;&gt; rc/yz\_solr.erl"},{line,192}]},{yz\_kv,index,7,[{file,"src/yz\_kv.erl"},{line,258}]},{yz\_kv,index,3,[{file,
&gt;&gt;
&gt;&gt; "src/yz\_kv.erl"},{line,193}]},{riak\_kv\_vnode,actual\_put,6,[{file,"src/riak\_kv\_vnode.erl"},{line,1416}]},
&gt;&gt;
&gt;&gt; {riak\_kv\_vnode,perform\_put,3,[{file,"src/riak\_kv\_vnode.erl"},{line,1404}]},{riak\_kv\_vnode,do\_put,7,[{fil
&gt;&gt;
&gt;&gt; e,"src/riak\_kv\_vnode.erl"},{line,1199}]},{riak\_kv\_vnode,handle\_command,3,[{file,"src/riak\_kv\_vnode.erl"}
&gt;&gt;
&gt;&gt; ,{line,485}]},{riak\_core\_vnode,vnode\_command,3,[{file,"src/riak\_core\_vnode.erl"},{line,345}]}]
&gt;&gt;
&gt;&gt; and the Java process uses a lot of CPU and eventually runs out of memory
&gt;&gt; or something like that and gets stuck. Killing the process gets the cluster
&gt;&gt; back up and running.
&gt;&gt;
&gt;&gt; I am guessing that it may be data corruption on the yz data on one node.
&gt;&gt;
&gt;&gt; Clearing away the yz data on that node and restarting riak makes the
&gt;&gt; system work again - and I guess AAE will rebuild the index.
&gt;&gt;
&gt;&gt;
&gt; This sounds very similar to the issue last week. I would certainly like to
&gt; rule out any sort of data corruption (are you thinking your disks are
&gt; corrupting the data or are you assuming Solr is?).
&gt;
&gt; However, it is also possible, like the last issue, that the node/cluster
&gt; simply does not have enough memory. When you delete the data Solr no longer
&gt; has anything to cache in-memory thus using significantly less. As
&gt; discussed, the recommended minimum
&gt;
&gt;
&gt;&gt; But I'm wondering why a crashing Java on one node practically takes down
&gt;&gt; the search on the cluster. Shouldn't Riak be more resilient than that?
&gt;&gt;
&gt;
&gt; The hard part here is, at least initially, the Java process doesn't crash,
&gt; it just starts to timeout. In distributed systems a slow-node is often
&gt; worse than a down node. Riak, prior to 1.4 had something called "health
&gt; check" that would mark a node down in this situation. Unfortunately in some
&gt; workloads, and I believe given your cluster's limited resources it would
&gt; happen here, this often results in excessive work being offloaded to
&gt; another node, which also does not have sufficient resources and around we
&gt; go until the entire cluster falls over. A capacity problem, typically, can
&gt; only be solved by adding more capacity.
&gt;
&gt;
&gt;&gt;
&gt;&gt; Is there a explicit reindex command for the full text search subsystem?
&gt;&gt;
&gt;&gt; Could Riak keep an eye on the java process and restart it if it crashes
&gt;&gt; or runs away?
&gt;&gt;
&gt;&gt;
&gt; Riak does manage the JVM process (starting/stopping/restarting) .I agree
&gt; that if we could include run-away process, like in your case, that would be
&gt; even better. I would have to think a bit more about how this would work (to
&gt; prevent the same problems mentioned above with the old-style health check)
&gt;
&gt; Jordan
&gt;
&gt;
&gt;
&gt;&gt; Chaim Solomon
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt; riak-users mailing list
&gt;&gt; riak-users@lists.basho.com
&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;&gt;
&gt;&gt;
&gt;
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

