---
title: "Re: Servers keep dying. How to understand why?"
description: ""
project: community
lastmod: 2013-05-15T14:30:49-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg11080"
mailinglist_parent_id: "msg11068"
author_name: "Julien Genestoux"
project_section: "mailinglistitem"
sent_date: 2013-05-15T14:30:49-07:00
---


Christian, all,

Not sure what kind of magic happend, but no server died in the last 2
days... and counting.
We have not changed a single line of code, which is quite odd...
I'm still monitoring everything and hope (sic!) for a failure soon so we
can fix the problem!

Thanks




--
\*Got a blog? Make following it simple: https://www.subtome.com/
\*

Julien Genestoux,
http://twitter.com/julien51

+1 (415) 830 6574
+33 (0)9 70 44 76 29


On Tue, May 14, 2013 at 12:31 PM, Julien Genestoux &lt;
julien.genest...@gmail.com&gt; wrote:

&gt; Thanks Christian.
&gt; We do indeed use mapreduce but it's a fairly simple function:
&gt; We retrieve a first object whose value is an array of at most 10 ids and
&gt; then we fetch all the values for these 10 ids.
&gt; However, this mapreduce job is quite rare (maybe 10 times a day at most at
&gt; this point...) so I don't think that's our issue.
&gt; I'll try to run the cluster without any call to that to see if that's
&gt; better, but I'd be very surprised. Also, we were doing this already even
&gt; before we allowed for multiple value and the cluster was stable back then.
&gt; We do not do key listing or anything like that.
&gt;
&gt; I'll try looking at the statistics too.
&gt;
&gt; Thanks,
&gt;
&gt;
&gt;
&gt;
&gt; On Tue, May 14, 2013 at 11:50 AM, Christian Dahlqvist  &gt; wrote:
&gt;
&gt;&gt; Hi Julien,
&gt;&gt;
&gt;&gt; The node appear to have crashed due to inability to allocate memory. How
&gt;&gt; are you accessing your data? Are you running any key listing or large
&gt;&gt; MapReduce jobs that could use up a lot of memory?
&gt;&gt;
&gt;&gt; In order to ensure that you are efficiently resolving siblings I would
&gt;&gt; recommend you monitor the statistics in Riak (
&gt;&gt; http://docs.basho.com/riak/latest/cookbooks/Statistics-and-Monitoring/).
&gt;&gt; Specifically look at node\_get\_fsm\_objsize\_\* and node\_get\_fsm\_siblings\_\*
&gt;&gt; statistics in order to identify objects that are very large or have lots of
&gt;&gt; siblings.
&gt;&gt;
&gt;&gt; Best regards,
&gt;&gt;
&gt;&gt; Christian
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt; On 13 May 2013, at 16:44, Julien Genestoux 
&gt;&gt; wrote:
&gt;&gt;
&gt;&gt; Christian, All,
&gt;&gt;
&gt;&gt; Bad news: my laptop is completely dead. Good news: I have a new one, and
&gt;&gt; it's now fully operational (backups FTW!).
&gt;&gt;
&gt;&gt; The log files have finally been uploaded:
&gt;&gt; https://www.dropbox.com/s/j7l3lniu0wogu29/riak-died.tar.gz
&gt;&gt;
&gt;&gt; I have attached to that mail our config.
&gt;&gt;
&gt;&gt; The machine is a virtual Xen instance at Linode with 4GB of memory. I
&gt;&gt; know it's probably not the very best setup, but 1) we're on a budget and 2)
&gt;&gt; we assumed that would fit our needs quite well.
&gt;&gt;
&gt;&gt; Just to put things in more details. Initially we did not use allow\_mult
&gt;&gt; and things worked out fine for a couple of days. As soon as we enabled 
&gt;&gt; allow\_mult,
&gt;&gt; we were not able to run the cluster for more then 5 hours without seeing
&gt;&gt; failing nodes, which is why I'm convinced we must be doing something wrong.
&gt;&gt; The question is: what?
&gt;&gt;
&gt;&gt; Thanks
&gt;&gt;
&gt;&gt;
&gt;&gt; On Sun, May 12, 2013 at 8:07 PM, Christian Dahlqvist &gt; &gt; wrote:
&gt;&gt;
&gt;&gt;&gt; Hi Julien,
&gt;&gt;&gt;
&gt;&gt;&gt; I was not able to access the logs based on the link you provided.
&gt;&gt;&gt;
&gt;&gt;&gt; Could you please attach a copy of your app.config file so we can get a
&gt;&gt;&gt; better understanding of the configuration of your cluster? Also, what is
&gt;&gt;&gt; the specification of the machines in the cluster?
&gt;&gt;&gt;
&gt;&gt;&gt; How much data do you have in the cluster and how are you querying it?
&gt;&gt;&gt;
&gt;&gt;&gt; Best regards,
&gt;&gt;&gt;
&gt;&gt;&gt; Christian
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; On 12 May 2013, at 19:11, Julien Genestoux 
&gt;&gt;&gt; wrote:
&gt;&gt;&gt;
&gt;&gt;&gt; Hi,
&gt;&gt;&gt;
&gt;&gt;&gt; We are running a cluster of 5 servers, or at least trying to, because
&gt;&gt;&gt; nodes seem to be dying 'randomly'
&gt;&gt;&gt; without us knowing any reason why. We don't have a great Erlang guy
&gt;&gt;&gt; aboard, and the error logs are not
&gt;&gt;&gt; that verbose.
&gt;&gt;&gt; So I've just .tgz the whole log directory and I was hoping somebody
&gt;&gt;&gt; could give us a clue.
&gt;&gt;&gt; It's there: 
&gt;&gt;&gt; https://www.dropbox.com/s/z9ezv0qlxgfhcyq/riak-died.tar.gz(might not be 
&gt;&gt;&gt; fully uploaded to dropbox yet!)
&gt;&gt;&gt;
&gt;&gt;&gt; I've looked at the archive and some people said their server was dying
&gt;&gt;&gt; because some object's size was just
&gt;&gt;&gt; too big to allocate the whole memory. Maybe that's what we're seeing?
&gt;&gt;&gt;
&gt;&gt;&gt; As one of our buckets is set with allow\_mult, I am tempted to think that
&gt;&gt;&gt; some object's size may be exploding.
&gt;&gt;&gt; However, we do actually try to resolve conflicts in our code. Any idea
&gt;&gt;&gt; how to confirm and then debug that we
&gt;&gt;&gt; have an issue there?
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; Thanks a lot for your precious help...
&gt;&gt;&gt;
&gt;&gt;&gt; Julien
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt; 
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

