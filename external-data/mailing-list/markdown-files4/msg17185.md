---
title: "Re: Yokozuna inconsistent search results"
description: ""
project: community
lastmod: 2016-04-04T07:17:27-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg17185"
author_name: "Oleksiy Krivoshey"
project_section: "mailinglistitem"
sent_date: 2016-04-04T07:17:27-07:00
---


Continuation...

The new index has the same inconsistent search results problem.
I was making a snapshot of `search aae-status` command almost each day.
There were absolutely no Yokozuna errors in logs.

I can see that some AAE trees were not expired (built &gt; 20 days ago). I can
also see that on two nodes (of 5) last AAE exchanges happened &gt; 20 days ago.

For now I have issued ` riak\_core\_util:rpc\_every\_member\_ann(yz\_entropy\_mgr,
expire\_trees, [], 5000).` on each node again. I will wait 10 days more but
I don't think that will fix anything.


On 25 March 2016 at 09:28, Oleksiy Krivoshey  wrote:

&gt; One interesting moment happened when I tried removing the index:
&gt;
&gt; - this index was associated with a bucket type, called fs\_chunks
&gt; - so I first called RpbSetBucketTypeReq to set search\_index: \_dont\_index\_
&gt; - i then tried to remove the index with RpbYokozunaIndexDeleteReq which
&gt; failed with "index is in use" and list of all buckets of the fs\_chunks type
&gt; - for some reason all these buckets had their own search\_index property
&gt; set to that same index
&gt;
&gt; How can this happen if I definitely never set the search\_index property
&gt; per bucket?
&gt;
&gt; On 24 March 2016 at 22:41, Oleksiy Krivoshey  wrote:
&gt;
&gt;&gt; OK!
&gt;&gt;
&gt;&gt; On 24 March 2016 at 21:11, Magnus Kessler  wrote:
&gt;&gt;
&gt;&gt;&gt; Hi Oleksiy,
&gt;&gt;&gt;
&gt;&gt;&gt; On 24 March 2016 at 14:55, Oleksiy Krivoshey  wrote:
&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Hi Magnus,
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Thanks! I guess I will go with index deletion because I've already
&gt;&gt;&gt;&gt; tried expiring the trees before.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Do I need to delete AAE data somehow or removing the index is enough?
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; If you expire the AAE trees with the commands I posted earlier, there
&gt;&gt;&gt; should be no need to remove the AAE data directories manually.
&gt;&gt;&gt;
&gt;&gt;&gt; I hope this works for you. Please monitor the tree rebuild and exchanges
&gt;&gt;&gt; with `riak-admin search aae-status` for the next few days. In particular
&gt;&gt;&gt; the exchanges should be ongoing on a continuous basis once all trees have
&gt;&gt;&gt; been rebuilt. If they don't, please let me know. At that point you should
&gt;&gt;&gt; also gather `riak-debug` output from all nodes before it gets rotated out
&gt;&gt;&gt; after 5 days by default.
&gt;&gt;&gt;
&gt;&gt;&gt; Kind Regards,
&gt;&gt;&gt;
&gt;&gt;&gt; Magnus
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; On 24 March 2016 at 13:28, Magnus Kessler  wrote:
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; Hi Oleksiy,
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; As a first step, I suggest to simply expire the Yokozuna AAE trees
&gt;&gt;&gt;&gt;&gt; again if the output of `riak-admin search aae-status` still suggests that
&gt;&gt;&gt;&gt;&gt; no recent exchanges have taken place. To do this, run `riak attach` on one
&gt;&gt;&gt;&gt;&gt; node and then
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; riak\_core\_util:rpc\_every\_member\_ann(yz\_entropy\_mgr, expire\_trees, [], 
&gt;&gt;&gt;&gt;&gt; 5000).
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; Exit from the riak console with `Ctrl+G q`.
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; Depending on your settings and amount of data the full index should be
&gt;&gt;&gt;&gt;&gt; rebuilt within the next 2.5 days (for a cluster with ring size 128 and
&gt;&gt;&gt;&gt;&gt; default settings). You can monitor the progress with `riak-admin search
&gt;&gt;&gt;&gt;&gt; aae-status` and also in the logs, which should have messages along the
&gt;&gt;&gt;&gt;&gt; lines of
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; 2016-03-24 10:28:25.372 [info]
&gt;&gt;&gt;&gt;&gt; &lt;0.4647.6477&gt;@yz\_exchange\_fsm:key\_exchange:179 Repaired 83055 keys during
&gt;&gt;&gt;&gt;&gt; active anti-entropy exchange of partition
&gt;&gt;&gt;&gt;&gt; 1210306043414653979137426502093171875652569137152 for preflist
&gt;&gt;&gt;&gt;&gt; {1164634117248063262943561351070788031288321245184,3}
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; Re-indexing can put additional strain on the cluster and may cause
&gt;&gt;&gt;&gt;&gt; elevated latency on a cluster already under heavy load. Please monitor the
&gt;&gt;&gt;&gt;&gt; response times while the cluster is re-indexing data.
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; If the cluster load allows it, you can force more rapid re-indexing by
&gt;&gt;&gt;&gt;&gt; changing a few parameters. Again at the `riak attach` console, run
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; riak\_core\_util:rpc\_every\_member\_ann(application, set\_env, [yokozuna, 
&gt;&gt;&gt;&gt;&gt; anti\_entropy\_build\_limit, {4, 60000}], 5000).
&gt;&gt;&gt;&gt;&gt; riak\_core\_util:rpc\_every\_member\_ann(application, set\_env, [yokozuna, 
&gt;&gt;&gt;&gt;&gt; anti\_entropy\_concurrency, 5], 5000).
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; This will allow up to 4 trees per node to be built/exchanged per hour,
&gt;&gt;&gt;&gt;&gt; with up to 5 concurrent exchanges throughout the cluster. To return back 
&gt;&gt;&gt;&gt;&gt; to
&gt;&gt;&gt;&gt;&gt; the default settings, use
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; riak\_core\_util:rpc\_every\_member\_ann(application, set\_env, [yokozuna, 
&gt;&gt;&gt;&gt;&gt; anti\_entropy\_build\_limit, {1, 360000}], 5000).
&gt;&gt;&gt;&gt;&gt; riak\_core\_util:rpc\_every\_member\_ann(application, set\_env, [yokozuna, 
&gt;&gt;&gt;&gt;&gt; anti\_entropy\_concurrency, 2], 5000).
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; If the cluster still doesn't make any progress with automatically
&gt;&gt;&gt;&gt;&gt; re-indexing data, the next steps are pretty much what you already
&gt;&gt;&gt;&gt;&gt; suggested, to drop the existing index and re-index from scratch. I'm
&gt;&gt;&gt;&gt;&gt; assuming that losing the indexes temporarily is acceptable to you at this
&gt;&gt;&gt;&gt;&gt; point.
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; Using any client API that supports RpbYokozunaIndexDeleteReq, you can
&gt;&gt;&gt;&gt;&gt; drop the index from all Solr instances, losing any data stored there
&gt;&gt;&gt;&gt;&gt; immediately. Next, you'll have to re-create the index. I have tried this
&gt;&gt;&gt;&gt;&gt; with the python API, where I deleted the index and re-created it with the
&gt;&gt;&gt;&gt;&gt; same already uploaded schema:
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; from riak import RiakClient
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; c = RiakClient()
&gt;&gt;&gt;&gt;&gt; c.delete\_search\_index('my\_index')
&gt;&gt;&gt;&gt;&gt; c.create\_search\_index('my\_index', 'my\_schema')
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; Note that simply deleting the index does not remove it's existing
&gt;&gt;&gt;&gt;&gt; association with any bucket or bucket type. Any PUT operations on these
&gt;&gt;&gt;&gt;&gt; buckets will lead to indexing failures being logged until the index has
&gt;&gt;&gt;&gt;&gt; been recreated. However, this also means that no separate operation in
&gt;&gt;&gt;&gt;&gt; `riak-admin` is required to associate the newly recreated index with the
&gt;&gt;&gt;&gt;&gt; buckets again.
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; After recreating the index expire the trees as explained previously.
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; Let us know if this solves your issue.
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; Kind Regards,
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; Magnus
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; On 24 March 2016 at 08:44, Oleksiy Krivoshey 
&gt;&gt;&gt;&gt;&gt; wrote:
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt; This is how things are looking after two weeks:
&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt; - there are no solr indexing issues for a long period (2 weeks)
&gt;&gt;&gt;&gt;&gt;&gt; - there are no yokozuna errors at all for 2 weeks
&gt;&gt;&gt;&gt;&gt;&gt; - there is an index with all empty schema, just \_yz\_\* fields, objects
&gt;&gt;&gt;&gt;&gt;&gt; stored in a bucket(s) are binary and so are not analysed by yokozuna
&gt;&gt;&gt;&gt;&gt;&gt; - same yokozuna query repeated gives different number for num\_found,
&gt;&gt;&gt;&gt;&gt;&gt; typically the difference between real number of keys in a bucket and
&gt;&gt;&gt;&gt;&gt;&gt; num\_found is about 25%
&gt;&gt;&gt;&gt;&gt;&gt; - number of keys repaired by AAE (according to logs) is about 1-2 per
&gt;&gt;&gt;&gt;&gt;&gt; few hours (number of keys "missing" in index is close to 1,000,000)
&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt; Should I now try to delete the index and yokozuna AAE data and wait
&gt;&gt;&gt;&gt;&gt;&gt; another 2 weeks? If yes - how should I delete the index and AAE data?
&gt;&gt;&gt;&gt;&gt;&gt; Will RpbYokozunaIndexDeleteReq be enough?
&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; --
&gt;&gt;&gt;&gt;&gt; Magnus Kessler
&gt;&gt;&gt;&gt;&gt; Client Services Engineer
&gt;&gt;&gt;&gt;&gt; Basho Technologies Limited
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; Registered Office - 8 Lincoln’s Inn Fields London WC2A 3BP Reg 07970431
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; --
&gt;&gt;&gt; Magnus Kessler
&gt;&gt;&gt; Client Services Engineer
&gt;&gt;&gt; Basho Technologies Limited
&gt;&gt;&gt;
&gt;&gt;&gt; Registered Office - 8 Lincoln’s Inn Fields London WC2A 3BP Reg 07970431
&gt;&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

