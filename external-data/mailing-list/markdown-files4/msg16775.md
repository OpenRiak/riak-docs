---
title: "Re: riak-cs sync buckets from S3 to Riak-cs"
description: ""
project: community
lastmod: 2015-11-16T18:52:47-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg16775"
mailinglist_parent_id: "msg16772"
author_name: "Shunichi Shinohara"
project_section: "mailinglistitem"
sent_date: 2015-11-16T18:52:47-08:00
---


Hi Alberto,

I didn't look into boto implementation, but I suspect that COPY Object API
does NOT work between different S3-like systems.
The actual interface definition of the API is [1] and source bucket/key
is just a string in the x-amz-copy-source header. The request went into the
system that includes rk02.ejemplo.com in your example, but it did not know
anything about source bucket/key because it does not have the bucket/key.
Object contents should be transferred in some way, e.g. GET Object from
source and PUT Object (or Multipart Upload for large objects) to target
system.

[1] http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectCOPY.html

Thanks,
Shino

2015-11-16 21:28 GMT+09:00 Alberto Ayllon :
&gt; Hello.
&gt;
&gt; Thanks for your help Dmitri.
&gt;
&gt; Perhaps this is not the correct place to ask this, but maybe someone had
&gt; have the same problem.
&gt;
&gt; I build a test environment with two nodes of RIAK-CS, not in cluster, and
&gt; I'm trying to move objects from one to another, to a bucket with the same
&gt; name (testbucket).
&gt; I'm using BOTO, and the copy\_key method, but it fails, I guess that problem
&gt; is in the HEAD call, this call checks if the key exists in the destination
&gt; bucket before make the PUT, that copy the key.
&gt;
&gt; Here is the code I'm using.
&gt;
&gt; from boto.s3.key import Key
&gt; from boto.s3.connection import S3Connection
&gt; from boto.s3.connection import OrdinaryCallingFormat
&gt;
&gt; apikey04='GFR3O0HFPXQ-BWSXEMAG'
&gt; secretkey04='eIiigR4Rov2O2kxuSHNW7WPoJE2KmrtMpzzqlg=='
&gt;
&gt; apikey02='J0TT\_C9MJPWPGHW-KEWY'
&gt; secretkey02='xcLOt3ANqyNJ0kAjP8Mxx68qr7kgyXG3eqJuMA=='
&gt; cf=OrdinaryCallingFormat()
&gt;
&gt; conn04=S3Connection(aws\_access\_key\_id=apikey04,aws\_secret\_access\_key=secretkey04,
&gt;
&gt; is\_secure=False,host='rk04.ejemplo.com',port=8080,calling\_format=cf)
&gt;
&gt; conn02=S3Connection(aws\_access\_key\_id=apikey02,aws\_secret\_access\_key=secretkey02,
&gt;
&gt; is\_secure=False,host='rk02.ejemplo.com',port=8080,calling\_format=cf)
&gt;
&gt; bucket04=conn04.get\_bucket('testbucket')
&gt; bucket02=conn02.get\_bucket('testbucket')
&gt;
&gt; rs04 = bucket04.list()
&gt;
&gt; for k in rs04:
&gt; print k.name
&gt; bucket02.copy\_key(k.key, bucket04, k.key)
&gt;
&gt;
&gt; When this script is executed it returns:
&gt;
&gt; Traceback (most recent call last):
&gt; File "s3\_connect\_2.py", line 38, in 
&gt; bucket02.copy\_key(k.key, bucket04, k.key)
&gt; File
&gt; "/home/alberto/.virtualenvs/boto/local/lib/python2.7/site-packages/boto/s3/bucket.py",
&gt; line 888, in copy\_key
&gt; response.reason, body)
&gt; boto.exception.S3ResponseError: S3ResponseError: 404 Not Found
&gt; xml version="1.0"
 encoding="UTF-8"?&gt;`NoSuchKey`The specified key
&gt; does not exist./&lt;Bucket: testbucket&gt;$
&gt;
&gt;
&gt;
&gt; The idea is copy all keys in testbucket from rk04.ejemplo.com, to testbucket
&gt; in test02.ejemplo.com, maybe someone can help me.
&gt;
&gt;
&gt; Thanks a lot.
&gt;
&gt;
&gt; 2015-11-13 17:06 GMT+01:00 Dmitri Zagidulin :
&gt;&gt;
&gt;&gt; Hi Alberto,
&gt;&gt;
&gt;&gt; From what I understand, the state of the art in terms of migration of
&gt;&gt; objects from Amazon S3 to Riak CS is -- writing migration scripts.
&gt;&gt; Either as shell scripts (using s3cmd), or language-specific libraries like
&gt;&gt; boto (or even just the S3 SDKs).
&gt;&gt; And the scripts would consist of:
&gt;&gt; 1) get a list of the buckets you want to migrate
&gt;&gt; 2) List the keys in those buckets
&gt;&gt; 3) Migrate each object from AWS to CS.
&gt;&gt;
&gt;&gt; You're right that mounting buckets as filesystems is a (distant)
&gt;&gt; possibility, but we have not seen much successful use of those (though if
&gt;&gt; anybody's made that work, let us know).
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt; On Thu, Nov 12, 2015 at 12:40 PM, Alberto Ayllon 
&gt;&gt; wrote:
&gt;&gt;&gt;
&gt;&gt;&gt; Hello.
&gt;&gt;&gt;
&gt;&gt;&gt; I'm new using Riak and Riak-cs, I have installed a Riak-cs cluster with 4
&gt;&gt;&gt; nodes and it works fine,
&gt;&gt;&gt;
&gt;&gt;&gt; Here is my question, the company where I work has some buckets in Amazon
&gt;&gt;&gt; s3, and I would like migrate objects from these buckets to our Riak-cs
&gt;&gt;&gt; installation, as far as I know I can do it using S3FUSE or S3BACKER,
&gt;&gt;&gt; mounting buckets as a filesystem, but would like avoid mount it as
&gt;&gt;&gt; filesystem. I tried it with boto python library, using the copy\_key method,
&gt;&gt;&gt; but it doesn't work.
&gt;&gt;&gt;
&gt;&gt;&gt; Has anybody try with success synchronize buckets from AS3 to Riak-CS?
&gt;&gt;&gt;
&gt;&gt;&gt; Thanks.
&gt;&gt;&gt;
&gt;&gt;&gt; P:D: Excuse for my English.
&gt;&gt;&gt;
&gt;&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;&gt;&gt;
&gt;&gt;
&gt;
&gt;
&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

