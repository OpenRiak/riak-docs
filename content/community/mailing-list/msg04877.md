---
title: "Re: diverting riak as a filesystem replacement"
description: ""
project: community
lastmod: 2011-09-26T10:45:42-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg04877"
mailinglist_parent_id: "msg04874"
author_name: "Jonathan Langevin"
project_section: "mailinglistitem"
sent_date: 2011-09-26T10:45:42-07:00
---


\\*
That fuse driver appears to be a bit more complete that the ruby-version I
linked earlier, nice find Mark.

Another possible option, is to simply use a solution such as Amazon's
EBS+ S3. You would use S3 for snapshot
backups to ensure data persistence.

 
 Jonathan Langevin
I.T. Manager
Loom Inc.
Wilmington, NC: (910) 241-0433 - jlange...@loomlearning.com -
www.loomlearning.com - Skype: intel352
\\*


On Mon, Sep 26, 2011 at 12:39 PM, Mark Phillips  wrote:

&gt; It's worth mentioning that there are also already two FUSE drivers written
&gt; to work with Riak [1]. They haven't been touched for a while, but at least
&gt; one was used heavily in production [2], and they might be a good place to
&gt; start for your use case.
&gt;
&gt; Mark
&gt;
&gt; 1 -
&gt; http://wiki.basho.com/Community-Developed-Libraries-and-Projects.html#Other-Tools-and-Projects
&gt; (towards the bottom of the list)
&gt; 2 - https://github.com/crucially/riakfuse
&gt;
&gt;
&gt; On Mon, Sep 26, 2011 at 9:23 AM, Jonathan Langevin &lt;
&gt; jlange...@loomlearning.com&gt; wrote:
&gt;
&gt;&gt; If you were to continue to pursue the use of Riak for a distributed FS,
&gt;&gt; and if you have any resources to toss at development, it may be possible to
&gt;&gt; build a FUSE driver that acts as a Riak client. FUSE = filesystem in
&gt;&gt; userspace, and can function across most any Linux/BSD variant (including Mac
&gt;&gt; OS X).
&gt;&gt;
&gt;&gt; More info: http://en.wikipedia.org/wiki/Filesystem\\_in\\_Userspace
&gt;&gt;
&gt;&gt; There is also a list of FUSE drivers at the above URL, several of which
&gt;&gt; mention "distributed" in the description. One of those may suffice for you
&gt;&gt; (if you've not already reviewed them). Otherwise, you could possibly use
&gt;&gt; their FUSE drivers as a basis for your own custom FUSE Riak driver.
&gt;&gt;
&gt;&gt; 
&gt;&gt; \\* Jonathan Langevin
&gt;&gt; Systems Administrator
&gt;&gt; Loom Inc.
&gt;&gt; Wilmington, NC: (910) 241-0433 - jlange...@loomlearning.com -
&gt;&gt; www.loomlearning.com - Skype: intel352 \\*
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt; On Sun, Sep 25, 2011 at 4:29 PM, Jeremiah Peschka &lt;
&gt;&gt; jeremiah.pesc...@gmail.com&gt; wrote:
&gt;&gt;
&gt;&gt;&gt; Responses inline
&gt;&gt;&gt; ---
&gt;&gt;&gt; Jeremiah Peschka - Founder, Brent Ozar PLF, LLC
&gt;&gt;&gt; Microsoft SQL Server MVP
&gt;&gt;&gt;
&gt;&gt;&gt; On Sep 25, 2011, at 5:30 AM, pille wrote:
&gt;&gt;&gt;
&gt;&gt;&gt; &gt; hi,
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; i'm quite new to riak and only know it from the docs available online.
&gt;&gt;&gt; &gt; to be honest, i did not search for a key/value store, but for a
&gt;&gt;&gt; reliable (HA) distributed, replicated filesystem that allows dynamic growth.
&gt;&gt;&gt;
&gt;&gt;&gt; To be honest, what you're looking for is a SAN. EMC's Isilon line, Dell's
&gt;&gt;&gt; Equallogic, and HP's Lefthand devices all meet your needs very well. They
&gt;&gt;&gt; don't require a lot of administrative knowledge, they're easy to set up and
&gt;&gt;&gt; maintain, and they are very easy to expand. SANs provide the features and
&gt;&gt;&gt; functionality that you're looking for and won't require any additional
&gt;&gt;&gt; development or maintenance. Yes, they cost money, but they do just sorta
&gt;&gt;&gt; work straight out of the box.
&gt;&gt;&gt;
&gt;&gt;&gt; That being said, I answered the rest of these questions as if you weren't
&gt;&gt;&gt; willing to just throw a bucket of money and SAN gear at your problem.
&gt;&gt;&gt;
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; all these filesystems i've dealt with are either immature, abandoned,
&gt;&gt;&gt; or are limited in features like dynamic scaling, snapshotting or fail in
&gt;&gt;&gt; out-of-diskspace scenarios (as they don't give you high availability and
&gt;&gt;&gt; data protection at the same time).
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; somehow i stumbled upon this project and liked its features, despite
&gt;&gt;&gt; not being a filesystem at all. i can live with its flat structure if it'll
&gt;&gt;&gt; bring me all the other features i need.
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; so i'm now at the point that after reading the online docs without any
&gt;&gt;&gt; hands-on experience leaves some questions unanswered.
&gt;&gt;&gt; &gt; since i'm used to storing all data in a filesystem, our application's
&gt;&gt;&gt; storage interface would need a complete rewrite to interface with riak and
&gt;&gt;&gt; provide the same services as before. therefore i'd like to ask you to share
&gt;&gt;&gt; your knowledge and experience.
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; 1) are snapshots provided?
&gt;&gt;&gt; &gt; i guess they aren't, but i'm more interested weather i can use the
&gt;&gt;&gt; vectorclocks for that.
&gt;&gt;&gt; &gt; i only need one snapshot and live data to provide an consistent old
&gt;&gt;&gt; view of the data for our staging instance.
&gt;&gt;&gt;
&gt;&gt;&gt; Snapshots are not provided. You could probably cook something up
&gt;&gt;&gt; yourself, but there's no snapshotting involved that I know of. Vector clocks
&gt;&gt;&gt; are used for determining object lineage and conflict resolution.
&gt;&gt;&gt;
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; 2) how does riak deal with different storage capacities of the
&gt;&gt;&gt; different nodes? is it a problem, if some nodes provide less space than
&gt;&gt;&gt; others? is data distributed uniformly accross all nodes or is its capacity
&gt;&gt;&gt; taken into account?
&gt;&gt;&gt;
&gt;&gt;&gt; AFAIK, data is distributed evenly across a number of virtual nodes (64 by
&gt;&gt;&gt; default). Those virtual nodes are then distributed evenly across your
&gt;&gt;&gt; physical nodes. I don't know of a way to change this, but I've been very
&gt;&gt;&gt; wrong before.
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; 3) we've got quite huge files for a database to store. is that a
&gt;&gt;&gt; problem? what storage backend do you propose?
&gt;&gt;&gt; &gt; currently we see the following distribution, but i expect more in the
&gt;&gt;&gt; range from 512MB to 4GB to come in future:
&gt;&gt;&gt; &gt; &lt; 1KB: 64053
&gt;&gt;&gt; &gt; 1KB - 1MB: 873795
&gt;&gt;&gt; &gt; 1MB - 2MB: 4776
&gt;&gt;&gt; &gt; 2MB - 4MB: 3131
&gt;&gt;&gt; &gt; 4MB - 8MB: 3136
&gt;&gt;&gt; &gt; 8MB - 16MB: 2842
&gt;&gt;&gt; &gt; 16MB - 32MB: 3136
&gt;&gt;&gt; &gt; 32MB - 64MB: 4032
&gt;&gt;&gt; &gt; 64MB - 128MB: 3118
&gt;&gt;&gt; &gt; 128MB - 256MB: 3361
&gt;&gt;&gt; &gt; 256MB - 512MB: 3221
&gt;&gt;&gt; &gt; 512MB - 1GB: 1423
&gt;&gt;&gt; &gt; 1GB - 2GB: 75
&gt;&gt;&gt;
&gt;&gt;&gt; Riak KV's max acceptable performance size is about 64MB for a file, but
&gt;&gt;&gt; performance would probably start degrading before that. Luwak is an
&gt;&gt;&gt; application built on top of Riak that probably meets your needs a lot better
&gt;&gt;&gt; than plain old Riak KV: http://wiki.basho.com/Luwak.html
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; 4) is range access possible to read parts of a file^W value or do i
&gt;&gt;&gt; need to stream the whole file through? this would not perform well on guge
&gt;&gt;&gt; values.
&gt;&gt;&gt;
&gt;&gt;&gt; With Luwak it's possible to get a portion of the object using the option
&gt;&gt;&gt; Range parameter: http://wiki.basho.com/HTTP-Fetch-Luwak-Object.html
&gt;&gt;&gt;
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; 5) to reduce the impact of a disk failure on the storage backend and
&gt;&gt;&gt; i'd like each disk of a server to be assigned to its own riak-node. i guess
&gt;&gt;&gt; healing the failed node ofter replacement is faster than raid recovery and
&gt;&gt;&gt; less data is at risk.
&gt;&gt;&gt; &gt; is it possible to reflect the hardware hierarchy in some way to
&gt;&gt;&gt; influence the place for replicas? CephFS offers this to make sure replicas
&gt;&gt;&gt; are hold on different hardware or even in different locations.
&gt;&gt;&gt; &gt; e.g. a STORAGE is in a SERVER, which is in a RACK, which is in a
&gt;&gt;&gt; DATACENTER. replicas of a file in a STORAGE should never be placed inside
&gt;&gt;&gt; the same SERVER, (or RACK, or DATACENTER).
&gt;&gt;&gt;
&gt;&gt;&gt; You can purchase Riak EDS which has multi-site replication. Otherwise,
&gt;&gt;&gt; Riak is just going to throw data into N nodes in your cluster and it will be
&gt;&gt;&gt; up to you to make sure those nodes are in different racks.
&gt;&gt;&gt;
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; 6) what happens, if less that R or W nodes report data? does it mean
&gt;&gt;&gt; not found or not available? even if the data is on an currently offline
&gt;&gt;&gt; node.
&gt;&gt;&gt;
&gt;&gt;&gt; If less than R nodes are present, your write will fail. The R value means
&gt;&gt;&gt; "this many nodes have to respond with data for it to be considered a
&gt;&gt;&gt; successful read." Anything less than R would, thusly, mean there was a
&gt;&gt;&gt; failure.
&gt;&gt;&gt;
&gt;&gt;&gt; If less than W nodes are able to write data, a hinted handoff will occur.
&gt;&gt;&gt;
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; 7) can he client applications connect to some random node?
&gt;&gt;&gt; &gt; should it simply retry the next one in the list upon failure?
&gt;&gt;&gt;
&gt;&gt;&gt; Client applications should connect to a random node, yes. Even better,
&gt;&gt;&gt; you should put a load balancing proxy server in front of your Riak cluster
&gt;&gt;&gt; so developers don't have to worry about writing their own load balancing
&gt;&gt;&gt; code.
&gt;&gt;&gt;
&gt;&gt;&gt; I'd retry on failure, but that's up to you. ;)
&gt;&gt;&gt;
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; 8) is the data reported back on read is compared/verifies with all
&gt;&gt;&gt; replicas to ensure consistency or just its metadata (if R&gt;1)
&gt;&gt;&gt;
&gt;&gt;&gt; Yes, R nodes have to respond with \\*the same\\* copy of the data before a
&gt;&gt;&gt; read is successful. You can quickly do this by comparing vector clocks and
&gt;&gt;&gt; other assorted metadata.
&gt;&gt;&gt;
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; 9) is data integrity in storage backend is secured through checksums?
&gt;&gt;&gt;
&gt;&gt;&gt; I think depends on the storage backend implementation. doing a quick grep
&gt;&gt;&gt; through the source code turns up the word "checksum" a lot, though.
&gt;&gt;&gt;
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; these are the questions puzzling me at the moment.
&gt;&gt;&gt; &gt; if you know some filesystem that matches my featurelist, please don't
&gt;&gt;&gt; hesitate to answer them off-topic ;-)
&gt;&gt;&gt;
&gt;&gt;&gt; Other options include HDFS and MogileFS (http://danga.com/mogilefs/).
&gt;&gt;&gt; Last.fm use MogileFS
&gt;&gt;&gt;
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; cheers
&gt;&gt;&gt; &gt; pille
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt;&gt; &gt; riak-users mailing list
&gt;&gt;&gt; &gt; riak-users@lists.basho.com
&gt;&gt;&gt; &gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt;&gt;
&gt;&gt;&gt;

&gt;&gt;&gt;
&gt;&gt;
&gt;
