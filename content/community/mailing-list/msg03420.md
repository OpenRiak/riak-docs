---
title: "Re: Riak doesn't use consistent hashing"
description: ""
project: community
lastmod: 2011-05-26T07:21:45-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg03420"
mailinglist_parent_id: "msg03419"
author_name: "Ben Tilly"
project_section: "mailinglistitem"
sent_date: 2011-05-26T07:21:45-07:00
---


Performance is fine. However requests get a "not found" response for an
extended period of time. See
http://lists.basho.com/pipermail/riak-users\\_lists.basho.com/2011-May/thread.html#4078for
previous discussion of what sounds like the same issue.

On Thu, May 26, 2011 at 6:57 AM, Jonathan Langevin &lt;
jlange...@loomlearning.com&gt; wrote:

&gt; That sounds quite disconcerting. What happens to the performance of the
&gt; cluster when this occurs?\\*
&gt;
&gt; 
&gt; Jonathan Langevin
&gt; Systems Administrator
&gt; Loom Inc.
&gt; Wilmington, NC: (910) 241-0433 - jlange...@loomlearning.com -
&gt; www.loomlearning.com - Skype: intel352
&gt; \\*
&gt;
&gt;
&gt; On Thu, May 26, 2011 at 1:54 AM, Greg Nelson  wrote:
&gt;
&gt;&gt; I've been doing some digging through the details of how a node joins a
&gt;&gt; cluster. When you hear that Riak uses consistent hashing, you'd expect it
&gt;&gt; to distribute keys to nodes by hashing keys onto the ring AND hashing nodes
&gt;&gt; onto the ring. Keys belong to the closest node on the ring, in the
&gt;&gt; clockwise direction. Add a node, it hashes onto the ring and takes over
&gt;&gt; some keys. Ordinarily the node would hash onto the ring in several places,
&gt;&gt; to achieve better spread. Some data (roughly 1 / #nodes) moves to the new
&gt;&gt; node from each of the other nodes, and everything else stays the same.
&gt;&gt;
&gt;&gt; In what Amazon describes as operationally simpler (strategy 3 in the
&gt;&gt; Dynamo paper), the ring is instead divided into equally-sized partitions.
&gt;&gt; Nodes are hashed onto the ring, and preflists are calculated by walking
&gt;&gt; clockwise from a partition, skipping partitions on already visited nodes.
&gt;&gt; Riak does something similar: it divides the ring into equally-sized
&gt;&gt; partitions, then nodes "randomly" claim partitions. However, the skipping
&gt;&gt; bit isn't part of Riak's preflist calculation. Instead, nodes claim
&gt;&gt; partitions in such a way as to be spaced out by target\\_n\\_val, to obviate the
&gt;&gt; need for skipping.
&gt;&gt;
&gt;&gt; Now, getting back to what happens when a node joins. The new node
&gt;&gt; calculates a new ring state that maintains the target\\_n\\_val invariant, as
&gt;&gt; well as trying to keep even spread of partitions per node. The algorithm
&gt;&gt; (default\\_choose\\_claim) is heuristic and greedy in nature, and recursively
&gt;&gt; transfers partitions to the new node until optimal spread is achieved,
&gt;&gt; maintaining target\\_n\\_val along the way. But if -- during one of those
&gt;&gt; recursive calls -- it can't meet the target\\_n\\_val, it will throw up its
&gt;&gt; hands and completely re-do the whole ring (by calling claim\\_rebalance\\_n).
&gt;&gt; Striping the partitions across nodes, in a round-robin fashion. When that
&gt;&gt; happens, most of the data needs to be handed off between nodes.
&gt;&gt;
&gt;&gt; This happens a lot, with many ring sizes. With ring\\_creation\\_size=128
&gt;&gt; (i.e., 128 partitions), it will happen when adding node 9 (87.5% of data
&gt;&gt; moves), adding node 12 (82%), adding node 15 (80%), adding node 19 (94%).
&gt;&gt; It happens with all ring sizes &gt;= 128 (256, 512, 1024, ...). It appears
&gt;&gt; that any ring\\_creation\\_size (64 by default) is safe for growing to 8 nodes
&gt;&gt; or so. But if you want to go beyond that... A ring size of &gt;= 128 with
&gt;&gt; more than 8 nodes doesn't seem all that unusual, surely someone has hit this
&gt;&gt; before? I've filed a bug report here:
&gt;&gt; https://issues.basho.com/show\\_bug.cgi?id=1111
&gt;&gt;
&gt;&gt; Anyway, this feels like a bit of a departure from consistent hashing. In
&gt;&gt; fact, could this not be replaced by normal hashing + a lookup table mapping
&gt;&gt; intervals of the hash space to nodes? And isn't that simply sharding?
&gt;&gt;
&gt;&gt; At any rate, I believe the claim algorithm can be improved to avoid those
&gt;&gt; "throw up hands and stripe everything" scenarios. In fact, here is such an
&gt;&gt; implementation: https://github.com/basho/riak\\_core/pull/55. It is still
&gt;&gt; heuristic and greedy, but it seems to do a better job of avoiding re-stripe.
&gt;&gt; Test results are attached in a zip on the bug linked above. I'd love to
&gt;&gt; get the riak\\_core gurus at Basho to look at this and help validate it. It
&gt;&gt; probably could use some cleaning up, but I want to make sure there aren't
&gt;&gt; other invariants or considerations I'm leaving out -- besides maintaining
&gt;&gt; target\\_n\\_val, keeping optimal partition spread, and minimizing handoff
&gt;&gt; between ring states.
&gt;&gt;
&gt;&gt; -Greg
&gt;&gt;
&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt; riak-users mailing list
&gt;&gt; riak-users@lists.basho.com
&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt;
&gt;&gt;
&gt;
&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;
&gt;
\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

