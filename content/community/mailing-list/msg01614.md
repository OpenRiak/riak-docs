---
title: "Re: Whole cluster times out if one node is gone"
description: ""
project: community
lastmod: 2010-11-23T14:56:20-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg01614"
mailinglist_parent_id: "msg01613"
author_name: "Neville Burnell"
project_section: "mailinglistitem"
sent_date: 2010-11-23T14:56:20-08:00
---


Just a thought ... have you verified your switch, cables, nics, etc

On 24 November 2010 09:33, Jay Adkisson  wrote:

&gt; (many profuse apologies to Dan - hit "reply" instead of "reply all")
&gt;
&gt; Alrighty, I've done a little more digging. When I throttle the writes
&gt; heavily (2/sec) and set R and W to 1 all around, the cluster works just fine
&gt; after I restart the node for about 15-20 seconds. Then the read request
&gt; hangs for about a minute, until node D disappears from connected\\_nodes in
&gt; riak-admin status, at which point it returns the desired value (although
&gt; sometimes I get a 503):
&gt;
&gt; --2010-11-23 13:\\*01:28\\*-- http://:8098/riak//?r=1
&gt; Resolving ... 
&gt; Connecting to ||:8098... connected.
&gt; HTTP request sent, awaiting response... \\* \\*200 OK
&gt; Length: 3684 (3.6K) [image/jpeg]
&gt; Saving to: `?r=1'
&gt;
&gt; 100%[======================================&gt;] 3,684 --.-K/s in 0s
&gt;
&gt; 2010-11-23 13:\\*02:21\\* (49.5 MB/s) - `?r=1' saved [3684/3684]
&gt;
&gt; --2010-11-23 13:02:23-- http://:8098/riak//?r=1
&gt; Resolving ... 
&gt; Connecting to ||:8098... connected.
&gt; HTTP request sent, awaiting response... 200 OK
&gt; Length: 3684 (3.6K) [image/jpeg]
&gt; Saving to: `?r=1'
&gt;
&gt; 100%[======================================&gt;] 3,684 --.-K/s in 0s
&gt;
&gt; 2010-11-23 13:02:23 (220 MB/s) - `?r=1' saved [3684/3684]
&gt;
&gt; Afterwards, node D comes back up and re-joins the cluster seamlessly.
&gt;
&gt; Any insights?
&gt;
&gt; --Jay
&gt;
&gt; On Mon, Nov 22, 2010 at 5:59 PM, Jay Adkisson  wrote:
&gt;
&gt;&gt; Hey Dan,
&gt;&gt;
&gt;&gt; Thanks for the response! I tried it again while watching `riak-admin
&gt;&gt; status` - basically, it takes about 30 seconds of node C being down before
&gt;&gt; riak realizes it's gone. During that time, if I'm writing to the cluster at
&gt;&gt; all (I throttled it to 2 writes per second for testing), both writes and
&gt;&gt; reads hang indefinitely, and sometimes time out.
&gt;&gt;
&gt;&gt; I'm using Ripple to do the writes, and wget to test reads, all on node A
&gt;&gt; for now, since I know it'll be up. I'm using the default R and W options
&gt;&gt; for now.
&gt;&gt;
&gt;&gt; Thanks for the help and clarification around ringready.
&gt;&gt;
&gt;&gt; --Jay
&gt;&gt;
&gt;&gt;
&gt;&gt; On Mon, Nov 22, 2010 at 5:15 PM, Dan Reverri  wrote:
&gt;&gt;
&gt;&gt;&gt; Your HTTP calls should not being timing out. Are you sending requests
&gt;&gt;&gt; directly to the Riak node or are you using a load balancer? How much load
&gt;&gt;&gt; are you placing on node A? Is it a write only load or are there reads as
&gt;&gt;&gt; well? Can you confirm "all" requests time out or is it a large subset of the
&gt;&gt;&gt; requests? How large are the objects being written? Are you setting R and W
&gt;&gt;&gt; in the request? Are you using a particular client (Ruby, Python, etc.)? Can
&gt;&gt;&gt; you provide the output of "riak-admin status" from node A?
&gt;&gt;&gt;
&gt;&gt;&gt; Regarding the ringready command; that is behaving as I would expect
&gt;&gt;&gt; considering a node is down.
&gt;&gt;&gt;
&gt;&gt;&gt; Thanks,
&gt;&gt;&gt; Dan
&gt;&gt;&gt;
&gt;&gt;&gt; Daniel Reverri
&gt;&gt;&gt; Developer Advocate
&gt;&gt;&gt; Basho Technologies, Inc.
&gt;&gt;&gt; d...@basho.com
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; On Mon, Nov 22, 2010 at 4:55 PM, Jay Adkisson  wrote:
&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Hey all,
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Here's what I'm seeing: I have four nodes A, B, C, and D. I'm loading
&gt;&gt;&gt;&gt; lots of data into node A, which is being distributed evenly across the
&gt;&gt;&gt;&gt; nodes. If I physically reboot node D, all my HTTP calls time out, and
&gt;&gt;&gt;&gt; `riak-admin ringready` complains that not all nodes are up. Is this
&gt;&gt;&gt;&gt; intended behavior? Is there a configuration option I can set so it fails
&gt;&gt;&gt;&gt; more gracefully?
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; --Jay
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;
&gt;

