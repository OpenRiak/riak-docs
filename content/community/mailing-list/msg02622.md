---
title: "Re: strange timeout errors"
description: ""
project: community
lastmod: 2011-03-11T14:58:49-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg02622"
mailinglist_parent_id: "msg02619"
author_name: "Greg Steffensen"
project_section: "mailinglistitem"
sent_date: 2011-03-11T14:58:49-08:00
---


FWIW, totally shutting down and restarting the cluster made this go away.
I'm assuming is that the problem was either on some particular machine, or
was related to some transient condition like rebalancing, and the restart
was either unnecessary or overkill.


On Fri, Mar 11, 2011 at 9:37 AM, Greg Steffensen
wrote:

&gt; I'm seeing Riak timeout consistently after 60 seconds when doing gets and
&gt; sets on particular keys (I've only tested in the REST interface). The
&gt; timeout happens inside Riak, not inside our HTTP client. It happens
&gt; regardless of whether the key already exists, and if writing, regardless of
&gt; what the value is- it just depends on the key. N is 3, and it happens
&gt; regardless of what R and W are. There also appear to be some patterns in
&gt; how likely the errors are to occur when given random keys of various
&gt; lengths. Here's the relationship of key length in random keys to timeout
&gt; likelihood in get requests- I've repeated the experiment several times and
&gt; the results, though not identical, have always been within a percentage
&gt; point of the values below, because most of the same keys timeout on each
&gt; run.
&gt;
&gt; 1: 10%
&gt; 2: 13%
&gt; 3: 17%
&gt; 4: 13%
&gt; 5: 17%
&gt; 6: 20%
&gt; 7: 12%
&gt; 8: 17%
&gt; 9: 14%
&gt; 10: 13%
&gt; 15: 20%
&gt; 20: 23%
&gt; 25: 12%
&gt; 26: 12%
&gt; 27: 17%
&gt; 28: 15%
&gt; 29: 19%
&gt; 30: 24%
&gt; 31: 9%
&gt; 32: 8%
&gt; 33: 8%
&gt; 34: 9%
&gt; 35: 9%
&gt; 36: 8%
&gt;
&gt; That was done on a ring using the default configuration with 13 physical
&gt; nodes that is experiencing lots of simultaneous write activity, and on which
&gt; one physical node has been down for days, but it still nominally a member of
&gt; the ring (I'm not sure whether this behavior was occurring before the
&gt; missing node went down).
&gt;
&gt; FWIW, I've tested write behavior like this with many clusters, and while
&gt; certainly most of them have behaved normally, this isn't the first time I've
&gt; seen this behavior. Has anyone else seen anything like this before?
&gt;
