---
title: "Re: Riak and Amazon EC2"
description: ""
project: community
lastmod: 2010-06-27T23:19:06-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg00659"
mailinglist_parent_id: "msg00657"
author_name: "Dmitry Demeshchuk"
project_section: "mailinglistitem"
sent_date: 2010-06-27T23:19:06-07:00
---


Hi Dan,

Thanks for the explanation. I haven't been looking into the MapReduce
wiki page for a while so I missed that point about link walking. Also,
thanks for the clarification that reduce phase is performed on the
initiating node. Though I didn't use reduce functionality for now,
it's an important fact to consider.

Thank you again.

On Sun, Jun 27, 2010 at 8:28 PM, Dan Reverri  wrote:
&gt; Hi Dmitry,
&gt; Regarding map reduce query performance in a cluster, map phases are run in
&gt; parallel so adding more machines to the cluster means more map functions can
&gt; be run simultaneously. Reduce phases currently only run on the node that
&gt; initiated the query so additional machines will not affect the performance
&gt; of reduce phases. More information regarding map reduce is available on the
&gt; wiki:
&gt; https://wiki.basho.com/display/RIAK/MapReduce
&gt; This portion of the wiki discusses how Riak spreads map reduce queries:
&gt; https://wiki.basho.com/display/RIAK/MapReduce#MapReduce-HowRiakSpreadsProcessing
&gt;
&gt; Regarding links, they are a special type of map phase. A link phase collects
&gt; the bucket/key values defined in the "Link" header of an object and passes
&gt; them to the next phase. Link phases are accessible to the Erlang clients by
&gt; defining them in a map reduce query. For example, the following can be run
&gt; from "riak console":
&gt; %% get a local client
&gt; {ok,C} = riak:local\\_client().
&gt; %% define a metadata dict with links
&gt; MD = dict:store(&lt;&lt;"Links"&gt;&gt;,
&gt; [{{&lt;&lt;"bucket"&gt;&gt;,&lt;&lt;"key2"&gt;&gt;},&lt;&lt;"tag"&gt;&gt;}],dict:new()).
&gt; RO = riak\\_object:new(&lt;&lt;"bucket"&gt;&gt;,&lt;&lt;"key1"&gt;&gt;,&lt;&lt;"value"&gt;&gt;).
&gt; RO1 = riak\\_object:update\\_metadata(RO, MD).
&gt; %% save the object
&gt; C:put(RO1,1).
&gt; %% run a link phase in a map reduce query
&gt; C:mapred([{&lt;&lt;"bucket"&gt;&gt;,&lt;&lt;"key1"&gt;&gt;}],[{link, &lt;&lt;"bucket"&gt;&gt;, '\\_', true}]).
&gt; The map reduce query returns: {ok,[[&lt;&lt;"bucket"&gt;&gt;,&lt;&lt;"key2"&gt;&gt;,&lt;&lt;"tag"&gt;&gt;]]}
&gt; \\* Note, the above query used '\\_' for the tag portion of the query to
&gt; indicate that any tag is acceptable.
&gt;
&gt; Thank you,
&gt; Dan
&gt; Daniel Reverri
&gt; Developer Advocate
&gt; Basho Technologies, Inc.
&gt; d...@basho.com
&gt;
&gt;
&gt; On Sun, Jun 27, 2010 at 6:36 AM, Dmitry Demeshchuk 
&gt; wrote:
&gt;&gt;
&gt;&gt; On Fri, Jun 25, 2010 at 8:25 PM, Ryan Tilder  wrote:
&gt;&gt; &gt; Hi, Dmitry.  There are some gaps in the information you included here
&gt;&gt; &gt; that
&gt;&gt; &gt; might help clarify what's going on so I'm going to just rattle off some
&gt;&gt; &gt; questions for clarification.
&gt;&gt; &gt; Is your test driver only making requests of a single EC2 instance?  Or
&gt;&gt; &gt; are
&gt;&gt; &gt; you querying all 7 nodes directly in so sort of load distribution?   If
&gt;&gt; &gt; you
&gt;&gt; &gt; aren't querying all 7 nodes directly, then you will likely see
&gt;&gt; &gt; performance
&gt;&gt; &gt; on par with a cluster with only a single "physical" node.
&gt;&gt;
&gt;&gt; I tried both ways: querying only one node and querying all the nodes.
&gt;&gt; The results were approximately the same. But as far as I understand,
&gt;&gt; for map-reduce queries it's an expected result, isn't it?
&gt;&gt;
&gt;&gt; &gt; Are you certain that the 7 nodes are communicating with each other?  The
&gt;&gt; &gt; output of the "riak-admin status" command should list the nodes in the
&gt;&gt; &gt; "ring\\_members" field.
&gt;&gt;
&gt;&gt; Yes, sure.
&gt;&gt;
&gt;&gt; &gt; Are the "documents" a separate key with Riak's built in links to the
&gt;&gt; &gt; "entities" or are they keys with a data blob that refer to the
&gt;&gt; &gt; entities?[1]
&gt;&gt; &gt;  If the latter, have you
&gt;&gt; &gt; read http://blog.basho.com/2010/02/24/link-walking-by-example/ ?
&gt;&gt;
&gt;&gt; To simplify, document data (I mean, value in Riak database) had
&gt;&gt; structure like this:
&gt;&gt;
&gt;&gt; [{entities, [123, 456, 745, 2352, 235 | ...]}].
&gt;&gt;
&gt;&gt; I actually used timestamps in microseconds for Ids but that doesn't
&gt;&gt; really matter.
&gt;&gt; And, regarding your last question, documents and entities were stored
&gt;&gt; in different buckets.
&gt;&gt;
&gt;&gt; What about links? Should they give better speed in that case? Also,
&gt;&gt; neither Erlang native API (I mean riak\\_client module) nor Erlang PBC
&gt;&gt; seem to have link-walking functions like REST API.
&gt;&gt;
&gt;&gt;
&gt;&gt; &gt; It's also important for me to note that EC2 instances do not necessarily
&gt;&gt; &gt; have the same characteristics of actual physical hardware when it comes
&gt;&gt; &gt; to
&gt;&gt; &gt; preventing resource contention.  Since EC2 instances are virtualized,
&gt;&gt; &gt; you
&gt;&gt; &gt; have no idea what other load the physical host of a given instance may
&gt;&gt; &gt; be
&gt;&gt; &gt; under.  As a result it is possible to have a Riak instance running on
&gt;&gt; &gt; the
&gt;&gt; &gt; same hardware as another IO and CPU intensive instance without your
&gt;&gt; &gt; knowledge, impeding each other to a certain degree.  We've had a number
&gt;&gt; &gt; of
&gt;&gt; &gt; users complain of performance problems with Riak clusters running on EC2
&gt;&gt; &gt; at
&gt;&gt; &gt; various times.  From my personal and anecdotal experience, EC2 seems to
&gt;&gt; &gt; be
&gt;&gt; &gt; pretty heavily oversubscribed much of the time which leads to
&gt;&gt; &gt; intermittent
&gt;&gt; &gt; performance issues for all kinds of applications.
&gt;&gt; &gt; All of that is just a long winded way of saying: don't expect shared
&gt;&gt; &gt; virtualized resources to provide the same performance as dedicated
&gt;&gt; &gt; physical
&gt;&gt; &gt; hardware.  But you should still see at least somewhat better performance
&gt;&gt; &gt; that you're seeing now if your testing harness is testing properly.
&gt;&gt;
&gt;&gt; Sure, I understand that. But I expected at least a bit better performance.
&gt;&gt;
&gt;&gt; Anyway, the day before yesterday I ran some tests using basho\\_bench.
&gt;&gt; These tests cheered me up a bit :)
&gt;&gt; Here's the link to the results:
&gt;&gt;
&gt;&gt; http://demmonoid.livejournal.com/4098.html
&gt;&gt;
&gt;&gt; Please let me know if you want me to add or correct any links to your
&gt;&gt; resources or add any more information about the tests.
&gt;&gt;
&gt;&gt; &gt; --Ryan
&gt;&gt; &gt; 1. I'm not certain if you're saying that the documents are stored in a
&gt;&gt; &gt; separate bucket from the entities in the same Riak cluster or a separate
&gt;&gt; &gt; Riak cluster entirely.
&gt;&gt; &gt; On Fri, Jun 25, 2010 at 12:02 AM, Dmitry Demeshchuk
&gt;&gt; &gt; 
&gt;&gt; &gt; wrote:
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; Greetings.
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; I tried running Riak with bitcask backend on 7 Amazon EC2 standard
&gt;&gt; &gt;&gt; large instances (7.5 GB RAM, 4 EC2 CPU units) and performed some
&gt;&gt; &gt;&gt; tests.
&gt;&gt; &gt;&gt; For comparison, I built up the following Riak clusters:
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; 7 physical nodes ring
&gt;&gt; &gt;&gt; 1 physical node ring (on one of the 7 instances, but I ran the tests
&gt;&gt; &gt;&gt; separately so the rings won't mess with each other)
&gt;&gt; &gt;&gt; 1 physical node ring on an extra large instance (15 GB RAM, 8EC2 CPU
&gt;&gt; &gt;&gt; units)
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; and ran a couple of tests with putting and getting data using Riak
&gt;&gt; &gt;&gt; native Erlang API (not PBC).
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; I had 2 buckets, the first one having small (averagely about 1KB)
&gt;&gt; &gt;&gt; values, but a lot of them (about several millions) called "entities",
&gt;&gt; &gt;&gt; and the second one having lists of keys from the first database,
&gt;&gt; &gt;&gt; called "documents". So, every document consists of a lot of entities
&gt;&gt; &gt;&gt; (I used 100 and 1000 for my tests). So, the approximate size of every
&gt;&gt; &gt;&gt; document was either 100KB or 1MB.
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; So, I performed tests of putting documents and entities to database
&gt;&gt; &gt;&gt; and then obtaining them. I tried to perform reads and writes using 10
&gt;&gt; &gt;&gt; and 100 concurrent Erlang processes (well, 100 was generally too much
&gt;&gt; &gt;&gt; as I ran out of CPU), first from only one machine and then from 2 and
&gt;&gt; &gt;&gt; 3 machines at the same time (for the 7-nodes ring). Of course, the
&gt;&gt; &gt;&gt; entities were obtained using map-reduce.
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; The first weird thing was that even with 10 concurrent reads and
&gt;&gt; &gt;&gt; writes the performance didn't differ for all three clusters. Okay, 1
&gt;&gt; &gt;&gt; large and 1 extra large nodes don't differ so much but the 7 nodes
&gt;&gt; &gt;&gt; should have given me some performance, shouldn't they?
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; The second thing was that the average read time for one document with
&gt;&gt; &gt;&gt; 1000 entities was about 5 seconds, and again, the number of machines
&gt;&gt; &gt;&gt; in the cluster didn't affect the result. I guess I just stumbled upon
&gt;&gt; &gt;&gt; the performance of the instance that sent all the map-reduce requests
&gt;&gt; &gt;&gt; and then collected the replies because when I ran tests on the other 2
&gt;&gt; &gt;&gt; instances, all three had the same performance.
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; The other strange thing was that during data writes most of the time
&gt;&gt; &gt;&gt; nodes were not io-loaded. If it was a one-stream write, it would be
&gt;&gt; &gt;&gt; obvious. But it were 10 and then 20 and 30 simultaneous writing
&gt;&gt; &gt;&gt; processes!
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; Unfortunately I cannot provide the detailed results now, they are
&gt;&gt; &gt;&gt; pretty messed up. I'm going to use basho\\_bench to make good graphs and
&gt;&gt; &gt;&gt; tables of these tests.
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; Any advises for the future tests or any explanations for such strange
&gt;&gt; &gt;&gt; performance?
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; Thank you in advance and sorry for a little messed up e-mail.
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; --
&gt;&gt; &gt;&gt; Best regards,
&gt;&gt; &gt;&gt; Dmitry Demeshchuk
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt; &gt;&gt; riak-users mailing list
&gt;&gt; &gt;&gt; riak-users@lists.basho.com
&gt;&gt; &gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt; --
&gt;&gt; Best regards,
&gt;&gt; Dmitry Demeshchuk
&gt;
&gt;


-- 
Best regards,
Dmitry Demeshchuk

