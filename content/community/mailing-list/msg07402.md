---
title: "Re: Riak cluster unresponsive after single node failure"
description: ""
project: community
lastmod: 2012-05-08T15:54:56-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg07402"
mailinglist_parent_id: "msg07401"
author_name: "Armon Dadgar"
project_section: "mailinglistitem"
sent_date: 2012-05-08T15:54:56-07:00
---


Hey Scott, 

My mistake, I was not sure if the claimant was responsible for convergence.

If this was a competition, it was not one that would ever finishâ€¦ The node went
down at about 1AM, and by 9AM when I started to resolve the issue it was in
the same state. I was unable to investigate the state of that machine, as it
was refusing any SSH connections.

Thanks for mentioning the key's. We've been thinking of doing just that
to get keys lexicographically near.

Best Regards,

Armon Dadgar


On Tuesday, May 8, 2012 at 3:26 PM, Scott Lystig Fritchie wrote:

&gt; &gt; &gt; &gt; "ar" == Armon Dadgar  &gt; &gt; &gt; (mailto:armon.dad...@gmail.com)&gt; wrote:
&gt; &gt; &gt; 
&gt; &gt; 
&gt; 
&gt; 
&gt; ar&gt; All the nodes appeared to have been blocked trying to talk to riak
&gt; ar&gt; 001 which was the ring claimant at the time. Doing this seems to
&gt; ar&gt; have cleared the state enough for the cluster to make progress
&gt; ar&gt; again.
&gt; 
&gt; Armon, it's quite unlikely that the ring claimant was doing anything
&gt; special because the claimant only acts when cluster membership changes.
&gt; 
&gt; Instead, it's quite likely that riak001 was busy doing a set of LevelDB 
&gt; compactions. There have been a number of changes recently to reduce the
&gt; amount of time that we've seen worst-case LevelDB compaction blocking Erlang
&gt; process schedulers which blocks \\*everything\\*, including the keep-alives
&gt; that are sent between Erlang nodes. The longest LevelDB-related
&gt; stoppage that I've seen was 7.5 minutes. :-( When that happens on a
&gt; node X, then all other nodes will complain (almost simultaneously) that 
&gt; node X is down. It's not \\*down\\*, it's just reallyreallyreally slow to
&gt; respond to messages ... which is effectively the same as being down.
&gt; 
&gt; Checking for big LevelDB compaction storms is pretty easy using
&gt; DTrace or SystemTap, but you're probably not using a kernel that
&gt; has user-space SystemTap available. There are compaction messages
&gt; in the "LOG" file of each LevelDB data directory. The hassle is the
&gt; need to look at all of them in parallel.
&gt; 
&gt; A secondary effect is watching write ops via "iostat -x 1": the
&gt; amount of data written spikes much higher than writes triggered only by
&gt; Riak client operations. (Read ops would go higher too, except that many
&gt; files input to a compaction are already cached by the OS.)
&gt; 
&gt; Your primary keys look UID'ish. If they are not lexigraphically adjacent
&gt; to other keys inserted at the same time, you will cause many more LevelDB
&gt; compaction events than if your keys were adjacent (e.g. prefixing them with
&gt; a wall-clock timestamp).
&gt; 
&gt; -Scott 

