---
title: "Re: Slow performance using linkwalk, help wanted"
description: ""
project: community
lastmod: 2010-11-09T07:26:09-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg01507"
mailinglist_parent_id: "msg01505"
author_name: "Karsten Thygesen"
project_section: "mailinglistitem"
sent_date: 2010-11-09T07:26:09-08:00
---


Hi

Each server have 18GB of memory and 8GB swap, which is not in use at all...

So there should be plenty of memory. Riak itself is using around 5-6GB of 
memory, so plenty to spare...

output from top:

last pid: 11849; load avg: 0.02, 0.02, 0.02; up 14+01:47:26 
 15:37:53
50 processes: 48 sleeping, 1 running, 1 on cpu
CPU states: 99.8% idle, 0.1% user, 0.2% kernel, 0.0% iowait, 0.0% swap
Memory: 18G phys mem, 697M free mem, 8192M swap, 8192M free swap

 PID USERNAME LWP PRI NICE SIZE RES STATE TIME CPU COMMAND
 8989 riak 77 59 0 5941M 5923M sleep 26:55 0.49% beam.smp
 3724 root 37 59 0 57M 42M sleep 48:53 0.19% splunkd
 3735 root 13 59 0 24M 20M sleep 18:05 0.08% python2.6



Karsten

On Nov 9, 2010, at 14:42 , Les Mikesell wrote:

&gt; On 11/9/10 4:10 AM, Karsten Thygesen wrote:
&gt;&gt; 
&gt;&gt; The cluster consists of 4 exactly similar nodes - all dedicated to riak use 
&gt;&gt; only
&gt;&gt; - no other zones or tasks going on. We use Riak-EE 0.13. The servers is HP
&gt;&gt; servers with 4 x 146GB 10K RPM SAS disks. There is a memorycache on the RAID
&gt;&gt; controller and it is used during both read and writes but the RAID iis built
&gt;&gt; usin Solaris-10u9 ZFS in a setup as such:
&gt;&gt; 
&gt;&gt; pool: pool01
&gt;&gt; state: ONLINE
&gt;&gt; scrub: scrub completed after 0h0m with 0 errors on Tue Oct 26 21:25:05 2010
&gt;&gt; config:
&gt;&gt; 
&gt;&gt; NAME STATE READ WRITE CKSUM
&gt;&gt; pool01 ONLINE 0 0 0
&gt;&gt; mirror-0 ONLINE 0 0 0
&gt;&gt; c0t0d0s7 ONLINE 0 0 0
&gt;&gt; c0t1d0s7 ONLINE 0 0 0
&gt;&gt; mirror-1 ONLINE 0 0 0
&gt;&gt; c0t2d0 ONLINE 0 0 0
&gt;&gt; c0t3d0 ONLINE 0 0 0
&gt;&gt; 
&gt;&gt; errors: No known data errors
&gt;&gt; 
&gt;&gt; metrics during load gives 5% CPU load and about 10% IO load (iostat reports 
&gt;&gt; 30
&gt;&gt; iops and the disks should be able to handle 300 iops each). So basically, the
&gt;&gt; servers is unloaded....
&gt;&gt; 
&gt;&gt; One question remains - we use ZFS with default blocksize of 128Kb - what is 
&gt;&gt; the
&gt;&gt; optimal blocksize with bitcask?
&gt;&gt; 
&gt;&gt; But I believe, that we should look somewhere else for the challenge - the
&gt;&gt; hardware is not loaded significant, so I suspect, that we have a faulty
&gt;&gt; datamodel or usage...?
&gt; 
&gt; How much RAM do you have for filesystem buffering? The difference in a first 
&gt; and a repeated query sounds like normal disk head motion when you have to go 
&gt; to disk for all the data to me. Disk benchmarks tend to use big files where 
&gt; database lookups are going to seek all over the place for things not in cache.
&gt; 
&gt; -- 
&gt; Les Mikesell
&gt; lesmikes...@gmail.com
&gt; 
&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com



smime.p7s
Description: S/MIME cryptographic signature
\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

