---
title: "Re: Riak performance problems when LevelDB database grows beyond 16GB"
description: ""
project: community
lastmod: 2012-10-18T15:32:59-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg08969"
mailinglist_parent_id: "msg08948"
author_name: "Evan Vigil-McClanahan"
project_section: "mailinglistitem"
sent_date: 2012-10-18T15:32:59-07:00
---


Ahoj, Jan,

Our leveldb developer, Matthew, sent along a reply. Reading it and
reading your last reply, I am at the limits of my ability to suggest
things, other than to note that if you're IO bound, running the disks
in RAID 0 rather than RAID 1 may help.

Please contact me off list if you have any issues getting those files
to Matthew because of their size.

On Wed, Oct 17, 2012 at 5:59 AM,  wrote:
&gt; Hi Evan,
&gt;
&gt; I corrected the setup according to your recommendations:
&gt;
&gt; - vm.swappiness is 0
&gt; - fs is ext4 on software RAID1, mounted with noatime
&gt; - disk scheduler is set to deadline (it was the default)
&gt; - eleveldb max\\_open\\_files is set to 200, cache is set to default
&gt;
&gt; (BTW, why is Riak not using the new O\\_NOATIME open(2) flag?)
&gt;
&gt; I restarted the last test with 3x40G and 1x14G DB, and it was able to sustain 
&gt; 1000 ops/sec for 5 minutes. Then node5 stalled with the call stack described 
&gt; in the original mail, 1 of 4 cores almost 100% busy. The node did write 29 
&gt; M/s (140 IOPs), with an occasional read (&lt;5 IOPs), with 252 open LevelDB 
&gt; files. The disk has 869G of free space.
&gt;
&gt; When I looked at the performance graphs 17 hours later, it still did write at 
&gt; cca 29M/s (120 IOPs), with the same call stack. The Riak node was busy even 
&gt; after 17 hours without any application requests, and it was not even 
&gt; connected to the rest of the Riak cluster (the node was not listed by 
&gt; erlang:nodes() on other nodes). I would suspect a bug in LevelDB, but people 
&gt; are using it in production, aren't they?
&gt;
&gt; I intend to retry the test without the software RAID. Any other hints?
&gt;
&gt; Best regards, Jan
&gt;
&gt; ---------- Původní zpráva ----------
&gt; Od: Evan Vigil-McClanahan
&gt; Datum: 12. 10. 2012
&gt; Předmět: Re: Re: Riak performance problems when LevelDB database grows beyond 
&gt; 16GB
&gt; Hi there, Jan,
&gt;
&gt; The lsof issue is that max\\_open\\_files is per backend, iirc, so if
&gt; you're maxed out you'll see vnode count \\* max\\_open\\_files.
&gt;
&gt; I think on the second try, you may have set the cache too high. I'd
&gt; drop it back to 8 or 16 MB, and possibly up the open files a bit more,
&gt; but you don't seem to be running into contention at this point.
&gt; There's a RAM cost, so maybe just leave it where it is for now, unless
&gt; you have quite a lot of memory.
&gt;
&gt; Another thing to check is that vm.swappiness is set to 0 and that your
&gt; disk scheduler is set to deadline for spinning disks and noop for
&gt; SSDs.
&gt;
&gt; On Fri, Oct 12, 2012 at 5:02 AM, wrote:
&gt;&gt;&gt; Can you attach the eleveldb portion of your app.config file?
&gt;&gt;&gt; Configuration problems, especially max\\_open\\_files being too low, can
&gt;&gt;&gt; often cause issues like this.
&gt;&gt;&gt;
&gt;&gt;&gt; If it isn't sensitive, the whole app.config and vm.args files are also
&gt;&gt;&gt; often helpful.
&gt;&gt;
&gt;&gt; Hello Evan,
&gt;&gt;
&gt;&gt; thanks for responding.
&gt;&gt;
&gt;&gt; I originally had default LevelDB settings. When the node stalled, I changed 
&gt;&gt; it
&gt;&gt; to
&gt;&gt;
&gt;&gt; {eleveldb, [
&gt;&gt; {data\\_root, "/home/riak/leveldb"},
&gt;&gt; {max\\_open\\_files, 132},
&gt;&gt; {cache\\_size, 377487360}
&gt;&gt; ]},
&gt;&gt;
&gt;&gt; on all nodes and I restarted them all. The application started to run with
&gt;&gt; about 1000 requests/second, after about 1 minute it dropped to &lt;500
&gt;&gt; requests/second, and the node stalled again after 41 minutes. BTW according 
&gt;&gt; to
&gt;&gt; lsof(1) it had 267 open LevelDB files which is more than the 132 files limit
&gt;&gt; (??).

