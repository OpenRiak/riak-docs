---
title: "Re: The power of the siblings...."
description: ""
project: community
lastmod: 2011-10-04T12:43:06-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg05033"
mailinglist_parent_id: "msg05027"
author_name: "Mike Oxford"
project_section: "mailinglistitem"
sent_date: 2011-10-04T12:43:06-07:00
---


Good info; thanks for taking the time to respond!
I can use now/0 and shard on the mod to spread them out in a
sub-second spread.

The only thing stopping me from doing so was that I'm lazy and
didn't want to have to write the correlation aspect over that many
buckets if I didn't have to. Enough other things to do. :)

Rock on.

-mox

On Tue, Oct 4, 2011 at 6:18 AM, Ryan Zezeski  wrote:
&gt;
&gt;
&gt; On Tue, Oct 4, 2011 at 12:07 AM, Mike Oxford  wrote:
&gt;&gt;
&gt;&gt; SSDs are an option, sure.  I have one in my laptop; we have a bunch
&gt;&gt; of X25s on the way already for the servers.  Yes, they're good.  But
&gt;&gt; IOPS is not the core issue since the whole thing can sit in RAM
&gt;&gt; which is faster yet.  Disk-flush "later" isn't time critical.  Getting the
&gt;&gt; data into the buckets is.
&gt;
&gt; If you're writing to bitcask, which I assumed, then IOPS is very much an
&gt; issue.  If using bitcask and the I/O throughput is not there you are going
&gt; to have major bakcups on the vnode mailbox.  If there are any selective
&gt; receives in the vnode implementation things will really get nasty.
&gt; Are you saying you're using an in-memory backend for these keys?
&gt;
&gt;&gt;
&gt;&gt; 5k per second per key, over multiple concurrent writers (3-6 initially,
&gt;&gt; possibly more later.) Pre-cache+flush doesn't work because you
&gt;&gt; lose the interleave from the multiple writers.  NTP's resolution is only
&gt;&gt; "so good." :)
&gt;
&gt; So for each key you have 3-6 concurrent writers averaging around 5kw/s.  How
&gt; many keys do you have like this?
&gt;
&gt;&gt;
&gt;&gt; The buckets can by cycled/sharded based on time, so slicing it into
&gt;&gt; "5 second buckets of children" is possible but this is just a
&gt;&gt; specialization
&gt;&gt; of the sharding ideology.
&gt;
&gt; I assume you mean that every 5s you would change the bucket name to avoid
&gt; overloading the same key space?  Yea, that would probably help but I still
&gt; think you'll have trouble with 5kw/s on a single key if using a durable
&gt; backend.
&gt;
&gt;&gt;
&gt;&gt; Point being: If it's basically used as an append-only-bucket (throw it
&gt;&gt; all in, sort it out later) how painful, underneath, is the child
&gt;&gt; resolution vs
&gt;&gt; the traditional "get it, write it" and then dealing with children ANYWAY
&gt;&gt; when you do get collisions (which, at 5kps, you ARE going to end up with.
&gt;
&gt; Yea, I agree either way you'll end up with children.  I would imagine you'd
&gt; have faster writes without the get/modify/put cycle but I've also never seen
&gt; anyone explode siblings that high on purpose so for all I know it will be
&gt; worse.  I'd be curious to see how Riak handles large sibling counts like
&gt; that but my gut says it won't do so well.
&gt;&gt;
&gt;&gt; This was touched on that it uses lists underneath.  Given high-end modern
&gt;&gt; hardware, (6 core CPUs, SSDs, etc.) ballpark, where would you guess the
&gt;&gt; red-line is?  10k children? 25k? 100k?  I won't hold anyone to it, but if
&gt;&gt; you say "hell no, children are really expensive" then I'll abort the idea
&gt;&gt; right here compared to "they're pretty efficient underneath, it might be
&gt;&gt; doable."
&gt;
&gt; I think it's a bad idea, no matter what the practical limit is.  Siblings,
&gt; when possible, are to be avoided.  They only exist because when you write a
&gt; distributed application like Riak there are certain scenarios where they
&gt; can't be avoided.  You can certainly try to use them as you describe, but I
&gt; can tell you the code was not written with that in mind.  Like I said, I'd
&gt; be curious to see the results.
&gt;&gt;
&gt;&gt; I'm familiar with all the HA/clustering "normal stuff" but I'm curious
&gt;&gt; about Riak in particular because while Riak isn't built to be fast,
&gt;&gt; I'm curious about how much load you can push a ring through before
&gt;&gt; the underlying architecture stresses.
&gt;
&gt; In most cases we expect Riak to be I/O bound.  If you're not stressing I/O
&gt; then my first instinct would be to raise the ring size so that each node has
&gt; more partitions.  There is no hard and fast rule about how many partitions a
&gt; node should have but is dependent on the type of disk you have.  Obviously,
&gt; SSDs and the like will handle more.  We even have some people that run SSDs
&gt; RAID 0.
&gt; Also, since ring size is something that you can't change once a cluster has
&gt; been created you need to do some form of capacity planning ahead of time to
&gt; guess what will be the best node/partition ratio.  In 1.0 we did some work
&gt; to make better use of I/O without relying on the ring size (such as async
&gt; folding and whatnot) but I'm not sure on all the details and I'm hoping one
&gt; of my colleagues can help me out if I'm missing something.
&gt;&gt;
&gt;&gt; I know Yammer was putting some load on theirs; something around 4k
&gt;&gt; per sec over a few boxes but not to a single key.
&gt;
&gt; The key part of that sentence: \\_not to a single key\\_.  Writing to a single
&gt; key is serialized and therefore it can only move as fast as the vnodes that
&gt; map to it.
&gt;
&gt;&gt;
&gt;&gt; The big "problem" is that you have to have "knowledge of the buckets"
&gt;&gt; to later correlate them. Listing buckets is expensive.  I don't want to
&gt;&gt; hard-code bucket names into the application space if I can help it.
&gt;&gt; Writing "list of buckets" to another key simply moves the bottleneck
&gt;&gt; from one key to another.  Shifting buckets based on time works, but
&gt;&gt; it's obnoxious to have to correlate at 10 second intervals ....
&gt;&gt; 8640 buckets worth of obnoxious.  Every day.  Much easier to sort a
&gt;&gt; large dataset all at once from a single bucket.
&gt;
&gt; I'm not sure if you realize this but "bucket" is really just a namespace in
&gt; the key.  Said another way =/.  The  is
&gt; what's hashed and determines the ring position.  There are no special
&gt; provisions for a bucket for the most part (one exception I can think of is
&gt; custom properties which get stored in the gossiped ring).  So while 8640
&gt; buckets seems wrong it really shouldn't make much of a difference to Riak.
&gt;  However, for places where we do treat buckets specially the code may not be
&gt; optimized for a large number of buckets.  Once again, something to try and
&gt; measure.
&gt;
&gt;&gt;
&gt;&gt; Assuming an entry size of 300 bytes that works out to around
&gt;&gt; ~130G per day, which will fit in RAM for the boxes.  Correlation can be
&gt;&gt; done on separate boxes later.  GigE cards bonded, etc.
&gt;&gt;
&gt;&gt; Removing the hardware limitations, where are the guesses on where
&gt;&gt; Riak itself will curl up in a corner, sob and not come out?
&gt;&gt;
&gt;&gt; If you had to do it, what suggestions would you all propose?
&gt;&gt; (Yes, I know I could just memcache with backup writes to
&gt;&gt; secondary/tertiary copies and flush later ... I'm interested in Riak.  :)
&gt;
&gt; I think, in general, writing to a single key 5k times a second will be
&gt; problematic.  Riak simply was not designed to modify a single key in a tight
&gt; loop. I'd love to be proven wrong.  I would either find a way to distribute
&gt; these writes across the key space better or batch them locally at the
&gt; application layer and send them in chunks that can be reconstructed later.
&gt; -Ryan

\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

