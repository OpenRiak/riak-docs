---
title: "Re: Slow performance using linkwalk, help wanted"
description: ""
project: community
lastmod: 2010-11-09T02:02:07-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg01502"
mailinglist_parent_id: "msg01496"
author_name: "Karsten Thygesen"
project_section: "mailinglistitem"
sent_date: 2010-11-09T02:02:07-08:00
---


Hi

OK, we will use a larger ringsize next time and will consider a data reload.

Regarding the metrics: the servers are dedicated to Riak use and it not used 
for anything else. They are new HP servers with 8 cores each and 4x146GB 10K 
RPM SAS disks in a contatenated mirror setup. We use Solaris with ZFS as 
filesystem and I have turned off atime update in the data partition.

The pool is built as such:

 pool: pool01
 state: ONLINE
 scrub: scrub completed after 0h0m with 0 errors on Tue Oct 26 21:25:05 2010
config:

 NAME STATE READ WRITE CKSUM
 pool01 ONLINE 0 0 0
 mirror-0 ONLINE 0 0 0
 c0t0d0s7 ONLINE 0 0 0
 c0t1d0s7 ONLINE 0 0 0
 mirror-1 ONLINE 0 0 0
 c0t2d0 ONLINE 0 0 0
 c0t3d0 ONLINE 0 0 0

errors: No known data errors

so it is as fast as possible. 

However - we use the ZFS default blocksize, which is 128Kb - is that optimal 
with bitcask as backend? It is rather large, but what is optimal with bitcask?

The cluster is 4 servers with gigabit connection located in the same datacenter 
on the same switch. The loadbalancer is a Zeus ZTM, which does quote a few http 
optimizations including extended reuse of http connections and we usually see 
far better response times using the loadbalancer than using a node directly.

When we run the test, each riak node is only about 100% cpu loaded (which on 
solaris means, that it only uses one of the 8 cores). We have seen spikes in 
the 160% area, but everything below 800% is not cpu bound. So all-in-all, the 
cpuload is between 5 and 10%.

Regarding IO, we see about 30 operations/sec pr. disk and we can expect around 
300 ops/sec, so again - the disks is only about 10% loaded.

So something else is the reason here, and we are quite lost. The current 
numbers makes Riak useless in our setup, so we need to get to the bottom of 
this...

Any ideas?

Best regards,
Karsten

On Nov 8, 2010, at 23:27 , Kevin Smith wrote:

&gt; The ring size constrains the parallelism for certain operations, such as 
&gt; MapReduce. Link walking is turned into a map phase internally. A small ring 
&gt; size can cause a stampede effect as many bucket/key pairs get hashed to the 
&gt; same vnode. 64 is a decent ring size but I'd personally opt for a larger ring 
&gt; size in the 128 or 256 range. The reason for this is a) it spreads out 
&gt; bucket/key pairs around the cluster in the case of large #'s of documents and 
&gt; b) provides room to expand the cluster in the future. Unfortunately, changing 
&gt; the ring size is fairly invasive and would require a data reload.
&gt; 
&gt; However, your overall performance seems slow to me. Do these timings reflect 
&gt; connecting via the load balancer? If so, could you re-run the write and link 
&gt; walk timings connecting directly to the cluster? It would also be good to 
&gt; know what the system metrics (load, memory usage, etc) on the nodes look like 
&gt; during the test.
&gt; 
&gt; --Kevin
&gt; On Nov 8, 2010, at 3:44 PM, Jan Buchholdt wrote:
&gt; 
&gt;&gt; Kevin -
&gt;&gt; 
&gt;&gt; The allow\\_mult is set to false. I'm quite sure that we doesn't omit the old 
&gt;&gt; entries vclock in the update. A typical vclock for an Person entry that have 
&gt;&gt; been updated 363 times (adding 363 links) have a length of 581 characters.
&gt;&gt; 
&gt;&gt; We haven't changed the number of partitions in the cluster. 64 is the 
&gt;&gt; number. What would you recommend considering that we have about 5 million 
&gt;&gt; people with 120 million documents?
&gt;&gt; 
&gt;&gt; Another information is that the first time I do a link walk (using curl) on 
&gt;&gt; a total idle cluster it takes 2.71 second for a person with 363 documents. 
&gt;&gt; If I repeat the request it takes 319 milliseconds. I would expect that the 
&gt;&gt; performance would be almost the same.
&gt;&gt; 
&gt;&gt; If I run my performance test with 20 treads, that randomly pick a Person 
&gt;&gt; from 5 millions, the minimum time is 2.8s, average 6.7s, 90% 8.9s and max 
&gt;&gt; 12.4s.
&gt;&gt; 
&gt;&gt; Would changing the ring\\_creation\\_size changing the read time to values near 
&gt;&gt; your test performance? Is there any way to change the ring\\_creation\\_size 
&gt;&gt; whiteout destroying our data? It takes 1-2 days to bootstrap all our data. 
&gt;&gt; Our write is down to about 500 documents/second. A bit disappointing but 
&gt;&gt; good enough for our application.
&gt;&gt; 
&gt;&gt; --
&gt;&gt; Jan Buchholdt
&gt;&gt; Software Pilot
&gt;&gt; Trifork A/S
&gt;&gt; Cell +45 50761121
&gt;&gt; 
&gt;&gt; 
&gt;&gt; 
&gt;&gt; On 2010-11-08 17:51, Kevin Smith wrote:
&gt;&gt;&gt; Jan -
&gt;&gt;&gt; 
&gt;&gt;&gt; I've run some tests using a 8 GB, 4-core Linux box I had handy along with 
&gt;&gt;&gt; my MBP as a client using riak-java-client over HTTP. For the test I 
&gt;&gt;&gt; configured a user record as you described linked to 250 1KB entries in a 
&gt;&gt;&gt; separate bucket named "documents". I spun up 5 Java threads to simulate 5 
&gt;&gt;&gt; concurrent users. Each thread performed the link walk from the user to the 
&gt;&gt;&gt; documents 2500 times. From that I was able to observe the follow 
&gt;&gt;&gt; performance (all times in milliseconds):
&gt;&gt;&gt; 
&gt;&gt;&gt; Average runtime: 124
&gt;&gt;&gt; 99th percentile: 220
&gt;&gt;&gt; 99.5th percentile: 263
&gt;&gt;&gt; 99.9th percentile: 949
&gt;&gt;&gt; 
&gt;&gt;&gt; The large difference between the 99.5th and 99.9th seems to correlate to 
&gt;&gt;&gt; the beginning of the run so I think those times might reflect the time 
&gt;&gt;&gt; required for Java's server JIT to fully kick in as well as GC times to 
&gt;&gt;&gt; stabilize.
&gt;&gt;&gt; 
&gt;&gt;&gt; I was able to reduce performance by triggering "vector clock explosion". 
&gt;&gt;&gt; Setting a bucket's "allow\\_mult" value to true and then overwriting existing 
&gt;&gt;&gt; entries with new values while omitting the old entries' vector clock 
&gt;&gt;&gt; information causes the object's vector clock data to bloat which will 
&gt;&gt;&gt; impact read times. Is there any chance this is occurring in your 
&gt;&gt;&gt; application?
&gt;&gt;&gt; 
&gt;&gt;&gt; Another possibility is the number of partitions in your cluster is not 
&gt;&gt;&gt; large enough to provide good parallelization for your workload. What's the 
&gt;&gt;&gt; value of ring\\_creation\\_size in your cluster's app.config? Riak will run 
&gt;&gt;&gt; with a default ring size of 64 partitions if the entry isn't present.
&gt;&gt;&gt; 
&gt;&gt;&gt; --Kevin
&gt;&gt;&gt; 
&gt;&gt;&gt; On Nov 8, 2010, at 9:45 AM, Jan Buchholdt wrote:
&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Kevin
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; We are using HTTP, (have tried PB without any performance gain) and
&gt;&gt;&gt;&gt; using riak-java-client as client lib.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; --
&gt;&gt;&gt;&gt; Jan Buchholdt
&gt;&gt;&gt;&gt; Software Pilot
&gt;&gt;&gt;&gt; Trifork A/S
&gt;&gt;&gt;&gt; Cell +45 50761121
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; On 2010-11-08 14:20, Kevin Smith wrote:
&gt;&gt;&gt;&gt;&gt; Jan -
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; Which protocol (HTTP or protocol buffers) and client lib are you using?
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; --Kevin
&gt;&gt;&gt;&gt;&gt; On Nov 8, 2010, at 6:36 AM, Jan Buchholdt wrote:
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; We are evaluating Riak for a project, but having a hard time making it 
&gt;&gt;&gt;&gt;&gt;&gt; fast enough for our need.
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; Our model is very simple and looks like this:
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; --------------------- \\* ---------------------
&gt;&gt;&gt;&gt;&gt;&gt; | Person | ------------------------&gt; | Document |
&gt;&gt;&gt;&gt;&gt;&gt; --------------------- ---------------------
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; We have a set of persons and each person can have many documents.
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; Our typical queries are:
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; Get an overview of all the persons documents. This query returns the 
&gt;&gt;&gt;&gt;&gt;&gt; person along with a subset of data from all the persons documents.
&gt;&gt;&gt;&gt;&gt;&gt; Get document by id.
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; Our requirements are that these quires should be performed under in 
&gt;&gt;&gt;&gt;&gt;&gt; under 100millis when we have 10 requests per second or less load.
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; The size of the data:
&gt;&gt;&gt;&gt;&gt;&gt; A document is approximately 1 kb
&gt;&gt;&gt;&gt;&gt;&gt; No data for a persons except the personidentifier
&gt;&gt;&gt;&gt;&gt;&gt; Around 6 million persons.
&gt;&gt;&gt;&gt;&gt;&gt; Each person has from from 0 to a couple of thousand documents.
&gt;&gt;&gt;&gt;&gt;&gt; All in all we have 120 mio documents.
&gt;&gt;&gt;&gt;&gt;&gt; Most persons don't have more than 1 to 10 documents, but then we have 
&gt;&gt;&gt;&gt;&gt;&gt; some few "heavy" persons having 500 to 1000 documents.
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; Riak setup:
&gt;&gt;&gt;&gt;&gt;&gt; 4 Nodes.
&gt;&gt;&gt;&gt;&gt;&gt; Hardware configuration for each node:
&gt;&gt;&gt;&gt;&gt;&gt; HP ProLiant DL360 G7
&gt;&gt;&gt;&gt;&gt;&gt; 18 gb ram
&gt;&gt;&gt;&gt;&gt;&gt; SAS discs
&gt;&gt;&gt;&gt;&gt;&gt; Intel(R) Xeon(R) CPU E5620 @ 2.40GHz Proc 1
&gt;&gt;&gt;&gt;&gt;&gt; Solaris 10 update 9
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; We use the default bitcask storage engine
&gt;&gt;&gt;&gt;&gt;&gt; We replicate data to 3 machines when it is written.
&gt;&gt;&gt;&gt;&gt;&gt; Reads are read from just one machine
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; We tried implementing our datamodel using Riak links as described below:
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; Persons are stored in a person bucket using their person identifier as 
&gt;&gt;&gt;&gt;&gt;&gt; key
&gt;&gt;&gt;&gt;&gt;&gt; /person/
&gt;&gt;&gt;&gt;&gt;&gt; {personid}
&gt;&gt;&gt;&gt;&gt;&gt; Documents are saved in another bucket
&gt;&gt;&gt;&gt;&gt;&gt; /document/
&gt;&gt;&gt;&gt;&gt;&gt; {documented}
&gt;&gt;&gt;&gt;&gt;&gt; At each person we store links to the persons documents.
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; We are having problems with the query fetching all the documents for a 
&gt;&gt;&gt;&gt;&gt;&gt; person. Reading all the documents for a person is done using a link 
&gt;&gt;&gt;&gt;&gt;&gt; walk. The linkwalk start reading all the document keys using the 
&gt;&gt;&gt;&gt;&gt;&gt; personid. It then fetches all documents.
&gt;&gt;&gt;&gt;&gt;&gt; For persons with 1 - 5 documents the response times are often over 100 
&gt;&gt;&gt;&gt;&gt;&gt; mills. And for the "heavy" persons with many documents response times 
&gt;&gt;&gt;&gt;&gt;&gt; are several seconds. But we are very new to Riak and are probably using 
&gt;&gt;&gt;&gt;&gt;&gt; a wrong approach.
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; Below are our thoughts (having almost no experience with Riak):
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; The chosen datamodel is good for writes. Writing a new document results 
&gt;&gt;&gt;&gt;&gt;&gt; in 3 operations against Riak. Writing the document using its id as key. 
&gt;&gt;&gt;&gt;&gt;&gt; Reading the Person to get all the persons document links. Append the new 
&gt;&gt;&gt;&gt;&gt;&gt; document's key to the persons links and write back the person.
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; Reading, using linkwalk, is slow because it is expensive to fetch many 
&gt;&gt;&gt;&gt;&gt;&gt; documents even though the linkwalk can read their keys right away by 
&gt;&gt;&gt;&gt;&gt;&gt; reading the links for the person. Even though we have 4 nodes and 
&gt;&gt;&gt;&gt;&gt;&gt; linkwalks are parallelized many documents need to be retrieved from one 
&gt;&gt;&gt;&gt;&gt;&gt; node. Having to fetch for example 100 documents on one node (one disc) 
&gt;&gt;&gt;&gt;&gt;&gt; is expensive. We do not know how data is stored but are afraid Riak is 
&gt;&gt;&gt;&gt;&gt;&gt; doing a lot of disk seeks.
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; We are considering another more denormalized approach where we write all 
&gt;&gt;&gt;&gt;&gt;&gt; the documents for a person in one "blob". But then we are afraid our 
&gt;&gt;&gt;&gt;&gt;&gt; writes become slow, because when adding a new document the blob must be 
&gt;&gt;&gt;&gt;&gt;&gt; read, the new document inserted and the blob written back.
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; We could really need some input. Is our assumptions wrong? (we have not 
&gt;&gt;&gt;&gt;&gt;&gt; yet dug into the problems). Is there a good datamodel for our 
&gt;&gt;&gt;&gt;&gt;&gt; requirements? etc?.
&gt;&gt;&gt;&gt;&gt;&gt; We haven't looked at Riak search at all. Maybe it could solve some of 
&gt;&gt;&gt;&gt;&gt;&gt; our problems.
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; -- --
&gt;&gt;&gt;&gt;&gt;&gt; Jan Buchholdt
&gt;&gt;&gt;&gt;&gt;&gt; Software Pilot
&gt;&gt;&gt;&gt;&gt;&gt; Trifork A/S
&gt;&gt;&gt;&gt;&gt;&gt; Cell +45 50761121
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt;&gt;&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt;&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt;&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt; 
&gt; 
&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com



smime.p7s
Description: S/MIME cryptographic signature
\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

