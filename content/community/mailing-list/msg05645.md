---
title: "Re: simulating physical node crash"
description: ""
project: community
lastmod: 2011-11-17T15:10:16-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg05645"
mailinglist_parent_id: "msg04918"
author_name: "francisco treacy"
project_section: "mailinglistitem"
sent_date: 2011-11-17T15:10:16-08:00
---


This morning one node went down (3-node 0.14 cluster) and I started getting
the dreaded `no\\_candidate\\_nodes,exhausted\\_prefist` error posted earlier.

If 2 nodes are remaining, and I always use N=3 R=1 ... why is it failing?
Something to do with my use of Search?

Thanks
Francisco


2011/9/28 Martin Woods 

&gt; Hi Francisco
&gt;
&gt; I've seen the same error in a dev environment running on a single Riak
&gt; node with an n\\_val of 1, so in my case it was nothing to do with a failing
&gt; node. I wasn't running Riak Search either. I posted a question about it to
&gt; this list a week or so ago but haven't seen a reply yet.
&gt;
&gt; So indeed, does anyone know what's causing this error and how we can avoid
&gt; it?
&gt;
&gt; Regards,
&gt; Martin.
&gt;
&gt;
&gt;
&gt; On 28 Sep 2011, at 20:39, francisco treacy &lt; 
&gt; francisco.tre...@gmail.com&gt; wrote:
&gt;
&gt; Regarding (3) I found a Forcing Read Repair contrib function 
&gt; (
&gt; http://contrib.basho.com/bucket\\_inspector.html) which should help.
&gt;
&gt; Otherwise for the m/r error, all of my buckets use default n\\_val and write
&gt; quorum. Could it be that some data never reached that particular node in
&gt; the cluster? That is, should've I used W=3? During the failure, many
&gt; assets were returning 404s which triggered read-repair (and were ok upon
&gt; subsequent request), but no luck with the Map/Reduce function (it kept on
&gt; failing). Could it have something to do with Riak Search?
&gt;
&gt; Thanks,
&gt;
&gt; Francisco
&gt;
&gt;
&gt; 2011/9/26 francisco treacy &lt; 
&gt; 
&gt; francisco.tre...@gmail.com&gt;
&gt;
&gt;&gt; Hi all,
&gt;&gt;
&gt;&gt; I have a 3-node Riak cluster, and I am simulating the scenario of
&gt;&gt; physical nodes crashing.
&gt;&gt;
&gt;&gt; When 2 nodes go down, and I query the remaining one, it fails with:
&gt;&gt;
&gt;&gt; {error,
&gt;&gt; {exit,
&gt;&gt; {{{error,
&gt;&gt; {no\\_candidate\\_nodes,exhausted\\_prefist,
&gt;&gt; [{riak\\_kv\\_mapred\\_planner,claim\\_keys,3},
&gt;&gt; {riak\\_kv\\_map\\_phase,schedule\\_input,5},
&gt;&gt; {riak\\_kv\\_map\\_phase,handle\\_input,3},
&gt;&gt; {luke\\_phase,executing,3},
&gt;&gt; {gen\\_fsm,handle\\_msg,7},
&gt;&gt; {proc\\_lib,init\\_p\\_do\\_apply,3}],
&gt;&gt; []}},
&gt;&gt; {gen\\_fsm,sync\\_send\\_event,
&gt;&gt; [&lt;0.31566.2330&gt;,
&gt;&gt; {inputs,
&gt;&gt;
&gt;&gt; (...)
&gt;&gt;
&gt;&gt; Here I'm doing a M/R, inputs being fed by Search.
&gt;&gt;
&gt;&gt; (1) All of the involved buckets have N=3, and all involved requests R=1
&gt;&gt; (I don't really need quorum for this usecase)
&gt;&gt;
&gt;&gt; Why is it failing? I'm sure i'm missing something basic here
&gt;&gt;
&gt;&gt; (2) Probably worth noting, those 3 nodes are spread across \\*two\\* physical
&gt;&gt; servers (1 on small one, 2 on beefier one). I've heard it is "not a good
&gt;&gt; idea", not sure why though. These two servers are definitely enough still
&gt;&gt; for our current load; should I consider adding a third one?
&gt;&gt;
&gt;&gt; (3) To overcome the aforementioned error, I added a new node to the
&gt;&gt; cluster (installed on the small server). Now the setup looks like: 4 nodes
&gt;&gt; = 2 on small server, 2 on beefier one.
&gt;&gt;
&gt;&gt; When 2 nodes go down, this works. Which brings me to another topic...
&gt;&gt; could you point me to good strategies to "pre-" invoke read-repair? Is it
&gt;&gt; up to clients to scan the keyspace forcing reads? It's a disaster
&gt;&gt; usability-wise when first users start getting 404s all over the place.
&gt;&gt;
&gt;&gt; Francisco
&gt;&gt;
&gt;
&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; 
&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;
&gt;
&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;
&gt;
\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

