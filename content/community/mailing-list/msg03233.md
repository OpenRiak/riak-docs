---
title: "Re: 'not found' after join"
description: ""
project: community
lastmod: 2011-05-05T10:40:02-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg03233"
mailinglist_parent_id: "msg03232"
author_name: "Ryan Zezeski"
project_section: "mailinglistitem"
sent_date: 2011-05-05T10:40:02-07:00
---


John,

All great points. The problem is that the ring changes immediately when a
node is added. So now, all the sudden, the preflist is potentially pointing
to nodes that don't have the data and they won't have that data until
handoff occurs. The faster that data gets transferred, the less time your
clients have to hit 'notfound'.

However, I agree completely with what you're saying. This is just a side
effect of how the system currently works. In a perfect world we wouldn't
care how long handoff takes and we would also do some sort of automatic
congestion control akin to TCP Reno or something. The preflist would still
point to the "old" partitions until all data has been successfully handed
off, and then and only then would we flip the switch for that vnode. I'm
pretty sure that's where we are heading (I say "pretty sure" b/c I just
joined the team and haven't been heavily involved in these specific talks
yet).

It's all coming down the pipe...

As for your specific I/O question re handoff\\_concurrecy, you might be right.
 I would think it depends on hardware/platform/etc. I was offering it as a
possible stopgap to minimize Greg's pain. It's certainly a cure to a
symptom, not the problem itself.

-Ryan

On Thu, May 5, 2011 at 1:10 PM, John D. Rowell  wrote:

&gt; Hi Ryan, Greg,
&gt;
&gt;
&gt; 2011/5/5 Ryan Zezeski 
&gt;
&gt;&gt; 1. For example, riak\\_core has a `handoff\\_concurrency` setting that
&gt;&gt; determines how many vnodes can concurrently handoff on a given node. By
&gt;&gt; default this is set to 4. That's going to take a while with your 2048
&gt;&gt; vnodes and all :)
&gt;&gt;
&gt;
&gt; Won't that make the handoff situation potentially worse? From the thread I
&gt; understood that the main problem was that the cluster was shuffling too much
&gt; data around and thus becoming unresponsive and/or returning unexpected
&gt; results (like "not founds"). I'm attributing the concerns more to an
&gt; excessive I/O situation than to how long the handoff takes. If the handoff
&gt; can be made transparent (no or little side effects) I don't think most
&gt; people will really care (e.g. the "fix the cluster tomorrow" anecdote).
&gt;
&gt; How about using a percentage of available I/O to throttle the vnode handoff
&gt; concurrency? Start with 1, and monitor the node's I/O (kinda like 'atop'
&gt; does, collection CPU, disk and network metrics), if it is below the expected
&gt; usage, then increase the vnode handoff concurrency, and vice-versa.
&gt;
&gt; I for one would be perfectly happy if the handoff took several hours (even
&gt; days) if we could maintain the core riak\\_kv characteristics intact during
&gt; those events. We've all seen looooong RAID rebuild times, and it's usually
&gt; better to just sit tight and keep the rebuild speed low (slower I/O) while
&gt; keeping all of the dependent systems running smoothly.
&gt;
&gt; cheers
&gt; -jd
&gt;
