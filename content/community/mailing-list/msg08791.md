---
title: "Re: Riak Memory Usage Constantly Growing"
description: ""
project: community
lastmod: 2012-10-02T09:31:48-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg08791"
mailinglist_parent_id: "msg08790"
author_name: "John E. Vincent"
project_section: "mailinglistitem"
sent_date: 2012-10-02T09:31:48-07:00
---


On Tue, Oct 2, 2012 at 8:51 AM, Shane McEwan  wrote:
&gt; Thanks John and Kelly. It's nice to know we're not the only ones. :-)
&gt;
&gt; As I said, we'll be upgrading to 1.2 in the coming weeks so it's good to
&gt; know that the memory issues might go away after that. It's not a showstopper
&gt; for us, more of a curiosity and concern it might develop into something
&gt; worse.
&gt;
&gt; I'll persist with the etop and see if I can get it to run and will report
&gt; back.
&gt;
&gt; We're still using key filters in our MapReduce functions but we plan to move
&gt; to 2i at the same time as upgrading to 1.2.
&gt;
&gt; The word "monitor" doesn't appear in any of our logs for the last 5 days.
&gt; Just lots of:
&gt;
&gt; 2012-10-02 00:10:47.869 [error] &lt;0.31890.1344&gt; gen\\_fsm &lt;0.31890.1344&gt; in
&gt; state wait\\_pipeline\\_shutdown terminated with reason: {sink\\_died,normal}
&gt; 2012-10-02 00:10:47.909 [error] &lt;0.31890.1344&gt; CRASH REPORT Process
&gt; &lt;0.31890.1344&gt; with 0 neighbours crashed with reason: {sink\\_died,normal}
&gt; 2012-10-02 00:10:47.981 [error] &lt;0.166.0&gt; Supervisor riak\\_pipe\\_builder\\_sup
&gt; had child undefined started with {riak\\_pipe\\_builder,start\\_link,undefined} at
&gt; &lt;0.31890.1344&gt; exit with reason {sink\\_died,normal} in context
&gt; child\\_terminated
&gt;
&gt; Thanks!
&gt;

We had this same error before the upgrade. It's much less noisy now
but same thing - sink\\_died
&gt;
&gt; On 02/10/12 15:55, Kelly McLaughlin wrote:
&gt;&gt;
&gt;&gt; John and Shane,
&gt;&gt;
&gt;&gt; I have been looking into some memory issues lately and I would be very
&gt;&gt; interested in more
&gt;&gt; information about your particular problems. If either of you are able to
&gt;&gt; get some output
&gt;&gt; from etop using the -sort memory option when you are having elevated
&gt;&gt; memory usage it
&gt;&gt; would be very helpful to see. I know that sometimes you get the
&gt;&gt; connection\\_lost message
&gt;&gt; when trying to use etop, but I have found that sometimes if you keep
&gt;&gt; trying it may succeed
&gt;&gt; after a few attempts.
&gt;&gt;
&gt;&gt; Are either of you using MapReduce? I see that John is using 2I. Shane, do
&gt;&gt; you also use 2I?
&gt;&gt; Finally, do you notice a lot of messages to the console or console log
&gt;&gt; that have the either the
&gt;&gt; phrase 'monitor large\\_heap' or 'monitor long\\_gc'?
&gt;&gt;
&gt;&gt; Kelly
&gt;&gt;
&gt;&gt; On Oct 2, 2012, at 6:11 AM, "John E. Vincent"
&gt;&gt;  wrote:
&gt;&gt;
&gt;&gt;&gt; I would highly suggest you upgrade to 1.2 when possible. We were, up
&gt;&gt;&gt; until recently, running on 1.4 and seeing the same problems you
&gt;&gt;&gt; describe. Take a look at this graph:
&gt;&gt;&gt;
&gt;&gt;&gt; http://i.imgur.com/0RtsU.png
&gt;&gt;&gt;
&gt;&gt;&gt; That's just one of our nodes but all of them exhibited the same
&gt;&gt;&gt; behavior. The falloffs are where we had to bounce riak.
&gt;&gt;&gt;
&gt;&gt;&gt; This is what one of our nodes looks like now and has looked like since
&gt;&gt;&gt; the upgrade:
&gt;&gt;&gt;
&gt;&gt;&gt; http://i.imgur.com/pm7Nk.png
&gt;&gt;&gt;
&gt;&gt;&gt; The change was SO dramatic that I seriously though /stats was broken.
&gt;&gt;&gt; I've verified outside of Riak and inside. The memory usage change was
&gt;&gt;&gt; very positive. Evidently there's even still a memory leak.
&gt;&gt;&gt;
&gt;&gt;&gt; We're heavy 2i users. No multi backend.
&gt;&gt;&gt;
&gt;&gt;&gt; On Tue, Oct 2, 2012 at 4:08 AM, Shane McEwan  wrote:
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; G'day!
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Just recently we've noticed memory usage in our Riak cluster constantly
&gt;&gt;&gt;&gt; increasing.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; The memory usage reported by the Riak stats "memory\\_total" parameter has
&gt;&gt;&gt;&gt; been less than 100MB for nearly a year but has recently increased to
&gt;&gt;&gt;&gt; over
&gt;&gt;&gt;&gt; 1GB.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; If we restart the cluster memory usage usually returns back to what we
&gt;&gt;&gt;&gt; would
&gt;&gt;&gt;&gt; call "normal" but after a week or so of stability the memory usage
&gt;&gt;&gt;&gt; starts
&gt;&gt;&gt;&gt; gradually growing again. Sometimes after a growth spurt over a few days
&gt;&gt;&gt;&gt; the
&gt;&gt;&gt;&gt; memory usage will plateau and be stable again for a week or two and then
&gt;&gt;&gt;&gt; put
&gt;&gt;&gt;&gt; on another growth spurt. The memory usage starts increasing at the same
&gt;&gt;&gt;&gt; moment on all 4 nodes.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; This graph [http://imagebin.org/230614] shows what I mean. The green
&gt;&gt;&gt;&gt; shows
&gt;&gt;&gt;&gt; the memory usage as reported by "memory\\_total" (left-hand y-axis scale).
&gt;&gt;&gt;&gt; The
&gt;&gt;&gt;&gt; red line shows the memory used by Riak's beam.smp process (right-hand
&gt;&gt;&gt;&gt; y-axis
&gt;&gt;&gt;&gt; scale).
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Also notice that the gradient of the recent growth seems to be
&gt;&gt;&gt;&gt; increasing
&gt;&gt;&gt;&gt; compared to the memory increases we had in August.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; We might have just assumed that the memory usage was normal Riak
&gt;&gt;&gt;&gt; behaviour.
&gt;&gt;&gt;&gt; Perhaps we have just tipped over some sort of internal buffer or cache
&gt;&gt;&gt;&gt; and
&gt;&gt;&gt;&gt; that causes some more memory to be allocated. However, whenever we
&gt;&gt;&gt;&gt; notice
&gt;&gt;&gt;&gt; the memory usage increasing it always coincides with the "riak-admin
&gt;&gt;&gt;&gt; top"
&gt;&gt;&gt;&gt; command failing to run.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; We try to run "riak-admin top" to diagnose what is using the memory but
&gt;&gt;&gt;&gt; it
&gt;&gt;&gt;&gt; returns: "Output server crashed: connection\\_lost". If we restart the
&gt;&gt;&gt;&gt; cluster
&gt;&gt;&gt;&gt; the top command works fine (but, of course, there's nothing interesting
&gt;&gt;&gt;&gt; to
&gt;&gt;&gt;&gt; see after a restart!).
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; So our theory at the moment is that some sort of instability or race
&gt;&gt;&gt;&gt; condition is causing Riak to start consuming more and more memory. A
&gt;&gt;&gt;&gt; side
&gt;&gt;&gt;&gt; effect of this instability is that the internal processes needed for
&gt;&gt;&gt;&gt; running
&gt;&gt;&gt;&gt; the top command are not working correctly. The actual functionality of
&gt;&gt;&gt;&gt; Riak
&gt;&gt;&gt;&gt; doesn't seem to be affected. Our application is running fine. We see a
&gt;&gt;&gt;&gt; slight increase in "FSM Put" times and CPU usage during the memory
&gt;&gt;&gt;&gt; growth
&gt;&gt;&gt;&gt; phases but all other parameters we're monitoring on the system seem
&gt;&gt;&gt;&gt; unaffected.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; There's nothing abnormal in the logs. We get a lot of
&gt;&gt;&gt;&gt; "riak\\_pipe\\_builder\\_sup
&gt;&gt;&gt;&gt; {sink\\_died,normal}" messages but they can be ignored, apparently. The
&gt;&gt;&gt;&gt; cluster is under constant load so we would expect to see either gradual
&gt;&gt;&gt;&gt; memory increase or a steady state but not both. Erlang process count,
&gt;&gt;&gt;&gt; open
&gt;&gt;&gt;&gt; file handles, etc are stable.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; So I was wondering if anyone has seen similar behaviour before?
&gt;&gt;&gt;&gt; Is there anything else we can do to diagnose the problem?
&gt;&gt;&gt;&gt; I'm accessing the stats URL once per minute, could that have any side
&gt;&gt;&gt;&gt; effects?
&gt;&gt;&gt;&gt; We'll be upgrading to Riak 1.2 and new hardware in the next few weeks so
&gt;&gt;&gt;&gt; should we just ignore it and hope it goes away?
&gt;&gt;&gt;&gt; Any other ideas?
&gt;&gt;&gt;&gt; Or is this just normal?
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Riak config:
&gt;&gt;&gt;&gt; 4 VMware nodes
&gt;&gt;&gt;&gt; ring\\_creation\\_size, 256
&gt;&gt;&gt;&gt; n\\_val, 3
&gt;&gt;&gt;&gt; eleveldb backend:
&gt;&gt;&gt;&gt; max\\_open\\_files, 20
&gt;&gt;&gt;&gt; cache\\_size, 15728640
&gt;&gt;&gt;&gt; "riak\\_kv\\_version":"1.1.1",
&gt;&gt;&gt;&gt; "riak\\_core\\_version":"1.1.1",
&gt;&gt;&gt;&gt; "stdlib\\_version":"1.17.4",
&gt;&gt;&gt;&gt; "kernel\\_version":"2.14.4"
&gt;&gt;&gt;&gt; Erlang R14B03 (erts-5.8.4)
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Thanks!
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Shane.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt;

&gt;&gt;
&gt;&gt;

&gt;

