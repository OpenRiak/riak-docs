---
title: "Re: High volume data series storage and queries"
description: ""
project: community
lastmod: 2011-08-09T11:02:28-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg04304"
mailinglist_parent_id: "msg04266"
author_name: "Ciprian Dorin Craciun"
project_section: "mailinglistitem"
sent_date: 2011-08-09T11:02:28-07:00
---


On Tue, Aug 9, 2011 at 04:17, Paul O  wrote:
&gt; I sent this earlier to Ciprian without cc-ing the list.
&gt; ---
&gt; Hi Ciprian,
&gt; Regarding the write penalty for batches, I plan to have a pre-write cache
&gt; (hence the query step will also have to include a step for this
&gt; verification, but I'm expecting the pre-cache volume to be small so I'd
&gt; consider a RDBMs for it.)

 Got it.


&gt; As I wrote in my clarifications for Jeremiah, I'm expecting lots of data
&gt; even per source, that's why I'd like to take advantage of the data kind and
&gt; limit the queries to avoid having to go through all values even those of a
&gt; single source.

 From what I've seen from your estimation, the data amount you're
going to store is huge. Not only that but also the bandwidth required
is quite a lot. (Assuming you have a 200MBit connection and you send
data over UDP (128 bytes in total = headers + payload), after a simple
calculation it results that you'll only be able to handle 16384
sensors. Thus maybe you should reduce the readings.)


&gt; Think about multiple readers and writers at the same time and I thought (but
&gt; am not sure) that storing opaque batches would be the most predictable and
&gt; would allow the data store to grow relatively linearly (this is where Riak
&gt; should help :-)
&gt; The strategy you're suggesting would eliminate the need for the intermediary
&gt; index, am I understanding this correctly?

 Yes, what I was describing would use Riak-core only for the
distribution of load and cluster management, and would eliminate the
need of any index -- the main data-store would be the index itself.


&gt; Anyway, I still have to think about your suggestions, using Riak-core and
&gt; then storing the individual data files in an embedded DB is a neat idea and
&gt; might allow more flexibility later (operations across all batches, etc.)
&gt; Regards,
&gt; Paul

 I wouldn't store the "data files" inside the embedded DB, but the
actual raw readings.

 Ciprian.


&gt; On Mon, Aug 8, 2011 at 3:34 PM, Ciprian Dorin Craciun
&gt;  wrote:
&gt;&gt;
&gt;&gt; On Mon, Aug 8, 2011 at 21:21, Paul O  wrote:
&gt;&gt; &gt; Hello Riak enthusiasts,
&gt;&gt; &gt; I am trying to design a solution for storing time series data coming
&gt;&gt; &gt; from a
&gt;&gt; &gt; very large number of potential high-frequency sources.
&gt;&gt; &gt; I thought Riak could be of help, though based on what I read about it I
&gt;&gt; &gt; can't use it without some other layer on top of it.
&gt;&gt; &gt; The problem is I need to be able to do range queries over this data, by
&gt;&gt; &gt; the
&gt;&gt; &gt; source. Hence, I want to be able to say "give me the N first data points
&gt;&gt; &gt; for
&gt;&gt; &gt; source S between time T1 and time T2."
&gt;&gt; &gt; I need to store this data for a rather long time, and the expected
&gt;&gt; &gt; volume
&gt;&gt; &gt; should grow more than what a "vanilla" RDBMS would support.
&gt;&gt; &gt; Another thing to note is that I can restrict the number of data points
&gt;&gt; &gt; to be
&gt;&gt; &gt; returned by a query, so no query would return more than MaxN data
&gt;&gt; &gt; points.
&gt;&gt; &gt; I thought about doing this the following way:
&gt;&gt; &gt; 1. bundle date time series in batches of MaxN, to ensure that any query
&gt;&gt; &gt; would require reading at most two batches. The batches would be store
&gt;&gt; &gt; inside
&gt;&gt; &gt; Riak.
&gt;&gt; &gt; 2. Store the start-time, end-time, size and Riak batch ID in a MySQL (or
&gt;&gt; &gt; PostgreSQL) DB.
&gt;&gt; &gt; My thinking is such a strategy would allow me to persist data in Riak
&gt;&gt; &gt; and
&gt;&gt; &gt; linearly grow with the data, and the index would be kept in a RDBM for
&gt;&gt; &gt; fast
&gt;&gt; &gt; range queries.
&gt;&gt; &gt; Does it sound sensible to use Riak this way? Does this make you
&gt;&gt; &gt; laugh/cry/shake your head in disbelief? Am I overlooking something from
&gt;&gt; &gt; Riak
&gt;&gt; &gt; which would make all this much better?
&gt;&gt; &gt; Thanks and best regards,
&gt;&gt; &gt; Paul
&gt;&gt;
&gt;&gt;    Hello all!
&gt;&gt;
&gt;&gt;    (Disclaimer: I'm not a Riak-KV user per-se, but I've built
&gt;&gt; something on-top of Riak-Core. Also the solution I'm proposing is not
&gt;&gt; just something "on-top" of Riak-KV, but ontop of Riak-Core.)
&gt;&gt;    (Context: Some time ago we had a similar problem in a project of
&gt;&gt; ours: store time-series of power consumption from a lot of devices.
&gt;&gt; The current solution is using Informix, but this is out of scope.)
&gt;&gt;
&gt;&gt;    First I would say that storing data in batches will have a high
&gt;&gt; penalty on writes -- and such an application I guess is write
&gt;&gt; intensive. (As I know there is no support for appending to values.
&gt;&gt; Or?)
&gt;&gt;
&gt;&gt;    Now because you have a "very large number of sources" means that
&gt;&gt; you could do -- on top of Riak-Core the following:
&gt;&gt;    \\* take the identifier of your data source and hash it -- just like
&gt;&gt; Riak-KV does with a key -- and use this as the key for Riak-Core;
&gt;&gt;    \\* build a Vnode module that handles writes and queries, and which
&gt;&gt; stores the data in either:
&gt;&gt;        \\* RRDtool -- one data-file per data-source;
&gt;&gt;        \\* or an embedded database allowing sorted data-sets; (we've
&gt;&gt; had some very nice experiments with BerkeleyDB;) just use as key the
&gt;&gt; concatenation from the source key and the timestamp, and being careful
&gt;&gt; to have the resulting key correctly sorted lexicographically (i.e.
&gt;&gt; keep the key and timestamp at fixed length (maybe padded) and
&gt;&gt; big-endian encoded);
&gt;&gt;    \\* all that remains to be solved is data-replication -- similar to
&gt;&gt; how Riak handles it;
&gt;&gt;
&gt;&gt;    Thinking a little bit about it, you could actually do also this:
&gt;&gt;    \\* Riak-KV uses 160bits for keys, thus you could "partition" this
&gt;&gt; key in two parts: "sensor-key" | "timestamp"; for example 96 bits for
&gt;&gt; sensor, and 64 bits for timestamp;
&gt;&gt;    \\* implement the riak\\_kv\\_backend module ontop of such an embedded
&gt;&gt; database as described above (i.e. BerkeleyDB, or LevelDB);
&gt;&gt;    \\* when storing and querying data you'll still need to bypass the
&gt;&gt; Riak-KV normal key hashing;
&gt;&gt;    \\* you'll have replication and balancing for free;
&gt;&gt;
&gt;&gt;    Ciprian.

