---
title: "Re: Two nodes cluster, 2i queries impossible when one node down?"
description: ""
project: community
lastmod: 2012-03-30T11:15:32-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg07091"
mailinglist_parent_id: "msg07090"
author_name: "Jared Morrow"
project_section: "mailinglistitem"
sent_date: 2012-03-30T11:15:32-07:00
---


Cedric,

The problem you are seeing is that the list of vnodes does not spread perfectly 
between physical nodes. So with N=2 on only a 2 node cluster, you aren't 
given assurances that those two copies of data will end up on two vnodes owned 
by different physical nodes. So if you have data X on vnode 15 and 18 
pretend, those two vnodes might both be owned by 192.168.0.21.

If you are running less (or equal) nodes than your N value, all bets are off 
when it comes to testing riak functionality. We suggest that you have a 
physical node count &gt; N +1 to see Riak's true capabilities. If you just 
want to test functionality on a small scale, you can use the 'make devrel' 
setup to run 4 nodes on a single machine for testing [1]. Please do not test 
speed or throughput in that setup, but if you are trying to see how 2i works, 
that's a better option than two nodes with N=2. 


[1] http://wiki.basho.com/Building-a-Development-Environment.html

Hope that helps,
Jared


On Mar 30, 2012, at 11:49 AM, Cedric Maion wrote:

&gt; Hey there,
&gt; 
&gt; I'm playing with a two node riak 1.1.1 cluster, but can't figure how to
&gt; make riak happy with only one node (and the other one failed).
&gt; 
&gt; My use case is the following:
&gt; - HTML pages gets stored in the riak cluster (it's used as a page cache)
&gt; - I'm using secondary indexes to tag those documents
&gt; - When I need to invalidate some HTML pages, I make a query on those
&gt; secondary indexes to retrieve the list of keys that needs to be deleted
&gt; (documents matching a specific tag value)
&gt; 
&gt; The bucket is configured with
&gt; {"props":{"n\\_val":2,"allow\\_mult":false,"r":1,"w":1,"dw":0,"rw":1}}.
&gt; 
&gt; Everything is working fine when both node are up and running.
&gt; However, if I turn one node off, secondary indexes queries returns
&gt; {error,{error,insufficient\\_vnodes\\_available}} errors.
&gt; 
&gt; I can't find a way to have the cluster converge to a stable state on only one 
&gt; node.
&gt; 
&gt; 
&gt; (192.168.0.21 is the node that has been turned off, by just stopping riak 
&gt; with /etc/init.d/riak stop)
&gt; 
&gt; 
&gt; root@192.168.0.51:~# riak-admin member\\_status
&gt; Attempting to restart script through sudo -u riak
&gt; ================================= Membership==================================
&gt; Status Ring Pending Node
&gt; -------------------------------------------------------------------------------
&gt; valid 50.0% -- 'riak@192.168.0.21'
&gt; valid 50.0% -- 'riak@192.168.0.51'
&gt; -------------------------------------------------------------------------------
&gt; Valid:2 / Leaving:0 / Exiting:0 / Joining:0 / Down:0
&gt; 
&gt; 
&gt; =&gt; 192.168.0.21 is not seen as being down, not sure if it's an issue or not
&gt; 
&gt; 
&gt; 
&gt; root@192.168.0.51:~# riak-admin transfers
&gt; Attempting to restart script through sudo -u riak
&gt; Nodes ['riak@192.168.0.21'] are currently down.
&gt; 'riak@192.168.0.51' waiting to handoff 5 partitions
&gt; 
&gt; 
&gt; =&gt; by creating/reading many keys, I finally get "waiting to handoff 32 
&gt; partitions", which seems OK to me (ring size is 64, so each node should 
&gt; normally own 32).
&gt; =&gt; however, secondary indexes queries always fails, until I turn the failed 
&gt; node ON again.
&gt; 
&gt; 
&gt; I tried to force "riak-admin down riak@192.168.0.21" from the valid node, but 
&gt; no luck either.
&gt; 
&gt; Not being able to use secondary indexes while a node is down is a real 
&gt; problem.
&gt; Is this expected behavior, or what am I missing?
&gt; 
&gt; 
&gt; Thanks in advance!
&gt; Kind regards,
&gt; 
&gt; Cedric
&gt; 

