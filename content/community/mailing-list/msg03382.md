---
title: "Re: Riak Recap for May 18 - 19"
description: ""
project: community
lastmod: 2011-05-21T18:45:32-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg03382"
mailinglist_parent_id: "msg03381"
author_name: "Greg Nelson"
project_section: "mailinglistitem"
sent_date: 2011-05-21T18:45:32-07:00
---


Hi Anthony,

You mentioned you're planning on growing to around 100 nodes. I'm curious what 
ring\\_creation\\_size you used? Also, how much data capacity per node are you 
planning on?

I've been spending a lot of time lately working through what happens when a 
node joins. There are a couple of big issues you will want to look out for in 
addition to what you've discovered, which boil down to essentially:
After a node joins, the data on partitions which change nodes in the new ring 
will be unavailable until the handoffs are complete. Currently this comes back 
as a 404 that's indistinguishable from a "true" 404.
At certain points in the progression of ring states from 1 to 100 nodes, a LOT 
more partitions move around than you'd expect from a consistent hashing scheme.

#2 obviously exacerbates #1, and if -- like us -- you plan to have a lot of 
data in the cluster, having most of it move around after a node joins is 
unrealistic.

I'm still trying to work through exactly what's happening with #2, but it seems 
like once you have more nodes than target\\_n\\_val, when adding a new node you 
usually get the consistent hashing property you want: that the new node takes 
some partitions from each of the other nodes, and that's it. But every once in 
a while (and really, not all that rarely), shit hits the fan and it decides to 
re-balance and completely change the ring. &gt;95% of partitions will move, in 
certain cases!

I have some erlang console code I've been using with riak\\_core to simulate our 
cluster, to get a deeper understanding of the rings at each phase. I might be 
able to clean that up and put it into a script to share.

-Greg 
On Saturday, May 21, 2011 at 9:31 AM, Anthony Molinaro wrote:
As I asked this question I thought I would pipe in with my experience (comments 
inline).
&gt; 
&gt; On May 20, 2011, at 3:17 PM, Mark Phillips  wrote:
&gt; 
&gt; &gt; 4) Q --- Lets say I have several new nodes to add, is the recommended
&gt; &gt; procedure to add them one at a time and wait for all transfers to
&gt; &gt; finish, or can you actually add several?
&gt; &gt; 
&gt; &gt; A --- The current recommended procedure is to add one node at a time
&gt; &gt; and wait for the partition transfers to finish before proceeding to
&gt; &gt; the next node addition.
&gt; 
&gt; I found that adding them one at a time would have taken about 4 hours per 
&gt; node and as I was doubling the size I felt there would be less shuffling of 
&gt; data if I added all at once (as suggested by aphyr on IRC). This proved to be 
&gt; exactly correct as I was able to add 4 new nodes in about 4 hours instead of 
&gt; 16.
&gt; 
&gt; &gt; Specifically:
&gt; &gt; 
&gt; &gt; \\* Use the "riak-admin join" command to kick off the cluster expansion
&gt; &gt; \\* Run "riak-admin transfers" periodically to keep an eye on the nodes
&gt; &gt; awaiting or passing off partitions (this may take a bit to complete);
&gt; &gt; an alternate (and less expensive) way to keep an eye on on this is to
&gt; &gt; just watch the logs.
&gt; 
&gt; Running "riak-admin transfers" hardly ever works I would say it times out 95% 
&gt; of the time when attempting to add a new node. I don't know why this is and I 
&gt; hope it is fixed someday but I would recommend never running it.
&gt; 
&gt; Unfortunately grepping logs is also tricky as you have to deal with lots of 
&gt; false positives if you done something like I did where you had a bunch of 
&gt; nodes crash then brought them up, only to realize you need to add capacity, 
&gt; so you add nodes. But now the logs on the first nodes have messages for 
&gt; transfers from the restart and the node addition.
&gt; 
&gt; &gt; \\* When "riak-admin ringready" prints "TRUE ..." to let you know that
&gt; &gt; all nodes agree on the ring state, you're good to go.
&gt; 
&gt; This actually returned true before transfers were complete IIRC so I think 
&gt; this may not quite be right.
&gt; 
&gt; &gt; (It's worth nothing that making this process smoother and more fluid
&gt; &gt; is high on our list of priorities.)
&gt; 
&gt; Good to know I look forward to this as I expect to be increasing my cluster 
&gt; up to close to 100 nodes by the end of this year.
&gt; 
&gt; -Anthony
&gt; 
&gt; 
&gt; 
&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt; 
\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

