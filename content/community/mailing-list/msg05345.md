---
title: "Re: Severe problems when adding a new node"
description: ""
project: community
lastmod: 2011-10-28T08:52:00-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg05345"
mailinglist_parent_id: "msg05343"
author_name: "Kelly McLaughlin"
project_section: "mailinglistitem"
sent_date: 2011-10-28T08:52:00-07:00
---


John,

It appears you've run into a race condition with adding and leaving nodes 
that's present in 1.0.1. The problem happens during handoff and can cause 
bitcask directories to be unexpectedly deleted. We have identified the issue 
and we are in the process of correcting it, testing, and generating a new point 
release containing the fix. In the meantime, we apologize for the inconvenience 
and irritation this has caused. 

Kelly


On Oct 28, 2011, at 9:14 AM, John Axel Eriksson wrote:

&gt; Last night we did two things. First we upgraded our entire cluster from 
&gt; riak-search 0.14.2 to 1.0.1. This process went
&gt; pretty well and the cluster was responding correctly after this was completed.
&gt; 
&gt; In our cluster we have around 40 000 files stored in Luwak (we also have 
&gt; about the same amount of keys, or more, in riak which is mostly
&gt; the metadata for the files in Luwak). The files are in sizes ranging from 
&gt; around 50K to around 400MB, most of the files are pretty small though. I
&gt; think we're up to a total of around 30GB now.
&gt; 
&gt; Anyway, upon adding a new node to the now 1.0.1 cluster I saw the beam.smp 
&gt; processes on all the servers, including the new one, taking
&gt; up almost all available cpu. It stayed in this state for around an hour and 
&gt; the cluster was slow to respond and occasionally timed out. During the
&gt; process Riak crashed on random nodes from time to time and I had to restart 
&gt; it. After about an hour things settled down. I added this
&gt; new node to our load-balancer so it too could serve requests. When testing 
&gt; our apps against the cluster we still got lots of timeouts and something
&gt; seemed very very wrong.
&gt; 
&gt; After a while I did a "riak-admin leave" on the node that was added (kind of 
&gt; a panic move I guess). Around 20 minutes after I did this, the cluster started
&gt; responding correctly again. All was not well though - files seemed to be 
&gt; corrupted(not sure what percentage but could be 1 % or more). I have no idea 
&gt; how
&gt; that could happen but files that we had accessed before now contained 
&gt; garbage. I haven't thoroughly researched exactly WHAT garbage they contain but
&gt; they're not in a usable state anymore. Is this something that could happen 
&gt; under any circumstances in Riak?
&gt; 
&gt; I'm afraid of adding a node at all now since it resulted in downtime and 
&gt; corruption when I tried it. I checked and rechecked the configuration files 
&gt; and really - they're
&gt; the same on all the nodes (except for vm.args where they have different names 
&gt; of course). Has anyone ever seen anything like this? Could it somehow be 
&gt; related to
&gt; the fact that I did an upgrade from 0.14.2 to 1.0.1 and maybe an hour later 
&gt; added a new 1.0.1 node?
&gt; 
&gt; Thanks for any input!
&gt; 
&gt; John
