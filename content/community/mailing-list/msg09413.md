---
title: "Re: Best practice -- duplicating and syncing objects"
description: ""
project: community
lastmod: 2012-11-29T13:09:11-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg09413"
mailinglist_parent_id: "msg09412"
author_name: "Eric Redmond"
project_section: "mailinglistitem"
sent_date: 2012-11-29T13:09:11-08:00
---

On Nov 29, 2012, at 9:46 AM, Felix Terkhorn  wrote:

&gt; Thanks, Eric. This makes sense. Right now we’re indeed handling the update 
&gt; on the client side.
&gt; 
&gt; Your point about considering normalization in cases where we have to 
&gt; duplicate a lot of data is well taken. This actually applies to a couple of 
&gt; use cases for us already – in one, we only have a single copy of the data; in 
&gt; another, we could see as many as 10-50 copies.
&gt; 
&gt; For the first case, we can stick to client-side updates of both copies for 
&gt; now. For the second case, if we actually see that many copies starting to 
&gt; crop up, we can certainly de-duplicate things a bit.
&gt; 
&gt; In case we do go about experimenting with post-commit hooks… my understanding 
&gt; was that the can only be written in erlang. I’m not entirely sure how this 
&gt; syncs up with your mention of “the number of pooled connections in 
&gt; javascript.” Forgive me if I’m missing something obvious, there. Is there 
&gt; some config setting that I can visit to find out how many pooled connections 
&gt; we have available to handle those post-commit hooks?

Sorry. You're correct, post-commit has no javascript option, only pre-commit.

&gt; -f
&gt; 
&gt; From: Eric Redmond [mailto:eredm...@basho.com] 
&gt; Sent: Thursday, November 29, 2012 11:17 AM
&gt; To: Felix Terkhorn
&gt; Cc: riak-users@lists.basho.com
&gt; Subject: Re: Best practice -- duplicating and syncing objects
&gt; 
&gt; There's no general best practice for keeping denormalized data in sync, 
&gt; beyond the obvious case, which is to update all values through whatever 
&gt; client you use to update one. If your number of keys are few, this is not 
&gt; going to be a hard hit on your updates. If you have an unbounded number of 
&gt; keys, you may consider normalizing your data model a bit to reduce duplicate 
&gt; data.
&gt; 
&gt; Correct, you do not have to wait for a post commit to fire (actually, you 
&gt; can't).
&gt; 
&gt; You could functionally update objects in a post-commit, though I don't know 
&gt; how commonly this is done. If the post-commit job is long running, you might 
&gt; run out of pooled connections in javascript. You'd also have to be very 
&gt; careful to avoid your aforementioned "infinity loop", since whether by link 
&gt; walking or postcommit hooks, you still run the risk of objects updating each 
&gt; other recursively.
&gt; 
&gt; Eric
&gt; 
&gt; On Nov 29, 2012, at 7:53 AM, Felix Terkhorn  wrote:
&gt; 
&gt; 
&gt; Greetings!
&gt; 
&gt; In the event that we have several documents, [A1, A2, …, An], which contain 
&gt; the same data accessed via different keys, what is the best practice for 
&gt; keeping the data in sync?
&gt; 
&gt; Also, do post commit hooks fire after the client receives a successful 201 or 
&gt; 200 status on a PUT? That is to say, we don’t have to wait for all 
&gt; post-commit hooks to fire, in order for our client to receive an HTTP success 
&gt; status, right?
&gt; 
&gt; That’s our assumption, and if true, we’d like to exploit that fact in order 
&gt; to keep the response time of the PUT low. Basically, client could PUT A1, 
&gt; and we could let Riak handle the necessary updates in a post-processing step.
&gt; 
&gt; We could keep the list [A1, A2, …, An] somewhere else, and simply walk that 
&gt; list every time any document in the list is updated, excluding the document 
&gt; itself. Is this a standard approach?
&gt; 
&gt; We thought of linking objects together, and having them update each other on 
&gt; post-commit, but that seems like it will bring us into infinite loop 
&gt; territory. :-D
&gt; 
&gt; Thanks,
&gt; Felix
 

