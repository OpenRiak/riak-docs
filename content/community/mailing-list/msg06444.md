---
title: "Re: Very (very) slow handoff, how to investigate?"
description: ""
project: community
lastmod: 2012-01-27T07:44:51-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg06444"
mailinglist_parent_id: "msg06443"
author_name: "Ian Plosker"
project_section: "mailinglistitem"
sent_date: 2012-01-27T07:44:51-08:00
---


Gal, 

You could try using `riak attach` and running the following to increase the 
handoff\\_concurrency from 1 to 4:

application:set\\_env(riak\\_core, handoff\\_concurrency, 4).

You will need to do this on all nodes. This will only remain in effect as long 
as the nodes remain running. If you wish to permanently increase the handoff 
concurrency you will have to do so in the app.config.

-- 
Ian Plosker 
Developer Advocate
Basho Technologies


On Friday, January 27, 2012 at 7:45 AM, Gal Barnea wrote:

&gt; Hi Ian
&gt; 
&gt; Thanks for the informative answer, I am using 1.0.3 indeed.
&gt; 
&gt; A day later, the cluster is making progress, but than I saw this in the 
&gt; console.log:
&gt; 2012-01-27 08:51:32.643 [info] 
&gt; &lt;0.30733.2881&gt;@riak\\_core\\_handoff\\_sender:start\\_fold:87 Handoff of partition 
&gt; riak\\_kv\\_vnode 50239118783249787813
&gt; 2516661246222288006726811648 from 
&gt; 'r...@ec2-107-21-156-59.compute-1.amazonaws.com 
&gt; (mailto:r...@ec2-107-21-156-59.compute-1.amazonaws.com)' to 
&gt; 'r...@ec2-leaving.compute-1.amazonaws.com 
&gt; (mailto:r...@ec2-leaving.compute-1.amazonaws.com)' completed: sent 5100479 
&gt; objects in 10596.49 seconds
&gt; 
&gt; 
&gt; so we've dropped 50% in rate and are now less than 500 records/second !
&gt; 
&gt; Frankly, I think this is problematic any way you look at it...If I need to 
&gt; wait days every time I manually remove a server from the cluster, it isn't 
&gt; really a valid solution from my perspective. 
&gt; 
&gt; Any thoughts?
&gt; 
&gt; Regards
&gt; Gal 
&gt; 
&gt; 
&gt; On Thu, Jan 26, 2012 at 11:35 PM, Ian Plosker  (mailto:i...@basho.com)&gt; wrote:
&gt; &gt; Gal, 
&gt; &gt; 
&gt; &gt; The limiting factor on EC2 will likely be IOPs (i.e. Disk throughput). EC2 
&gt; &gt; is a IOPs constrained environment, especially if you're using EBS. Further, 
&gt; &gt; doing a leave can induce a large number of ownership changes to ensure that 
&gt; &gt; preflists maintain the appropriate n\\_vals. The number of partitions that 
&gt; &gt; need to be shuffled can exceed 80% of all partitions. In short, it can take 
&gt; &gt; a while for the rebalance to complete. Assuming you're using a &gt;=1.0 
&gt; &gt; release, you're cluster should still correctly respond to all incoming 
&gt; &gt; requests. 
&gt; &gt; 
&gt; &gt; Which version of Riak are you using? As of Riak 1.0.3, 
&gt; &gt; `handoff\\_concurrency`, the number of outgoing handoffs per node, is set to 
&gt; &gt; 1. This will reduce the rate at which the rebalance occurs, but it reduces 
&gt; &gt; the impact of the rebalance on your cluster. 
&gt; &gt; 
&gt; &gt; -- 
&gt; &gt; Ian Plosker 
&gt; &gt; Developer Advocate
&gt; &gt; Basho Technologies, Inc.
&gt; &gt; 
&gt; &gt; 
&gt; &gt; 
&gt; &gt; On Thursday, January 26, 2012 at 3:43 PM, Gal Barnea wrote:
&gt; &gt; 
&gt; &gt; 
&gt; &gt; 
&gt; &gt; &gt; Ok, so now I can see in the "leaving" node logs:
&gt; &gt; &gt; 2012-01-26 19 (tel:2012-01-26%2019):18:23.015 [info] 
&gt; &gt; &gt; &lt;0.32148.2873&gt;@riak\\_core\\_handoff\\_sender:start\\_fold:39 Starting handoff of 
&gt; &gt; &gt; partition riak\\_kv\\_vnode 685078892498860742907977265335757665463718379520 
&gt; &gt; &gt; from 'r...@ec2-leaving.compute-1.amazonaws.com 
&gt; &gt; &gt; (mailto:r...@ec2-leaving.compute-1.amazonaws.com)' to 
&gt; &gt; &gt; 'r...@ec2-othernode.compute-1.amazonaws.com 
&gt; &gt; &gt; (mailto:r...@ec2-othernode.compute-1.amazonaws.com)'
&gt; &gt; &gt; 2012-01-26 19 (tel:2012-01-26%2019):24:17.798 [info] &lt;0.31620.2873&gt; 
&gt; &gt; &gt; alarm\\_handler: {set,{system\\_memory\\_high\\_watermark,[]}}
&gt; &gt; &gt; 2012-01-26 20 (tel:2012-01-26%2020):23:28.991 [info] 
&gt; &gt; &gt; &lt;0.32148.2873&gt;@riak\\_core\\_handoff\\_sender:start\\_fold:87 Handoff of 
&gt; &gt; &gt; partition riak\\_kv\\_vnode 685078892498860742907977265335757665463718379520 
&gt; &gt; &gt; from 'r...@leaving.compute-1.amazonaws.com 
&gt; &gt; &gt; (mailto:r...@leaving.compute-1.amazonaws.com)' to 
&gt; &gt; &gt; 'r...@ec2-othernode.compute-1.amazonaws.com 
&gt; &gt; &gt; (mailto:r...@ec2-othernode.compute-1.amazonaws.com)' completed: sent 
&gt; &gt; &gt; 5110665 objects in 3905.97 seconds
&gt; &gt; &gt; 
&gt; &gt; &gt; so things \\*are\\* moving but at a rate of 1308 records per second.
&gt; &gt; &gt; This sounds very slow to me, accounting for the small record size, the 
&gt; &gt; &gt; high bw rate inside ec2 and practically 0% load on the servers
&gt; &gt; &gt; 
&gt; &gt; &gt; any thoughts? 
&gt; &gt; &gt; 
&gt; &gt; &gt; 
&gt; &gt; &gt; 
&gt; &gt; &gt; On Thu, Jan 26, 2012 at 10:12 PM, Gal Barnea  &gt; &gt; (mailto:g...@eyeviewdigital.com)&gt; wrote:
&gt; &gt; &gt; &gt; Hi all
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; I have a 6 server cluster running on ec2 (m1.large) - this is an 
&gt; &gt; &gt; &gt; evaluation environment, so practically no load besides the existing 
&gt; &gt; &gt; &gt; data (~200 million records, ~1k each) 
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; after running "riak-admin leave" on one of the node, I noticed that for 
&gt; &gt; &gt; &gt; more than 3 hours
&gt; &gt; &gt; &gt; 1 - member\\_status showed that there is one "leaving" node and pending 
&gt; &gt; &gt; &gt; data to handoff on the rest but the numbers never changed
&gt; &gt; &gt; &gt; 2 - riak-admin transfers - showed handoffs waiting, but nothing changed
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; at this point, I restarted the "leaving" node, so now the status is 
&gt; &gt; &gt; &gt; 1 - member\\_status - still stuck with the same numbers
&gt; &gt; &gt; &gt; 2 - transfers - are slowly changing
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; The leaving server's logs are showing that a single handoff started 
&gt; &gt; &gt; &gt; after the restart,but nothing since (roughly an hour ago)
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; Interestingly, the leaving server is pretty idle while the remaining 
&gt; &gt; &gt; &gt; servers are working hard at 50%-60% cpu 
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; so, the question now is where should I dig around to try and understand 
&gt; &gt; &gt; &gt; what's going on. Any thoughts? 
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; Thanks
&gt; &gt; &gt; &gt; Gal
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; 
&gt; &gt; &gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; &gt; &gt; riak-users mailing list
&gt; &gt; &gt; riak-users@lists.basho.com (mailto:riak-users@lists.basho.com)
&gt; &gt; &gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt; &gt; &gt; 
&gt; &gt; &gt; 
&gt; &gt; &gt; 
&gt; &gt; 
&gt; &gt; 
&gt; 

\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

