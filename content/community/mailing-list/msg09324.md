---
title: "Re: mysterious Riak problems"
description: ""
project: community
lastmod: 2012-11-14T10:43:53-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg09324"
mailinglist_parent_id: "msg09323"
author_name: "Matthew Von-Maszewski"
project_section: "mailinglistitem"
sent_date: 2012-11-14T10:43:53-08:00
---


Dave,

Ok, heavy writes. Let's see if leveldb has hit one of its intentional "stalls":

&gt; sort /var/db/riak/leveldb/\\*/LOG\\* | grep -i waiting

See if that shows any indication of stall in the LOG files of leveldb. If so, 
pick one server and send me a combined LOG file from that server:

sort /var/db/riak/leveldb/\\*/LOG\\* &gt;LOG.all

That will tar zip really well.

Matthew


On Nov 14, 2012, at 1:34 PM, David Lowell wrote:

&gt; Thanks Matthew. I've run both greps with no hits, unfortunately.
&gt; 
&gt; A couple of details that I want to highlight. Since I first posted about this 
&gt; issue, we upgraded from Riak 1.2.0, to 1.2.1. Following that upgrade, we 
&gt; continue see these periods of instability with errors in the logs like 
&gt; "riak\\_kv\\_vnode worker pool crashed", but we started seeing lots of new error 
&gt; records in the logs about "Unrecognized message" as well. In both cases, we 
&gt; see tons of these "long\\_gc" monitoring messages and several 
&gt; "system\\_memory\\_high\\_watermark" alarms during these periods. The client also 
&gt; has connection problems such as timeouts and connections being refused.
&gt; 
&gt; The logs from the last 18 hours on this server are really a mess, with very 
&gt; high levels of all of these errors. I'd be happy to send them along if you 
&gt; think that would help.
&gt; 
&gt; It's also worth noting that our application is pretty write heavy, has lots 
&gt; of parallel processes generating those writes (so not a lot of flow control 
&gt; if Riak bogs down, at least not yet). It's probably pushing Riak fairly hard.
&gt; 
&gt; Dave
&gt; 
&gt; --
&gt; Dave Lowell
&gt; d...@connectv.com
&gt; 
&gt; On Nov 14, 2012, at 8:51 AM, Matthew Von-Maszewski wrote:
&gt; 
&gt;&gt; Dave,
&gt;&gt; 
&gt;&gt; Just getting my head back into the game. Was away for a few days. Random 
&gt;&gt; thought, maybe there is a hard drive with a read problem. That can cause 
&gt;&gt; issues similar to this. 1.2.1 does NOT percolate the read errors seen in 
&gt;&gt; leveldb to riak-admin (yes, that should start to happen in 1.3).
&gt;&gt; 
&gt;&gt; I will assume your leveldb "data\\_root" is "/var/db/riak/leveldb" for this 
&gt;&gt; script. Please substitute your appropriate path from app.config and try 
&gt;&gt; these two commands on each physical server (node):
&gt;&gt; 
&gt;&gt; sort /var/db/riak/leveldb/\\*/LOG\\* | grep corrupted
&gt;&gt; sort /var/db/riak/leveldb/\\*/LOG\\* | grep checksum
&gt;&gt; 
&gt;&gt; If you get hits on either, we have found the problem.
&gt;&gt; 
&gt;&gt; The "LOCK file unavailable" is more of a statement about the internal 
&gt;&gt; condition of the code instead of an error. The message is saying that the 
&gt;&gt; first attempt to re-open a vnode failed because the prior instance is still 
&gt;&gt; closing (or more likely waiting for Erlang's garbage collection to finish 
&gt;&gt; destroying things). This message is new to 1.2 code base.
&gt;&gt; 
&gt;&gt; Matthew
&gt;&gt; 
&gt;&gt; 
&gt;&gt; 
&gt;&gt; 
&gt;&gt; On Nov 7, 2012, at 6:56 PM, David Lowell wrote:
&gt;&gt; 
&gt;&gt;&gt; After further thought, I want to add more color to this issue. I 
&gt;&gt;&gt; hypothesize the symptoms I described here were continued fallout of an 
&gt;&gt;&gt; earlier crash. So I've waded further back into to the logs to try to shed 
&gt;&gt;&gt; light on how the Riak process was doing prior to this time. It was unhappy. 
&gt;&gt;&gt; It appears to have failed and stopped several times in the 1/2 day prior. 
&gt;&gt;&gt; The pattern revealed in the logs looks something like this:
&gt;&gt;&gt; 
&gt;&gt;&gt; - Following a clean startup, the service runs for a while
&gt;&gt;&gt; 
&gt;&gt;&gt; - Usually the first log entry of any kind near the beginning of a long 
&gt;&gt;&gt; block of error logs is "alarm\\_handler: 
&gt;&gt;&gt; {set,{system\\_memory\\_high\\_watermark,[]}}" 
&gt;&gt;&gt; ( Is this indicating excessive memory use? )
&gt;&gt;&gt; 
&gt;&gt;&gt; - Within a few minutes we see several log messages warning about "long\\_gc", 
&gt;&gt;&gt; which I assume is an indication that garbage collection took longer than 
&gt;&gt;&gt; some threshold
&gt;&gt;&gt; 
&gt;&gt;&gt; - Within the next minute or two, we start to see the legions of errors, 
&gt;&gt;&gt; "riak\\_kv\\_vnode worker pool crashed", and "gen\\_fsm" having some sort of 
&gt;&gt;&gt; timeout when trying to communicate with the eleveldb backend
&gt;&gt;&gt; 
&gt;&gt;&gt; - Eventually we see a log record indicating "Failed to start 
&gt;&gt;&gt; riak\\_kv\\_eleveldb" because of a leveldb LOCK file being temporarily 
&gt;&gt;&gt; unavailable
&gt;&gt;&gt; 
&gt;&gt;&gt; - Then Riak starts to exit: riak:stop:46 "backend module failed to start." 
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; So, not really knowing the Riak internals, it's a little difficult to piece 
&gt;&gt;&gt; together the story here. Could be we're running low on memory. Hard to know 
&gt;&gt;&gt; why riak\\_kv\\_workers are failing, or why this leveldb LOCK file is 
&gt;&gt;&gt; unavailable. To those more learned, do these log records tell a story?
&gt;&gt;&gt; 
&gt;&gt;&gt; For the record, we're using the default 8 MB of leveldb cache per vnode, so 
&gt;&gt;&gt; that ought to cap cache for our 512 vnodes at 4 GB. Our host has 32 GB of 
&gt;&gt;&gt; physical memory. Are there other pieces of Riak that can use a lot of 
&gt;&gt;&gt; memory that we need to look out for?
&gt;&gt;&gt; 
&gt;&gt;&gt; I'll include a few of the actual log records for reference, below.
&gt;&gt;&gt; 
&gt;&gt;&gt; Dave
&gt;&gt;&gt; 
&gt;&gt;&gt; --
&gt;&gt;&gt; Dave Lowell
&gt;&gt;&gt; d...@connectv.com
&gt;&gt;&gt; 
&gt;&gt;&gt; Representative logs, many similar ones deleted for brevity:
&gt;&gt;&gt; 
&gt;&gt;&gt; 2012-11-07 01:41:02.395 [info] &lt;0.51.0&gt; alarm\\_handler: 
&gt;&gt;&gt; {set,{system\\_memory\\_high\\_watermark,[]}}
&gt;&gt;&gt; 2012-11-07 01:55:50.517 [info] 
&gt;&gt;&gt; &lt;0.73.0&gt;@riak\\_core\\_sysmon\\_handler:handle\\_event:85 monitor long\\_gc 
&gt;&gt;&gt; &lt;0.18585.32&gt; 
&gt;&gt;&gt; [{initial\\_call,{riak\\_core\\_coverage\\_fsm,init,1}},{almost\\_current\\_function,{riak\\_core\\_coverage\\_plan,'-next\\_vnode/2-fun-0-',2}},{message\\_queue\\_len,0}]
&gt;&gt;&gt; 
&gt;&gt;&gt; [{timeout,219},{old\\_heap\\_block\\_size,0},{heap\\_block\\_size,28657},{mbuf\\_size,0},{stack\\_size,48},{old\\_heap\\_size,0},{heap\\_size,11430}]
&gt;&gt;&gt; 2012-11-07 01:56:20.303 [error] &lt;0.9231.0&gt;@riak\\_core\\_vnode:handle\\_info:510 
&gt;&gt;&gt; 1258832464966656615093408225054454710289582522368 riak\\_kv\\_vnode worker pool 
&gt;&gt;&gt; crashed 
&gt;&gt;&gt; {timeout,{gen\\_fsm,sync\\_send\\_event,[&lt;0.9234.0&gt;,{checkout,false,5000},5000]}}
&gt;&gt;&gt; 2012-11-07 01:56:22.382 [error] &lt;0.10002.0&gt; gen\\_fsm &lt;0.10002.0&gt; in state 
&gt;&gt;&gt; ready terminated with reason: 
&gt;&gt;&gt; {timeout,{gen\\_server,call,[&lt;0.10005.0&gt;,{work,&lt;0.10003.0&gt;,{fold,#Fun,#Fun},{fsm,{40916762,{1398702738851840683437120250060505233655091691520,'riak@10.0.3.11'}},&lt;0.18429.32&gt;}}]}}
&gt;&gt;&gt; 2012-11-07 01:57:11.833 [error] &lt;0.19755.32&gt;@riak\\_kv\\_vnode:init:265 Failed 
&gt;&gt;&gt; to start riak\\_kv\\_eleveldb\\_backend Reason: {db\\_open,"IO error: lock 
&gt;&gt;&gt; /var/data/ctv/riak/leveldb/1258832464966656615093408225054454710289582522368/LOCK:
&gt;&gt;&gt; Resource temporarily unavailable"}
&gt;&gt;&gt; 2012-11-07 01:57:27.425 [info] 
&gt;&gt;&gt; &lt;0.73.0&gt;@riak\\_core\\_sysmon\\_handler:handle\\_event:85 monitor long\\_gc 
&gt;&gt;&gt; &lt;0.19181.32&gt; 
&gt;&gt;&gt; [{initial\\_call,{riak\\_core\\_coverage\\_fsm,init,1}},{almost\\_current\\_function,{gen\\_fsm,loop,7}},{message\\_queue\\_len,0}]
&gt;&gt;&gt; 
&gt;&gt;&gt; [{timeout,109},{old\\_heap\\_block\\_size,0},{heap\\_block\\_size,28657},{mbuf\\_size,0},{stack\\_size,47},{old\\_heap\\_size,0},{heap\\_size,9171}]
&gt;&gt;&gt; 2012-11-07 01:57:51.109 [error] &lt;0.10002.0&gt; CRASH REPORT Process 
&gt;&gt;&gt; &lt;0.10002.0&gt; with 0 neighbours exited with reason: 
&gt;&gt;&gt; {timeout,{gen\\_server,call,[&lt;0.10005.0&gt;,{work,&lt;0.10003.0&gt;,{fold,#Fun,#Fun},{fsm,{40916762,{1398702738851840683437120250060505233655091691520,'riak@10.0.3.11'}},&lt;0.18429.32&gt;}}]}}
&gt;&gt;&gt; in gen\\_fsm:terminate/7 line 
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; On Nov 7, 2012, at 11:34 AM, David Lowell wrote:
&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Hello Riak Folks,
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; The last three days, we've been having a string of problems with Riak. An 
&gt;&gt;&gt;&gt; otherwise healthy server running our full application stack will suddenly 
&gt;&gt;&gt;&gt; start throwing a bunch of errors in the logs. Although the Riak processes 
&gt;&gt;&gt;&gt; stay up, most or all requests to Riak fail during these periods.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; The errors in the logs are predominantly describing "riak\\_kv\\_vnode worker 
&gt;&gt;&gt;&gt; pool crashed" and timeout conditions. This morning, we had this crashy 
&gt;&gt;&gt;&gt; behavior start immediately after a clean Riak startup, and making a single 
&gt;&gt;&gt;&gt; call to our API, so the logs are quite free of other noise. I've 
&gt;&gt;&gt;&gt; summarized those logs below for curious parties, and can attach the full 
&gt;&gt;&gt;&gt; set of logs if needed.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; I forgot to check this morning, but during a similar outage on Monday, the 
&gt;&gt;&gt;&gt; Riak server was refusing connections to new clients.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Interestingly, after giving Riak a while with no traffic at all today 
&gt;&gt;&gt;&gt; (like 15-30 minutes), it appears to have recovered without a restart. 
&gt;&gt;&gt;&gt; We've had similar recoveries during other "outages" of this type since 
&gt;&gt;&gt;&gt; Sunday evening. Facilitating this sort of recovery does seem to require 
&gt;&gt;&gt;&gt; shutting down all application KV requests for a while.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; We're suspicious of some kind of corruption in the eleveldb on-disk files, 
&gt;&gt;&gt;&gt; because in past outages of this type, we've observed that the condition 
&gt;&gt;&gt;&gt; persists over reboots. But we don't have much more evidence than that. Is 
&gt;&gt;&gt;&gt; there a command we can run that will check over eleveldb files for 
&gt;&gt;&gt;&gt; corruption or inconsistency?
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Other than that, what can cause "worker pool crashed" events? What do you 
&gt;&gt;&gt;&gt; know about the "timeouts" that are in these logs?
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; For the record, we're running Riak 1.2.0 on Ubuntu 10.04, eleveldb 
&gt;&gt;&gt;&gt; backend with 512 partitions. We're running predominantly in a single-node 
&gt;&gt;&gt;&gt; configuration on a bunch of isolated dev boxes at the moment, on our way 
&gt;&gt;&gt;&gt; to spreading out our 512 vnodes onto 5 hosts in production.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Thanks for your help,
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Dave
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; --
&gt;&gt;&gt;&gt; Dave Lowell
&gt;&gt;&gt;&gt; d...@connectv.com
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 2012-11-07 18:11:03.398 [info] &lt;0.7.0&gt; Application lager started on node 
&gt;&gt;&gt;&gt; 'riak@10.0.3.11'
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; ... normal startup messages ...
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 2012-11-07 18:11:50.109 [info] 
&gt;&gt;&gt;&gt; &lt;0.10582.0&gt;@riak\\_core:wait\\_for\\_application:419 Wait complete for 
&gt;&gt;&gt;&gt; application riak\\_search (0 seconds)
&gt;&gt;&gt;&gt; 2012-11-07 18:22:18.509 [error] &lt;0.2897.0&gt;@riak\\_core\\_vnode:handle\\_info:510 
&gt;&gt;&gt;&gt; 105616329260241031198313161739262640092323250176 riak\\_kv\\_vnode worker pool 
&gt;&gt;&gt;&gt; crashed 
&gt;&gt;&gt;&gt; {timeout,{gen\\_server,call,[&lt;0.2902.0&gt;,{work,&lt;0.2900.0&gt;,{fold,#Fun,#Fun},{fsm,{96247562,{105616329260241031198313161739262640092323250176,'riak@10.0.3.11'}},&lt;0.15324.0&gt;}}]}}
&gt;&gt;&gt;&gt; 2012-11-07 18:22:18.509 [error] &lt;0.2899.0&gt; gen\\_fsm &lt;0.2899.0&gt; in state 
&gt;&gt;&gt;&gt; ready terminated with reason: 
&gt;&gt;&gt;&gt; {timeout,{gen\\_server,call,[&lt;0.2902.0&gt;,{work,&lt;0.2900.0&gt;,{fold,#Fun,#Fun},{fsm,{96247562,{105616329260241031198313161739262640092323250176,'riak@10.0.3.11'}},&lt;0.15324.0&gt;}}]}}
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; ... 13 more "riak\\_kv\\_vnode worker pool crashed" messages...
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 2012-11-07 18:22:21.245 [error] &lt;0.2899.0&gt; CRASH REPORT Process &lt;0.2899.0&gt; 
&gt;&gt;&gt;&gt; with 0 neighbours exited with reason: 
&gt;&gt;&gt;&gt; {timeout,{gen\\_server,call,[&lt;0.2902.0&gt;,{work,&lt;0.2900.0&gt;,{fold,#Fun,#Fun},{fsm,{96247562,{105616329260241031198313161739262640092323250176,'riak@10.0.3.11'}},&lt;0.15324.0&gt;}}]}}
&gt;&gt;&gt;&gt; in gen\\_fsm:terminate/7 line 611
&gt;&gt;&gt;&gt; 2012-11-07 18:22:21.844 [error] &lt;0.2944.0&gt; gen\\_fsm &lt;0.2944.0&gt; in state 
&gt;&gt;&gt;&gt; ready terminated with reason: 
&gt;&gt;&gt;&gt; {timeout,{gen\\_server,call,[&lt;0.2947.0&gt;,{work,&lt;0.2945.0&gt;,{fold,#Fun,#Fun},{fsm,{96247562,{114179815416476790484662877555959610910619729920,'riak@10.0.3.11'}},&lt;0.15324.0&gt;}}]}}
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; ... 13 more "CRASH REPORT Process  with 0 neighbours exited with 
&gt;&gt;&gt;&gt; reason" and "gen\\_fsm &lt;0.2989.0&gt; in state ready terminated with reason" 
&gt;&gt;&gt;&gt; message pairs
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 2012-11-07 18:23:21.427 [error] &lt;0.15322.0&gt; gen\\_server &lt;0.15322.0&gt; 
&gt;&gt;&gt;&gt; terminated with reason: 
&gt;&gt;&gt;&gt; {error,{case\\_clause,{error,timeout,[[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[&lt;&lt;"1352256943.4983411"&gt;&gt;],[],[],[],...]}},...}
&gt;&gt;&gt;&gt; 2012-11-07 18:23:21.495 [error] &lt;0.15322.0&gt; CRASH REPORT Process 
&gt;&gt;&gt;&gt; &lt;0.15322.0&gt; with 0 neighbours exited with reason: 
&gt;&gt;&gt;&gt; {error,{case\\_clause,{error,timeout,[[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[&lt;&lt;"1352256943.4983411"&gt;&gt;],[],[],[],...]}},...}
&gt;&gt;&gt;&gt; in gen\\_server:terminate/6 line 747
&gt;&gt;&gt;&gt; 2012-11-07 18:23:21.525 [error] &lt;0.10590.0&gt; Supervisor riak\\_api\\_pb\\_sup had 
&gt;&gt;&gt;&gt; child undefined started with {riak\\_api\\_pb\\_server,start\\_link,undefined} at 
&gt;&gt;&gt;&gt; &lt;0.15322.0&gt; exit with reason 
&gt;&gt;&gt;&gt; {error,{case\\_clause,{error,timeout,[[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[&lt;&lt;"1352256943.4983411"&gt;&gt;],[],[],[],...]}},...}
&gt;&gt;&gt;&gt; in context child\\_terminated
&gt;&gt;&gt; 
&gt;&gt; 
&gt; 

