---
title: "Re: avg write io wait time regression in 1.2.1"
description: ""
project: community
lastmod: 2012-11-06T17:28:03-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg09213"
mailinglist_parent_id: "msg09210"
author_name: "Matthew Von-Maszewski"
project_section: "mailinglistitem"
sent_date: 2012-11-06T17:28:03-08:00
---


Dietrich,

I forgot to mention that I noticed your app.config has the cache size at 500M. 
That will work against you in 1.2.x. I personally introduced a bug that hurts 
performance on that setting. My apologies. I recommend you take it below 100M 
until release notes publish that the bug is fixed.

Matthew


On Nov 6, 2012, at 7:04 PM, Dietrich Featherston wrote:

&gt; Thanks for the feedback. We haven't noticed any drop in riak 
&gt; responsiveness--quite the opposite. We were just alarmed at some of the 
&gt; iostat information we were seeing which may very well result from, as you 
&gt; pointed out, greater concurrency in layers above the disk subsystem. It's not 
&gt; of concern at the moment.
&gt; 
&gt; As for the cluster itself--we are running 9 physical notes with a ring size 
&gt; of 64. Each with a 2.93Ghz 8-core Xeon and ~2 TB of RAID0 SSD storage across 
&gt; 6 physical drives.
&gt; 
&gt; Thanks again for looking into this.
&gt; 
&gt; D
&gt; 
&gt; 
&gt; On Tue, Nov 6, 2012 at 3:19 PM, Matthew Von-Maszewski  
&gt; wrote:
&gt; Dietrich,
&gt; 
&gt; I finally reviewed your LOG.all today. The basic analysis is:
&gt; 
&gt; - you have a really fast disk subsystem, and
&gt; - your machine is bored.
&gt; 
&gt; I make the first comment based upon the fact that your Level-0 file creations 
&gt; take less than 200 ms on files of 40Mbyte with 10,000 keys (or more). I 
&gt; would like to know more specifics about your hardware and operating system 
&gt; (and disk parameters). That information is likely worthy of sharing with all.
&gt; 
&gt; I make the second comment after running a shallow analysis of all compaction 
&gt; activity per minute in LOG.all. Your current ingest rate (rate of adding 
&gt; data to leveldb) is not very high compared to other sites I have seen. So we 
&gt; should continue this discussion if you still believe there is an overall slow 
&gt; down. 
&gt; 
&gt; I will note that the authors of Erlang have given us some suggested changes 
&gt; in our Erlang to leveldb interface this past weekend. That information could 
&gt; also give leveldb a throughput boost if proven valid. Keep you posted.
&gt; 
&gt; But at this time I see nothing that yells "massive slow down". I am of 
&gt; course open to being wrong.
&gt; 
&gt; Matthew
&gt; 
&gt; 
&gt; On Nov 2, 2012, at 10:32 AM, Dietrich Featherston wrote:
&gt; 
&gt;&gt; Here's the level output from one of our upgraded nodes. Included our 
&gt;&gt; app.config as well. Will continue looking for clues. I can put together some 
&gt;&gt; snapshots of our sst file sizes across the cluster if you think that would 
&gt;&gt; help as well.
&gt;&gt; 
&gt;&gt; On Fri, Nov 2, 2012 at 6:19 AM, Matthew Von-Maszewski  
&gt;&gt; wrote:
&gt;&gt; Dietrich,
&gt;&gt; 
&gt;&gt; I can make two guesses into the increased disk writes. But I am also 
&gt;&gt; willing to review your actual LOG files to isolate root cause. If you could 
&gt;&gt; run the following and post the resulting file from one server, I will review 
&gt;&gt; it over the weekend or early next week:
&gt;&gt; 
&gt;&gt; sort /var/lib/riak/leveldb/\\*/LOG &gt;LOG.all
&gt;&gt; 
&gt;&gt; The file will compress well. And no need to stop the server, just gather 
&gt;&gt; the LOG data live.
&gt;&gt; 
&gt;&gt; Guess 1: your data is in a transition phase. 1.1 used 2 Megabyte files 
&gt;&gt; exclusively. 1.2 is resizing the files to much larger sizes during a 
&gt;&gt; compaction. You could be seeing a larger number of files than usual 
&gt;&gt; participating in each compaction as the file sizes change. While this is 
&gt;&gt; possible, I have doubts … hence this is a guess.
&gt;&gt; 
&gt;&gt; Guess 2: I increased the various leveldb file sizes to reduce the number of 
&gt;&gt; open and closes, both for writes and random reads. This helped latencies in 
&gt;&gt; both the compactions and random reads. Any compaction in 1.2 is likely to 
&gt;&gt; reread and write larger total number of bytes. While this is possible, I 
&gt;&gt; again have doubts … the number of read operations should also go up if this 
&gt;&gt; guess is correct. Your read operations have not increased. This guess 
&gt;&gt; might still be valid if the read operations were satisfied by the Linux 
&gt;&gt; memory data cache. I do not how those would be counted or not counted.
&gt;&gt; 
&gt;&gt; 
&gt;&gt; Matthew
&gt;&gt; 
&gt;&gt; 
&gt;&gt; On Nov 1, 2012, at 10:01 PM, Dietrich Featherston wrote:
&gt;&gt; 
&gt;&gt;&gt; Will check on that.
&gt;&gt;&gt; 
&gt;&gt;&gt; Can you think of anything that would explain the 5x increase in disk writes 
&gt;&gt;&gt; we are seeing with the same workload?
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; On Thu, Nov 1, 2012 at 6:03 PM, Matthew Von-Maszewski  
&gt;&gt;&gt; wrote:
&gt;&gt;&gt; Look for any activity in the LOG. Level-0 "creations" are fast and not 
&gt;&gt;&gt; typically relevant. You would be most interested in LOG lines containing 
&gt;&gt;&gt; "Compacting" (start) and "Compacted" (end). The time in between will 
&gt;&gt;&gt; throttle. The idea is that these compaction events can pile up, one after 
&gt;&gt;&gt; another and multiple overlapping. It is these heavy times where the 
&gt;&gt;&gt; throttle saves the user experience.
&gt;&gt;&gt; 
&gt;&gt;&gt; Matthew
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; On Nov 1, 2012, at 8:54 PM, Dietrich Featherston wrote:
&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Thanks. The amortized stalls may very well describe what we are seeing. If 
&gt;&gt;&gt;&gt; I combine leveldb logs from all partitions on one of the upgraded nodes 
&gt;&gt;&gt;&gt; what should I look for in terms of compaction activity to verify this?
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; On Thu, Nov 1, 2012 at 5:48 PM, Matthew Von-Maszewski  
&gt;&gt;&gt;&gt; wrote:
&gt;&gt;&gt;&gt; Dietrich,
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; I can see your concern with the write IOS statistic. Let me comment on 
&gt;&gt;&gt;&gt; the easy question first: block\\_size.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; The block\\_size parameter in 1.1 was not getting passed to leveldb from the 
&gt;&gt;&gt;&gt; erlang layer. You were using a 4096 byte block parameter no matter what 
&gt;&gt;&gt;&gt; you typed in the app.config. The block\\_size is used by leveldb as a 
&gt;&gt;&gt;&gt; threshold. Once you accumulate enough data above that threshold, the 
&gt;&gt;&gt;&gt; current block is written to disk and a new one started. If you have 10k 
&gt;&gt;&gt;&gt; data values, your get one data item per block and its size is ~10k. If 
&gt;&gt;&gt;&gt; you have 1k data values, you get about four per block and the block is 
&gt;&gt;&gt;&gt; about 4k.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; We recommend 4k blocks to help read performance. The entire block has to 
&gt;&gt;&gt;&gt; run through decompression and potentially CRC calculation when it comes 
&gt;&gt;&gt;&gt; off the disk. That CPU time really kills any disk performance gains by 
&gt;&gt;&gt;&gt; having larger blocks. Ok, that might change in 1.3 as we enable hardware 
&gt;&gt;&gt;&gt; CRC … but only if you have "verify\\_checksums, true" in app.config.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Back to performance: I have not seen the change your graph details when 
&gt;&gt;&gt;&gt; testing with SAS drives under moderate load. I am only today starting 
&gt;&gt;&gt;&gt; qualification tests with SSD drives.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; But my 1.2 and 1.3 tests focus on drive / Riak saturation. 1.1 has the 
&gt;&gt;&gt;&gt; nasty tendency to stall (intentionally) when we saturate the write side of 
&gt;&gt;&gt;&gt; leveldb, . The stall was measured in seconds or even minutes in 1.1. 
&gt;&gt;&gt;&gt; 1.2.1 has a write throttle that forecasts leveldb's stall state and 
&gt;&gt;&gt;&gt; incrementally slows the individual writes to prevent the stalls. Maybe 
&gt;&gt;&gt;&gt; that is what is being seen in the graph. The only way to know for sure is 
&gt;&gt;&gt;&gt; to get an dump of your leveldb LOG files, combined them, then compare 
&gt;&gt;&gt;&gt; compaction activity to your graph.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Write stalls are detailed here: 
&gt;&gt;&gt;&gt; http://basho.com/blog/technical/2012/10/30/leveldb-in-riak-1p2/
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; How can I better assist you at this point?
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Matthew
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; On Nov 1, 2012, at 8:13 PM, Dietrich Featherston wrote:
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; We've just gone through the process of upgrading two riak clusters from 
&gt;&gt;&gt;&gt;&gt; 1.1 to 1.2.1. Both are on the leveldb backend backed by RAID0'd SSDs. 
&gt;&gt;&gt;&gt;&gt; The process has gone smoothly and we see that latencies as measured at 
&gt;&gt;&gt;&gt;&gt; the gen\\_fsm level are largely unaffected.
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; However, we are seeing some troubling disk statistics and I'm looking for 
&gt;&gt;&gt;&gt;&gt; an explanation before we upgrade the remainder of our nodes. The source 
&gt;&gt;&gt;&gt;&gt; of the worry seems to be a huge amplification in the number of writes 
&gt;&gt;&gt;&gt;&gt; serviced by the disk which may be the cause of rising io wait times.
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; My first thought was that this could be due to some leveldb tuning in 
&gt;&gt;&gt;&gt;&gt; 1.2.1 which increases file sizes per the release notes 
&gt;&gt;&gt;&gt;&gt; (https://github.com/basho/riak/blob/master/RELEASE-NOTES.md). But nodes 
&gt;&gt;&gt;&gt;&gt; that were upgraded yesterday are still showing this symptom. I would have 
&gt;&gt;&gt;&gt;&gt; expected any block re-writing to have subsided by now.
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; Next hypothesis has to do with block size overriding in app.config. In 
&gt;&gt;&gt;&gt;&gt; 1.1, we had specified custom block sizes of 256k. We removed this prior 
&gt;&gt;&gt;&gt;&gt; to upgrading to 1.2.1 at the advice of #riak since block size 
&gt;&gt;&gt;&gt;&gt; configuration was ignored prior to 1.2 ('"block\\_size" parameter within 
&gt;&gt;&gt;&gt;&gt; app.config for leveldb was ignored. This parameter is now properly 
&gt;&gt;&gt;&gt;&gt; passed to leveldb.' --&gt; 
&gt;&gt;&gt;&gt;&gt; https://github.com/basho/riak/commit/f12596c221a9d942cc23d8e4fd83c9ca46e02105).
&gt;&gt;&gt;&gt;&gt; I'm wondering if the block size parameter really was being passed to 
&gt;&gt;&gt;&gt;&gt; leveldb, and having removed it, blocks are now being rewritten to a new 
&gt;&gt;&gt;&gt;&gt; size, perhaps different from what they were being written as before 
&gt;&gt;&gt;&gt;&gt; (https://github.com/basho/riak\\_kv/commit/ad192ee775b2f5a68430d230c0999a2caabd1155)
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; Here is the output of the following script showing the increased writes 
&gt;&gt;&gt;&gt;&gt; to disk (https://gist.github.com/37319a8ed2679bb8b21d)
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; --an upgraded 1.2.1 node--
&gt;&gt;&gt;&gt;&gt; read ios: 238406742
&gt;&gt;&gt;&gt;&gt; write ios: 4814320281
&gt;&gt;&gt;&gt;&gt; read/write ratio: .04952033
&gt;&gt;&gt;&gt;&gt; avg wait: .10712340
&gt;&gt;&gt;&gt;&gt; read wait: .49174364
&gt;&gt;&gt;&gt;&gt; write wait: .42695475
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; --a node still running 1.1--
&gt;&gt;&gt;&gt;&gt; read ios: 267770032
&gt;&gt;&gt;&gt;&gt; write ios: 944170656
&gt;&gt;&gt;&gt;&gt; read/write ratio: .28360342
&gt;&gt;&gt;&gt;&gt; avg wait: .34237204
&gt;&gt;&gt;&gt;&gt; read wait: .47222371
&gt;&gt;&gt;&gt;&gt; write wait: 1.83283749
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; And here's what munin is showing us in terms of avg io wait times.
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; Any thoughts on what might explain this?
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; Thanks,
&gt;&gt;&gt;&gt;&gt; D
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt; 
&gt;&gt; 
&gt;&gt; 
&gt; 
&gt; 

