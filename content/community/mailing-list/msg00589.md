---
title: "Re: [ANN] Riak Release 0.11.0"
description: ""
project: community
lastmod: 2010-06-14T17:32:41-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg00589"
mailinglist_parent_id: "msg00587"
author_name: "Alan McConnell"
project_section: "mailinglistitem"
sent_date: 2010-06-14T17:32:41-07:00
---


Justin,
Thanks a ton for the quick response, that makes perfect sense. It also
explains why I've only seen this behavior in our test environment. It may
make sense to note this info in the "Replication" node of the wiki, perhaps
in the "What does N=3 really mean?" section (there's an example of a 2-node
cluster with N=3 in there). I think our case of "Let's quickly test this
thing out on more than 1 box with default config" is probably common and
this behavior could lead some to believe that Riak isn't replicating
properly.

Thanks again.

-Alan

On Mon, Jun 14, 2010 at 12:54 PM, Justin Sheehy  wrote:

&gt; Hi, Alan.
&gt;
&gt; Your replicas do in fact exist on both nodes. However, I understand
&gt; that the situation you are observing is confusing. I will attempt to
&gt; explain.
&gt;
&gt; Quite some time ago, something surprising was noticed by some of our
&gt; users during their pre-production testing. Some intentional failure
&gt; scenarios (with busted nodes, etc) would fail much more slowly when
&gt; R=1 than when R=2. This was due to the fact that to satisfy a R=1
&gt; request with a non-object response (timeout or notfound), we would
&gt; wait for all N nodes to reply. With R=2, we could send this response
&gt; as soon as N-1 nodes reply. In some situations this is a dramatic
&gt; difference in time.
&gt;
&gt; To remove this perceived problem we implemented what we refer to as
&gt; "basic quorum". If a simple majority of vnodes have produced
&gt; non-successful internal replies, we return a non-success value such as
&gt; a notfound. This means that if there is only one copy of the object
&gt; out there, and the node holding it is slowest to respond, the client
&gt; will not see that object in their response but will instead get the
&gt; notfound instead of waiting for the last node to respond or time out.
&gt;
&gt; (note that read-repair will still occur in any case)
&gt;
&gt; This could be avoided if we considered "not found" to be a success
&gt; condition, but then in the above situation you would see not founds
&gt; even with R=2. That would simply be defined as another kind of
&gt; "successful" response. Either way, it is a tradeoff of different
&gt; kinds of surprise.
&gt;
&gt; I hope that this explanation helps with your understanding.
&gt;
&gt; On another note, it's not useful to run Riak with a number of physical
&gt; hosts less than your N value unless you're planning on expanding it
&gt; soon. So: testing with 2 hosts and N=3 means that you are testing
&gt; against a very much not-recommended configuration. I suggest either
&gt; using more hosts or else changing your default bucket N value to 2.
&gt;
&gt; -Justin
&gt;
&gt;
&gt; On Mon, Jun 14, 2010 at 1:59 PM, Alan McConnell 
&gt; wrote:
&gt; &gt; Hey Dan,
&gt; &gt; I have a 2-node cluster with default bucket settings (N=3, etc.), and if
&gt; I
&gt; &gt; take one of the boxes down (and perform reads with R=1) I get tons of
&gt; "key
&gt; &gt; not found" errors for keys I know exist in the cluster. Seems like for
&gt; many
&gt; &gt; keys, all 3 replicas live on one host. From what you've written here
&gt; &gt; though, it seems like that should not happen. Do you know of any way my
&gt; &gt; cluster could have gotten into this state?
&gt; &gt; I did run a restore on this cluster using a riak-admin backup from a
&gt; &gt; different, single-node cluster. I wonder if that caused an uneven
&gt; &gt; distribution.
&gt; &gt; Any help would be appreciated. As it stands now our 2-node cluster has
&gt; &gt; serious read problems if either node goes down.
&gt; &gt; -Alan
&gt;
