---
title: "Re: Whole cluster times out if one node is gone"
description: ""
project: community
lastmod: 2010-11-27T15:21:42-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg01635"
mailinglist_parent_id: "msg01634"
author_name: "Sean Cribbs"
project_section: "mailinglistitem"
sent_date: 2010-11-27T15:21:42-08:00
---


1) Riak detects node outage the same way any Erlang system does - when a 
message fails to deliver, or the heartbeat maintained by epmd fails. The 
default timeout in epmd is 1 minute, which is probably why you're seeing it 
take 1 minute to be detected.
2) If it takes too long (the vnode is overloaded, perhaps, or is just starting 
up as a hint partition) to retrieve from any node, the request can time out.9
3) You could probably configure epmd to timeout sooner, but then you become 
more vulnerable to temporary partitions. YMMV

Sean Cribbs 
Developer Advocate
Basho Technologies, Inc.
http://basho.com/

On Nov 27, 2010, at 3:21 PM, Jay Adkisson wrote:

&gt; Neville, I'm not sure how you mean. The network gear is all functional, 
&gt; otherwise I wouldn't be able to interact with the machines at all (they're at 
&gt; our colo). But as far as I understand, if I hard reboot a box (or, in a 
&gt; real-world scenario, the pdu fails), the switch will happily continue 
&gt; forwarding packets into nothingness, causing HTTP requests to hang 
&gt; indefinitely until they time out. From what Dan said, I would expect that 
&gt; Riak handles that sort of situation intelligently. I guess my remaining 
&gt; questions are:
&gt; 
&gt; \\* How does Riak detect that a node is down, and what could cause that to take 
&gt; a full minute?
&gt; \\* When N=3, what about a single node failure could cause a read with R=1 to 
&gt; time out?
&gt; \\* Is there a way to configure the strictness of when nodes are assumed dead? 
&gt; I'm thinking like a "timeout" config option or something.
&gt; 
&gt; Peace,
&gt; --Jay
&gt; 
&gt; On Tue, Nov 23, 2010 at 2:55 PM, Neville Burnell  
&gt; wrote:
&gt; Just a thought ... have you verified your switch, cables, nics, etc
&gt; 
&gt; 
&gt; On 24 November 2010 09:33, Jay Adkisson  wrote:
&gt; (many profuse apologies to Dan - hit "reply" instead of "reply all")
&gt; 
&gt; Alrighty, I've done a little more digging. When I throttle the writes 
&gt; heavily (2/sec) and set R and W to 1 all around, the cluster works just fine 
&gt; after I restart the node for about 15-20 seconds. Then the read request 
&gt; hangs for about a minute, until node D disappears from connected\\_nodes in 
&gt; riak-admin status, at which point it returns the desired value (although 
&gt; sometimes I get a 503):
&gt; 
&gt; --2010-11-23 13:01:28-- http://:8098/riak//?r=1
&gt; Resolving ... 
&gt; Connecting to ||:8098... connected.
&gt; HTTP request sent, awaiting response...  200 OK
&gt; Length: 3684 (3.6K) [image/jpeg]
&gt; Saving to: `?r=1'
&gt; 
&gt; 100%[======================================&gt;] 3,684 --.-K/s in 0s
&gt; 
&gt; 2010-11-23 13:02:21 (49.5 MB/s) - `?r=1' saved [3684/3684]
&gt; 
&gt; --2010-11-23 13:02:23-- http://:8098/riak//?r=1
&gt; Resolving ... 
&gt; Connecting to ||:8098... connected.
&gt; HTTP request sent, awaiting response... 200 OK
&gt; Length: 3684 (3.6K) [image/jpeg]
&gt; Saving to: `?r=1'
&gt; 
&gt; 100%[======================================&gt;] 3,684 --.-K/s in 0s
&gt; 
&gt; 2010-11-23 13:02:23 (220 MB/s) - `?r=1' saved [3684/3684]
&gt; 
&gt; Afterwards, node D comes back up and re-joins the cluster seamlessly.
&gt; 
&gt; Any insights? 
&gt; 
&gt; --Jay
&gt; 
&gt; On Mon, Nov 22, 2010 at 5:59 PM, Jay Adkisson  wrote:
&gt; Hey Dan,
&gt; 
&gt; Thanks for the response! I tried it again while watching `riak-admin status` 
&gt; - basically, it takes about 30 seconds of node C being down before riak 
&gt; realizes it's gone. During that time, if I'm writing to the cluster at all 
&gt; (I throttled it to 2 writes per second for testing), both writes and reads 
&gt; hang indefinitely, and sometimes time out.
&gt; 
&gt; I'm using Ripple to do the writes, and wget to test reads, all on node A for 
&gt; now, since I know it'll be up. I'm using the default R and W options for now.
&gt; 
&gt; Thanks for the help and clarification around ringready.
&gt; 
&gt; --Jay
&gt; 
&gt; 
&gt; On Mon, Nov 22, 2010 at 5:15 PM, Dan Reverri  wrote:
&gt; Your HTTP calls should not being timing out. Are you sending requests 
&gt; directly to the Riak node or are you using a load balancer? How much load are 
&gt; you placing on node A? Is it a write only load or are there reads as well? 
&gt; Can you confirm "all" requests time out or is it a large subset of the 
&gt; requests? How large are the objects being written? Are you setting R and W in 
&gt; the request? Are you using a particular client (Ruby, Python, etc.)? Can you 
&gt; provide the output of "riak-admin status" from node A?
&gt; 
&gt; Regarding the ringready command; that is behaving as I would expect 
&gt; considering a node is down.
&gt; 
&gt; Thanks,
&gt; Dan
&gt; 
&gt; Daniel Reverri
&gt; Developer Advocate
&gt; Basho Technologies, Inc.
&gt; d...@basho.com
&gt; 
&gt; 
&gt; On Mon, Nov 22, 2010 at 4:55 PM, Jay Adkisson  wrote:
&gt; Hey all,
&gt; 
&gt; Here's what I'm seeing: I have four nodes A, B, C, and D. I'm loading lots 
&gt; of data into node A, which is being distributed evenly across the nodes. If 
&gt; I physically reboot node D, all my HTTP calls time out, and `riak-admin 
&gt; ringready` complains that not all nodes are up. Is this intended behavior? 
&gt; Is there a configuration option I can set so it fails more gracefully?
&gt; 
&gt; --Jay
&gt; 
 
&gt; 
&gt; 
&gt; 
&gt; 
 
&gt; 
&gt; 

