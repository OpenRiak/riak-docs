---
title: "Re: Problem with deleting keys"
description: ""
project: community
lastmod: 2011-06-16T08:42:02-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg03709"
mailinglist_parent_id: "msg03708"
author_name: "Greg Nelson"
project_section: "mailinglistitem"
sent_date: 2011-06-16T08:42:02-07:00
---


Well, it is kind of Riak specific. An implementation that treated DELETEs like 
PUTs (tombstones w/ vector clocks for ordering), then this would not be an 
issue, right? When no primary nodes are down, the tombstones can be physically 
deleted on the backend. A logical delete could never reappear if that were how 
it worked.

Is this essentially what is on the current master branch (not yet released)?

On Thursday, June 16, 2011 at 8:22 AM, Nico Meyer wrote:

&gt; The problem with unreachable nodes still remains, since you don't know 
&gt; how long they will be gone. The only 'safe' minimum time to keep deleted 
&gt; values is forever. This can be easily emulated in the application layer 
&gt; by using a special value (or use Riak metadata for example).
&gt; So it essentially a trade off like most things. If you are sure that no 
&gt; node will ever be down for more than 24 hours, your solution would work.
&gt; 
&gt; If it is really essential for an application that deleted keys don't 
&gt; ever reappear, you should just store this information explicitly (that 
&gt; way you also know when the key was deleted btw.). If not, then one can 
&gt; live with the current behaviour, which is much simpler implementation wise.
&gt; 
&gt; I would just separate the two issues of logically deleting and 
&gt; physically deleting (which is just an operational issue as opposed to an 
&gt; issue for your application design). The latter could be handled by the 
&gt; storage backend. Bitcask already has a key expiration feature. If it 
&gt; where fixed, so that expired key are actually counted towards the 
&gt; triggering of merges, and the ttl could be set per key, you would be 
&gt; good to go ;-).
&gt; 
&gt; Btw, this whole issue is not really Riak specific. It is essentially a 
&gt; consequence of eventual consistency, where you have to make a trade off 
&gt; between the amount of bookkeeping information you want to store and the 
&gt; maximum amount of time (or number of updates) any part of the system can 
&gt; diverge from the rest of the system before you get undesired results.
&gt; 
&gt; Cheers,
&gt; Nico
&gt; 
&gt; Am 16.06.2011 16:50, schrieb Kresten Krab Thorup:
&gt; &gt; ...when doing a delete, Riak actually stores a "deleted" record, but then 
&gt; &gt; it is too eagerly deleting it for real after that. There should be a 
&gt; &gt; configurable "zombie time" between requesting a delete and the "deleted 
&gt; &gt; record" being deleted for real; so that the deleted record's vector clock 
&gt; &gt; will show that the delete is more recent than the other value(s) in case 
&gt; &gt; those are later reconciled. The current infrastructure just doesn't have a 
&gt; &gt; good place to "enqueue" such a "delete this for real in 24 hours"-ish 
&gt; &gt; request.
&gt; &gt; 
&gt; &gt; Also, the master branch now has support for specifying a vector clock with 
&gt; &gt; a delete (in 14.x releases you can in stead do a PUT w/ X-Riak-Deleted=true 
&gt; &gt; and a proper vector clock, and an empty content). That's better (more 
&gt; &gt; consistent), but not a real fix.
&gt; &gt; 
&gt; &gt; Kresten
&gt; &gt; 
&gt; &gt; On 16/06/2011, at 11.58, "Nico 
&gt; &gt; Meyer"&gt; wrote:
&gt; &gt; 
&gt; &gt; Hello David,
&gt; &gt; 
&gt; &gt; this behaviour is quite expected if you think about how Riak works.
&gt; &gt; Assuming you use the default replication factor of n=3, each key is stored 
&gt; &gt; on all of your three nodes. If you delete a key while one node (let's call 
&gt; &gt; it A) is down, the key is deleted from the two nodes that are still up 
&gt; &gt; (let's call them B and C), and remains on the downed node A.
&gt; &gt; Once node A is up again, the situation is indistinguishable from B and C 
&gt; &gt; having a hard drive crash and loosing all their data, in that A has the key 
&gt; &gt; and B and C know nothing about it.
&gt; &gt; 
&gt; &gt; If you do a GET of the deleted key at this point, the result depends on the 
&gt; &gt; r-value that you choose. For r&gt;1 you will get a not\\_found on the first get. 
&gt; &gt; For r=1 you might get the data or a not\\_found, depending on which two nodes 
&gt; &gt; answer first (see 
&gt; &gt; https://issues.basho.com/show\\_bug.cgi?id=992 about basic quorum for an 
&gt; &gt; explanation). Also, at that point read repair will kick in and re-replicate 
&gt; &gt; the key to all nodes, so subsequent GETs will always return the original 
&gt; &gt; datum.
&gt; &gt; 
&gt; &gt; listing keys on the other hand does not use quorum but just does a set 
&gt; &gt; union of all keys of all the nodes in you cluster. Essentially it is 
&gt; &gt; equivalent to r=1 without basic quorum. The same is true for map/reduce 
&gt; &gt; queries to my knowledge
&gt; &gt; 
&gt; &gt; The essential problem is that a real physical delete is indistinguishable 
&gt; &gt; from data loss (or never having had the data in the first place), while 
&gt; &gt; those two things are logically different.
&gt; &gt; If you want to be sure that a key is deleted with all its replicas you must 
&gt; &gt; delete it with a write quorum setting of w=n. Also you need to tell Riak 
&gt; &gt; not to count fallback vnodes toward you write quorum. This feature is quite 
&gt; &gt; new and I believe only available in the head revision. Also I forgot the 
&gt; &gt; name of the parameter and don't know if it is even applicable for DELETEs.
&gt; &gt; Anyhow, if you do all this, your DELETEs will simply fail if any of the 
&gt; &gt; nodes that has a copy of the key is down (so in your case, if any node is 
&gt; &gt; down).
&gt; &gt; 
&gt; &gt; If you only want to logically delete, and don't care about freeing the disk 
&gt; &gt; space and RAM that is used by the key, you should use a special value, 
&gt; &gt; which is interpreted by your application as a not found. That way you also 
&gt; &gt; get proper conflict resolution between DELETEs and PUTs (say one client 
&gt; &gt; deletes a key while another one updates it).
&gt; &gt; 
&gt; &gt; Cheers,
&gt; &gt; Nico
&gt; &gt; 
&gt; &gt; Am 16.06.2011 00:55, schrieb David Mitchell:
&gt; &gt; Erlang: R13B04
&gt; &gt; Riak: 0.14.2
&gt; &gt; 
&gt; &gt; I have a three node cluster, and while one node was down, I deleted every 
&gt; &gt; key in a certain bucket. Then, I started the node that was down, and it 
&gt; &gt; joined the cluster.
&gt; &gt; 
&gt; &gt; Now, when do a listing on these keys in this bucket, and I get the entire 
&gt; &gt; list. I can also get the values of the bucket. However, when I try to 
&gt; &gt; delete the keys, the keys are not deleted.
&gt; &gt; 
&gt; &gt; Can anyone help me get the nodes back in a consistent state? I have tried 
&gt; &gt; restarting the nodes.
&gt; &gt; 
&gt; &gt; David
&gt; &gt; 
&gt; &gt; 
&gt; &gt; 
&gt; &gt; 
&gt; &gt; 
&gt; &gt; 
&gt; &gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; &gt; riak-users mailing list
&gt; &gt; riak-users@lists.basho.com
&gt; &gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt; &gt; 
&gt; &gt; 
&gt; &gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; &gt; riak-users mailing list
&gt; &gt; riak-users@lists.basho.com
&gt; &gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt; 
&gt; 
&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com (mailto:riak-users@lists.basho.com)
&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

