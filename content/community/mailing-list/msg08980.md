---
title: "Re: Is Riak suitable for s small-record write-intensive	billion-records application?"
description: ""
project: community
lastmod: 2012-10-19T08:32:33-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg08980"
mailinglist_parent_id: "msg08979"
author_name: "Les Mikesell"
project_section: "mailinglistitem"
sent_date: 2012-10-19T08:32:33-07:00
---


On Fri, Oct 19, 2012 at 8:02 AM, Guido Medina  wrote:
&gt; It depends, if you have siblings enabled at the bucket, then you need to
&gt; resolve the conflicts using the object vclock,

How does that work for simultaneous initial inserts?

&gt; if you are not using
&gt; siblings, last write wins, either way, I haven't got any good results by
&gt; delegating that tasks to Riak, with siblings, eventually I ran Riak out in
&gt; speed of the writes making Riak fail (Due to LevelDB write speed?). And with
&gt; last write wins then I don't think you would want unexpected results, and
&gt; hence my recommendation: We use two things to resolve such issues; in-memory
&gt; cache + locking mechanism.

The problem is where the inserting client should handle new keys and
updates differently, or at least be aware that its insert failed or
will be ignored later.

&gt; For the last quote, the locking mechanism if well designed will always take
&gt; care of that.

If it is easy, why doesn't riak handle it?

-- 
 Les Mikesell
 lesmikes...@gmail.com

