---
title: "Re: Storing large collections."
description: ""
project: community
lastmod: 2012-03-05T20:27:52-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg06885"
mailinglist_parent_id: "msg06884"
author_name: "Jeremiah Peschka"
project_section: "mailinglistitem"
sent_date: 2012-03-05T20:27:52-08:00
---


On Mar 5, 2012, at 7:09 PM, Eric Siegel wrote:

&gt; Originally, I had planned to map each of my items to their own key. 
&gt; This was foolish as I estimate that I'll have around 6 billion keys, and this 
&gt; simply won't fit into memory.

This is only an issue if you're using bitcask (which is the default back end 
for Riak). If you're willing to consider one of the alternative storage 
backends (LevelDB or InnoDB), then you can store as much data as you want, as 
long as you have disk space to hold it.

&gt; 
&gt; My next plan of attack is store a collection of items to a given key, 
&gt; approximately 1million keys each with 6000 values.

This sounds cumbersome.

&gt; 
&gt; I was wondering how much performance will degrade as the value size gets 
&gt; larger. Also, I'm worried about
&gt; having to merge the new value into the collection.

I would too - there's no guarantee that two writes to the same collection won't 
happen enough together to cause a conflict.

&gt; 
&gt; Does anyone have any experience with this problem and perhaps some advice?
&gt; 
&gt; eric
&gt; 


---
Jeremiah Peschka - Managing Director, Brent Ozar PLF, LLC
Microsoft SQL Server MVP
