---
title: "Re: Spikes in node_(get|put)_fsm_time_100"
description: ""
project: community
lastmod: 2011-02-09T09:12:41-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg02245"
mailinglist_parent_id: "msg02225"
author_name: "Anthony Molinaro"
project_section: "mailinglistitem"
sent_date: 2011-02-09T09:12:41-08:00
---


Hi,

 Any thoughts on this? I added a timeout to my client so its not impacted
(other than missing some data, but that's okay). However, I still see large
spikes in the node\\_(get|put)\\_fsm\\_time\\_100 stats (normal operation seems to be
about 1200, and I see spikes up to 200000).
 
 One thing I thought of is upping the number of async threads. I did
increase the number of partitions to 1024 and with only 4 nodes in the
ring I could be hitting some sort of locking at the bitcask layer.

 Are there any maintenance tasks that happen with bitcask that could
cause lag? For instance in our frequency server which uses riak\\_core
with a linked in driver for a backend, we have to grow the file every
so often which lead to these sort of spikes, so maybe bitcask has some
thing similar?

Thanks,

-Anthony

On Tue, Feb 08, 2011 at 12:09:34PM -0800, Anthony Molinaro wrote:
&gt; Hi,
&gt; 
&gt; I have a 4 node cluster using riak\\_kv\\_multi\\_backend with one backend
&gt; configured to use riak\\_kv\\_bitcask\\_backend. I'm using the multi backend
&gt; because eventually I want to also run a cache backend. I'm sampling
&gt; the statistics once per minute and viewing them in rrd and noticed
&gt; something odd. The node\\_(get|put)\\_fsm\\_time\\_100 sometimes spike to
&gt; 60 seconds while 99.99% of the time it's less than 2 milliseconds.
&gt; 
&gt; I'm going to work around by lowering the timeouts in riak-erlang-client
&gt; but this seems like it could continue to be a problem if the get/put
&gt; fsms continue to run even if the client times out.
&gt; 
&gt; Anyway, just curious if others have experienced this sort of long tail
&gt; spikiness.
&gt; 
&gt; -Anthony
&gt; 
&gt; -- 
&gt; ------------------------------------------------------------------------
&gt; Anthony Molinaro 
&gt; 

-- 
------------------------------------------------------------------------
Anthony Molinaro 

