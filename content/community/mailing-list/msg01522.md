---
title: "Re: Understanding Riaks rebalancing and handoff behaviour"
description: ""
project: community
lastmod: 2010-11-11T07:05:19-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg01522"
mailinglist_parent_id: "msg01521"
author_name: "Nico Meyer"
project_section: "mailinglistitem"
sent_date: 2010-11-11T07:05:19-08:00
---


Hi,

Am Donnerstag, den 11.11.2010, 07:59 +0100 schrieb Sven Riedel:
&gt; Hi,
&gt; thanks for the detailed reply. So you would suggest that somehow the
&gt; partition allocation got into an incosistent state across nodes. I'll
&gt; have to check the logs to see if anything similar to your dump pops
&gt; up.
&gt; 
&gt; &gt; So I compared the ring states manually using the console, and in the
&gt; &gt; ring state on the removed node quite a few partitions where assigned
&gt; &gt; to
&gt; &gt; different nodes than what the other nodes thought.
&gt; &gt; After I manually synced the ring on the leaving node with the rest
&gt; &gt; of
&gt; &gt; the cluster by doing this on the console:
&gt; &gt; 
&gt; &gt; {ok,Ring} = rpc:call('r...@othernode', riak\\_core\\_ring\\_manger,
&gt; &gt; get\\_my\\_ring, []).
&gt; &gt; riak\\_core\\_ring\\_manager:set\\_my\\_ring(R).
&gt; &gt; 
&gt; &gt; 
&gt; 
&gt; 
&gt; That ought to be 
&gt; 
&gt; 
&gt; riak\\_core\\_ring\\_manager:set\\_my\\_ring( Ring ).
&gt; 
&gt; 
&gt; right? Just verifying because my Erlang is rather rudimentary :)

You are right.

&gt; &gt; Also riak-admin ringready will not recognize this problem, as far as
&gt; &gt; I
&gt; &gt; read the code, because only the ring states of the current ring
&gt; &gt; members
&gt; &gt; are compared. I haven't tried it, cause I am still on 0.12.0. 
&gt; &gt; The same is apparently true for riak-admin transfers, which might
&gt; &gt; tell
&gt; &gt; you that there are no handoffs left, even if the removed node still
&gt; &gt; has
&gt; &gt; data.
&gt; &gt; 
&gt; 
&gt; 
&gt; I'm running 0.13.0, so if we're stumbling over the same cause it's
&gt; still there.
&gt; 
&gt; &gt; 
&gt; &gt; I discovered another problem while debugging this. I you restart (or
&gt; &gt; it
&gt; &gt; crashes) a node that you removed from the cluster which still has
&gt; &gt; data,
&gt; &gt; it won't start handing off it's data afterwards. The reason being,
&gt; &gt; that
&gt; &gt; is the node watcher also does not get notified that the other nodes
&gt; &gt; are
&gt; &gt; up, and so all of them are considered down. This also can only be
&gt; &gt; worked
&gt; &gt; around manually via the erlang console.
&gt; &gt; 
&gt; 
&gt; 
&gt; Why would that have to be worked around at all? My understanding is
&gt; through the data duplication within the ring having a single node
&gt; encounter a messy and fatal accident shouldn't destabilize the entire
&gt; ring. The nodes which contain the duplicate data would just take over
&gt; until a replacement node gets added, and the newly dead node is
&gt; removed (ok, via console).
&gt; 

It's not a problem right away. But since the replicated data is not
actively synchronized in the background the keys that were not copied
until the node dies have one less replica. That is until they are read
at least once, at which point read repair does replicate the key again.
So it depends on your setup and requirements, if this is acceptable or
not.

&gt; 
&gt; So this still leaves me with some of my original questions open:
&gt; &gt; &gt; 
&gt; &gt; &gt; 1. What would normally trigger a rebalancing of the nodes? 
&gt; &gt; &gt; 2. Is there a way to manually trigger a rebalancing?
&gt; &gt; &gt; 3. Did I do anything wrong with the procedure described above to
&gt; &gt; &gt; be left in the current odd state by riak?
&gt; 

Every vnode, which is responsible for one partition, checks after 60
seconds of inactivity if its is residing one the node where it should
be, according to the ring state. If not, the data is send to the correct
node. So the rebalancing of data is triggered by rebalancing the
partitions among the nodes in the ring.
The ring is balanced during the gossiping of the ring state, which is
done by every node with another random node at every 0-60 (also
randomly) seconds.
In the worst case it could take some minutes before the ring stabilizes,
but its statistically likely to converge faster.

So there's is nothing to trigger manually really. One problem I see has
to do again with restarting a node, which still has data that should be
handed off to another node. Initially only the vnodes that are owned by
a node started, which by definition don't include the ones to be handed
off. But if the vnodes are never started, they won't perform the
handoff.
It kind of works anyway, but the vnodes are started and transfered
sequentially. Normally four partitions are transferred in parallel, so I
don't know if this is by design or by accident. The details are
convoluted enough to suspect the latter.
In any case this would also make also have the effect that those
partitions won't show up in the output of riak-admin transfers, since
only running vnodes are considered.

I also forgot, that I patched another problem in my own version of riak,
which will prevent any further handoff of data after four of them failed
with an error or timeout. This probably happened in your case, if your A
nodes became unresponsive for 30 minutes (did the machine swap by the
way?).
I should probably create a bug report for this, with my patch attached.
Stupid laziness!

After reading your original post again, I think almost all of the things
you saw can be explained by the bug that I mentioned in my first answer
(the ring status of removed nodes is not synchronized with the remaining
nodes). The problem obviously becomes worse if you remove several nodes
at a time.


I should really file some bug reports for all the problems I found, but
it is just too much effort for me right now. I have fixed the problems
that bugged us the most myself, so I should at least provide the patches
like a good open source citizen :-)

Bye,

Nico

