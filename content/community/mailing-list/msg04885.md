---
title: "Re: Performance Issues with LevelDB Backend on 1.0.0RC1"
description: ""
project: community
lastmod: 2011-09-26T14:25:47-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg04885"
mailinglist_parent_id: "msg04884"
author_name: "Dan Reverri"
project_section: "mailinglistitem"
sent_date: 2011-09-26T14:25:47-07:00
---


Hi Patrick,

Did you restart Riak after changing the configuration option in app.config?
Also, where in the config file did you add the option? Would it be possible
to see your app.config file? How many of the file descriptors listed by lsof
reference the "data/leveldb" directory?

Thanks,
Dan

Daniel Reverri
Developer Advocate
Basho Technologies, Inc.
d...@basho.com


On Mon, Sep 26, 2011 at 12:23 PM, Patrick Van Stee
wrote:

&gt; Good suggestion. Erlang only has 39 ports open. I initially tried
&gt; increasing max\\_open\\_files to a huge number just to see what would happen,
&gt; but there was no noticeable difference in performance. Also lsof -u riak
&gt; shows over 12000 fds open even after limiting max\\_open\\_files back to 20 and
&gt; the number continues to grow until it hits the system limit which then
&gt; causes that "Accept failed error". I'm ok with increasing the max\\_open\\_files
&gt; limit but it doesn't really seem to improve performance at least in my case.
&gt; Also even when I try to limit the amount of file descriptors with
&gt; max\\_open\\_files, riak still opens new ones until it crashes.
&gt;
&gt; On Sep 26, 2011, at 2:54 PM, Jon Meredith wrote:
&gt;
&gt; Hi Patrick,
&gt;
&gt; I suggested increasing ports as you had an emfile on a socket accept call.
&gt; Erlang uses ports for things like network sockets and file handles when
&gt; opened by \\*erlang\\* processes. However, the leveldb library manages it's own
&gt; sockets as it is a C++ library dynamically loaded by the emulator and so
&gt; doesn't count towards ports.
&gt;
&gt; Is it possible that Riak started getting more client load if request
&gt; latency increased? Changing max\\_open\\_files will keep the number of
&gt; process-level file handles lower, but will cause more opening and closing of
&gt; files to search them. If you have a nice modern OS with lots of file
&gt; handles available, you may be able to increase max\\_open\\_files for increased
&gt; performance.
&gt;
&gt; If you want to check how many ports you are using you can run this from the
&gt; riak console.
&gt;
&gt; (dev1@127.0.0.1)7&gt; length(erlang:ports()).
&gt; 39
&gt;
&gt; Try increasing your max\\_open\\_ports and checking how many file handles are
&gt; in use using a tool like lsof and check the number of ports you have opened.
&gt;
&gt; Cheers, Jon.
&gt;
&gt; On Mon, Sep 26, 2011 at 12:44 PM, Patrick Van Stee  &gt; wrote:
&gt;
&gt;&gt; Thanks for the quick response Jon. I bumped it from 4096 up to the max I
&gt;&gt; have set in /etc/riak/defaults and writes actually slowed down a little bit
&gt;&gt; (~10 less writes per second). Shouldn't the max\\_open\\_files setting keep the
&gt;&gt; total amount of fd's pretty low? Maybe I'm misunderstanding what that option
&gt;&gt; is used for.
&gt;&gt;
&gt;&gt; Patrick
&gt;&gt;
&gt;&gt; On Sep 26, 2011, at 2:34 PM, Jon Meredith wrote:
&gt;&gt;
&gt;&gt; Hi Patrick,
&gt;&gt;
&gt;&gt; You may be running out of ports which erlang uses for TCP sockets - try
&gt;&gt; increasing ERL\\_MAX\\_PORTS in etc/vm.args
&gt;&gt;
&gt;&gt; Cheers, Jon
&gt;&gt; Basho Technologies.
&gt;&gt;
&gt;&gt; On Mon, Sep 26, 2011 at 12:17 PM, Patrick Van Stee &lt;
&gt;&gt; vans...@highgroove.com&gt; wrote:
&gt;&gt;
&gt;&gt;&gt; We're running a small, 2 node riak cluster (on 2 m1.large boxes) using
&gt;&gt;&gt; the LevelDB backend and have been trying to write ~250 keys a second at it.
&gt;&gt;&gt; With a small dataset everything was running smoothly. However, after storing
&gt;&gt;&gt; several hundred thousand keys some performance issues started to show up.
&gt;&gt;&gt;
&gt;&gt;&gt; \\* We're running out of file descriptors which is causing nodes to crash
&gt;&gt;&gt; with the following error:
&gt;&gt;&gt;
&gt;&gt;&gt; 2011-09-24 00:23:52.097 [error] &lt;0.110.0&gt; CRASH REPORT Process [] with 0
&gt;&gt;&gt; neighbours crashed with reason: {error,accept\\_failed}
&gt;&gt;&gt; 2011-09-24 00:23:52.098 [error] &lt;0.121.0&gt; application: mochiweb, "Accept
&gt;&gt;&gt; failed error", "{error,emfile}
&gt;&gt;&gt;
&gt;&gt;&gt; Setting the max\\_open\\_files limit in the app.config doesn't seem to help.
&gt;&gt;&gt;
&gt;&gt;&gt; \\* Writes have slowed down by an order of magnitude. I even set the n\\_val,
&gt;&gt;&gt; w, and dw bucket properties to 1 without any noticeable difference. Also we
&gt;&gt;&gt; switched to using protocol buffers to make sure there wasn't any extra
&gt;&gt;&gt; overhead when using HTTP.
&gt;&gt;&gt;
&gt;&gt;&gt; \\* Running map reduce jobs that use a range query on a secondary index
&gt;&gt;&gt; started returning an error, {"error":"map\\_reduce\\_error"}, once our dataset
&gt;&gt;&gt; increased in size. Feeding a list of keys works fine, but querying the index
&gt;&gt;&gt; for keys seems to be timing out:
&gt;&gt;&gt;
&gt;&gt;&gt; 2011-09-26 16:37:57.192 [error] &lt;0.136.0&gt; Supervisor
&gt;&gt;&gt; riak\\_pipe\\_fitting\\_sup had child undefined started with
&gt;&gt;&gt; {riak\\_pipe\\_fitting,start\\_link,undefined} at &lt;0.3497.0&gt; exit with reason
&gt;&gt;&gt; {timeout,{gen\\_server,call,[{riak\\_pipe\\_vnode\\_master,'riak@10.206.105.52
&gt;&gt;&gt; '},{return\\_vnode,{'riak\\_vnode\\_req\\_v1',502391187832497878132516661246222288006726811648,{raw,#Ref&lt;0.0.1.88700&gt;,&lt;0.3500.0&gt;},{cmd\\_enqueue,{fitting,&lt;0.3499.0&gt;,#Ref&lt;0.0.1.88700&gt;,#Fun,#Fun},{&lt;&lt;"ip\\_queries"&gt;&gt;,&lt;&lt;"uaukXZn5rZQ0LrSED3pi-fE-JjU"&gt;&gt;},infinity,[{502391187832497878132516661246222288006726811648,'
&gt;&gt;&gt; riak@10.206.105.52'}]}}}]}} in context child\\_terminated
&gt;&gt;&gt;
&gt;&gt;&gt; Is anyone familiar with these problems or is there anything else I can
&gt;&gt;&gt; try to increase the performance when using LevelDB?

&gt;&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;
&gt;

