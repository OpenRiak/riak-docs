---
title: "Re: More Migration Questions"
description: ""
project: community
lastmod: 2012-11-13T09:16:44-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg09305"
mailinglist_parent_id: "msg09304"
author_name: "Thomas Santero"
project_section: "mailinglistitem"
sent_date: 2012-11-13T09:16:44-08:00
---


Hi Shane,

I'm sorry for the delay on this. Over the weekend I was working to
replicate your setup so I can answer your question from experience. Alas,
time got the best of me and I have not yet finished.

That said, I'm inclined to suggest upgrading riak on your current cluster
first and then using riak-admin replace to move off of the VM's and onto
metal.

\\* In this scenario, do a rolling upgrade (including making backups) of the
current cluster.
\\* Install riak onto the new machines
\\* join the first machine to the cluster
\\* use riak-admin replace to replace one of the old nodes with the new node
\\* wait for ring-ready, then repeat for the other nodes.

Tom

On Tue, Nov 13, 2012 at 11:59 AM, Shane McEwan  wrote:

&gt; Anyone? Beuller? :-)
&gt;
&gt; Installing Riak 1.1.1 on the new nodes, copying the data directories from
&gt; the old nodes, issuing a "reip" on all the new nodes, starting up, waiting
&gt; for partition handoffs to complete, shutting down, upgrading to 1.2.1 and
&gt; starting up again got us to where we want to be. But this is not very
&gt; convenient.
&gt;
&gt; What do I do when I come to creating our test environment where I'll be
&gt; wanting to copy production data onto the test nodes on a regular basis? At
&gt; that point I won't have the "luxury" of downgrading to 1.1.1 to have a
&gt; working "reip" command.
&gt;
&gt; Surely there's gotta be an easier way to spin up a new cluster with new
&gt; names and IPs but with old data?
&gt;
&gt; Shane.
&gt;
&gt;
&gt; On 08/11/12 21:10, Shane McEwan wrote:
&gt;
&gt;&gt; G'day!
&gt;&gt;
&gt;&gt; Just to add to the list of people asking questions about migrating to
&gt;&gt; 1.2.1 . . .
&gt;&gt;
&gt;&gt; We're about to migrate our 4 node production Riak database from 1.1.1 to
&gt;&gt; 1.2.1. At the same time we're also migrating from virtual machines to
&gt;&gt; physical machines. These machines will have new names and IP addresses.
&gt;&gt;
&gt;&gt; The process of doing rolling upgrades is well documented but I'm unsure
&gt;&gt; of the correct procedure for moving to an entirely new cluster.
&gt;&gt;
&gt;&gt; We have the luxury of a maintenance window so we don't need to keep
&gt;&gt; everything running during the migration. Therefore the current plan is
&gt;&gt; to stop the current cluster, copy the Riak data directories to the new
&gt;&gt; machines and start up the new cluster. The hazy part of the process is
&gt;&gt; how we "reip" the database so it will work in the new cluster.
&gt;&gt;
&gt;&gt; We've tried using the "riak-admin reip" command but were left with one
&gt;&gt; of our nodes in "(legacy)" mode according to "riak-admin member-status".
&gt;&gt; From an earlier E-Mail thread[1] it seems like "reip" is deprecated and
&gt;&gt; we should be doing a "cluster force replace" instead.
&gt;&gt;
&gt;&gt; So, would the new procedure be the following?
&gt;&gt;
&gt;&gt; 1. Shutdown old cluster
&gt;&gt; 2. Copy data directory
&gt;&gt; 3. Start new cluster (QUESTION: The new nodes don't own any of the
&gt;&gt; partitions in the data directory. What does it do?) (QUESTION: The new
&gt;&gt; nodes won't be part of a cluster yet. Do I need to "join" them before I
&gt;&gt; can do any of the following commands? Or do I just put all the joins and
&gt;&gt; force-replace commands into the same plan and commit it all together?)
&gt;&gt; 3. Issue "riak-admin cluster force-replace old-node1 new-node1"
&gt;&gt; (QUESTION: Do I run this command just on "new-node1" or on all nodes?)
&gt;&gt; 4. Issue "force-replace" commands for the remaining three nodes.
&gt;&gt; 5. Issue a "cluster plan" and "cluster commit" to commit the changes.
&gt;&gt; 6. Cross fingers.
&gt;&gt;
&gt;&gt; In my mind the "replace" and/or "force-replace" commands are something
&gt;&gt; we would use it we had a failed node and needed to bring a spare online
&gt;&gt; to take over. It doesn't feel like something you would do if you don't
&gt;&gt; already have a cluster in place and are needing to "replace" ALL nodes.
&gt;&gt;
&gt;&gt; Of course, we want to test this procedure before doing it for real. What
&gt;&gt; are the risks of doing the above procedure while the old cluster is
&gt;&gt; still running? While the new nodes are on a segregated network and
&gt;&gt; shouldn't be able to contact the old nodes what would happen if we did
&gt;&gt; the above and found the network wasn't as segregated as we originally
&gt;&gt; thought? Would the new nodes start trying to communicate with the old
&gt;&gt; nodes before the "force-replace" can take effect? Or, because all the
&gt;&gt; cluster changes are atomic there won't be any risk of that?
&gt;&gt;
&gt;&gt; Sorry for all the questions. I'm just trying to get a clear procedure
&gt;&gt; for moving an entire cluster to new hardware and hopefully this thread
&gt;&gt; will help other people in the future.
&gt;&gt;
&gt;&gt; Thanks in advance!
&gt;&gt;
&gt;&gt; Shane.
&gt;&gt;
&gt;&gt; [1] 
&gt;&gt; http://comments.gmane.org/\\*\\*gmane.comp.db.riak.user/8418
&gt;&gt;
&gt;&gt;
&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\*\\*\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt; riak-users mailing list
&gt;&gt; riak-users@lists.basho.com
&gt;&gt; http://lists.basho.com/\\*\\*mailman/listinfo/riak-users\\_\\*\\*lists.basho.com
&gt;&gt;
&gt;
&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\*\\*\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/\\*\\*mailman/listinfo/riak-users\\_\\*\\*lists.basho.com
&gt;

-- 
@tsantero 
Technical Evangelist
Basho Technologies
347-571-3995
