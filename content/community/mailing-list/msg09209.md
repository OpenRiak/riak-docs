---
title: "Re: How to make Riak work faster (writing)"
description: ""
project: community
lastmod: 2012-11-06T14:50:44-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg09209"
author_name: "Jared Morrow"
project_section: "mailinglistitem"
sent_date: 2012-11-06T14:50:44-08:00
---


Uruka,

Well at least now your numbers make sense. I wasn't trying to be snide in
my response, that wasn't my intention, so sorry if I appeared that way. I
was attempting, poorly, to say that Memory and Bitcask shouldn't be close
to each other so you have other issues.

Now that you have apparently eliminated your network & load bottlenecks,
you can take a look at
http://docs.basho.com/riak/latest/cookbooks/Linux-Performance-Tuning/paying
particular attention to mounts, scheduler, and swapiness settings.
 Just today someone at Basho was mentioning they changed their scheduler
from CFQ to deadline and saw their load average go from 30.00 to 4.00.

After tuning mentioned above, Riak will perform better along side increased
disk performance. I benchmarked on VM's, but those VM's were on SmartOS
running ZFS with a pretty fast raid-z setup.

-Jared

On Tue, Nov 6, 2012 at 3:37 PM, Uruka Dark  wrote:

&gt; First of all, thank you for your reply.
&gt;
&gt; Well, if you tell me that I can beat myself up trying to get what another
&gt; person gets in a benchmark, then I don't understand what's the whole point
&gt; in post your results here. I thought that you were trying to tell me that
&gt; in an similar setup, you could do much better, and that I probably had some
&gt; problem on my setup (and you were right). Considering how different our
&gt; results are, I imagine that there is something wrong - by "wrong" I mean: I
&gt; have a setup problem; my hardware is not up to the task and our
&gt; environments are not that similar; etc. That's what I'm trying to figure
&gt; out and also, I'm trying to know the tool.
&gt;
&gt; I have a few different scenarios to face: heavy write, heavy read, both of
&gt; them, etc. Now, I'm considering the heavy write scenario and later I'll
&gt; deal with others. If I jump from one scenario to another without at least a
&gt; minimum solid conclusion, it will not help me.
&gt;
&gt; Based on what you said, I replaced the machine that was producing the load
&gt; by a new one identical to the cluster machines (intel core i3 2.3GHz, 4GB
&gt; RAM 1TB HD). Now I have 3 machines with the same setup in a gigabit network.
&gt;
&gt; I started using bitcask backend:
&gt; https://dl.dropbox.com/u/308392/sum\\_bit.png
&gt;
&gt; Then I tried memory backend:
&gt; https://dl.dropbox.com/u/308392/sum\\_mem.png
&gt;
&gt; Now we can see a major impact on the results. The memory backend could do
&gt; much better with roughly 4000 ops/sec, but bitcask not so much, about +200
&gt; ops/sec than the last result. Changing the third machine really worked.
&gt;
&gt; Just to see what would happen, I decided to try the benchmark locally (of
&gt; corse, I knew that it would put a heavy load over the CPU). I ran the test
&gt; from one of the cluster machines.
&gt; Results:
&gt;
&gt; Bitcask:
&gt; https://dl.dropbox.com/u/308392/sum\\_bit\\_local.png
&gt;
&gt; Memory:
&gt; https://dl.dropbox.com/u/308392/sum\\_mem\\_local.png
&gt;
&gt; Well, it seems that my bottleneck is related to my disks.
&gt; Once again, thank you.
&gt; You helped me alot.
&gt;
&gt;
&gt; On Mon, Nov 5, 2012 at 4:36 PM, Jared Morrow  wrote:
&gt;
&gt;&gt; So if you are getting close to similar numbers with the memory and
&gt;&gt; bitcask backends, you know that file IO isn't your bottleneck in the load
&gt;&gt; test. My guess is that the machine you are running the load with (where you
&gt;&gt; are running basho\\_bench) can't keep up with the test. I'm running
&gt;&gt; basho\\_bench on a core i7 on the same switch as the riak nodes, so that
&gt;&gt; could be enough of the difference. Again though, you can beat yourself up
&gt;&gt; trying to just get what another person gets in a benchmark, so it really
&gt;&gt; comes down to what you want out of it. What are your app's requirements?
&gt;&gt; What do you expect to be the requirements a year from now? What do your
&gt;&gt; access patterns look like, more read heavy or more write heavy?
&gt;&gt;
&gt;&gt; Right now, you are only doing bulk load experiments, what about
&gt;&gt; Write/Read/Update that simulate your expected usage pattern? What we have
&gt;&gt; done in the past as a good test is to do a bulk load like you are doing
&gt;&gt; with say 10 or 100 million keys. That'll make sure Riak is loaded with a
&gt;&gt; real world amount of data to start. Then run another pareto distribution
&gt;&gt; run (using something like {key\\_generator, {int\\_to\\_bin, {pareto\\_int,
&gt;&gt; 10000000}}}.) with a spread of puts/gets/updates like {operations,
&gt;&gt; [{get, 4},{put, 1},{update, 1}]}.. I'll tell you right now with a two
&gt;&gt; node cluster you probably aren't going to be happy with the results as that
&gt;&gt; is not how Riak was designed to work at its best. If you can swing it, add
&gt;&gt; two more nodes to the test, set N=2 instead of N=1 so you are getting at
&gt;&gt; least some data safety and run the above. If it meets the need of your app,
&gt;&gt; awesome. if it doesn't come back and discuss it or try another solution
&gt;&gt; with the same requirements.
&gt;&gt;
&gt;&gt; -Jared
&gt;&gt;
&gt;&gt;
&gt;&gt; 
&gt;
&gt;
&gt;
