---
title: "Re: Bitcask & Innostore Benchmark @ EC2"
description: ""
project: community
lastmod: 2010-07-12T06:29:21-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg00713"
mailinglist_parent_id: "msg00712"
author_name: "James Sadler"
project_section: "mailinglistitem"
sent_date: 2010-07-12T06:29:21-07:00
---


Hi Sean,

So the mental model I have of the benchmark framework is wrong.

I thought that 'concurrent' was the setting for how many requests
could be in progress at any one time (I assumed 1 erlang process
(worker) per request), which is essentially a total deliberate
overestimate of our eventual production concurrency levels; not
particularly scientific and to be taken with a pinch of salt, but it
was consistent across a number of benchmarking runs.

With my mental model of what concurrent meant, I just took mode 'max'
to mean 'don't bother rate throttling'.

Anyway, even if I did hammer the server too hard, BitCask seemed to
handle it just fine!

Regarding the worker processes, does each erlang process handle &gt; 1
request at a time?

Thanks,

James


On 12 July 2010 23:03, Sean Cribbs  wrote:
&gt; James,
&gt;
&gt; One thing you might try is lowering the concurrency rate. "max" mode at even 
&gt; 5-10 workers is enough to saturate most networks (the most I've ever run has 
&gt; been 15).  Whether that is an accurate representation of your production load 
&gt; is another story entirely, and an "exercise for the reader".
&gt;
&gt; Sean Cribbs 
&gt; Developer Advocate
&gt; Basho Technologies, Inc.
&gt; http://basho.com/
&gt;
&gt; On Jul 11, 2010, at 11:52 PM, James Sadler wrote:
&gt;
&gt;&gt; Hi All,
&gt;&gt;
&gt;&gt; I've been benchmarking Riak using basho\\_bench on an EC2 m1.large
&gt;&gt; instance and also running locally on my iMac inside VirtualBox to
&gt;&gt; assess performance of the Bitcask and Innostore backends
&gt;&gt;
&gt;&gt; The test configuration looks like this:
&gt;&gt;
&gt;&gt; {mode, max}.
&gt;&gt; {duration, 10}.
&gt;&gt; {concurrent, 50}.
&gt;&gt; {driver, basho\\_bench\\_driver\\_http\\_raw}.
&gt;&gt; {code\\_paths, ["deps/stats",
&gt;&gt;              "deps/ibrowse"]}.
&gt;&gt; %% a composite key composed of 4 IDs, each of which is a 16 char hex string,
&gt;&gt; %% specific to our data model.
&gt;&gt; {key\\_generator, {random\\_dynamo\\_style\\_string, 35000}}.
&gt;&gt; {value\\_generator, {fixed\\_bin, 1000}}.
&gt;&gt; {operations, [{get, 1}, {update, 2}]}.
&gt;&gt; {http\\_raw\\_ips, ["127.0.0.1"]}.
&gt;&gt;
&gt;&gt; Also, to generate the 'random\\_dynamo\\_style\\_string', I made the
&gt;&gt; following changes to the bash\\_bench source:
&gt;&gt;
&gt;&gt; diff --git a/src/basho\\_bench\\_keygen.erl b/src/basho\\_bench\\_keygen.erl
&gt;&gt; index 4849bbe..639e90b 100644
&gt;&gt; --- a/src/basho\\_bench\\_keygen.erl
&gt;&gt; +++ b/src/basho\\_bench\\_keygen.erl
&gt;&gt; @@ -54,6 +54,13 @@ new({pareto\\_int, MaxKey}, \\_Id) -&gt;
&gt;&gt; new({pareto\\_int\\_bin, MaxKey}, \\_Id) -&gt;
&gt;&gt;     Pareto = pareto(trunc(MaxKey \\* 0.2), ?PARETO\\_SHAPE),
&gt;&gt;     fun() -&gt; &lt;&lt;(Pareto()):32/native&gt;&gt; end;
&gt;&gt; +new({random\\_dynamo\\_style\\_string, MaxKey}, \\_Id) -&gt;
&gt;&gt; +    fun() -&gt; lists:concat([
&gt;&gt; +                    get\\_random\\_string(16, "0123456789abcdef"), "-",
&gt;&gt; +                    get\\_random\\_string(16, "0123456789abcdef"), "-",
&gt;&gt; +                    get\\_random\\_string(16, "0123456789abcdef"), "-",
&gt;&gt; +                    get\\_random\\_string(16, "0123456789abcdef")])
&gt;&gt; +    end;
&gt;&gt; new(Other, \\_Id) -&gt;
&gt;&gt;     ?FAIL\\_MSG("Unsupported key generator requested: ~p\\n", [Other]).
&gt;&gt;
&gt;&gt; @@ -74,10 +81,17 @@ dimension({pareto\\_int, \\_}) -&gt;
&gt;&gt;     0.0;
&gt;&gt; dimension({pareto\\_int\\_bin, \\_}) -&gt;
&gt;&gt;     0.0;
&gt;&gt; +dimension({random\\_dynamo\\_style\\_string, MaxKey}) -&gt;
&gt;&gt; +    0.0;
&gt;&gt; dimension(Other) -&gt;
&gt;&gt;     ?FAIL\\_MSG("Unsupported key generator dimension requested: ~p\\n", 
&gt;&gt; [Other]).
&gt;&gt;
&gt;&gt; -
&gt;&gt; +get\\_random\\_string(Length, AllowedChars) -&gt;
&gt;&gt; +    lists:foldl(fun(\\_, Acc) -&gt;
&gt;&gt; +                        [lists:nth(random:uniform(length(AllowedChars)),
&gt;&gt; +                                   AllowedChars)]
&gt;&gt; +                            ++ Acc
&gt;&gt; +                end, [], lists:seq(1, Length)).
&gt;&gt;
&gt;&gt;
&gt;&gt; %% ====================================================================
&gt;&gt;
&gt;&gt;
&gt;&gt; As of now, I've only been running the benchmark with a 'cluster' of
&gt;&gt; one single Riak node, and I have benchmarked with bitcask and
&gt;&gt; innostore backends on the latest version of Riak (0.11.0-1344) and
&gt;&gt; innostore (1.0.0-88) on Ubuntu Lucid.  I have also been running the
&gt;&gt; basho\\_bench on the same host as the Riak node.
&gt;&gt;
&gt;&gt; The benchmarks are showing very high get and update latencies in the
&gt;&gt; 95th percentile and beyond when using Innostore as the backend.
&gt;&gt; Bitcask performance is much better.
&gt;&gt;
&gt;&gt; While running the benchmarks, I had an iostat process reporting IO
&gt;&gt; every 1 second.  It clearly showed heavy writes during the benchmark,
&gt;&gt; but practically zero reads.  I expect that this was because of the
&gt;&gt; disk cache.  What I found very surprising was that the latencies for
&gt;&gt; innostore gets during the benchmark were very high, even though the
&gt;&gt; disk was not being hit for reads at all.  This was reproducible on
&gt;&gt; both EC2 and on a local VM on my iMac.
&gt;&gt;
&gt;&gt; Observations:
&gt;&gt;
&gt;&gt; ## Innostore backend
&gt;&gt;
&gt;&gt; - Latencies for innostore are high across the board. Even for reads,
&gt;&gt; \\_\\_when iostat is reporting that no reads are hitting the disk\\_\\_.
&gt;&gt;
&gt;&gt; - 95th percentile read/write latencies are up to 1500/2000 millis.
&gt;&gt;
&gt;&gt; - 99th percentile reads/writes are up to 2000/4000 millis.
&gt;&gt;
&gt;&gt; - The difference between update and get latency is small.
&gt;&gt;
&gt;&gt; - There are some failed (timeouts) updates/gets in log file produced
&gt;&gt; by basho\\_bench
&gt;&gt;
&gt;&gt; - Throughput with innostore backend is 150-200 req/sec
&gt;&gt;
&gt;&gt; - Mounting the filesystem with noatime doesn't seem to make much of a
&gt;&gt; difference.
&gt;&gt;
&gt;&gt; - Getting values from disk cache has huge latency (iostat reporting no
&gt;&gt; reads on the device).  This is somewhat bizarre.
&gt;&gt;
&gt;&gt; ## Bitcask backend
&gt;&gt;
&gt;&gt; - There are zero errors in the log produced by bash\\_bench (no timeouts
&gt;&gt; like with innostore)
&gt;&gt;
&gt;&gt; - Throughput is much higher: 620 req/sec
&gt;&gt;
&gt;&gt; - The 99.9th percentile latencies are 200ms for writes, and for reads 100ms
&gt;&gt;
&gt;&gt; - The 95th percentile latencies are 160ms for writes and 60ms for reads
&gt;&gt;
&gt;&gt; - Mean & median latencies are 115ms for writes and 20ms for reads.
&gt;&gt;
&gt;&gt; Summary charts from basho\\_bench are attached.
&gt;&gt;
&gt;&gt;
&gt;&gt; NOTE:
&gt;&gt;
&gt;&gt; I haven't included benchmark results from my own local VM.  FWIW, I
&gt;&gt; observed the approximately same characteristics in the EC2 and local
&gt;&gt; benchmarks.
&gt;&gt;
&gt;&gt; In summary, it looks like there are significant performance issues
&gt;&gt; with Innostore in terms of throughput and latency.  Latency is the
&gt;&gt; biggest issue for our ad serving product at Lexer, so it looks like
&gt;&gt; we'll be using Bitcask in production.
&gt;&gt;
&gt;&gt; Hopefully these results will be useful to others.
&gt;&gt;
&gt;&gt; Also, given our large key size of 67 characters, combined with the
&gt;&gt; Bitcask's padding and storage layout, how many keys should we be able
&gt;&gt; to manager per node per GB?
&gt;&gt;
&gt;&gt; Looking forward to any comments.
&gt;&gt;
&gt;&gt; Thanks.
&gt;&gt;
&gt;&gt; --
&gt;&gt; James
&gt;
&gt;

-- 
James

