---
title: "Re: 'not found' after join"
description: ""
project: community
lastmod: 2011-05-05T14:26:54-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg03245"
author_name: "Greg Nelson"
project_section: "mailinglistitem"
sent_date: 2011-05-05T14:26:54-07:00
---


There's no concurrency for writes to these objects, which is what I was hoping 
would simplify the problem. But it sounds like I'll have to turn on allow\\_mult 
and resolve conflicts anyway.
On Thursday, May 5, 2011 at 2:23 PM, Bob Ippolito wrote: 
&gt; It's not necessarily as much application logic as you might think,
&gt; you've just described what statebox [1] is an abstraction for (but it
&gt; encapsulates change history in the value). It's all Erlang, but the
&gt; technique could be applied in any language. That said, it's really
&gt; frustrating that data is unavailable during hand-off, but at least you
&gt; can mitigate it with a smart model (you should probably have this
&gt; anyway). We're also really looking forward to having this issue
&gt; resolved.
&gt; 
&gt; Greg's usage pattern sounds like it's fundamentally inconsistent even
&gt; in the normal case when no handoff is occurring (assuming that there's
&gt; any concurrency for writes).
&gt; 
&gt; [1] http://github.com/mochi/statebox
&gt; 
&gt; On Thu, May 5, 2011 at 2:07 PM, Ben Tilly  wrote:
&gt; &gt; There are solutions to that consistency issue. You can set
&gt; &gt; allow\\_multi true, have each object have a link to a change history,
&gt; &gt; and have each change have a record of what changed. The change
&gt; &gt; history could be done as a singly linked list, where each change is
&gt; &gt; inserted into a bucket with a randomly generated key.
&gt; &gt; 
&gt; &gt; And then on reading an object, if you find siblings, you can go look
&gt; &gt; at the change histories, merge them, and come up with a resolved
&gt; &gt; object.
&gt; &gt; 
&gt; &gt; This is a \\*lot\\* of application logic, but it should be doable.
&gt; &gt; 
&gt; &gt; On Thu, May 5, 2011 at 1:14 PM, Greg Nelson  wrote:
&gt; &gt; &gt; The future I'd like to see is basically what I initially expected. That 
&gt; &gt; &gt; is,
&gt; &gt; &gt; I can add a single node to an online cluster and clients should not even 
&gt; &gt; &gt; see
&gt; &gt; &gt; any effects of this or need to know that it's even happening -- except of
&gt; &gt; &gt; course the side effects like the added load on the cluster incurred by
&gt; &gt; &gt; gossiping new ring state, handing off data, etc. But if no data has
&gt; &gt; &gt; actually been lost, I don't believe data should ever be unavailable,
&gt; &gt; &gt; temporarily or not. And I'd like to be able to, as someone else mentioned,
&gt; &gt; &gt; add a node and throttle the handoffs and let it trickle over hours or even
&gt; &gt; &gt; days.
&gt; &gt; &gt; 
&gt; &gt; &gt; Waving hands and saying that eventually the data will make it is true in
&gt; &gt; &gt; principle, but in practice if you are following a read/modify/write 
&gt; &gt; &gt; pattern
&gt; &gt; &gt; for some objects, you could easily lose data. e.g., my application writes
&gt; &gt; &gt; JSON arrays to certain objects, and when it wishes to append something to
&gt; &gt; &gt; the array, it will read/append/write back. If that initial read returns
&gt; &gt; &gt; 404, then a new empty array is created. This is normal operation. But if
&gt; &gt; &gt; that 404 is not a "normal" 404, it will happily create a new empty array,
&gt; &gt; &gt; append, and write back a single-element array to that key. Of course there
&gt; &gt; &gt; could have been a 100 element array in Riak that was just unavailable at 
&gt; &gt; &gt; the
&gt; &gt; &gt; time which is now effectively lost.
&gt; &gt; &gt; 
&gt; &gt; &gt; Anyhow, I do understand the importance of knowing what will happen when
&gt; &gt; &gt; doing something operationally like adding a node, and I understand that 
&gt; &gt; &gt; one
&gt; &gt; &gt; can't naively expect everything to just work like magic. But the current
&gt; &gt; &gt; behavior is pretty poorly documented and surprising. I don't think it was
&gt; &gt; &gt; even mentioned in the operations webinar! (Ok, I'll stop beating a dead
&gt; &gt; &gt; horse. :))
&gt; &gt; &gt; 
&gt; &gt; &gt; On Thursday, May 5, 2011 at 12:22 PM, Alexander Sicular wrote:
&gt; &gt; &gt; 
&gt; &gt; &gt; I'm really loving this thread. Generating great ideas for the way
&gt; &gt; &gt; things should be... in the future. It seems to me that "the ring
&gt; &gt; &gt; changes immediately" is actually the problem as Ryan astutely
&gt; &gt; &gt; mentions. One way the future could look is :
&gt; &gt; &gt; 
&gt; &gt; &gt; - a new node comes online
&gt; &gt; &gt; - introductions are made
&gt; &gt; &gt; - candidate vnodes are selected for migration (&lt;- insert pixie dust magic
&gt; &gt; &gt; here)
&gt; &gt; &gt; - the number of simultaneous migrations are configurable, fewer for
&gt; &gt; &gt; limited interruption or more for quicker completion
&gt; &gt; &gt; - vnodes are migrated
&gt; &gt; &gt; - once migration is completed, ownership is claimed
&gt; &gt; &gt; 
&gt; &gt; &gt; Selecting vnodes for migration is where the unicorn cavalry attack the
&gt; &gt; &gt; dragons den. If done right(er) the algorithm could be swappable to
&gt; &gt; &gt; optimize for different strategies. Don't ask me how to implement it,
&gt; &gt; &gt; I'm only a yellow belt in erlang-fu.
&gt; &gt; &gt; 
&gt; &gt; &gt; Cheers,
&gt; &gt; &gt; Alexander
&gt; &gt; &gt; 
&gt; &gt; &gt; On Thu, May 5, 2011 at 13:33, Ryan Zezeski  wrote:
&gt; &gt; &gt; 
&gt; &gt; &gt; John,
&gt; &gt; &gt; All great points. The problem is that the ring changes immediately when a
&gt; &gt; &gt; node is added. So now, all the sudden, the preflist is potentially 
&gt; &gt; &gt; pointing
&gt; &gt; &gt; to nodes that don't have the data and they won't have that data until
&gt; &gt; &gt; handoff occurs. The faster that data gets transferred, the less time your
&gt; &gt; &gt; clients have to hit 'notfound'.
&gt; &gt; &gt; However, I agree completely with what you're saying. This is just a side
&gt; &gt; &gt; effect of how the system currently works. In a perfect world we wouldn't
&gt; &gt; &gt; care how long handoff takes and we would also do some sort of automatic
&gt; &gt; &gt; congestion control akin to TCP Reno or something. The preflist would still
&gt; &gt; &gt; point to the "old" partitions until all data has been successfully handed
&gt; &gt; &gt; off, and then and only then would we flip the switch for that vnode. I'm
&gt; &gt; &gt; pretty sure that's where we are heading (I say "pretty sure" b/c I just
&gt; &gt; &gt; joined the team and haven't been heavily involved in these specific talks
&gt; &gt; &gt; yet).
&gt; &gt; &gt; It's all coming down the pipe...
&gt; &gt; &gt; As for your specific I/O question re handoff\\_concurrecy, you might be 
&gt; &gt; &gt; right.
&gt; &gt; &gt; I would think it depends on hardware/platform/etc. I was offering it as a
&gt; &gt; &gt; possible stopgap to minimize Greg's pain. It's certainly a cure to a
&gt; &gt; &gt; symptom, not the problem itself.
&gt; &gt; &gt; -Ryan
&gt; &gt; &gt; 
&gt; &gt; &gt; On Thu, May 5, 2011 at 1:10 PM, John D. Rowell  wrote:
&gt; &gt; &gt; 
&gt; &gt; &gt; Hi Ryan, Greg,
&gt; &gt; &gt; 
&gt; &gt; &gt; 2011/5/5 Ryan Zezeski 
&gt; &gt; &gt; 
&gt; &gt; &gt; 1. For example, riak\\_core has a `handoff\\_concurrency` setting that
&gt; &gt; &gt; determines how many vnodes can concurrently handoff on a given node. By
&gt; &gt; &gt; default this is set to 4. That's going to take a while with your 2048
&gt; &gt; &gt; vnodes and all :)
&gt; &gt; &gt; 
&gt; &gt; &gt; Won't that make the handoff situation potentially worse? From the thread I
&gt; &gt; &gt; understood that the main problem was that the cluster was shuffling too 
&gt; &gt; &gt; much
&gt; &gt; &gt; data around and thus becoming unresponsive and/or returning unexpected
&gt; &gt; &gt; results (like "not founds"). I'm attributing the concerns more to an
&gt; &gt; &gt; excessive I/O situation than to how long the handoff takes. If the handoff
&gt; &gt; &gt; can be made transparent (no or little side effects) I don't think most
&gt; &gt; &gt; people will really care (e.g. the "fix the cluster tomorrow" anecdote).
&gt; &gt; &gt; 
&gt; &gt; &gt; How about using a percentage of available I/O to throttle the vnode
&gt; &gt; &gt; handoff concurrency? Start with 1, and monitor the node's I/O (kinda like
&gt; &gt; &gt; 'atop' does, collection CPU, disk and network metrics), if it is below the
&gt; &gt; &gt; expected usage, then increase the vnode handoff concurrency, and 
&gt; &gt; &gt; vice-versa.
&gt; &gt; &gt; 
&gt; &gt; &gt; I for one would be perfectly happy if the handoff took several hours (even
&gt; &gt; &gt; days) if we could maintain the core riak\\_kv characteristics intact during
&gt; &gt; &gt; those events. We've all seen looooong RAID rebuild times, and it's usually
&gt; &gt; &gt; better to just sit tight and keep the rebuild speed low (slower I/O) while
&gt; &gt; &gt; keeping all of the dependent systems running smoothly.
&gt; &gt; &gt; 
&gt; &gt; &gt; cheers
&gt; &gt; &gt; -jd
&gt; &gt; &gt; 
&gt; &gt; &gt; 
&gt; &gt; &gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; &gt; &gt; riak-users mailing list
&gt; &gt; &gt; riak-users@lists.basho.com
&gt; &gt; &gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt; &gt; &gt; 
&gt; &gt; &gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; &gt; &gt; riak-users mailing list
&gt; &gt; &gt; riak-users@lists.basho.com
&gt; &gt; &gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt; &gt; &gt; 
&gt; &gt; &gt; 
&gt; &gt; &gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; &gt; &gt; riak-users mailing list
&gt; &gt; &gt; riak-users@lists.basho.com
&gt; &gt; &gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt; &gt; 
&gt; 
