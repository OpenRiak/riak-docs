---
title: "Re: bitcask and innostore overheads"
description: ""
project: community
lastmod: 2010-10-04T12:20:58-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg01187"
mailinglist_parent_id: "msg00683"
author_name: "Colin Surprenant"
project_section: "mailinglistitem"
sent_date: 2010-10-04T12:20:58-07:00
---


Hi!

I just got this OMG moment after reading Sean's comment on Innostore's
buffer pool invalidation when using random patterns for the key space.
 I am at the point where the relative poor write performance of my
Riak setup has started to bite me. I am using Innostore and not using
Bitcask because of my huge & growing keys volume.

Now, this is not a rigorous benchmark but, in my staging environment,
which uses a single node "large" EC2 instance (4 EC2 compute units,
7.5GB ram) I was using MD5 style hash keys and my Riak insertion rate
was about 40-60 items per second (using 5 writer threads over the REST
api) and the load average on my system was getting very high, around
10.

After seing this comment, I changed my key format to use a simple
increasing integer number and, bingo, my insertion rate increased
approx 10 fold, with a negligible impact of the system load.

I think it would be worth point this out in the doc somewhere. This
very simple fact does have a \\*huge\\* impact on Innostore's performance.

Colin

On Tue, Jul 6, 2010 at 4:18 PM, Sean Cribbs  wrote:
&gt; Jeremy,
&gt;
&gt; I'm glad to see you're still looking at Riak.
&gt;
&gt; Regarding your bitcask question, that does seem to be in the correct range of 
&gt; sizes.  Dave (@dizzyco) tells me the actual figure is 24 bytes + the 
&gt; hashtable overhead.
&gt;
&gt; Inno does pad things to fixed-size pages, so yes, you could end up with 
&gt; wasted disk.  However, I would suspect the greater concern would be excessive 
&gt; invalidation of the buffer pool from the essentially random/uniform shape of 
&gt; your key-space, making it difficult to get good throughput.  Inno works best 
&gt; when keys are inserted in sequential order.
&gt;
&gt; Sean Cribbs 
&gt; Developer Advocate
&gt; Basho Technologies, Inc.
&gt; http://basho.com/
&gt;
&gt; On Jul 6, 2010, at 3:36 PM, Jeremy Hinegardner wrote:
&gt;
&gt;&gt; Hi all,
&gt;&gt;
&gt;&gt; I am doing some sizing estimates for a possible transition to riak of
&gt;&gt; our document store.  I've mentioned it before on this list before and
&gt;&gt; in #riak and this is a snippet of a conversation I had with @seancribbs:
&gt;&gt;
&gt;&gt;    https://gist.github.com/c3838e5c421d6ab21c93
&gt;&gt;
&gt;&gt; I have also reviewed the bitcask-intro.pdf and http://gist.github.com/438065
&gt;&gt;
&gt;&gt; Quick and dirty info, I am looking to store billions of documents, starting
&gt;&gt; with 2 billion initially, and a linear growth of around 10 million per day.
&gt;&gt;
&gt;&gt; The key is a 64bit number as a string (generaly about 20 bytes) and the 
&gt;&gt; value is
&gt;&gt; a text/xml document of an average size of 1.5KiB.  This size long tails out 
&gt;&gt; to
&gt;&gt; maybe 5 MiB.
&gt;&gt;
&gt;&gt; Our system is write once. A key/value pair should never be overwritten once 
&gt;&gt; it
&gt;&gt; is initially inserted, and it is accessed fairly often for about a day, and 
&gt;&gt; then
&gt;&gt; a long tail drop off.  The pair must be available for retrieval at any time.
&gt;&gt;
&gt;&gt; == Bitcask ==
&gt;&gt;
&gt;&gt; I went into the source of bitcask to confirm the 32 bytes per key minimum
&gt;&gt; memory requirements mentioned in http://gist.github.com/438065 and turned
&gt;&gt; up:
&gt;&gt;
&gt;&gt;    http://github.com/basho/bitcask/blob/master/c\\_src/bitcask\\_nifs.c#L37
&gt;&gt;
&gt;&gt; If my calculations are correct, the actual memory overhead, per key using
&gt;&gt; bitcask is 72+N bytes on a 64bit system:
&gt;&gt;
&gt;&gt;    UT\\_hash\\_handle -&gt;  50 bytes, (6 pointers and 2 chars)
&gt;&gt;    file\\_id        -&gt;   4 bytes,
&gt;&gt;    total\\_sz       -&gt;   4 bytes,
&gt;&gt;    offset         -&gt;   8 bytes,
&gt;&gt;    tstamp         -&gt;   4 bytes,
&gt;&gt;    key\\_sz         -&gt;   2 bytes,
&gt;&gt;    key            -&gt;   N bytes - how big is this?  is this the riak key,
&gt;&gt;                                  or a hash of the riak key?
&gt;&gt;
&gt;&gt; This adds up to 72 bytes + the size of the key, per key/value in bitcask.
&gt;&gt;
&gt;&gt; If I assume that the key is 20 bytes, then we are talking 92 bytes of memory
&gt;&gt; overhead per document. That means I can store, ~11 million documents per GiB 
&gt;&gt; of
&gt;&gt; free memory (1024^3 / 92),  Or, if I have 32GiB of free ram on a machine
&gt;&gt; to dedicate to riak w/bitcask (the rest would be used for diskcache) I
&gt;&gt; can store ~373 Million documents.
&gt;&gt;
&gt;&gt; Are my calculations correct?
&gt;&gt;
&gt;&gt; It also does not look like bitcask pads values on disk, so there is no wasted
&gt;&gt; disk space.  Is this correct?
&gt;&gt;
&gt;&gt; == Innostore ==
&gt;&gt;
&gt;&gt; For Innostore I'm not so worried about the memory overhead as insertion 
&gt;&gt; overhead
&gt;&gt; and wasted disk space.
&gt;&gt;
&gt;&gt; Since InnoDB stores data in key order and our keys are esssentially random 
&gt;&gt; 64bit
&gt;&gt; numbers as strings, are we going to have a significant overhead in our
&gt;&gt; insertions?
&gt;&gt;
&gt;&gt; Using innostore, will there be any key/value padding on the data which
&gt;&gt; would cause an overhead per row of disk usage?
&gt;&gt;
&gt;&gt; Also, we currently compress the data on on disk, and I would interested in
&gt;&gt; hearing how the compression of disk pages with innostore works.
&gt;&gt;
&gt;&gt; thanks,
&gt;&gt;
&gt;&gt; -jeremy
&gt;&gt;
&gt;&gt; --
&gt;&gt; ========================================================================
&gt;&gt; Jeremy Hinegardner                              jer...@hinegardner.org
&gt;&gt;
&gt;
&gt;
