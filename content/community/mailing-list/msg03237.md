---
title: "Re: 'not found' after join"
description: ""
project: community
lastmod: 2011-05-05T13:07:00-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg03237"
mailinglist_parent_id: "msg03233"
author_name: "Andy Gross"
project_section: "mailinglistitem"
sent_date: 2011-05-05T13:07:00-07:00
---


Alex's description roughly matches up with some of our plans to address this
issue.

As with almost anything, this comes down to a tradeoff between consistency
and availability. In the case of joining nodes, making the
join/handoff/ownership claim process more "atomic" requires a higher degree
of consensus from the machines in the cluster. The current process (which
is clearly non-optimal) allows nodes to join the ring as long as they can
contact one current ring member. A more atomic process would introduce
consensus issues that might prevent nodes from joining in partitioned
scenarios.

A good solution would probably involve some consistency knobs around the
join process to deal with a spectrum of failure/partition scenarios.

This is something of which we are acutely aware and are actively pursuing
solutions for a near-term release.

- Andy


On Thu, May 5, 2011 at 12:22 PM, Alexander Sicular wrote:

&gt; I'm really loving this thread. Generating great ideas for the way
&gt; things should be... in the future. It seems to me that "the ring
&gt; changes immediately" is actually the problem as Ryan astutely
&gt; mentions. One way the future could look is :
&gt;
&gt; - a new node comes online
&gt; - introductions are made
&gt; - candidate vnodes are selected for migration (&lt;- insert pixie dust magic
&gt; here)
&gt; - the number of simultaneous migrations are configurable, fewer for
&gt; limited interruption or more for quicker completion
&gt; - vnodes are migrated
&gt; - once migration is completed, ownership is claimed
&gt;
&gt; Selecting vnodes for migration is where the unicorn cavalry attack the
&gt; dragons den. If done right(er) the algorithm could be swappable to
&gt; optimize for different strategies. Don't ask me how to implement it,
&gt; I'm only a yellow belt in erlang-fu.
&gt;
&gt; Cheers,
&gt; Alexander
&gt;
&gt; On Thu, May 5, 2011 at 13:33, Ryan Zezeski  wrote:
&gt; &gt; John,
&gt; &gt; All great points. The problem is that the ring changes immediately when
&gt; a
&gt; &gt; node is added. So now, all the sudden, the preflist is potentially
&gt; pointing
&gt; &gt; to nodes that don't have the data and they won't have that data until
&gt; &gt; handoff occurs. The faster that data gets transferred, the less time
&gt; your
&gt; &gt; clients have to hit 'notfound'.
&gt; &gt; However, I agree completely with what you're saying. This is just a side
&gt; &gt; effect of how the system currently works. In a perfect world we wouldn't
&gt; &gt; care how long handoff takes and we would also do some sort of automatic
&gt; &gt; congestion control akin to TCP Reno or something. The preflist would
&gt; still
&gt; &gt; point to the "old" partitions until all data has been successfully handed
&gt; &gt; off, and then and only then would we flip the switch for that vnode. I'm
&gt; &gt; pretty sure that's where we are heading (I say "pretty sure" b/c I just
&gt; &gt; joined the team and haven't been heavily involved in these specific talks
&gt; &gt; yet).
&gt; &gt; It's all coming down the pipe...
&gt; &gt; As for your specific I/O question re handoff\\_concurrecy, you might be
&gt; right.
&gt; &gt; I would think it depends on hardware/platform/etc. I was offering it as
&gt; a
&gt; &gt; possible stopgap to minimize Greg's pain. It's certainly a cure to a
&gt; &gt; symptom, not the problem itself.
&gt; &gt; -Ryan
&gt; &gt;
&gt; &gt; On Thu, May 5, 2011 at 1:10 PM, John D. Rowell  wrote:
&gt; &gt;&gt;
&gt; &gt;&gt; Hi Ryan, Greg,
&gt; &gt;&gt;
&gt; &gt;&gt; 2011/5/5 Ryan Zezeski 
&gt; &gt;&gt;&gt;
&gt; &gt;&gt;&gt; 1. For example, riak\\_core has a `handoff\\_concurrency` setting that
&gt; &gt;&gt;&gt; determines how many vnodes can concurrently handoff on a given node.
&gt; By
&gt; &gt;&gt;&gt; default this is set to 4. That's going to take a while with your 2048
&gt; &gt;&gt;&gt; vnodes and all :)
&gt; &gt;&gt;
&gt; &gt;&gt; Won't that make the handoff situation potentially worse? From the thread
&gt; I
&gt; &gt;&gt; understood that the main problem was that the cluster was shuffling too
&gt; much
&gt; &gt;&gt; data around and thus becoming unresponsive and/or returning unexpected
&gt; &gt;&gt; results (like "not founds"). I'm attributing the concerns more to an
&gt; &gt;&gt; excessive I/O situation than to how long the handoff takes. If the
&gt; handoff
&gt; &gt;&gt; can be made transparent (no or little side effects) I don't think most
&gt; &gt;&gt; people will really care (e.g. the "fix the cluster tomorrow" anecdote).
&gt; &gt;&gt;
&gt; &gt;&gt; How about using a percentage of available I/O to throttle the vnode
&gt; &gt;&gt; handoff concurrency? Start with 1, and monitor the node's I/O (kinda
&gt; like
&gt; &gt;&gt; 'atop' does, collection CPU, disk and network metrics), if it is below
&gt; the
&gt; &gt;&gt; expected usage, then increase the vnode handoff concurrency, and
&gt; vice-versa.
&gt; &gt;&gt;
&gt; &gt;&gt; I for one would be perfectly happy if the handoff took several hours
&gt; (even
&gt; &gt;&gt; days) if we could maintain the core riak\\_kv characteristics intact
&gt; during
&gt; &gt;&gt; those events. We've all seen looooong RAID rebuild times, and it's
&gt; usually
&gt; &gt;&gt; better to just sit tight and keep the rebuild speed low (slower I/O)
&gt; while
&gt; &gt;&gt; keeping all of the dependent systems running smoothly.
&gt; &gt;&gt;
&gt; &gt;&gt; cheers
&gt; &gt;&gt; -jd
&gt; &gt;
&gt; &gt;
&gt; &gt;
&gt; &gt;
&gt;

