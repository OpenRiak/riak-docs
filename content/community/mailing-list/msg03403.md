---
title: "Re: Issues with capacity planning pages on wiki"
description: ""
project: community
lastmod: 2011-05-25T09:55:25-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg03403"
mailinglist_parent_id: "msg03402"
author_name: "Jonathan Langevin"
project_section: "mailinglistitem"
sent_date: 2011-05-25T09:55:25-07:00
---


That was one hell of a response. You need to post that as a Wiki article or
such, after all that work :-O\\*


Jonathan Langevin
Systems Administrator
Loom Inc.
Wilmington, NC: (910) 241-0433 - jlange...@loomlearning.com -
www.loomlearning.com - Skype: intel352
\\*


On Wed, May 25, 2011 at 12:22 PM, Nico Meyer  wrote:

&gt; Hi Anthony,
&gt;
&gt; I think, I can explain at least a big chunk of the difference in RAM and
&gt; disk consumption you see.
&gt;
&gt; Let start with RAM. I could of course be wrong here, but I believe the 
&gt; \\*'static
&gt; bitcask per key overhead\\*' is just plainly too small. Let me explain why.
&gt; The bitcask\\_keydir\\_entry struct for each entry looks like this:
&gt;
&gt; typedef struct
&gt; {
&gt; uint32\\_t file\\_id;
&gt; uint32\\_t total\\_sz;
&gt; uint64\\_t offset;
&gt; uint32\\_t tstamp;
&gt; uint16\\_t key\\_sz;
&gt; char key[0];
&gt; } bitcask\\_keydir\\_entry;
&gt;
&gt;
&gt; This has indeed a size of 22 bytes (The array 'key' has zero entries
&gt; because the key is written to the memory address directly after the keydir
&gt; entry).
&gt; As is done int the capacity planner, you need to add the size of the bucket
&gt; and key to get the size of the keydir entry, but that is not the whole
&gt; story.
&gt;
&gt; The thing that is actually stored in key is the result of this Erlang
&gt; expression:
&gt;
&gt; erlang:term\\_to\\_binary( {&lt;&lt;"bucket"&gt;&gt;, &lt;&lt;"key"&gt;&gt;} )
&gt;
&gt; that is, a tuple of two binaries converted to the Erlang external term
&gt; format.
&gt;
&gt; So lets see:
&gt;
&gt; 1&gt; term\\_to\\_binary({&lt;&lt;&gt;&gt;,&lt;&lt;&gt;&gt;}).
&gt; &lt;&lt;131,104,2,109,0,0,0,0,109,0,0,0,0&gt;&gt;
&gt; 2&gt; iolist\\_size(term\\_to\\_binary({&lt;&lt;&gt;&gt;,&lt;&lt;&gt;&gt;})).
&gt; 13
&gt; 3&gt; iolist\\_size(term\\_to\\_binary({&lt;&lt;"a"&gt;&gt;,&lt;&lt;"b"&gt;&gt;})).
&gt; 15
&gt; 4&gt; iolist\\_size(term\\_to\\_binary({&lt;&lt;"aa"&gt;&gt;,&lt;&lt;"b"&gt;&gt;})).
&gt; 16
&gt; 5&gt; iolist\\_size(term\\_to\\_binary({&lt;&lt;"aa"&gt;&gt;,&lt;&lt;"bb"&gt;&gt;})).
&gt; 17
&gt;
&gt; so even an empty bucket/key pair take 13 bytes to store.
&gt;
&gt; Also, since the hashtable storing the keydir entries is essentially an
&gt; array of pointers to bitcask\\_keydir\\_entry objects, there is another 8 bytes
&gt; of overhead per key, assuming you are running a 64bit system.
&gt;
&gt; so the real static overhead per key is not 22 but 22+13+8 = 43 bytes.
&gt;
&gt; Lets run the numbers for your predicted memory consumption again:
&gt;
&gt; ( 43 + 10 + 36 ) \\* 183915891 \\* 3 = 49105542897 = 45.7 GB
&gt;
&gt;
&gt; Your actual RAM consumption of 70 GB seems to be at odd with the output of
&gt; erlang:memory/0 that you sent:
&gt;
&gt; {total,7281790968} =&gt; RAM: 7281790968 \\* 8 = 54.3 GB
&gt;
&gt;
&gt; So that is much closer, within about 20 percent. Some additional overhead
&gt; is to be expected, but it is hard to say how much of that is due to Erlangs
&gt; internal usage and how much due to bitcask.
&gt;
&gt; So lets examine the disk consumption next.
&gt; As you rightly concluded the equation here
&gt; http://wiki.basho.com/Cluster-Capacity-Planning.html is somewhat
&gt; simplified, and your are also right, that the real equation would be
&gt;
&gt; ( 14 + Key + Value ) \\* Num Entries \\* N\\_Val
&gt;
&gt; On the other hand 14 bytes + keysize might be quite irrelevant if your
&gt; values have a size of at least 2KB (as in the example), which seems to be
&gt; the general assumption in some aspects of the design of riak and bitcask.
&gt; As you also noticed, this additional small overhead brings you nowhere near
&gt; the disk usage that you observe.
&gt;
&gt; First, the key that is stored in the bitcask files is not the key part of
&gt; the bucket/key pair that riak calls a key, but the serialized bucket/key
&gt; pair described above, so the calculation becomes:
&gt;
&gt; ( 14 + ( 13 + Bucket + Key) + Value ) \\* Num Entries \\* N\\_Val
&gt;
&gt; ( 14 + ( 13 + 10 + 36) + 36 ) \\* 183915891 \\* 3 = 56 GB
&gt;
&gt; Still not enough :-/.
&gt; So next lets examine what is actually stored as the value in bitcask. It is
&gt; not simply the data you provide, but a riak object (r\\_object record) which
&gt; is again serialized by the erlang:term\\_to\\_binary/1 function. So lets see. I
&gt; create a new riak object with zero byte bucket, key and value:
&gt;
&gt; 3&gt; Obj = riak\\_object:new(&lt;&lt;&gt;&gt;,&lt;&lt;&gt;&gt;,&lt;&lt;&gt;&gt;).
&gt; {r\\_object,&lt;&lt;&gt;&gt;,&lt;&lt;&gt;&gt;,
&gt; [{r\\_content,{dict,0,16,16,8,80,48,
&gt; {[],[],[],[],[],[],[],[],[],[],[],[],[],[],...},
&gt; {{[],[],[],[],[],[],[],[],[],[],[],[],...}}},
&gt; &lt;&lt;&gt;&gt;}],
&gt; [],
&gt; {dict,1,16,16,8,80,48,
&gt; {[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],...},
&gt; {{[],[],[],[],[],[],[],[],[],[],[],[],[],...}}},
&gt; undefined}
&gt; 4&gt; iolist\\_size(erlang:term\\_to\\_binary(Obj)).\\*
&gt; 205\\*
&gt;
&gt; Also, bucket and key are contained int the riak object itself (and
&gt; therefore in the bitcask notion of the value). So with this information the
&gt; predicted disk usage becomes:
&gt;
&gt; ( 14 + ( 13 + Bucket + Key ) + ( 205 + Bucket + Key + Value ) ) \\* Num Entries 
&gt; \\* N\\_Val
&gt;
&gt; ( 14 + ( 13 + 10 + 36) + ( 205 + 10 + 36 ) ) \\* 183915891 \\* 3 = 166.5 GB
&gt;
&gt; which is way closer to the 341 GB you observe.
&gt;
&gt; But we can get even closer, although the detailes become somewhat more
&gt; fuzzy. But bear with me.
&gt; I again create a riak object, but this time with a non empty bucket/key so
&gt; I can store it in riak:
&gt;
&gt; (ctag@172.20.1.31)7&gt; Obj = riak\\_object:new(&lt;&lt;"a"&gt;&gt;,&lt;&lt;"a"&gt;&gt;,&lt;&lt;&gt;&gt;).
&gt; {r\\_object,&lt;&lt;"a"&gt;&gt;,&lt;&lt;"a"&gt;&gt;,
&gt; [{r\\_content,{dict,0,16,16,8,80,48,
&gt; {[],[],[],[],[],[],[],[],[],[],[],[],[],[],...},
&gt; {{[],[],[],[],[],[],[],[],[],[],[],[],...}}},
&gt; &lt;&lt;&gt;&gt;}],
&gt; [],
&gt; {dict,1,16,16,8,80,48,
&gt; {[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],...},
&gt; {{[],[],[],[],[],[],[],[],[],[],[],[],[],...}}},
&gt; undefined}
&gt;
&gt; (ctag@172.20.1.31)8&gt; iolist\\_size(erlang:term\\_to\\_binary(Obj)).\\*207\\*
&gt;
&gt; (ctag@172.20.1.31)9&gt; {ok,C}=riak:local\\_client().
&gt; {ok,{riak\\_client,'ctag@172.20.1.31',&lt;&lt;2,123,179,255&gt;&gt;}}
&gt; (ctag@172.20.1.31)10&gt; C:put(Obj,1,1).
&gt; ok
&gt;
&gt; (ctag@172.20.1.31)12&gt; {ok,ObjStored} = C:get(&lt;&lt;"a"&gt;&gt;,&lt;&lt;"a"&gt;&gt;, 1).
&gt; {ok,{r\\_object,&lt;&lt;"a"&gt;&gt;,&lt;&lt;"a"&gt;&gt;,
&gt; [{r\\_content,{dict,2,16,16,8,80,48,
&gt; {[],[],[],[],[],[],[],[],[],[],[],[],...},
&gt; {{[],[],[],[],[],[],[],[],[],[],...}}},
&gt; &lt;&lt;&gt;&gt;}],
&gt; [{&lt;&lt;2,123,179,255&gt;&gt;,{1,63473554112}}],
&gt; {dict,1,16,16,8,80,48,
&gt; {[],[],[],[],[],[],[],[],[],[],[],[],[],...},
&gt; {{[],[],[],[],[],[],[],[],[],[],[],...}}},
&gt; undefined}}
&gt; (ctag@172.20.1.31)13&gt; iolist\\_size(erlang:term\\_to\\_binary(ObjStored)).\\*358\\*
&gt;
&gt; Ok? What happened? The object we retrieved is considerably larger than the
&gt; one we stored. One culprit is the vector clock data, which was an empty list
&gt; for Obj, and now has one entry:
&gt;
&gt; (ctag@172.20.1.31)14&gt; riak\\_object:vclock(Obj).
&gt; []
&gt; (ctag@172.20.1.31)15&gt; riak\\_object:vclock(ObjStored).
&gt; [{&lt;&lt;2,123,179,255&gt;&gt;,{1,63473554112}}]
&gt; (ctag@172.20.1.31)23&gt; iolist\\_size(term\\_to\\_binary(riak\\_object:vclock(Obj))).
&gt; 2
&gt; (ctag@172.20.1.31)24&gt; 
&gt; iolist\\_size(term\\_to\\_binary(riak\\_object:vclock(ObjStored))).
&gt; 30
&gt;
&gt; So thats 28 bytes each time the object is updated with a new client ID (so
&gt; alway use a meaningful client ID!!!!), until the vclock pruning sets in. The
&gt; default bucket property is {big\\_vclock,50}, so in the worst case this could
&gt; account for 28\\*50=1400 byte!
&gt; But each object that has been stored somehow has at least one entry in the
&gt; vclock, so another 28 bytes of overhead
&gt;
&gt; The other part of the growth stems from some standard entries, which are
&gt; added to the object metadata during the put operation:
&gt;
&gt; (ctag@172.20.1.31)35&gt; dict:to\\_list(riak\\_object:get\\_metadata(Obj)).
&gt; []
&gt; (ctag@172.20.1.31)37&gt; 
&gt; iolist\\_size(term\\_to\\_binary(riak\\_object:get\\_metadata(Obj))).
&gt; 60
&gt;
&gt; (ctag@172.20.1.31)36&gt; dict:to\\_list(riak\\_object:get\\_metadata(ObjStored)).
&gt; [{&lt;&lt;"X-Riak-VTag"&gt;&gt;,"7PoD9FEMUBzNmQeMnjUbas"},
&gt; {&lt;&lt;"X-Riak-Last-Modified"&gt;&gt;,{1306,334912,424099}}]
&gt; (ctag@172.20.1.31)38&gt; 
&gt; iolist\\_size(term\\_to\\_binary(riak\\_object:get\\_metadata(ObjStored))).
&gt; 183
&gt;
&gt; So there are the other 123 bytes.
&gt;
&gt; In total this 356 byte\\* overhead per object leads us to the following
&gt; calculation: (\\* 2 bytes from the above 358 came from the bucket and key
&gt; which are already accounted for)
&gt;
&gt; ( 14 + ( 13 + Bucket + Key ) + ( 356 + Bucket + Key + Value ) ) \\* Num Entries 
&gt; \\* N\\_Val
&gt;
&gt; ( 14 + ( 13 + 10 + 36) + ( 356 + 10 + 36 ) ) \\* 183915891 \\* 3 = 244 GB
&gt;
&gt;
&gt; We are getting closer!
&gt; If you loaded the data via the REST API the overhead is somewhat larger
&gt; still, since the object will also contain 'content-type', 'X-Riak-Meta' and
&gt; 'Link' metadata entries:
&gt;
&gt; xxxx@node2:~$ curl -v -d '' -H "Content-Type: text/plain" 
&gt; http://127.0.0.1:8098/riak/a/a
&gt;
&gt;
&gt; (ctag@172.20.1.31)44&gt; {ok,ObjStored} = C:get(&lt;&lt;"a"&gt;&gt;,&lt;&lt;"a"&gt;&gt;, 1).
&gt; {ok,{r\\_object,&lt;&lt;"a"&gt;&gt;,&lt;&lt;"a"&gt;&gt;,
&gt; [{r\\_content,{dict,5,16,16,8,80,48,
&gt; {[],[],[],[],[],[],[],[],[],[],[],[],...},
&gt; 
&gt; {{[],[],[[&lt;&lt;"Links"&gt;&gt;]],[],[],[],[],[],[],[],...}}},
&gt; &lt;&lt;&gt;&gt;}],
&gt; [{&lt;&lt;5,134,53,93&gt;&gt;,{1,63473557230}}],
&gt; {dict,1,16,16,8,80,48,
&gt; {[],[],[],[],[],[],[],[],[],[],[],[],[],...},
&gt; {{[],[],[],[],[],[],[],[],[],[],[],...}}},
&gt; undefined}}
&gt; (ctag@172.20.1.31)45&gt; dict:to\\_list(riak\\_object:get\\_metadata(ObjStored)).
&gt; [{&lt;&lt;"Links"&gt;&gt;,[]},
&gt; {&lt;&lt;"X-Riak-VTag"&gt;&gt;,"3TQzJznzXXWtZefntWXPDR"},
&gt; {&lt;&lt;"content-type"&gt;&gt;,"text/plain"},
&gt; {&lt;&lt;"X-Riak-Last-Modified"&gt;&gt;,{1306,338030,682871}},
&gt; {&lt;&lt;"X-Riak-Meta"&gt;&gt;,[]}]
&gt;
&gt; (ctag@172.20.1.31)46&gt; iolist\\_size(erlang:term\\_to\\_binary(ObjStored)). 
&gt; \\*
&gt; 449\\*
&gt;
&gt;
&gt; Which leads to: (remember again to subtract 2 bytes)
&gt;
&gt; ( 14 + ( 13 + Bucket + Key ) + ( 447 + Bucket + Key + Value ) ) \\* Num Entries 
&gt; \\* N\\_Val
&gt;
&gt; ( 14 + ( 13 + 10 + 36) + ( 447 + 10 + 36 ) ) \\* 183915891 \\* 3 = 290.8 GB
&gt;
&gt;
&gt; Nearly there!
&gt;
&gt; Now there are also the hintfiles, which are a kind of an index into the
&gt; bitcask data files to speedup the start of a riak node. The hintfiles
&gt; contain one entry per key and the code that creates one entry looks like
&gt; this:
&gt;
&gt; [&lt;&gt;, &lt;&gt;,
&gt; &lt;&gt;, &lt;&gt;, Key].
&gt;
&gt;
&gt; So thats 4 + 2 + 4 + 8 + KeySize (= 18 + KeySize) additonal bytes per key.
&gt; So the final result if you inserted the key via the Rest API is:
&gt;
&gt; ( 14 + ( 13 + Bucket + Key ) + ( 447 + Bucket + Key + Value ) + (18 + ( 13 + 
&gt; Bucket + Key ) ) ) \\* Num Entries \\* N\\_Val = \\*( 505 + 3 \\* (Bucket + Key) + 
&gt; Value ) \\* Num Entries \\* N\\_Val\\*
&gt;
&gt; ( 505 + 3 \\* (10 + 36) + 36 ) \\* 183915891 \\* 3 = 374636669967 = 348.9 GB
&gt;
&gt;
&gt; And if you used Erlang (or probably any ProtocolBuffers client):
&gt;
&gt; ( 14 + ( 13 + Bucket + Key ) + ( 356 + Bucket + Key + Value ) + (18 + ( 13 + 
&gt; Bucket + Key ) ) ) \\* Num Entries \\* N\\_Val = \\*( 414 + 3 \\* (Bucket + Key) + 
&gt; Value ) \\* Num Entries \\* N\\_Val\\*
&gt;
&gt; ( 414 + 3 \\* (10 + 36) + 36 ) \\* 183915891 \\* 3 = 324427631724 = 302.1 GB
&gt;
&gt;
&gt; So the truth is somewhere in between. But as David wrote, there can be
&gt; additional overhead due to the append only nature on bitcask.
&gt;
&gt; Cheers,
&gt; Nico
&gt;
&gt; Am 24.05.2011 23:48, schrieb Anthony Molinaro:
&gt;
&gt; Just curious if anyone has any ideas, for the moment, I'm just taking
&gt; the RAM calculation and multiplying by 2 and the Disk calculation and
&gt; multiplying by 8, based on my findings with my current cluster. But
&gt; I would like to know why my values are so much higher than those I should
&gt; be getting.
&gt;
&gt; Also, I'd still like to know how the forms calculate things as the disk
&gt; calculation there does not match reality or the formula.
&gt;
&gt; Also, waiting to hear if there is any way to force merge to run so I can
&gt; more accurately gauge whether multiple copies are effecting disk usage.
&gt;
&gt; Thanks,
&gt;
&gt; -Anthony
&gt;
&gt; On Mon, May 23, 2011 at 11:06:31PM -0700, Anthony Molinaro wrote:
&gt;
&gt; On Mon, May 23, 2011 at 10:53:29PM -0700, Anthony Molinaro wrote:
&gt;
&gt; On Mon, May 23, 2011 at 09:57:25PM -0600, David Smith wrote:
&gt;
&gt; On Mon, May 23, 2011 at 9:39 PM, Anthony Molinaro
&gt; Thus, depending on
&gt; your merge triggers, more space can be used than is strictly necessary
&gt; to store the data.
&gt;
&gt; So the lack of any overhead in the calculation is expected? I mean
&gt; according to http://wiki.basho.com/Cluster-Capacity-Planning.html
&gt;
&gt; Disk = Estimated Total Objects \\* Average Object Size \\* n\\_val
&gt;
&gt; Which just seems wrong, doesn't it? I don't quite understand the
&gt; bitcask code well enough yet to see what the actual data it stores is,
&gt; but the whitepaper suggested several things were involved in the on
&gt; disk representation.
&gt;
&gt; Okay, finally found the code for this part, I kept looking in the nif
&gt; but that's only the keydir, not the data files. It looks like
&gt;
&gt; %% Setup io\\_list for writing -- avoid merging binaries if we can help it
&gt; Bytes0 = [&lt;&gt;, &lt;&gt;,
&gt; &lt;&gt;, Key, Value],
&gt; Bytes = [&lt;&lt;(erlang:crc32(Bytes0)):?CRCSIZEFIELD&gt;&gt; | Bytes0],
&gt;
&gt; And looking at the header, it seems that there's 14 bytes of overhead
&gt; (4 for CRC, 4 for timestamp, 2 for keysize, 4 for valsize).
&gt;
&gt; So disk calculation should be
&gt;
&gt; ( 14 + Key + Value ) \\* Num Entries \\* N\\_Val
&gt;
&gt; So using my numbers from before that gives
&gt;
&gt; ( 14 + 36 + 36 ) \\* 183915891 \\* 3 = 47450299878 = 44.1 GB
&gt;
&gt; which actually isn't much closer to 341 GB than the previous calculation :(
&gt;
&gt; So all my questions from the previous email still apply.
&gt;
&gt; -Anthony
&gt;
&gt; --
&gt; ------------------------------------------------------------------------
&gt; Anthony Molinaro  
&gt; 
&gt;
&gt;
&gt;

