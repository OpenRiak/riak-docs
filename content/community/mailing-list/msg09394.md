---
title: "Re: riak on SSDs - how to manage potential SSD failures"
description: ""
project: community
lastmod: 2012-11-27T13:14:19-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg09394"
mailinglist_parent_id: "msg09392"
author_name: "Matthew Von-Maszewski"
project_section: "mailinglistitem"
sent_date: 2012-11-27T13:14:19-08:00
---


Alex,

Your question is outside my knowledge space. I had to ask around. This is the 
message that came back:

You can grab \\_ALL\\_ stats for a node over HTTP at HOST:PORT/stats. This gets you 
a big JSON blob of all stats. The stat in question is in the JSON with key 
'leveldb\\_read\\_block\\_error', the value will be either "undefined" or an integer. 
The former if there is no leveldb backend.


Matthew


On Nov 27, 2012, at 1:37 PM, Alex Babkin  wrote:

&gt; Thank you for quick response Matt
&gt; 
&gt; So you are saying that i will have facilities in Riak 1.3 to handle these 
&gt; errors in application layer? automatically by riak?
&gt; 
&gt; Alex
&gt; 
&gt; 
&gt; On Mon, Nov 26, 2012 at 2:09 PM, Matthew Von-Maszewski  
&gt; wrote:
&gt; Alex,
&gt; 
&gt; The eleveldb backend creates a CRC for every item placed on the disk. You 
&gt; can activate the test of the CRC on every read by adding:
&gt; 
&gt; {verify\\_checksums, true},
&gt; 
&gt; to the "{eleveldb " portion of app.config. With riak 1.2, you must manually 
&gt; monitor each vnode directory for the lost/BLOCKS.bad file changing size. It 
&gt; only increases upon read operations detecting a CRC and/or compression 
&gt; corruption error.
&gt; 
&gt; Manually monitoring the BLOCKS.bad file is tacky (my apologies). The 
&gt; upcoming 1.3 release will populate riak admin with a counter of errors seen. 
&gt; But that code is still weeks from release.
&gt; 
&gt; Matthew
&gt; 
&gt; On Nov 26, 2012, at 1:25 PM, Alex Babkin  wrote:
&gt; 
&gt; &gt; Hi all
&gt; &gt;
&gt; &gt; first post here, so please be kind :)
&gt; &gt;
&gt; &gt; I have plans to build an experimental riak cluster out of cheap ARM 
&gt; &gt; computing parts and consumer grade SSDs to measure performance and 
&gt; &gt; experiment to assess production viability
&gt; &gt; I plan to use levelDB as the backend
&gt; &gt;
&gt; &gt; One thing to be concerned of, in light of various SSD failure stories, is 
&gt; &gt; of course a scenario of SSD failure and also the way it fails (some parts 
&gt; &gt; of SSD space just aren't writable anymore, but still readable, i.e stuck at 
&gt; &gt; some constant value). This may potentially result in a scenario where a 
&gt; &gt; replicated record on two clusters, one with working SSD and one with 
&gt; &gt; faulty, will have different data. Will riak try to account for this 
&gt; &gt; scenario?
&gt; &gt;
&gt; &gt; I'm trying to think of ways to mitigate this risk of nodes failing due to 
&gt; &gt; these SSD failures or at least get an early indication of a failure 
&gt; &gt; (however insignificant it may be).
&gt; &gt; Guess my first question should be "Does riak provide any form of checksums 
&gt; &gt; or what not on the data it reads/writes, or it blindly trusts that the 
&gt; &gt; backend/filesystem reads/writes data correctly?"
&gt; &gt;
&gt; &gt; If not, are there any other tricks people use to trigger some alarm bells 
&gt; &gt; that an SSD is 'going' ?
&gt; &gt;
&gt; &gt; Thanks
&gt; &gt; Alex
&gt; &gt;
&gt; &gt;
&gt; &gt;
&gt; &gt;
&gt; &gt;
&gt; 
&gt; 

