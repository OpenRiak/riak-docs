---
title: "Re: avg write io wait time regression in 1.2.1"
description: ""
project: community
lastmod: 2012-11-06T16:04:28-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg09211"
mailinglist_parent_id: "msg09210"
author_name: "Dietrich Featherston"
project_section: "mailinglistitem"
sent_date: 2012-11-06T16:04:28-08:00
---


Thanks for the feedback. We haven't noticed any drop in riak
responsiveness--quite the opposite. We were just alarmed at some of the
iostat information we were seeing which may very well result from, as you
pointed out, greater concurrency in layers above the disk subsystem. It's
not of concern at the moment.

As for the cluster itself--we are running 9 physical notes with a ring size
of 64. Each with a 2.93Ghz 8-core Xeon and ~2 TB of RAID0 SSD storage
across 6 physical drives.

Thanks again for looking into this.

D


On Tue, Nov 6, 2012 at 3:19 PM, Matthew Von-Maszewski wrote:

&gt; Dietrich,
&gt;
&gt; I finally reviewed your LOG.all today. The basic analysis is:
&gt;
&gt; - you have a really fast disk subsystem, and
&gt; - your machine is bored.
&gt;
&gt; I make the first comment based upon the fact that your Level-0 file
&gt; creations take less than 200 ms on files of 40Mbyte with 10,000 keys (or
&gt; more). I would like to know more specifics about your hardware and
&gt; operating system (and disk parameters). That information is likely worthy
&gt; of sharing with all.
&gt;
&gt; I make the second comment after running a shallow analysis of all
&gt; compaction activity per minute in LOG.all. Your current ingest rate (rate
&gt; of adding data to leveldb) is not very high compared to other sites I have
&gt; seen. So we should continue this discussion if you still believe there is
&gt; an overall slow down.
&gt;
&gt; I will note that the authors of Erlang have given us some suggested
&gt; changes in our Erlang to leveldb interface this past weekend. That
&gt; information could also give leveldb a throughput boost if proven valid.
&gt; Keep you posted.
&gt;
&gt; But at this time I see nothing that yells "massive slow down". I am of
&gt; course open to being wrong.
&gt;
&gt; Matthew
&gt;
&gt;
&gt; On Nov 2, 2012, at 10:32 AM, Dietrich Featherston wrote:
&gt;
&gt; Here's the level output from one of our upgraded nodes. Included our
&gt; app.config as well. Will continue looking for clues. I can put together
&gt; some snapshots of our sst file sizes across the cluster if you think that
&gt; would help as well.
&gt;
&gt; On Fri, Nov 2, 2012 at 6:19 AM, Matthew Von-Maszewski 
&gt; wrote:
&gt;
&gt;&gt; Dietrich,
&gt;&gt;
&gt;&gt; I can make two guesses into the increased disk writes. But I am also
&gt;&gt; willing to review your actual LOG files to isolate root cause. If you
&gt;&gt; could run the following and post the resulting file from one server, I will
&gt;&gt; review it over the weekend or early next week:
&gt;&gt;
&gt;&gt; sort /var/lib/riak/leveldb/\\*/LOG &gt;LOG.all
&gt;&gt;
&gt;&gt; The file will compress well. And no need to stop the server, just gather
&gt;&gt; the LOG data live.
&gt;&gt;
&gt;&gt; Guess 1: your data is in a transition phase. 1.1 used 2 Megabyte files
&gt;&gt; exclusively. 1.2 is resizing the files to much larger sizes during a
&gt;&gt; compaction. You could be seeing a larger number of files than usual
&gt;&gt; participating in each compaction as the file sizes change. While this is
&gt;&gt; possible, I have doubts … hence this is a guess.
&gt;&gt;
&gt;&gt; Guess 2: I increased the various leveldb file sizes to reduce the number
&gt;&gt; of open and closes, both for writes and random reads. This helped
&gt;&gt; latencies in both the compactions and random reads. Any compaction in 1.2
&gt;&gt; is likely to reread and write larger total number of bytes. While this is
&gt;&gt; possible, I again have doubts … the number of read operations should also
&gt;&gt; go up if this guess is correct. Your read operations have not increased.
&gt;&gt; This guess might still be valid if the read operations were satisfied by
&gt;&gt; the Linux memory data cache. I do not how those would be counted or not
&gt;&gt; counted.
&gt;&gt;
&gt;&gt;
&gt;&gt; Matthew
&gt;&gt;
&gt;&gt;
&gt;&gt; On Nov 1, 2012, at 10:01 PM, Dietrich Featherston wrote:
&gt;&gt;
&gt;&gt; Will check on that.
&gt;&gt;
&gt;&gt; Can you think of anything that would explain the 5x increase in disk
&gt;&gt; writes we are seeing with the same workload?
&gt;&gt;
&gt;&gt;
&gt;&gt; On Thu, Nov 1, 2012 at 6:03 PM, Matthew Von-Maszewski &gt; &gt; wrote:
&gt;&gt;
&gt;&gt;&gt; Look for any activity in the LOG. Level-0 "creations" are fast and not
&gt;&gt;&gt; typically relevant. You would be most interested in LOG lines containing
&gt;&gt;&gt; "Compacting" (start) and "Compacted" (end). The time in between will
&gt;&gt;&gt; throttle. The idea is that these compaction events can pile up, one after
&gt;&gt;&gt; another and multiple overlapping. It is these heavy times where the
&gt;&gt;&gt; throttle saves the user experience.
&gt;&gt;&gt;
&gt;&gt;&gt; Matthew
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; On Nov 1, 2012, at 8:54 PM, Dietrich Featherston wrote:
&gt;&gt;&gt;
&gt;&gt;&gt; Thanks. The amortized stalls may very well describe what we are seeing.
&gt;&gt;&gt; If I combine leveldb logs from all partitions on one of the upgraded nodes
&gt;&gt;&gt; what should I look for in terms of compaction activity to verify this?
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; On Thu, Nov 1, 2012 at 5:48 PM, Matthew Von-Maszewski &lt;
&gt;&gt;&gt; matth...@basho.com&gt; wrote:
&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Dietrich,
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; I can see your concern with the write IOS statistic. Let me comment on
&gt;&gt;&gt;&gt; the easy question first: block\\_size.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; The block\\_size parameter in 1.1 was not getting passed to leveldb from
&gt;&gt;&gt;&gt; the erlang layer. You were using a 4096 byte block parameter no matter
&gt;&gt;&gt;&gt; what you typed in the app.config. The block\\_size is used by leveldb as a
&gt;&gt;&gt;&gt; threshold. Once you accumulate enough data above that threshold, the
&gt;&gt;&gt;&gt; current block is written to disk and a new one started. If you have 10k
&gt;&gt;&gt;&gt; data values, your get one data item per block and its size is ~10k. If you
&gt;&gt;&gt;&gt; have 1k data values, you get about four per block and the block is about 
&gt;&gt;&gt;&gt; 4k.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; We recommend 4k blocks to help read performance. The entire block has
&gt;&gt;&gt;&gt; to run through decompression and potentially CRC calculation when it comes
&gt;&gt;&gt;&gt; off the disk. That CPU time really kills any disk performance gains by
&gt;&gt;&gt;&gt; having larger blocks. Ok, that might change in 1.3 as we enable hardware
&gt;&gt;&gt;&gt; CRC … but only if you have "verify\\_checksums, true" in app.config.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Back to performance: I have not seen the change your graph details
&gt;&gt;&gt;&gt; when testing with SAS drives under moderate load. I am only today starting
&gt;&gt;&gt;&gt; qualification tests with SSD drives.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; But my 1.2 and 1.3 tests focus on drive / Riak saturation. 1.1 has the
&gt;&gt;&gt;&gt; nasty tendency to stall (intentionally) when we saturate the write side of
&gt;&gt;&gt;&gt; leveldb, . The stall was measured in seconds or even minutes in 1.1.
&gt;&gt;&gt;&gt; 1.2.1 has a write throttle that forecasts leveldb's stall state and
&gt;&gt;&gt;&gt; incrementally slows the individual writes to prevent the stalls. Maybe
&gt;&gt;&gt;&gt; that is what is being seen in the graph. The only way to know for sure is
&gt;&gt;&gt;&gt; to get an dump of your leveldb LOG files, combined them, then compare
&gt;&gt;&gt;&gt; compaction activity to your graph.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Write stalls are detailed here:
&gt;&gt;&gt;&gt; http://basho.com/blog/technical/2012/10/30/leveldb-in-riak-1p2/
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; How can I better assist you at this point?
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Matthew
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; On Nov 1, 2012, at 8:13 PM, Dietrich Featherston wrote:
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; We've just gone through the process of upgrading two riak clusters from
&gt;&gt;&gt;&gt; 1.1 to 1.2.1. Both are on the leveldb backend backed by RAID0'd SSDs. The
&gt;&gt;&gt;&gt; process has gone smoothly and we see that latencies as measured at the
&gt;&gt;&gt;&gt; gen\\_fsm level are largely unaffected.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; However, we are seeing some troubling disk statistics and I'm looking
&gt;&gt;&gt;&gt; for an explanation before we upgrade the remainder of our nodes. The source
&gt;&gt;&gt;&gt; of the worry seems to be a huge amplification in the number of writes
&gt;&gt;&gt;&gt; serviced by the disk which may be the cause of rising io wait times.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; My first thought was that this could be due to some leveldb tuning in
&gt;&gt;&gt;&gt; 1.2.1 which increases file sizes per the release notes (
&gt;&gt;&gt;&gt; https://github.com/basho/riak/blob/master/RELEASE-NOTES.md). But nodes
&gt;&gt;&gt;&gt; that were upgraded yesterday are still showing this symptom. I would have
&gt;&gt;&gt;&gt; expected any block re-writing to have subsided by now.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Next hypothesis has to do with block size overriding in app.config. In
&gt;&gt;&gt;&gt; 1.1, we had specified custom block sizes of 256k. We removed this prior to
&gt;&gt;&gt;&gt; upgrading to 1.2.1 at the advice of #riak since block size configuration
&gt;&gt;&gt;&gt; was ignored prior to 1.2 ('"block\\_size" parameter within app.config for
&gt;&gt;&gt;&gt; leveldb was ignored. This parameter is now properly passed to leveldb.'
&gt;&gt;&gt;&gt; --&gt;
&gt;&gt;&gt;&gt; https://github.com/basho/riak/commit/f12596c221a9d942cc23d8e4fd83c9ca46e02105).
&gt;&gt;&gt;&gt; I'm wondering if the block size parameter really was being passed to
&gt;&gt;&gt;&gt; leveldb, and having removed it, blocks are now being rewritten to a new
&gt;&gt;&gt;&gt; size, perhaps different from what they were being written as before (
&gt;&gt;&gt;&gt; https://github.com/basho/riak\\_kv/commit/ad192ee775b2f5a68430d230c0999a2caabd1155
&gt;&gt;&gt;&gt; )
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Here is the output of the following script showing the increased writes
&gt;&gt;&gt;&gt; to disk (https://gist.github.com/37319a8ed2679bb8b21d)
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; --an upgraded 1.2.1 node--
&gt;&gt;&gt;&gt; read ios: 238406742
&gt;&gt;&gt;&gt; write ios: 4814320281
&gt;&gt;&gt;&gt; read/write ratio: .04952033
&gt;&gt;&gt;&gt; avg wait: .10712340
&gt;&gt;&gt;&gt; read wait: .49174364
&gt;&gt;&gt;&gt; write wait: .42695475
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; --a node still running 1.1--
&gt;&gt;&gt;&gt; read ios: 267770032
&gt;&gt;&gt;&gt; write ios: 944170656
&gt;&gt;&gt;&gt; read/write ratio: .28360342
&gt;&gt;&gt;&gt; avg wait: .34237204
&gt;&gt;&gt;&gt; read wait: .47222371
&gt;&gt;&gt;&gt; write wait: 1.83283749
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; And here's what munin is showing us in terms of avg io wait times.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Any thoughts on what might explain this?
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Thanks,
&gt;&gt;&gt;&gt; D
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt; 
&gt;
&gt;
&gt;
