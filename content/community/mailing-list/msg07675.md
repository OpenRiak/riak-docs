---
title: "Re: full cluster failure"
description: ""
project: community
lastmod: 2012-06-12T21:43:05-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg07675"
mailinglist_parent_id: "msg07665"
author_name: "Gil Teixeira"
project_section: "mailinglistitem"
sent_date: 2012-06-12T21:43:05-07:00
---


Let me try and place some more information about my problem here.
Like I explain the full cluster becomes unresponsive in this scenario.(when one 
node abruptly fails)

This is the console.log of one cluster node:
right after boot
2012-06-13 12:25:19.548 [info] &lt;0.1721.0&gt; Application bitcask started on node 
'riak@192.168.1.201'
2012-06-13 12:25:19.574 [info] &lt;0.1975.0&gt;@riak\\_core:wait\\_for\\_application:390 
Wait complete for application riak\\_pipe (0 seconds)
2012-06-13 12:25:19.579 [info] &lt;0.2119.0&gt;@riak\\_core:wait\\_for\\_application:396 
Waiting for application riak\\_kv to start (0 seconds).
2012-06-13 12:25:19.581 [info] &lt;0.1721.0&gt; Application riak\\_kv started on node 
'riak@192.168.1.201'
2012-06-13 12:25:19.583 [info] &lt;0.1721.0&gt; Application riak\\_search started on 
node 'riak@192.168.1.201'
2012-06-13 12:25:19.583 [info] &lt;0.1721.0&gt; Application basho\\_stats started on 
node 'riak@192.168.1.201'
2012-06-13 12:25:19.591 [info] &lt;0.1721.0&gt; Application runtime\\_tools started on 
node 'riak@192.168.1.201'
2012-06-13 12:25:19.652 [info] &lt;0.1906.0&gt;@riak\\_core:wait\\_for\\_service:410 Wait 
complete for service riak\\_pipe (0 seconds)
2012-06-13 12:25:19.680 [info] &lt;0.2119.0&gt;@riak\\_core:wait\\_for\\_application:390 
Wait complete for application riak\\_kv (0 seconds)
2012-06-13 12:25:19.802 [info] &lt;0.2009.0&gt;@riak\\_core:wait\\_for\\_service:410 Wait 
complete for service riak\\_kv (0 seconds)


Seams normal to me ;)
nothing on error.log and crash.log 

I disconnect a node from the network and when i try to retrieve data from the 
cluster:


2012-06-13 12:34:55.714 [info] &lt;0.7.0&gt; Application lager started on node 
'riak@192.168.1.201'
2012-06-13 12:34:55.742 [info] &lt;0.7.0&gt; Application public\\_key started on node 
'riak@192.168.1.201'
2012-06-13 12:34:55.749 [info] &lt;0.7.0&gt; Application ssl started on node 
'riak@192.168.1.201'
2012-06-13 12:34:55.757 [info] &lt;0.7.0&gt; Application riak\\_core started on node 
'riak@192.168.1.201'
2012-06-13 12:34:55.786 [info] &lt;0.7.0&gt; Application riak\\_control started on node 
'riak@192.168.1.201'
2012-06-13 12:34:55.787 [info] &lt;0.7.0&gt; Application basho\\_metrics started on 
node 'riak@192.168.1.201'
2012-06-13 12:34:55.788 [info] &lt;0.7.0&gt; Application cluster\\_info started on node 
'riak@192.168.1.201'
2012-06-13 12:34:55.794 [info] &lt;0.7.0&gt; Application merge\\_index started on node 
'riak@192.168.1.201'
2012-06-13 12:34:55.799 [info] &lt;0.192.0&gt;@riak\\_core:wait\\_for\\_service:416 Waiting 
for service riak\\_pipe to start (0 seconds)
2012-06-13 12:34:55.809 [info] &lt;0.261.0&gt;@riak\\_core:wait\\_for\\_application:396 
Waiting for application riak\\_pipe to start (0 seconds).
2012-06-13 12:34:55.810 [info] &lt;0.7.0&gt; Application riak\\_pipe started on node 
'riak@192.168.1.201'
2012-06-13 12:34:55.820 [info] &lt;0.7.0&gt; Application inets started on node 
'riak@192.168.1.201'
2012-06-13 12:34:55.822 [info] &lt;0.7.0&gt; Application mochiweb started on node 
'riak@192.168.1.201'
2012-06-13 12:34:55.825 [info] &lt;0.7.0&gt; Application erlang\\_js started on node 
'riak@192.168.1.201'
2012-06-13 12:34:55.829 [info] &lt;0.7.0&gt; Application luke started on node 
'riak@192.168.1.201'
2012-06-13 12:34:55.837 [info] &lt;0.295.0&gt;@riak\\_core:wait\\_for\\_service:416 Waiting 
for service riak\\_kv to start (0 seconds)
2012-06-13 12:34:55.855 [info] &lt;0.312.0&gt;@riak\\_kv\\_js\\_vm:init:76 Spidermonkey VM 
(thread stack: 16MB, max heap: 8MB, pool: riak\\_kv\\_js\\_map) host starting 
(&lt;0.312.0&gt;)
2012-06-13 12:34:55.857 [info] &lt;0.313.0&gt;@riak\\_kv\\_js\\_vm:init:76 Spidermonkey VM 
(thread stack: 16MB, max heap: 8MB, pool: riak\\_kv\\_js\\_map) host starting 
(&lt;0.313.0&gt;)
2012-06-13 12:34:55.858 [info] &lt;0.314.0&gt;@riak\\_kv\\_js\\_vm:init:76 Spidermonkey VM 
(thread stack: 16MB, max heap: 8MB, pool: riak\\_kv\\_js\\_map) host starting 
(&lt;0.314.0&gt;)
2012-06-13 12:34:55.860 [info] &lt;0.315.0&gt;@riak\\_kv\\_js\\_vm:init:76 Spidermonkey VM 
(thread stack: 16MB, max heap: 8MB, pool: riak\\_kv\\_js\\_map) host starting 
(&lt;0.315.0&gt;)
2012-06-13 12:34:55.862 [info] &lt;0.316.0&gt;@riak\\_kv\\_js\\_vm:init:76 Spidermonkey VM 
(thread stack: 16MB, max heap: 8MB, pool: riak\\_kv\\_js\\_map) host starting 
(&lt;0.316.0&gt;)
2012-06-13 12:34:55.864 [info] &lt;0.317.0&gt;@riak\\_kv\\_js\\_vm:init:76 Spidermonkey VM 
(thread stack: 16MB, max heap: 8MB, pool: riak\\_kv\\_js\\_map) host starting 
(&lt;0.317.0&gt;)
2012-06-13 12:34:55.866 [info] &lt;0.318.0&gt;@riak\\_kv\\_js\\_vm:init:76 Spidermonkey VM 
(thread stack: 16MB, max heap: 8MB, pool: riak\\_kv\\_js\\_map) host starting 
(&lt;0.318.0&gt;)
2012-06-13 12:34:55.868 [info] &lt;0.319.0&gt;@riak\\_kv\\_js\\_vm:init:76 Spidermonkey VM 
(thread stack: 16MB, max heap: 8MB, pool: riak\\_kv\\_js\\_map) host starting 
(&lt;0.319.0&gt;)
2012-06-13 12:34:55.870 [info] &lt;0.321.0&gt;@riak\\_kv\\_js\\_vm:init:76 Spidermonkey VM 
(thread stack: 16MB, max heap: 8MB, pool: riak\\_kv\\_js\\_reduce) host starting 
(&lt;0.321.0&gt;)
2012-06-13 12:34:55.871 [info] &lt;0.322.0&gt;@riak\\_kv\\_js\\_vm:init:76 Spidermonkey VM 
(thread stack: 16MB, max heap: 8MB, pool: riak\\_kv\\_js\\_reduce) host starting 
(&lt;0.322.0&gt;)
2012-06-13 12:34:55.873 [info] &lt;0.323.0&gt;@riak\\_kv\\_js\\_vm:init:76 Spidermonkey VM 
(thread stack: 16MB, max heap: 8MB, pool: riak\\_kv\\_js\\_reduce) host starting 
(&lt;0.323.0&gt;)
2012-06-13 12:34:55.875 [info] &lt;0.324.0&gt;@riak\\_kv\\_js\\_vm:init:76 Spidermonkey VM 
(thread stack: 16MB, max heap: 8MB, pool: riak\\_kv\\_js\\_reduce) host starting 
(&lt;0.324.0&gt;)
2012-06-13 12:34:55.877 [info] &lt;0.325.0&gt;@riak\\_kv\\_js\\_vm:init:76 Spidermonkey VM 
(thread stack: 16MB, max heap: 8MB, pool: riak\\_kv\\_js\\_reduce) host starting 
(&lt;0.325.0&gt;)
2012-06-13 12:34:55.879 [info] &lt;0.326.0&gt;@riak\\_kv\\_js\\_vm:init:76 Spidermonkey VM 
(thread stack: 16MB, max heap: 8MB, pool: riak\\_kv\\_js\\_reduce) host starting 
(&lt;0.326.0&gt;)
2012-06-13 12:34:55.880 [info] &lt;0.328.0&gt;@riak\\_kv\\_js\\_vm:init:76 Spidermonkey VM 
(thread stack: 16MB, max heap: 8MB, pool: riak\\_kv\\_js\\_hook) host starting 
(&lt;0.328.0&gt;)
2012-06-13 12:34:55.882 [info] &lt;0.329.0&gt;@riak\\_kv\\_js\\_vm:init:76 Spidermonkey VM 
(thread stack: 16MB, max heap: 8MB, pool: riak\\_kv\\_js\\_hook) host starting 
(&lt;0.329.0&gt;)
2012-06-13 12:34:55.887 [info] &lt;0.7.0&gt; Application bitcask started on node 
'riak@192.168.1.201'
2012-06-13 12:34:55.922 [info] &lt;0.261.0&gt;@riak\\_core:wait\\_for\\_application:390 
Wait complete for application riak\\_pipe (0 seconds)
2012-06-13 12:34:55.934 [info] &lt;0.431.0&gt;@riak\\_core:wait\\_for\\_application:396 
Waiting for application riak\\_kv to start (0 seconds).
2012-06-13 12:34:55.938 [info] &lt;0.7.0&gt; Application riak\\_kv started on node 
'riak@192.168.1.201'
2012-06-13 12:34:55.940 [info] &lt;0.7.0&gt; Application riak\\_search started on node 
'riak@192.168.1.201'
2012-06-13 12:34:55.941 [info] &lt;0.7.0&gt; Application basho\\_stats started on node 
'riak@192.168.1.201'
2012-06-13 12:34:55.945 [info] &lt;0.7.0&gt; Application runtime\\_tools started on 
node 'riak@192.168.1.201'
2012-06-13 12:34:56.015 [info] &lt;0.192.0&gt;@riak\\_core:wait\\_for\\_service:410 Wait 
complete for service riak\\_pipe (0 seconds)
2012-06-13 12:34:56.047 [info] &lt;0.431.0&gt;@riak\\_core:wait\\_for\\_application:390 
Wait complete for application riak\\_kv (0 seconds)
2012-06-13 12:34:56.156 [info] &lt;0.295.0&gt;@riak\\_core:wait\\_for\\_service:410 Wait 
complete for service riak\\_kv (0 seconds)
2012-06-13 12:35:24.556 [error] &lt;0.729.0&gt; gen\\_fsm &lt;0.729.0&gt; in state 
wait\\_pipeline\\_shutdown terminated with reason: {sink\\_died,shutdown}
2012-06-13 12:35:24.564 [error] &lt;0.729.0&gt; CRASH REPORT Process &lt;0.729.0&gt; with 0 
neighbours crashed with reason: {sink\\_died,shutdown}
2012-06-13 12:35:24.572 [error] &lt;0.193.0&gt; Supervisor riak\\_pipe\\_builder\\_sup had 
child undefined started with {riak\\_pipe\\_builder,start\\_link,undefined} at 
&lt;0.729.0&gt; exit with reason {sink\\_died,shutdown} in context child\\_terminated
2012-06-13 12:35:24.573 [error] &lt;0.724.0&gt; gen\\_fsm &lt;0.724.0&gt; in state 
wait\\_pipeline\\_shutdown terminated with reason: {sink\\_died,normal}
2012-06-13 12:35:24.575 [error] &lt;0.724.0&gt; CRASH REPORT Process &lt;0.724.0&gt; with 2 
neighbours crashed with reason: {sink\\_died,normal}
2012-06-13 12:35:24.577 [error] &lt;0.193.0&gt; Supervisor riak\\_pipe\\_builder\\_sup had 
child undefined started with {riak\\_pipe\\_builder,start\\_link,undefined} at 
&lt;0.724.0&gt; exit with reason {sink\\_died,normal} in context child\\_terminated
2012-06-13 12:35:24.578 [error] &lt;0.195.0&gt; Supervisor riak\\_pipe\\_qcover\\_sup had 
child undefined started with {riak\\_core\\_coverage\\_fsm,start\\_link,undefined} at 
&lt;0.731.0&gt; exit with reason {sink\\_died,normal} in context child\\_terminated


on error.log i get:

2012-06-13 12:34:41.710 [error] &lt;0.1880.0&gt; Supervisor riak\\_control\\_sup had 
child riak\\_control\\_session started with riak\\_control\\_session:start\\_link() at 
&lt;0.1881.0&gt; exit with reason killed in context shutdown\\_error
2012-06-13 12:35:24.556 [error] &lt;0.729.0&gt; gen\\_fsm &lt;0.729.0&gt; in state 
wait\\_pipeline\\_shutdown terminated with reason: {sink\\_died,shutdown}
2012-06-13 12:35:24.564 [error] &lt;0.729.0&gt; CRASH REPORT Process &lt;0.729.0&gt; with 0 
neighbours crashed with reason: {sink\\_died,shutdown}
2012-06-13 12:35:24.572 [error] &lt;0.193.0&gt; Supervisor riak\\_pipe\\_builder\\_sup had 
child undefined started with {riak\\_pipe\\_builder,start\\_link,undefined} at 
&lt;0.729.0&gt; exit with reason {sink\\_died,shutdown} in context child\\_terminated
2012-06-13 12:35:24.573 [error] &lt;0.724.0&gt; gen\\_fsm &lt;0.724.0&gt; in state 
wait\\_pipeline\\_shutdown terminated with reason: {sink\\_died,normal}
2012-06-13 12:35:24.575 [error] &lt;0.724.0&gt; CRASH REPORT Process &lt;0.724.0&gt; with 2 
neighbours crashed with reason: {sink\\_died,normal}
2012-06-13 12:35:24.577 [error] &lt;0.193.0&gt; Supervisor riak\\_pipe\\_builder\\_sup had 
child undefined started with {riak\\_pipe\\_builder,start\\_link,undefined} at 
&lt;0.724.0&gt; exit with reason {sink\\_died,normal} in context child\\_terminated
2012-06-13 12:35:24.578 [error] &lt;0.195.0&gt; Supervisor riak\\_pipe\\_qcover\\_sup had 
child undefined started with {riak\\_core\\_coverage\\_fsm,start\\_link,undefined} at 
&lt;0.731.0&gt; exit with reason {sink\\_died,normal} in context child\\_terminated
2012-06-13 12:36:25.529 [error] &lt;0.159.0&gt; \\*\\* Node 'riak@192.168.1.204' not 
responding \\*\\*
\\*\\* Removing (timedout) connection \\*\\*


Some more info:
This is the state after disconnecting one node:
root@db1:/var/log/riak# riak-admin member\\_status
Attempting to restart script through sudo -u riak
================================= Membership ==================================
Status Ring Pending Node
-------------------------------------------------------------------------------
valid 18.8% -- 'riak@192.168.1.201'
valid 18.8% -- 'riak@192.168.1.202'
valid 18.8% -- 'riak@192.168.1.203'
valid 25.0% -- 'riak@192.168.1.204'
valid 18.8% -- 'riak@192.168.1.205'
-------------------------------------------------------------------------------
Valid:5 / Leaving:0 / Exiting:0 / Joining:0 / Down:0
root@db1:/var/log/riak# riak-admin ring\\_status
Attempting to restart script through sudo -u riak
================================== Claimant ===================================
Claimant: 'riak@192.168.1.201'
Status: up
Ring Ready: true

============================== Ownership Handoff ==============================
No pending changes.

============================== Unreachable Nodes ==============================
The following nodes are unreachable: ['riak@192.168.1.204']

WARNING: The cluster state will not converge until all nodes
are up. Once the above nodes come back online, convergence
will continue. If the outages are long-term or permanent, you
can either mark the nodes as down (riak-admin down NODE) or
forcibly remove the nodes from the cluster (riak-admin
force-remove NODE) to allow the remaining nodes to settle.
root@db1:/var/log/riak# 







On Jun 12, 2012, at 11:10 PM, Gil Teixeira wrote:

&gt; Anything… please do help me in regard to what files i should send… 
&gt; 
&gt; On Jun 12, 2012, at 10:55 PM, Mark Phillips wrote:
&gt; 
&gt;&gt; Hi Gil,
&gt;&gt; 
&gt;&gt; Anything in the logs you can share?
&gt;&gt; 
&gt;&gt; Mark
&gt;&gt; 
&gt;&gt; 
&gt;&gt; On Jun 12, 2012, at 1:28, Gil Teixeira  wrote:
&gt;&gt; 
&gt;&gt;&gt; Hi,
&gt;&gt;&gt; 
&gt;&gt;&gt; Hellow, I am faily new to RIAK, and am experiencing a very peculiar 
&gt;&gt;&gt; problem. 
&gt;&gt;&gt; We are conducting a RIAK evaluation to see if it fits our purposes, because 
&gt;&gt;&gt; on paper it seams to be a perfect fit.
&gt;&gt;&gt; 
&gt;&gt;&gt; To create an evaluating scenario i have setup 5 nodes under vmware(the 
&gt;&gt;&gt; network is bridged)
&gt;&gt;&gt; 
&gt;&gt;&gt; The cluster runs just fine, and i can down nodes for maintenance as 
&gt;&gt;&gt; expected perfectly, while the cluster keeps serving the data as expected. 
&gt;&gt;&gt; Node reboots are also tolerated well by the cluster as expected.
&gt;&gt;&gt; 
&gt;&gt;&gt; But if a note unexpectedly fails (hard power off or sudden network 
&gt;&gt;&gt; disconnection) all nodes get exited and the full cluster becomes 
&gt;&gt;&gt; inaccessible until the failed node is back up or until i manually mark the 
&gt;&gt;&gt; failed node as down with risk-admin.
&gt;&gt;&gt; 
&gt;&gt;&gt; I was expecting a cluster (n3) with 5 nodes to simply tolerate 1 node 
&gt;&gt;&gt; failure transparently. 
&gt;&gt;&gt; 
&gt;&gt;&gt; Is there something i may be doing wrong? Is this the expected behavior?
&gt;&gt;&gt; 
&gt;&gt;&gt; I would appreciate any light anyone could shine on this subject.
&gt;&gt;&gt; 
&gt;&gt;&gt; Thank you,
&gt;&gt;&gt; Gil
&gt;&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt; 
&gt; 
&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com


\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

