---
title: "Re: mysterious Riak problems"
description: ""
project: community
lastmod: 2012-11-14T13:46:15-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg09328"
mailinglist_parent_id: "msg09327"
author_name: "David Lowell"
project_section: "mailinglistitem"
sent_date: 2012-11-14T13:46:15-08:00
---


The 512 vnodes will run on 5 physical nodes in production, but we're running 
all 512 on a single node in dev. And it's on one of these single node 
"clusters" that we're seeing these issues.

Dave

--
Dave Lowell
d...@connectv.com

On Nov 14, 2012, at 1:35 PM, Matthew Von-Maszewski wrote:

&gt; Dave,
&gt; 
&gt; The problem seems most pronounced when your are averaging 6 megabyte values. 
&gt; Honestly, my previous test suite only included 150k values. The 
&gt; WriteThrottle is NOT giving you the support you need in this situation (you 
&gt; need it to be throttling a tad more). I need to think on how to help here.
&gt; 
&gt; I see from the old emails that you have 32G of physical memory and this log 
&gt; suggests you have 512 vnodes per physical server. Is this correct? Also, 
&gt; how many physical servers?
&gt; 
&gt; Matthew
&gt; 
&gt; On Nov 14, 2012, at 2:00 PM, David Lowell wrote:
&gt; 
&gt;&gt; Thanks Matthew. Yep, there are quite a few hits on 'waiting'. Interesting. 
&gt;&gt; I'll send the merged log separately.
&gt;&gt; 
&gt;&gt; Dave
&gt;&gt; 
&gt;&gt; --
&gt;&gt; Dave Lowell
&gt;&gt; d...@connectv.com
&gt;&gt; 
&gt;&gt; On Nov 14, 2012, at 10:43 AM, Matthew Von-Maszewski wrote:
&gt;&gt; 
&gt;&gt;&gt; Dave,
&gt;&gt;&gt; 
&gt;&gt;&gt; Ok, heavy writes. Let's see if leveldb has hit one of its intentional 
&gt;&gt;&gt; "stalls":
&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; sort /var/db/riak/leveldb/\\*/LOG\\* | grep -i waiting
&gt;&gt;&gt; 
&gt;&gt;&gt; See if that shows any indication of stall in the LOG files of leveldb. If 
&gt;&gt;&gt; so, pick one server and send me a combined LOG file from that server:
&gt;&gt;&gt; 
&gt;&gt;&gt; sort /var/db/riak/leveldb/\\*/LOG\\* &gt;LOG.all
&gt;&gt;&gt; 
&gt;&gt;&gt; That will tar zip really well.
&gt;&gt;&gt; 
&gt;&gt;&gt; Matthew
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; On Nov 14, 2012, at 1:34 PM, David Lowell wrote:
&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Thanks Matthew. I've run both greps with no hits, unfortunately.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; A couple of details that I want to highlight. Since I first posted about 
&gt;&gt;&gt;&gt; this issue, we upgraded from Riak 1.2.0, to 1.2.1. Following that upgrade, 
&gt;&gt;&gt;&gt; we continue see these periods of instability with errors in the logs like 
&gt;&gt;&gt;&gt; "riak\\_kv\\_vnode worker pool crashed", but we started seeing lots of new 
&gt;&gt;&gt;&gt; error records in the logs about "Unrecognized message" as well. In both 
&gt;&gt;&gt;&gt; cases, we see tons of these "long\\_gc" monitoring messages and several 
&gt;&gt;&gt;&gt; "system\\_memory\\_high\\_watermark" alarms during these periods. The client 
&gt;&gt;&gt;&gt; also has connection problems such as timeouts and connections being 
&gt;&gt;&gt;&gt; refused.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; The logs from the last 18 hours on this server are really a mess, with 
&gt;&gt;&gt;&gt; very high levels of all of these errors. I'd be happy to send them along 
&gt;&gt;&gt;&gt; if you think that would help.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; It's also worth noting that our application is pretty write heavy, has 
&gt;&gt;&gt;&gt; lots of parallel processes generating those writes (so not a lot of flow 
&gt;&gt;&gt;&gt; control if Riak bogs down, at least not yet). It's probably pushing Riak 
&gt;&gt;&gt;&gt; fairly hard.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Dave
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; --
&gt;&gt;&gt;&gt; Dave Lowell
&gt;&gt;&gt;&gt; d...@connectv.com
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; On Nov 14, 2012, at 8:51 AM, Matthew Von-Maszewski wrote:
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; Dave,
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; Just getting my head back into the game. Was away for a few days. 
&gt;&gt;&gt;&gt;&gt; Random thought, maybe there is a hard drive with a read problem. That 
&gt;&gt;&gt;&gt;&gt; can cause issues similar to this. 1.2.1 does NOT percolate the read 
&gt;&gt;&gt;&gt;&gt; errors seen in leveldb to riak-admin (yes, that should start to happen in 
&gt;&gt;&gt;&gt;&gt; 1.3).
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; I will assume your leveldb "data\\_root" is "/var/db/riak/leveldb" for this 
&gt;&gt;&gt;&gt;&gt; script. Please substitute your appropriate path from app.config and try 
&gt;&gt;&gt;&gt;&gt; these two commands on each physical server (node):
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; sort /var/db/riak/leveldb/\\*/LOG\\* | grep corrupted
&gt;&gt;&gt;&gt;&gt; sort /var/db/riak/leveldb/\\*/LOG\\* | grep checksum
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; If you get hits on either, we have found the problem.
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; The "LOCK file unavailable" is more of a statement about the internal 
&gt;&gt;&gt;&gt;&gt; condition of the code instead of an error. The message is saying that 
&gt;&gt;&gt;&gt;&gt; the first attempt to re-open a vnode failed because the prior instance is 
&gt;&gt;&gt;&gt;&gt; still closing (or more likely waiting for Erlang's garbage collection to 
&gt;&gt;&gt;&gt;&gt; finish destroying things). This message is new to 1.2 code base.
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; Matthew
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; On Nov 7, 2012, at 6:56 PM, David Lowell wrote:
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; After further thought, I want to add more color to this issue. I 
&gt;&gt;&gt;&gt;&gt;&gt; hypothesize the symptoms I described here were continued fallout of an 
&gt;&gt;&gt;&gt;&gt;&gt; earlier crash. So I've waded further back into to the logs to try to 
&gt;&gt;&gt;&gt;&gt;&gt; shed light on how the Riak process was doing prior to this time. It was 
&gt;&gt;&gt;&gt;&gt;&gt; unhappy. It appears to have failed and stopped several times in the 1/2 
&gt;&gt;&gt;&gt;&gt;&gt; day prior. The pattern revealed in the logs looks something like this:
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; - Following a clean startup, the service runs for a while
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; - Usually the first log entry of any kind near the beginning of a long 
&gt;&gt;&gt;&gt;&gt;&gt; block of error logs is "alarm\\_handler: 
&gt;&gt;&gt;&gt;&gt;&gt; {set,{system\\_memory\\_high\\_watermark,[]}}" 
&gt;&gt;&gt;&gt;&gt;&gt; ( Is this indicating excessive memory use? )
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; - Within a few minutes we see several log messages warning about 
&gt;&gt;&gt;&gt;&gt;&gt; "long\\_gc", which I assume is an indication that garbage collection took 
&gt;&gt;&gt;&gt;&gt;&gt; longer than some threshold
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; - Within the next minute or two, we start to see the legions of errors, 
&gt;&gt;&gt;&gt;&gt;&gt; "riak\\_kv\\_vnode worker pool crashed", and "gen\\_fsm" having some sort of 
&gt;&gt;&gt;&gt;&gt;&gt; timeout when trying to communicate with the eleveldb backend
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; - Eventually we see a log record indicating "Failed to start 
&gt;&gt;&gt;&gt;&gt;&gt; riak\\_kv\\_eleveldb" because of a leveldb LOCK file being temporarily 
&gt;&gt;&gt;&gt;&gt;&gt; unavailable
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; - Then Riak starts to exit: riak:stop:46 "backend module failed to 
&gt;&gt;&gt;&gt;&gt;&gt; start." 
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; So, not really knowing the Riak internals, it's a little difficult to 
&gt;&gt;&gt;&gt;&gt;&gt; piece together the story here. Could be we're running low on memory. 
&gt;&gt;&gt;&gt;&gt;&gt; Hard to know why riak\\_kv\\_workers are failing, or why this leveldb LOCK 
&gt;&gt;&gt;&gt;&gt;&gt; file is unavailable. To those more learned, do these log records tell a 
&gt;&gt;&gt;&gt;&gt;&gt; story?
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; For the record, we're using the default 8 MB of leveldb cache per vnode, 
&gt;&gt;&gt;&gt;&gt;&gt; so that ought to cap cache for our 512 vnodes at 4 GB. Our host has 32 
&gt;&gt;&gt;&gt;&gt;&gt; GB of physical memory. Are there other pieces of Riak that can use a lot 
&gt;&gt;&gt;&gt;&gt;&gt; of memory that we need to look out for?
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; I'll include a few of the actual log records for reference, below.
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; Dave
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; --
&gt;&gt;&gt;&gt;&gt;&gt; Dave Lowell
&gt;&gt;&gt;&gt;&gt;&gt; d...@connectv.com
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; Representative logs, many similar ones deleted for brevity:
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; 2012-11-07 01:41:02.395 [info] &lt;0.51.0&gt; alarm\\_handler: 
&gt;&gt;&gt;&gt;&gt;&gt; {set,{system\\_memory\\_high\\_watermark,[]}}
&gt;&gt;&gt;&gt;&gt;&gt; 2012-11-07 01:55:50.517 [info] 
&gt;&gt;&gt;&gt;&gt;&gt; &lt;0.73.0&gt;@riak\\_core\\_sysmon\\_handler:handle\\_event:85 monitor long\\_gc 
&gt;&gt;&gt;&gt;&gt;&gt; &lt;0.18585.32&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; [{initial\\_call,{riak\\_core\\_coverage\\_fsm,init,1}},{almost\\_current\\_function,{riak\\_core\\_coverage\\_plan,'-next\\_vnode/2-fun-0-',2}},{message\\_queue\\_len,0}]
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; [{timeout,219},{old\\_heap\\_block\\_size,0},{heap\\_block\\_size,28657},{mbuf\\_size,0},{stack\\_size,48},{old\\_heap\\_size,0},{heap\\_size,11430}]
&gt;&gt;&gt;&gt;&gt;&gt; 2012-11-07 01:56:20.303 [error] 
&gt;&gt;&gt;&gt;&gt;&gt; &lt;0.9231.0&gt;@riak\\_core\\_vnode:handle\\_info:510 
&gt;&gt;&gt;&gt;&gt;&gt; 1258832464966656615093408225054454710289582522368 riak\\_kv\\_vnode worker 
&gt;&gt;&gt;&gt;&gt;&gt; pool crashed 
&gt;&gt;&gt;&gt;&gt;&gt; {timeout,{gen\\_fsm,sync\\_send\\_event,[&lt;0.9234.0&gt;,{checkout,false,5000},5000]}}
&gt;&gt;&gt;&gt;&gt;&gt; 2012-11-07 01:56:22.382 [error] &lt;0.10002.0&gt; gen\\_fsm &lt;0.10002.0&gt; in state 
&gt;&gt;&gt;&gt;&gt;&gt; ready terminated with reason: 
&gt;&gt;&gt;&gt;&gt;&gt; {timeout,{gen\\_server,call,[&lt;0.10005.0&gt;,{work,&lt;0.10003.0&gt;,{fold,#Fun,#Fun},{fsm,{40916762,{1398702738851840683437120250060505233655091691520,'riak@10.0.3.11'}},&lt;0.18429.32&gt;}}]}}
&gt;&gt;&gt;&gt;&gt;&gt; 2012-11-07 01:57:11.833 [error] &lt;0.19755.32&gt;@riak\\_kv\\_vnode:init:265 
&gt;&gt;&gt;&gt;&gt;&gt; Failed to start riak\\_kv\\_eleveldb\\_backend Reason: {db\\_open,"IO error: 
&gt;&gt;&gt;&gt;&gt;&gt; lock 
&gt;&gt;&gt;&gt;&gt;&gt; /var/data/ctv/riak/leveldb/1258832464966656615093408225054454710289582522368/LOCK:
&gt;&gt;&gt;&gt;&gt;&gt; Resource temporarily unavailable"}
&gt;&gt;&gt;&gt;&gt;&gt; 2012-11-07 01:57:27.425 [info] 
&gt;&gt;&gt;&gt;&gt;&gt; &lt;0.73.0&gt;@riak\\_core\\_sysmon\\_handler:handle\\_event:85 monitor long\\_gc 
&gt;&gt;&gt;&gt;&gt;&gt; &lt;0.19181.32&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; [{initial\\_call,{riak\\_core\\_coverage\\_fsm,init,1}},{almost\\_current\\_function,{gen\\_fsm,loop,7}},{message\\_queue\\_len,0}]
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; [{timeout,109},{old\\_heap\\_block\\_size,0},{heap\\_block\\_size,28657},{mbuf\\_size,0},{stack\\_size,47},{old\\_heap\\_size,0},{heap\\_size,9171}]
&gt;&gt;&gt;&gt;&gt;&gt; 2012-11-07 01:57:51.109 [error] &lt;0.10002.0&gt; CRASH REPORT Process 
&gt;&gt;&gt;&gt;&gt;&gt; &lt;0.10002.0&gt; with 0 neighbours exited with reason: 
&gt;&gt;&gt;&gt;&gt;&gt; {timeout,{gen\\_server,call,[&lt;0.10005.0&gt;,{work,&lt;0.10003.0&gt;,{fold,#Fun,#Fun},{fsm,{40916762,{1398702738851840683437120250060505233655091691520,'riak@10.0.3.11'}},&lt;0.18429.32&gt;}}]}}
&gt;&gt;&gt;&gt;&gt;&gt; in gen\\_fsm:terminate/7 line 
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; On Nov 7, 2012, at 11:34 AM, David Lowell wrote:
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Hello Riak Folks,
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; The last three days, we've been having a string of problems with Riak. 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; An otherwise healthy server running our full application stack will 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; suddenly start throwing a bunch of errors in the logs. Although the 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Riak processes stay up, most or all requests to Riak fail during these 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; periods.
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; The errors in the logs are predominantly describing "riak\\_kv\\_vnode 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; worker pool crashed" and timeout conditions. This morning, we had this 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; crashy behavior start immediately after a clean Riak startup, and 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; making a single call to our API, so the logs are quite free of other 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; noise. I've summarized those logs below for curious parties, and can 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; attach the full set of logs if needed.
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; I forgot to check this morning, but during a similar outage on Monday, 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; the Riak server was refusing connections to new clients.
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Interestingly, after giving Riak a while with no traffic at all today 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; (like 15-30 minutes), it appears to have recovered without a restart. 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; We've had similar recoveries during other "outages" of this type since 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Sunday evening. Facilitating this sort of recovery does seem to require 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; shutting down all application KV requests for a while.
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; We're suspicious of some kind of corruption in the eleveldb on-disk 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; files, because in past outages of this type, we've observed that the 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; condition persists over reboots. But we don't have much more evidence 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; than that. Is there a command we can run that will check over eleveldb 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; files for corruption or inconsistency?
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Other than that, what can cause "worker pool crashed" events? What do 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; you know about the "timeouts" that are in these logs?
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; For the record, we're running Riak 1.2.0 on Ubuntu 10.04, eleveldb 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; backend with 512 partitions. We're running predominantly in a 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; single-node configuration on a bunch of isolated dev boxes at the 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; moment, on our way to spreading out our 512 vnodes onto 5 hosts in 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; production.
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Thanks for your help,
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Dave
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; --
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Dave Lowell
&gt;&gt;&gt;&gt;&gt;&gt;&gt; d...@connectv.com
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 2012-11-07 18:11:03.398 [info] &lt;0.7.0&gt; Application lager started on 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; node 'riak@10.0.3.11'
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; ... normal startup messages ...
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 2012-11-07 18:11:50.109 [info] 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; &lt;0.10582.0&gt;@riak\\_core:wait\\_for\\_application:419 Wait complete for 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; application riak\\_search (0 seconds)
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 2012-11-07 18:22:18.509 [error] 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; &lt;0.2897.0&gt;@riak\\_core\\_vnode:handle\\_info:510 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 105616329260241031198313161739262640092323250176 riak\\_kv\\_vnode worker 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; pool crashed 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; {timeout,{gen\\_server,call,[&lt;0.2902.0&gt;,{work,&lt;0.2900.0&gt;,{fold,#Fun,#Fun},{fsm,{96247562,{105616329260241031198313161739262640092323250176,'riak@10.0.3.11'}},&lt;0.15324.0&gt;}}]}}
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 2012-11-07 18:22:18.509 [error] &lt;0.2899.0&gt; gen\\_fsm &lt;0.2899.0&gt; in state 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; ready terminated with reason: 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; {timeout,{gen\\_server,call,[&lt;0.2902.0&gt;,{work,&lt;0.2900.0&gt;,{fold,#Fun,#Fun},{fsm,{96247562,{105616329260241031198313161739262640092323250176,'riak@10.0.3.11'}},&lt;0.15324.0&gt;}}]}}
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; ... 13 more "riak\\_kv\\_vnode worker pool crashed" messages...
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 2012-11-07 18:22:21.245 [error] &lt;0.2899.0&gt; CRASH REPORT Process 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; &lt;0.2899.0&gt; with 0 neighbours exited with reason: 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; {timeout,{gen\\_server,call,[&lt;0.2902.0&gt;,{work,&lt;0.2900.0&gt;,{fold,#Fun,#Fun},{fsm,{96247562,{105616329260241031198313161739262640092323250176,'riak@10.0.3.11'}},&lt;0.15324.0&gt;}}]}}
&gt;&gt;&gt;&gt;&gt;&gt;&gt; in gen\\_fsm:terminate/7 line 611
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 2012-11-07 18:22:21.844 [error] &lt;0.2944.0&gt; gen\\_fsm &lt;0.2944.0&gt; in state 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; ready terminated with reason: 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; {timeout,{gen\\_server,call,[&lt;0.2947.0&gt;,{work,&lt;0.2945.0&gt;,{fold,#Fun,#Fun},{fsm,{96247562,{114179815416476790484662877555959610910619729920,'riak@10.0.3.11'}},&lt;0.15324.0&gt;}}]}}
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; ... 13 more "CRASH REPORT Process  with 0 neighbours exited with 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; reason" and "gen\\_fsm &lt;0.2989.0&gt; in state ready terminated with reason" 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; message pairs
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 2012-11-07 18:23:21.427 [error] &lt;0.15322.0&gt; gen\\_server &lt;0.15322.0&gt; 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; terminated with reason: 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; {error,{case\\_clause,{error,timeout,[[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[&lt;&lt;"1352256943.4983411"&gt;&gt;],[],[],[],...]}},...}
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 2012-11-07 18:23:21.495 [error] &lt;0.15322.0&gt; CRASH REPORT Process 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; &lt;0.15322.0&gt; with 0 neighbours exited with reason: 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; {error,{case\\_clause,{error,timeout,[[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[&lt;&lt;"1352256943.4983411"&gt;&gt;],[],[],[],...]}},...}
&gt;&gt;&gt;&gt;&gt;&gt;&gt; in gen\\_server:terminate/6 line 747
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 2012-11-07 18:23:21.525 [error] &lt;0.10590.0&gt; Supervisor riak\\_api\\_pb\\_sup 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; had child undefined started with 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; {riak\\_api\\_pb\\_server,start\\_link,undefined} at &lt;0.15322.0&gt; exit with 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; reason 
&gt;&gt;&gt;&gt;&gt;&gt;&gt; {error,{case\\_clause,{error,timeout,[[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[&lt;&lt;"1352256943.4983411"&gt;&gt;],[],[],[],...]}},...}
&gt;&gt;&gt;&gt;&gt;&gt;&gt; in context child\\_terminated
&gt;&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt;&gt;&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt;&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt;&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt; 
&gt; 

\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

