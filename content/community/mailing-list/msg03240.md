---
title: "Re: 'not found' after join"
description: ""
project: community
lastmod: 2011-05-05T14:04:31-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg03240"
mailinglist_parent_id: "msg03233"
author_name: "Ben Tilly"
project_section: "mailinglistitem"
sent_date: 2011-05-05T14:04:31-07:00
---


On Thu, May 5, 2011 at 1:06 PM, Andy Gross  wrote:
&gt;
&gt; Alex's description roughly matches up with some of our plans to address this
&gt; issue.
&gt; As with almost anything, this comes down to a tradeoff between consistency
&gt; and availability.   In the case of joining nodes, making the
&gt; join/handoff/ownership claim process more "atomic" requires a higher degree
&gt; of consensus from the machines in the cluster.  The current process (which
&gt; is clearly non-optimal) allows nodes to join the ring as long as they can
&gt; contact one current ring member.  A more atomic process would introduce
&gt; consensus issues that might prevent nodes from joining in partitioned
&gt; scenarios.

Why would the whole ring need to know about the join? Suppose that
there exists the option when you ask for a piece of data to reply, "I
don't have it, this data moved to X and the boundary between us is at
Y." Now all that needs to know about a handoff is the two nodes
involved, and everyone else can find out about it lazily.

Add to that the ability for any node to add another node to its right,
and any node to tell a node that it is on their left. You only need
to reach one node to be able to join the ring.

&gt; A good solution would probably involve some consistency knobs around the
&gt; join process to deal with a spectrum of failure/partition scenarios.
&gt; This is something of which we are acutely aware and are actively pursuing
&gt; solutions for a near-term release.
&gt; - Andy

How near term?

I'm in the early stages of a development effort that is aiming to
release something this year. We were hoping to use Riak, but this
could be a show stopper for us.

&gt; On Thu, May 5, 2011 at 12:22 PM, Alexander Sicular 
&gt; wrote:
&gt;&gt;
&gt;&gt; I'm really loving this thread. Generating great ideas for the way
&gt;&gt; things should be... in the future. It seems to me that "the ring
&gt;&gt; changes immediately" is actually the problem as Ryan astutely
&gt;&gt; mentions. One way the future could look is :
&gt;&gt;
&gt;&gt; - a new node comes online
&gt;&gt; - introductions are made
&gt;&gt; - candidate vnodes are selected for migration (&lt;- insert pixie dust magic
&gt;&gt; here)
&gt;&gt; - the number of simultaneous migrations are configurable, fewer for
&gt;&gt; limited interruption or more for quicker completion
&gt;&gt; - vnodes are migrated
&gt;&gt; - once migration is completed, ownership is claimed
&gt;&gt;
&gt;&gt; Selecting vnodes for migration is where the unicorn cavalry attack the
&gt;&gt; dragons den. If done right(er) the algorithm could be swappable to
&gt;&gt; optimize for different strategies. Don't ask me how to implement it,
&gt;&gt; I'm only a yellow belt in erlang-fu.
&gt;&gt;
&gt;&gt; Cheers,
&gt;&gt; Alexander
&gt;&gt;
&gt;&gt; On Thu, May 5, 2011 at 13:33, Ryan Zezeski  wrote:
&gt;&gt; &gt; John,
&gt;&gt; &gt; All great points.  The problem is that the ring changes immediately when
&gt;&gt; &gt; a
&gt;&gt; &gt; node is added.  So now, all the sudden, the preflist is potentially
&gt;&gt; &gt; pointing
&gt;&gt; &gt; to nodes that don't have the data and they won't have that data until
&gt;&gt; &gt; handoff occurs.  The faster that data gets transferred, the less time
&gt;&gt; &gt; your
&gt;&gt; &gt; clients have to hit 'notfound'.
&gt;&gt; &gt; However, I agree completely with what you're saying.  This is just a
&gt;&gt; &gt; side
&gt;&gt; &gt; effect of how the system currently works.  In a perfect world we
&gt;&gt; &gt; wouldn't
&gt;&gt; &gt; care how long handoff takes and we would also do some sort of automatic
&gt;&gt; &gt; congestion control akin to TCP Reno or something.  The preflist would
&gt;&gt; &gt; still
&gt;&gt; &gt; point to the "old" partitions until all data has been successfully
&gt;&gt; &gt; handed
&gt;&gt; &gt; off, and then and only then would we flip the switch for that vnode.
&gt;&gt; &gt;  I'm
&gt;&gt; &gt; pretty sure that's where we are heading (I say "pretty sure" b/c I just
&gt;&gt; &gt; joined the team and haven't been heavily involved in these specific
&gt;&gt; &gt; talks
&gt;&gt; &gt; yet).
&gt;&gt; &gt; It's all coming down the pipe...
&gt;&gt; &gt; As for your specific I/O question re handoff\\_concurrecy, you might be
&gt;&gt; &gt; right.
&gt;&gt; &gt;  I would think it depends on hardware/platform/etc.  I was offering it
&gt;&gt; &gt; as a
&gt;&gt; &gt; possible stopgap to minimize Greg's pain.  It's certainly a cure to a
&gt;&gt; &gt; symptom, not the problem itself.
&gt;&gt; &gt; -Ryan
&gt;&gt; &gt;
&gt;&gt; &gt; On Thu, May 5, 2011 at 1:10 PM, John D. Rowell  wrote:
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; Hi Ryan, Greg,
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; 2011/5/5 Ryan Zezeski 
&gt;&gt; &gt;&gt;&gt;
&gt;&gt; &gt;&gt;&gt; 1. For example, riak\\_core has a `handoff\\_concurrency` setting that
&gt;&gt; &gt;&gt;&gt; determines how many vnodes can concurrently handoff on a given node.
&gt;&gt; &gt;&gt;&gt;  By
&gt;&gt; &gt;&gt;&gt; default this is set to 4.  That's going to take a while with your 2048
&gt;&gt; &gt;&gt;&gt; vnodes and all :)
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; Won't that make the handoff situation potentially worse? From the
&gt;&gt; &gt;&gt; thread I
&gt;&gt; &gt;&gt; understood that the main problem was that the cluster was shuffling too
&gt;&gt; &gt;&gt; much
&gt;&gt; &gt;&gt; data around and thus becoming unresponsive and/or returning unexpected
&gt;&gt; &gt;&gt; results (like "not founds"). I'm attributing the concerns more to an
&gt;&gt; &gt;&gt; excessive I/O situation than to how long the handoff takes. If the
&gt;&gt; &gt;&gt; handoff
&gt;&gt; &gt;&gt; can be made transparent (no or little side effects) I don't think most
&gt;&gt; &gt;&gt; people will really care (e.g. the "fix the cluster tomorrow" anecdote).
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; How about using a percentage of available I/O to throttle the vnode
&gt;&gt; &gt;&gt; handoff concurrency? Start with 1, and monitor the node's I/O (kinda
&gt;&gt; &gt;&gt; like
&gt;&gt; &gt;&gt; 'atop' does, collection CPU, disk and network metrics), if it is below
&gt;&gt; &gt;&gt; the
&gt;&gt; &gt;&gt; expected usage, then increase the vnode handoff concurrency, and
&gt;&gt; &gt;&gt; vice-versa.
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; I for one would be perfectly happy if the handoff took several hours
&gt;&gt; &gt;&gt; (even
&gt;&gt; &gt;&gt; days) if we could maintain the core riak\\_kv characteristics intact
&gt;&gt; &gt;&gt; during
&gt;&gt; &gt;&gt; those events. We've all seen looooong RAID rebuild times, and it's
&gt;&gt; &gt;&gt; usually
&gt;&gt; &gt;&gt; better to just sit tight and keep the rebuild speed low (slower I/O)
&gt;&gt; &gt;&gt; while
&gt;&gt; &gt;&gt; keeping all of the dependent systems running smoothly.
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; cheers
&gt;&gt; &gt;&gt; -jd
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt; &gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt; &gt; riak-users mailing list
&gt;&gt; &gt; riak-users@lists.basho.com
&gt;&gt; &gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;
&gt;
