---
title: "Re: Map reduce error on large bucket"
description: ""
project: community
lastmod: 2010-09-13T11:12:11-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg01070"
mailinglist_parent_id: "msg01069"
author_name: "Alexander Sicular"
project_section: "mailinglistitem"
sent_date: 2010-09-13T11:12:11-07:00
---


Hmm... sounds to me like your'e getting close to a magic comp sci number. 2^16 
or 65536. If this isn't a timeout thing, which it could be [1], it could be a 
bug.

-Alexander

[1] https://issues.basho.com/show\\_bug.cgi?id=367

On Sep 13, 2010, at 2:05 PM, SKester wrote:

&gt; I am trying to run a map-reduce job against all keys in a bucket. The job is 
&gt; working fine on buckets with ~60,000 or less entries. However on buckets 
&gt; with &gt; 63,000 keys I get the following error every time:
&gt; 
&gt; Input:
&gt; 
&gt; curl -X POST -H "content-type: application/json" 
&gt; http://testdw0b01.be.weather.com:8098/mapred?chunked=true --data @-
&gt; {"inputs":"profile\\_63000","query":[{"map":{"language":"javascript","source":"function(v)
&gt; {var data = Riak.mapValuesJson(v)[0]; var r=[];for(var i in data.locations){ 
&gt; var 
&gt; o = {}; o[data.locations[i]] = 1; r.push(o); } return r; 
&gt; }"}},{"reduce":{"language":"javascript","source":"function(v) { var r = {}; 
&gt; for (var i in v) { for(var w in v[i])
&gt; { if (w in r) r[w] += v[i][w]; else r[w] = v[i][w]; } } return 
&gt; [r];}"}}],"timeout": 600000}
&gt; 
&gt; Output:
&gt; 
&gt; {"error":"map\\_reduce\\_error"}
&gt; 
&gt; Any ideas? I am running on a 4 box Centos cluster with Riak installed via 
&gt; 64bit RPMâ€™s, and default settings.
&gt; 
&gt; Thanks,
&gt; Scott
