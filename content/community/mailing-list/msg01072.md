---
title: "Re: Map reduce error on large bucket"
description: ""
project: community
lastmod: 2010-09-13T12:18:17-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg01072"
mailinglist_parent_id: "msg01069"
author_name: "Kevin Smith"
project_section: "mailinglistitem"
sent_date: 2010-09-13T12:18:17-07:00
---


One problem with using the default number of partitions, which is 64, is that 
many bucket/key pairs wind up hashing to the same few partitions. This has the 
effect of concentrating the majority of the map function invocations onto a few 
processes which winds up causing read function outs. These time outs are 
currently interpreted as "not found" results by the MapReduce machinery.

When a job encounters a "not found" it retries on each partition holding a 
replica until it runs out of partitions. Your reduce function might be getting 
called with one of these "not found" values which will cause your function to 
fail. The Javascript environment also includes a global function, ejsLog, which 
you can use to perform printf-style debugging. ejsLog takes two arguments: the 
full path to a log file and the text to log. It then writes the text to the log 
file along with a timestamp.

There are two workarounds you can use to improve MapReduce performance and 
reduce the chance of "not found" results caused by timeouts:

1) Bump up the vnode\\_mr\\_timeout value in app.config to a larger value. This is 
the amount of time, specified in milliseconds, a MapReduce job will wait for a 
map function to complete. Turning this up to 3000 - 5000 has helped other users 
in the past.

2) Configure more partitions than the default value in your cluster. The number 
of partitions must be a power of 2. This is not an easy thing to change after 
you've loaded data into a running cluster so if you'd need to be able to dump 
and reload your data for this to be an option. 

Also, as Sean pointed out, we have several nice fixes for listing keys and 
improvements to MapReduce error reporting in tip and will be available in the 
next release if you can't run unreleased code.

--Kevin
On Sep 13, 2010, at 2:05 PM, SKester wrote:

&gt; I am trying to run a map-reduce job against all keys in a bucket. The job is 
&gt; working fine on buckets with ~60,000 or less entries. However on buckets 
&gt; with &gt; 63,000 keys I get the following error every time:
&gt; 
&gt; Input:
&gt; 
&gt; curl -X POST -H "content-type: application/json" 
&gt; http://testdw0b01.be.weather.com:8098/mapred?chunked=true --data @-
&gt; {"inputs":"profile\\_63000","query":[{"map":{"language":"javascript","source":"function(v)
&gt; {var data = Riak.mapValuesJson(v)[0]; var r=[];for(var i in data.locations){ 
&gt; var 
&gt; o = {}; o[data.locations[i]] = 1; r.push(o); } return r; 
&gt; }"}},{"reduce":{"language":"javascript","source":"function(v) { var r = {}; 
&gt; for (var i in v) { for(var w in v[i])
&gt; { if (w in r) r[w] += v[i][w]; else r[w] = v[i][w]; } } return 
&gt; [r];}"}}],"timeout": 600000}
&gt; 
&gt; Output:
&gt; 
&gt; {"error":"map\\_reduce\\_error"}
&gt; 
&gt; Any ideas? I am running on a 4 box Centos cluster with Riak installed via 
&gt; 64bit RPMâ€™s, and default settings.
&gt; 
&gt; Thanks,
&gt; Scott
