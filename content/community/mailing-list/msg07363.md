---
title: "Re: Geospatial advice?"
description: ""
project: community
lastmod: 2012-05-01T11:25:14-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg07363"
mailinglist_parent_id: "msg07362"
author_name: "Alexander Sicular"
project_section: "mailinglistitem"
sent_date: 2012-05-01T11:25:14-07:00
---


Hey, I'm as up for a good and clever hack as anybody. But the question is
just because you can, should you? Who will maintain your hack after your'e
dead? I'm still maintaing crap I wrote years ago. Even though I'm paid,
sometimes I would rather not have the headache. Why would you use a product
that specifically does not support such hackery? Scaling postgres or mongo
are known and solvable problems especially concerning bounded data sets,
likes, say, all points on a globe. Now if you were storing checkins, that
would be a different problem. One suitable for, say, Riak.

On Tue, May 1, 2012 at 14:09, Mark Rose  wrote:

&gt; Well, I'd be indexing items over the entire globe. I'd be be looking at
&gt; resolutions from an entire world view down to city block. I'm thinking of
&gt; using geohashes as an index to restrict the result set, then further
&gt; filtering and sorting by mapreducing the remaining items. So I only need
&gt; enough granularity to reduce the number of items to a reasonable amount. At
&gt; the world view level, I'd filter out most results using mapreduce, but the
&gt; local-level queries would be far more common so an index would be highly
&gt; advantageous. The geometry I'd want to query would be a window that
&gt; arbitrarily overlaps one or more geohash regions. Basically, think plotting
&gt; items in say, Google Maps.
&gt;
&gt; Can you use a secondary index inside mapreduce? I haven't seen any
&gt; examples of it. I have only seen a secondary index being used to feed a
&gt; mapreduce. I am new to Riak.
&gt;
&gt; I imagine my number of points would be at most 100 items per square km,
&gt; but typically less than 1 per square km. A 1 km resolution would be
&gt; sufficient. A 32 bit geohash would cover that fine. Vast regions of the
&gt; Earth would contain no points at all.
&gt;
&gt; -Mark
&gt;
&gt;
&gt; On Tue, May 1, 2012 at 1:16 PM, Sean Cribbs  wrote:
&gt;
&gt;&gt; In contrast to Alexander's assessment, I'd say "it depends". I have built
&gt;&gt; some geospatial indexes on top of Riak using a geohashing scheme based on
&gt;&gt; the Hilbert space-filling curve. However, I had to choose specific levels
&gt;&gt; of "zoom" and precompute them. Now that we have secondary indexes, you
&gt;&gt; could perhaps bypass the precomputation step. In general, if you know the
&gt;&gt; geometry of the space you want to query, you can fairly trivially compute
&gt;&gt; the names of the geohashes you need to look up and then either fetch
&gt;&gt; individual keys for those (if you precompute them), or use MapReduce to
&gt;&gt; fetch a range of them. It's not automatic, for sure, but the greatest
&gt;&gt; complexity will be in deciding which granularities of index to support.
&gt;&gt;
&gt;&gt; On Tue, May 1, 2012 at 12:44 PM, Alexander Sicular wrote:
&gt;&gt;
&gt;&gt;&gt; My advice is to not use Riak. Check mongo or Postgres.
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; @siculars on twitter
&gt;&gt;&gt; http://siculars.posterous.com
&gt;&gt;&gt;
&gt;&gt;&gt; Sent from my iRotaryPhone
&gt;&gt;&gt;
&gt;&gt;&gt; On May 1, 2012, at 9:18, Mark Rose  wrote:
&gt;&gt;&gt;
&gt;&gt;&gt; &gt; Hello everyone!
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; I'm going to be implementing Riak as a storage engine for geographic
&gt;&gt;&gt; data. Research has lead me to using geohashing as a useful way to filter
&gt;&gt;&gt; out results outside of a region of interest. However, I've run into some
&gt;&gt;&gt; stumbling blocks and I'm looking for advice on the best way to proceed.
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; Querying efficiently by geohash involves querying several regions
&gt;&gt;&gt; around a point. From what I can tell, Riak offers no way to query a
&gt;&gt;&gt; secondary index with multiple ranges. Having to query a several ranges,
&gt;&gt;&gt; merge them in the application layer, then pass them off to mapreduce seems
&gt;&gt;&gt; rather silly (and could mean passing GBs of data). Alternatively, I could
&gt;&gt;&gt; start straight with mapreduce, but key filtering seems to work only with
&gt;&gt;&gt; the primary key, which would force me into using the geohashed location as
&gt;&gt;&gt; the primary key (which would lead to collisions if two things existed at
&gt;&gt;&gt; the same point). I'd also like to avoid using the primary key as the
&gt;&gt;&gt; geohash as if the item moves I'd have to change all the references to it.
&gt;&gt;&gt; Lastly, I could do a less efficient mapreduce over a less precise geohash,
&gt;&gt;&gt; but this doesn't solve the issue of the equator (anything near the equator
&gt;&gt;&gt; would require mapreducing the entire dataset).
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; Is there any way to query multiple ranges with a secondary index and
&gt;&gt;&gt; pass that off to mapreduce? Or should I just stick with the less efficient
&gt;&gt;&gt; mapreduce, and when near the equator, run two queries and later merge them?
&gt;&gt;&gt; Or am I going about this the wrong way?
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; In any case, the final stage of my queries will involve mapreduce as
&gt;&gt;&gt; I'll need to further filter the items found in a region.
&gt;&gt;&gt; &gt;
&gt;&gt;&gt; &gt; Thank you,
&gt;&gt;&gt; &gt; Mark
&gt;&gt;&gt; &gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt;&gt; &gt; riak-users mailing list
&gt;&gt;&gt; &gt; riak-users@lists.basho.com
&gt;&gt;&gt; &gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt;&gt;

&gt;&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt; --
&gt;&gt; Sean Cribbs 
&gt;&gt; Software Engineer
&gt;&gt; Basho Technologies, Inc.
&gt;&gt; http://basho.com/
&gt;&gt;
&gt;&gt;
&gt;
