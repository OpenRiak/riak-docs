---
title: "Re: Key Filter Timeout"
description: ""
project: community
lastmod: 2011-10-24T07:38:18-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg05268"
author_name: "Jim Adler"
project_section: "mailinglistitem"
sent_date: 2011-10-24T07:38:18-07:00
---


Yes, using 1.0.1 with LevelDB. I moved to it from Bitcask in the hopes of 
better performance.

Good to hear about your 60M key use-case. Can you share any key access 
performance numbers?

Jim

On Oct 24, 2011, at 7:23 AM, Mark Steele  wrote:

&gt; Just curious Kyle, you using the 1.0 series?
&gt; 
&gt; I've done some informal testing on a 3 node 1.0 cluster and key listing was 
&gt; working just peachy on 60 million keys using bitcask as the backend.
&gt; 
&gt; Cheers,
&gt; 
&gt; Mark
&gt; 
&gt; On Sunday 23 October 2011 12:26:35 Aphyr wrote:
&gt;&gt; On 10/23/2011 12:11 PM, Jim Adler wrote:
&gt;&gt;&gt; I will be loosening the key filter criterion after I get the basics
&gt;&gt;&gt; working, which I thought would be a simple equality check. 8M keys
&gt;&gt;&gt; isn't really a large data set, is it? I thought that keys were stored
&gt;&gt;&gt; in memory and key filters just operated on those memory keys and not
&gt;&gt;&gt; data.
&gt;&gt;&gt; 
&gt;&gt;&gt; Jim
&gt;&gt; 
&gt;&gt; That's about where we started seeing timeouts in list-keys. Around 25
&gt;&gt; million keys, list-keys started to take down the cluster. (6 nodes, 1024
&gt;&gt; partitions). You may not encounter these problems, but were I in your
&gt;&gt; position and planning to grow... I would prepare to stop using key
&gt;&gt; filters, bucket listing, and key listing early.
&gt;&gt; 
&gt;&gt; Our current strategy is to store the keys in Redis, and synchronize them
&gt;&gt; with post-commit hooks and a process that reads over bitcask. With
&gt;&gt; ionice 3, it's fairly low-impact. https://github.com/aphyr/bitcask-ruby
&gt;&gt; may be useful.
&gt;&gt; 
&gt;&gt; --Kyle
&gt;&gt; 
&gt;&gt; # Simplified code, extracted from our bitcask scanner:
&gt;&gt; def run
&gt;&gt; `renice 10 #{Process.pid}`
&gt;&gt; `ionice -c 3 -p #{Process.pid}`
&gt;&gt; 
&gt;&gt; begin
&gt;&gt; bitcasks\\_dir = '/var/lib/riak/bitcask'
&gt;&gt; dirs = Dir.entries(bitcasks\\_dir).select do |dir|
&gt;&gt; dir =~ /^\\d+$/
&gt;&gt; end.map do |dir|
&gt;&gt; File.join(bitcasks\\_dir, dir)
&gt;&gt; end
&gt;&gt; 
&gt;&gt; dirs.each do |dir|
&gt;&gt; scan dir
&gt;&gt; GC.start
&gt;&gt; end
&gt;&gt; log.info "Completed run"
&gt;&gt; rescue =&gt; e
&gt;&gt; log.error "#{e}\\n#{e.backtrace.join "\\n"}"
&gt;&gt; sleep 10
&gt;&gt; end
&gt;&gt; end
&gt;&gt; end
&gt;&gt; 
&gt;&gt; def scan(dir)
&gt;&gt; log.info "Loading #{dir}"
&gt;&gt; b = Bitcask.new dir
&gt;&gt; b.load
&gt;&gt; 
&gt;&gt; log.info "Updating #{dir}"
&gt;&gt; b.keydir.each do |key, e|
&gt;&gt; bucket, key = BERT.decode(key).map { |x|
&gt;&gt; Rack::Utils.unescape x
&gt;&gt; }
&gt;&gt; # Handle determines what to do with this particular bucket/key
&gt;&gt; # combo; e.g. insert into redis.
&gt;&gt; handle bucket, key, e
&gt;&gt; end
&gt;&gt; end
&gt;&gt; 
&gt; 

