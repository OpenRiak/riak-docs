---
title: "Re: Riak Recap for Dec. 13 - 14"
description: ""
project: community
lastmod: 2010-12-16T18:42:14-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg01810"
mailinglist_parent_id: "msg01809"
author_name: "Dan Reverri"
project_section: "mailinglistitem"
sent_date: 2010-12-16T18:42:14-08:00
---


Hi Daniel,

Clients do not specify the partition when making a request. A client can
request any key from any node in the cluster and Riak will return the
associated value.

Thanks,
Dan

Daniel Reverri
Developer Advocate
Basho Technologies, Inc.
d...@basho.com


On Thu, Dec 16, 2010 at 6:01 PM, Daniel Woo  wrote:

&gt; Hi Mark,
&gt;
&gt; Thanks for your explanation, so in this case the partitions would be
&gt; re-distributed
&gt;
&gt; \\*from\\*
&gt;
&gt; Node1: p1 ~ p16
&gt; Node2: p17 ~ p32
&gt; Node3: p33 ~ p48
&gt; Node4: p49 ~ p64
&gt;
&gt; \\*to \\*
&gt; Node1: p1 ~ p13 (remove 3 partitions)
&gt; Node2: p17 ~ p29 (remove 3 partitions)
&gt; Node3: p33 ~ p45 (remove 3 partitions)
&gt; Node4: p49 ~ p61 (remove 3 partitions)
&gt; Node5: p14, 15, 16, 30, 31, 32, 46, 47, 48, 62, 63, 64 (aprox 1/5
&gt; partitions will be transferred to this new node)
&gt;
&gt; Since there is no centralized node in Riak, how do we know the partition 32
&gt; is moved to node 5 from the client caller? Cassandra seems break half of the
&gt; adjacent node's data into the new node, that will be easy for the client to
&gt; search for the datum just around the node-circle clockwise, although it
&gt; causes unbalanced data distribution and you have to move them by command
&gt; lines. Riak seems to have this solved by moving partitions into new nodes
&gt; equally, that's very interesting, how do you guys make it? If the client
&gt; caller queries for partition 32 which was originally on node 2, how do the
&gt; client know it's on a new node now?
&gt;
&gt; Thanks,
&gt; Daniel
&gt;
&gt;
&gt; On Fri, Dec 17, 2010 at 6:56 AM, Mark Phillips  wrote:
&gt;
&gt;&gt; Hey Daniel,
&gt;&gt;
&gt;&gt; [snip]
&gt;&gt;
&gt;&gt; &gt; So, I guess Riak would have to re-hash the whole partitions into all the
&gt;&gt; 5
&gt;&gt; &gt; nodes, right? Is this done lazily when the node finds the requested data
&gt;&gt; is
&gt;&gt; &gt; missing?
&gt;&gt; &gt; Or is there a way to handle this with consistent re-hashing so we can
&gt;&gt; avoid
&gt;&gt; &gt; moving data around when new nodes added?
&gt;&gt; &gt;
&gt;&gt;
&gt;&gt; Riak won't rehash all the partitions in the ring when new nodes are
&gt;&gt; added. When you go from 4 -&gt; 5 nodes, for example, approx. 1/5 of the
&gt;&gt; existing partitions are transferred to the new node. The other 4/5s of
&gt;&gt; the partitions will remain unchanged. As far as moving data around
&gt;&gt; when new nodes are added, this is impossible to avoid. Data needs to
&gt;&gt; be handed off to be spread around the ring.
&gt;&gt;
&gt;&gt; Hope that helps.
&gt;&gt;
&gt;&gt; Mark
&gt;&gt;
&gt;
&gt;
&gt;
&gt; --
&gt; Thanks & Regards,
&gt; Daniel
&gt;

