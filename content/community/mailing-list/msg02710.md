---
title: "Re: Multiple disks"
description: ""
project: community
lastmod: 2011-03-22T10:49:38-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg02710"
mailinglist_parent_id: "msg02707"
author_name: "Jeremiah Peschka"
project_section: "mailinglistitem"
sent_date: 2011-03-22T10:49:38-07:00
---


As an aside about RAID 10:

If you have a decent RAID controller, you may get reads that are 2x as fast 
because the controller will read from both stripes at the same time. It's not 
just a redundancy issue: it's also fast. Like a cheetah. On meth.

RAID 10 will really improve performance if you're doing a lot a map reduce 
queries that iterate across an entire bucket. Not sure if it makes any 
difference, but RAID 10 is the best practice for drive configuration in the 
data warehouse world for that very reason: scans become stupidly fast.
-- 
Jeremiah Peschka
Microsoft SQL Server MVP
MCITP: Database Developer, DBA
On Tuesday, March 22, 2011 at 10:10 AM, Greg Nelson wrote: 
&gt; Thanks for all the responses!
&gt; 
&gt; Regarding RAID 10, I think for now that's out because we can't afford to cut 
&gt; storage capacity in half when we're already using 2-3x for Riak level 
&gt; redundancy. And adding more redundancy at the RAID layer just seems... 
&gt; incorrect.
&gt; 
&gt; Running multiple instances on each box is probably not an option either since 
&gt; -- as Luke pointed out -- we risk storing N replicas of an object on one 
&gt; machine. And given that we're considering something like 12 or even 24 disks 
&gt; per machine, we can't realistically make N larger than that.
&gt; 
&gt; RAID 5 seems like it may be a decent option, as we'd only lose out on the 
&gt; capacity of one drive. e.g., if we had 12 1TB disks per machine we'd get 11TB 
&gt; of capacity per machine. Of course, we need to consider the failure rates 
&gt; with that many disks in RAID 5 and whether that sufficiently addresses the 
&gt; problem of exposing each machine to a higher surface of failure. But again, 
&gt; we can lose a node -- that's what Riak is for! We also need to investigate 
&gt; the write performance of such a setup; that said, even though we have a 
&gt; write-heavy application, we'll probably not be that write-heavy.
&gt; 
&gt; Joseph: your patch looks interesting, as that's exactly what I first thought 
&gt; of. I figured the directories under the main storage dir corresponded to 
&gt; partitions. I'll definitely give your code a looksy. One thing that 
&gt; immediately jumped out at me is: what happens when a disk fails? Does the 
&gt; handoff mechanism really work seamlessly there? Or is the assumption that a 
&gt; failure can only happen at the node layer (rather than subsets of vnodes 
&gt; within a node) baked into the fundamental design of Riak?
&gt; 
&gt; Greg
&gt; On Tuesday, March 22, 2011 at 8:53 AM, Alexander Sicular wrote:
&gt; &gt; Ya, my original message just highlighted the standard 0,1,5 that most 
&gt; &gt; people/hardware should know/be able to support. There are better options 
&gt; &gt; and 10 would be one of them. 
&gt; &gt; 
&gt; &gt; 
&gt; &gt; @siculars on twitter
&gt; &gt; http://siculars.posterous.com
&gt; &gt; 
&gt; &gt; Sent from my iPhone
&gt; &gt; 
&gt; &gt; On Mar 22, 2011, at 8:43, Ryan Zezeski  wrote:
&gt; &gt; 
&gt; &gt; &gt; 
&gt; &gt; &gt; 
&gt; &gt; &gt; On Tue, Mar 22, 2011 at 10:01 AM, Alexander Sicular  
&gt; &gt; &gt; wrote:
&gt; &gt; &gt; &gt; Save your ops dudes the headache and just use raid 5 and be done with 
&gt; &gt; &gt; &gt; it.
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; 
&gt; &gt; &gt; Depending on the number of disks available I might even argue running 
&gt; &gt; &gt; software RAID 10 for better throughput and less chance of data loss (as 
&gt; &gt; &gt; long as you can afford to cut your avail storage in half on every 
&gt; &gt; &gt; machine). It's not too hard to setup on modern Linux distros (mdadm); at 
&gt; &gt; &gt; least I was doing it 5 years ago and I'm no sys admin. 
&gt; &gt; &gt; 
&gt; &gt; &gt; -Ryan 
&gt; &gt; 
 
