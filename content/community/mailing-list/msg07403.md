---
title: "Re: Riak cluster unresponsive after single node failure"
description: ""
project: community
lastmod: 2012-05-08T15:58:39-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg07403"
mailinglist_parent_id: "msg07402"
author_name: "Armon Dadgar"
project_section: "mailinglistitem"
sent_date: 2012-05-08T15:58:39-07:00
---


Where by "competition", I meant "compaction". Derp.

Best Regards,

Armon Dadgar


On Tuesday, May 8, 2012 at 3:54 PM, Armon Dadgar wrote:

&gt; Hey Scott, 
&gt; 
&gt; My mistake, I was not sure if the claimant was responsible for convergence.
&gt; 
&gt; If this was a competition, it was not one that would ever finishâ€¦ The node 
&gt; went
&gt; down at about 1AM, and by 9AM when I started to resolve the issue it was in
&gt; the same state. I was unable to investigate the state of that machine, as it
&gt; was refusing any SSH connections.
&gt; 
&gt; Thanks for mentioning the key's. We've been thinking of doing just that
&gt; to get keys lexicographically near.
&gt; 
&gt; Best Regards,
&gt; 
&gt; Armon Dadgar
&gt; 
&gt; 
&gt; On Tuesday, May 8, 2012 at 3:26 PM, Scott Lystig Fritchie wrote:
&gt; 
&gt; &gt; &gt; &gt; &gt; "ar" == Armon Dadgar  &gt; &gt; &gt; &gt; (mailto:armon.dad...@gmail.com)&gt; wrote:
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; 
&gt; &gt; 
&gt; &gt; 
&gt; &gt; ar&gt; All the nodes appeared to have been blocked trying to talk to riak
&gt; &gt; ar&gt; 001 which was the ring claimant at the time. Doing this seems to
&gt; &gt; ar&gt; have cleared the state enough for the cluster to make progress
&gt; &gt; ar&gt; again.
&gt; &gt; 
&gt; &gt; Armon, it's quite unlikely that the ring claimant was doing anything
&gt; &gt; special because the claimant only acts when cluster membership changes.
&gt; &gt; 
&gt; &gt; Instead, it's quite likely that riak001 was busy doing a set of LevelDB 
&gt; &gt; compactions. There have been a number of changes recently to reduce the
&gt; &gt; amount of time that we've seen worst-case LevelDB compaction blocking Erlang
&gt; &gt; process schedulers which blocks \\*everything\\*, including the keep-alives
&gt; &gt; that are sent between Erlang nodes. The longest LevelDB-related
&gt; &gt; stoppage that I've seen was 7.5 minutes. :-( When that happens on a
&gt; &gt; node X, then all other nodes will complain (almost simultaneously) that 
&gt; &gt; node X is down. It's not \\*down\\*, it's just reallyreallyreally slow to
&gt; &gt; respond to messages ... which is effectively the same as being down.
&gt; &gt; 
&gt; &gt; Checking for big LevelDB compaction storms is pretty easy using
&gt; &gt; DTrace or SystemTap, but you're probably not using a kernel that
&gt; &gt; has user-space SystemTap available. There are compaction messages
&gt; &gt; in the "LOG" file of each LevelDB data directory. The hassle is the
&gt; &gt; need to look at all of them in parallel.
&gt; &gt; 
&gt; &gt; A secondary effect is watching write ops via "iostat -x 1": the
&gt; &gt; amount of data written spikes much higher than writes triggered only by
&gt; &gt; Riak client operations. (Read ops would go higher too, except that many
&gt; &gt; files input to a compaction are already cached by the OS.)
&gt; &gt; 
&gt; &gt; Your primary keys look UID'ish. If they are not lexigraphically adjacent
&gt; &gt; to other keys inserted at the same time, you will cause many more LevelDB
&gt; &gt; compaction events than if your keys were adjacent (e.g. prefixing them with
&gt; &gt; a wall-clock timestamp).
&gt; &gt; 
&gt; &gt; -Scott 
&gt; 

