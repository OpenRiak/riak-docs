---
title: "Re: 'not found' after join"
description: ""
project: community
lastmod: 2011-05-05T13:14:12-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg03238"
mailinglist_parent_id: "msg03233"
author_name: "Greg Nelson"
project_section: "mailinglistitem"
sent_date: 2011-05-05T13:14:12-07:00
---


The future I'd like to see is basically what I initially expected. That is, I 
can add a single node to an online cluster and clients should not even see any 
effects of this or need to know that it's even happening -- except of course 
the side effects like the added load on the cluster incurred by gossiping new 
ring state, handing off data, etc. But if no data has actually been lost, I 
don't believe data should ever be unavailable, temporarily or not. And I'd like 
to be able to, as someone else mentioned, add a node and throttle the handoffs 
and let it trickle over hours or even days.

Waving hands and saying that eventually the data will make it is true in 
principle, but in practice if you are following a read/modify/write pattern for 
some objects, you could easily lose data. e.g., my application writes JSON 
arrays to certain objects, and when it wishes to append something to the array, 
it will read/append/write back. If that initial read returns 404, then a new 
empty array is created. This is normal operation. But if that 404 is not a 
"normal" 404, it will happily create a new empty array, append, and write back 
a single-element array to that key. Of course there could have been a 100 
element array in Riak that was just unavailable at the time which is now 
effectively lost.

Anyhow, I do understand the importance of knowing what will happen when doing 
something operationally like adding a node, and I understand that one can't 
naively expect everything to just work like magic. But the current behavior is 
pretty poorly documented and surprising. I don't think it was even mentioned in 
the operations webinar! (Ok, I'll stop beating a dead horse. :))
On Thursday, May 5, 2011 at 12:22 PM, Alexander Sicular wrote: 
&gt; I'm really loving this thread. Generating great ideas for the way
&gt; things should be... in the future. It seems to me that "the ring
&gt; changes immediately" is actually the problem as Ryan astutely
&gt; mentions. One way the future could look is :
&gt; 
&gt; - a new node comes online
&gt; - introductions are made
&gt; - candidate vnodes are selected for migration (&lt;- insert pixie dust magic 
&gt; here)
&gt; - the number of simultaneous migrations are configurable, fewer for
&gt; limited interruption or more for quicker completion
&gt; - vnodes are migrated
&gt; - once migration is completed, ownership is claimed
&gt; 
&gt; Selecting vnodes for migration is where the unicorn cavalry attack the
&gt; dragons den. If done right(er) the algorithm could be swappable to
&gt; optimize for different strategies. Don't ask me how to implement it,
&gt; I'm only a yellow belt in erlang-fu.
&gt; 
&gt; Cheers,
&gt; Alexander
&gt; 
&gt; On Thu, May 5, 2011 at 13:33, Ryan Zezeski  wrote:
&gt; &gt; John,
&gt; &gt; All great points. The problem is that the ring changes immediately when a
&gt; &gt; node is added. So now, all the sudden, the preflist is potentially pointing
&gt; &gt; to nodes that don't have the data and they won't have that data until
&gt; &gt; handoff occurs. The faster that data gets transferred, the less time your
&gt; &gt; clients have to hit 'notfound'.
&gt; &gt; However, I agree completely with what you're saying. This is just a side
&gt; &gt; effect of how the system currently works. In a perfect world we wouldn't
&gt; &gt; care how long handoff takes and we would also do some sort of automatic
&gt; &gt; congestion control akin to TCP Reno or something. The preflist would still
&gt; &gt; point to the "old" partitions until all data has been successfully handed
&gt; &gt; off, and then and only then would we flip the switch for that vnode. I'm
&gt; &gt; pretty sure that's where we are heading (I say "pretty sure" b/c I just
&gt; &gt; joined the team and haven't been heavily involved in these specific talks
&gt; &gt; yet).
&gt; &gt; It's all coming down the pipe...
&gt; &gt; As for your specific I/O question re handoff\\_concurrecy, you might be right.
&gt; &gt; I would think it depends on hardware/platform/etc. I was offering it as a
&gt; &gt; possible stopgap to minimize Greg's pain. It's certainly a cure to a
&gt; &gt; symptom, not the problem itself.
&gt; &gt; -Ryan
&gt; &gt; 
&gt; &gt; On Thu, May 5, 2011 at 1:10 PM, John D. Rowell  wrote:
&gt; &gt; &gt; 
&gt; &gt; &gt; Hi Ryan, Greg,
&gt; &gt; &gt; 
&gt; &gt; &gt; 2011/5/5 Ryan Zezeski 
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; 1. For example, riak\\_core has a `handoff\\_concurrency` setting that
&gt; &gt; &gt; &gt; determines how many vnodes can concurrently handoff on a given node. By
&gt; &gt; &gt; &gt; default this is set to 4. That's going to take a while with your 2048
&gt; &gt; &gt; &gt; vnodes and all :)
&gt; &gt; &gt; 
&gt; &gt; &gt; Won't that make the handoff situation potentially worse? From the thread I
&gt; &gt; &gt; understood that the main problem was that the cluster was shuffling too 
&gt; &gt; &gt; much
&gt; &gt; &gt; data around and thus becoming unresponsive and/or returning unexpected
&gt; &gt; &gt; results (like "not founds"). I'm attributing the concerns more to an
&gt; &gt; &gt; excessive I/O situation than to how long the handoff takes. If the handoff
&gt; &gt; &gt; can be made transparent (no or little side effects) I don't think most
&gt; &gt; &gt; people will really care (e.g. the "fix the cluster tomorrow" anecdote).
&gt; &gt; &gt; 
&gt; &gt; &gt; How about using a percentage of available I/O to throttle the vnode
&gt; &gt; &gt; handoff concurrency? Start with 1, and monitor the node's I/O (kinda like
&gt; &gt; &gt; 'atop' does, collection CPU, disk and network metrics), if it is below the
&gt; &gt; &gt; expected usage, then increase the vnode handoff concurrency, and 
&gt; &gt; &gt; vice-versa.
&gt; &gt; &gt; 
&gt; &gt; &gt; I for one would be perfectly happy if the handoff took several hours (even
&gt; &gt; &gt; days) if we could maintain the core riak\\_kv characteristics intact during
&gt; &gt; &gt; those events. We've all seen looooong RAID rebuild times, and it's usually
&gt; &gt; &gt; better to just sit tight and keep the rebuild speed low (slower I/O) while
&gt; &gt; &gt; keeping all of the dependent systems running smoothly.
&gt; &gt; &gt; 
&gt; &gt; &gt; cheers
&gt; &gt; &gt; -jd
&gt; &gt; 
&gt; &gt; 
&gt; 
 
