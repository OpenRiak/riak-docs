---
title: "Re: Scaling up or out"
description: ""
project: community
lastmod: 2012-12-06T10:21:47-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg09452"
mailinglist_parent_id: "msg09449"
author_name: "Sean Carey"
project_section: "mailinglistitem"
sent_date: 2012-12-06T10:21:47-08:00
---


If your jumping between 5-25% iowait, I'd add nodes. Also tuning pdflush will 
help with the jumpy iowait.

Lucid by default uses up to 20% of your ram before flushing. 

So say you have 10gb of ram, your system could be trying to flush 2gb of data 
causing huge iowait spikes.


Sean Carey
@densone


On Thursday, December 6, 2012 at 11:49, Ken Perkins wrote:

&gt; We're around ~20% Swapping, IO Wait in the 5-25% range, depending on the 
&gt; machine.
&gt; 
&gt; We're running Lucid, with the deadline scheduler. I'm strongly biasing 
&gt; towards adding a few more nodes, but I'm not married to it :) 
&gt; 
&gt; 
&gt; On Wed, Dec 5, 2012 at 9:22 PM, Sean Carey  (mailto:ca...@basho.com)&gt; wrote:
&gt; &gt; So Ken, 
&gt; &gt; Fair amount? &lt; 5% or &gt; 20% 
&gt; &gt; 
&gt; &gt; If there's iowait and memory issues, adding nodes could alleviate that. If 
&gt; &gt; there's almost no iowait or minimal iowait, adding memory will help. Also, 
&gt; &gt; tuning vm.dirty on linux might get you more memory and less iowait. Or at 
&gt; &gt; least more consistent iowait. 
&gt; &gt; 
&gt; &gt; 
&gt; &gt; Which linux distro are you on and which scheduler are you using? 
&gt; &gt; 
&gt; &gt; 
&gt; &gt; -Sean 
&gt; &gt; 
&gt; &gt; 
&gt; &gt; 
&gt; &gt; 
&gt; &gt; 
&gt; &gt; 
&gt; &gt; On Thursday, December 6, 2012 at 12:15 AM, Ken Perkins wrote:
&gt; &gt; 
&gt; &gt; &gt; VMs, not the same host, rackspace has VM affinity to protect against 
&gt; &gt; &gt; that. We do see a fair amount of IO Wait.
&gt; &gt; &gt; 
&gt; &gt; &gt; Rackspace has a new affinity based SSD block device service that I plan 
&gt; &gt; &gt; to evaluate, but I'm not ready for that in production. 
&gt; &gt; &gt; 
&gt; &gt; &gt; 
&gt; &gt; &gt; On Wed, Dec 5, 2012 at 7:45 PM, Sean Carey  &gt; &gt; (mailto:ca...@basho.com)&gt; wrote:
&gt; &gt; &gt; &gt; Ken, 
&gt; &gt; &gt; &gt; Are your vms on different bare metal? Could they potentially be on the 
&gt; &gt; &gt; &gt; same bare metal? 
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; Are you seeing any io contention? 
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; Sean Carey
&gt; &gt; &gt; &gt; @densone
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; On Wednesday, December 5, 2012 at 20:41, Ken Perkins wrote:
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; &gt; Yes, we're thrashing on all of the boxes, due to disk access when 
&gt; &gt; &gt; &gt; &gt; looking through merge\\_index. It's not noisy neighbors, given how 
&gt; &gt; &gt; &gt; &gt; consistent the thrashing is. We had a box with a corrupted index (we 
&gt; &gt; &gt; &gt; &gt; had to remove merge\\_index and rebuild) and that machine instantly 
&gt; &gt; &gt; &gt; &gt; went to 0% thrashing. So we have a pretty good indication of the 
&gt; &gt; &gt; &gt; &gt; source.
&gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; &gt; The cost for 10 8GB VMs is roughly equivalent to 5 16GB ones.
&gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; &gt; Thanks for your input Michael!
&gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; &gt; Ken
&gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; &gt; On Wed, Dec 5, 2012 at 4:47 PM, Michael Johnson  &gt; &gt; &gt; &gt; (mailto:m...@mediatemple.net)&gt; wrote:
&gt; &gt; &gt; &gt; &gt; &gt; There are a lot of things that go into this, but I would tend to 
&gt; &gt; &gt; &gt; &gt; &gt; suggest in a hosted VM senario, upping the ram is likely the right 
&gt; &gt; &gt; &gt; &gt; &gt; solution.
&gt; &gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; &gt; &gt; You mention thrashing, but what is that thrashing coming from? I 
&gt; &gt; &gt; &gt; &gt; &gt; assume all the boxes are thrashing and not just one or two of them? 
&gt; &gt; &gt; &gt; &gt; &gt; Is it due to swapping or is it just the raw disk access? Maybe 
&gt; &gt; &gt; &gt; &gt; &gt; you logging too aggressively? 
&gt; &gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; &gt; &gt; Perhaps your are suffering from a bad neighbor effect. If this is 
&gt; &gt; &gt; &gt; &gt; &gt; the case, increasing the amount of ram will likely put you on a 
&gt; &gt; &gt; &gt; &gt; &gt; physical host with few customers and thus you would be less likely 
&gt; &gt; &gt; &gt; &gt; &gt; to have a bad neighbor. 
&gt; &gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; &gt; &gt; Cost-wise in the VM world, you might be better off adding a few 
&gt; &gt; &gt; &gt; &gt; &gt; nodes rather than increasing the ram in your existing vm's.
&gt; &gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; &gt; &gt; But then we are talking VMs and thus it should be fairly painless 
&gt; &gt; &gt; &gt; &gt; &gt; to experiment. I would try adding ram first and if that doesn't 
&gt; &gt; &gt; &gt; &gt; &gt; work, add a few nodes. Someone else my have a different opinion, 
&gt; &gt; &gt; &gt; &gt; &gt; but that is my two cents. 
&gt; &gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; &gt; &gt; On Wed, Dec 5, 2012 at 4:33 PM, Ken Perkins  &gt; &gt; &gt; &gt; &gt; (mailto:k...@clipboard.com)&gt; wrote:
&gt; &gt; &gt; &gt; &gt; &gt; &gt; Hello all,
&gt; &gt; &gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; &gt; &gt; &gt; We're seeing enough thrashing and low-memory on our production 
&gt; &gt; &gt; &gt; &gt; &gt; &gt; ring that we've decided to upgrade our hardware. The real 
&gt; &gt; &gt; &gt; &gt; &gt; &gt; question is should we scale up or out.
&gt; &gt; &gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; &gt; &gt; &gt; Currently our ring is 512 partitions. We know that it's a 
&gt; &gt; &gt; &gt; &gt; &gt; &gt; sub-optimal size but we can't easily solve that now. We're 
&gt; &gt; &gt; &gt; &gt; &gt; &gt; currently running a search-heavy app on 5 8GB VMs. I'm debating 
&gt; &gt; &gt; &gt; &gt; &gt; &gt; between moving the VMs up to 16GB, or adding a few more 8GB VMs. 
&gt; &gt; &gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; &gt; &gt; &gt; Some of the talk in #riak has pushed me towards adding more 
&gt; &gt; &gt; &gt; &gt; &gt; &gt; machines (thus lowering the per node number of partitions) but I 
&gt; &gt; &gt; &gt; &gt; &gt; &gt; wanted to do a quick sanity check here with folks that it's 
&gt; &gt; &gt; &gt; &gt; &gt; &gt; better than scaling up my current machines. 
&gt; &gt; &gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; &gt; &gt; &gt; Thanks!
&gt; &gt; &gt; &gt; &gt; &gt; &gt; Ken Perkins
&gt; &gt; &gt; &gt; &gt; &gt; &gt; clipboard.com (http://clipboard.com)
&gt; &gt; &gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; &gt; &gt; &gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; &gt; &gt; &gt; &gt; &gt; &gt; riak-users mailing list
&gt; &gt; &gt; &gt; &gt; &gt; &gt; riak-users@lists.basho.com (mailto:riak-users@lists.basho.com)
&gt; &gt; &gt; &gt; &gt; &gt; &gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt; &gt; &gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; &gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; &gt; &gt; &gt; &gt; riak-users mailing list
&gt; &gt; &gt; &gt; &gt; riak-users@lists.basho.com (mailto:riak-users@lists.basho.com)
&gt; &gt; &gt; &gt; &gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; 
&gt; &gt; 
&gt; 

