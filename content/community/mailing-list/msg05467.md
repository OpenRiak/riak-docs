---
title: "Re: 2i for single-result lookup"
description: ""
project: community
lastmod: 2011-11-07T20:31:27-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg05467"
mailinglist_parent_id: "msg05466"
author_name: "Nate Lawson"
project_section: "mailinglistitem"
sent_date: 2011-11-07T20:31:27-08:00
---


I appreciate you sharing your design. But I can never understand why people go 
to such great lengths to add transactions to an eventually-consistent db.

You could possibly solve the same problem by using memcached or Redis. Example:

1. Look up the user in Riak. If that lookup succeeds, abort.
2. Store a copy of the new username via memcached add() with a generous expiry 
interval.
3. If it exists already (atomic in memcached), you lost a race with another 
user and error out. If not, also store the username in Riak.
4. The memcached key will expire at some point after Riak becomes consistent, 
keeping your memory use reasonable.

Is user creation such a high percentage of your transaction volume that you 
have to build something almost as complicated as a VCS on top of it in order to 
scale? For robustness, isn't it simpler to just disable user creation if your 
memcached single-point-of-failure is down and you weren't able to failover to 
the spare?

I'm not criticizing you since you may have some unique need I overlooked. But 
would the above work for you?

-Nate

On Nov 7, 2011, at 7:16 PM, Justin Karneges wrote:

&gt; Hmm, but if the username (or email) must be unique then I think this problem 
&gt; may be more than just indexing. There's also an id reservation issue.
&gt; 
&gt; At least in my case, I did not think 2i would work for me, because I needed 
&gt; one of the indexes (email) to be unique and reservable on a first-come basis, 
&gt; yet I wanted people to be able to change email addresses. The resulting 
&gt; algorithm is a bit disgusting but it essentially works like this:
&gt; 
&gt; 1) All keys are tagged with extra metadata:
&gt; a) creation date (Riak only has modified date)
&gt; b) random "value" id (I'm using a UUID for this). this is meant to indicate 
&gt; a particular generation of a key's value. updates to a key should retain 
&gt; this 
&gt; value, but overwrites should have new ids.
&gt; 
&gt; 2) User keys (where the indexes point) must contain extra metadata: a set of 
&gt; index references (index creation date / value id pairs).
&gt; 
&gt; The idea with #1 and #2 here is that indexes point to users and users point 
&gt; to 
&gt; indexes. Notably, users point to specific index key \\*versions\\* thanks to the 
&gt; extra metadata. This aids in conflict resolution (more on that later).
&gt; 
&gt; 3) If you want to claim an index, then you first checks to see if a key 
&gt; already 
&gt; exists with that name. If it does then you can't claim ("account already 
&gt; exists"). If a key with that name does not exist then you make the index key 
&gt; with a link pointing to the user data key. Both the index key and user keys 
&gt; need to have creation date and value id metadata. Additionally, the user key 
&gt; needs to add a reference to this index into its set of references. Of course 
&gt; a conflict can occur if two clients try to claim indexes simultaneously. 
&gt; That's fine, you'll end up with two values (siblings) at the same index with 
&gt; different creation dates and value ids, and conflict resolution will deal 
&gt; with 
&gt; that.
&gt; 
&gt; 4) When retrieving a user, you must confirm the relationship between the 
&gt; index 
&gt; and the object. Essentially a valid keypair is the earliest dated index 
&gt; sibling that points to a user key with the latest dated index reference that 
&gt; points back to it. Determining this validity requires retrieving both keys, 
&gt; regardless of which one you start from, and in certain conflict cases may 
&gt; require crawling through to other keys. If you cannot verify the 
&gt; relationship 
&gt; between index and object then you must treat this case as if neither exist.
&gt; 
&gt; What does this all mean?
&gt; 
&gt; You can create users, unique by some value, on a first come basis. If two 
&gt; clients try to create accounts with the same name at the same time, then an 
&gt; arbitrary one will win. If two existing users try to change to the same 
&gt; name, 
&gt; an arbitrary one will win, and the loser \\*will not lose his original 
&gt; reservation\\*. I mapped out some pretty awful race conditions like if three 
&gt; existing users (A, B, and C) exist and A and B suddenly try to change to the 
&gt; same name at the same moment C tries to change to B's original name, and B 
&gt; loses to A, then B is guaranteed to get his index back and C's claim becomes 
&gt; invalid.
&gt; 
&gt; Oh, and finally, whichever index key is valid becomes the authoritative value 
&gt; of that data field. For example, in my case I'm indexing by email. This 
&gt; means 
&gt; if I want to look up a user's email address based on the direct user ID, I 
&gt; first look up the user object key directly, then I determine its valid index 
&gt; (needed to be done anyway just to confirm the user's validity to exist) and 
&gt; then use \\*that\\* index value as the user's email address. You would \\*not\\* use 
&gt; some email field value in the JSON blob. In fact, better to not have your 
&gt; index field in the JSON blob at all since conflict resolution won't be able 
&gt; to 
&gt; fix it.
&gt; 
&gt; OH, and one last thing. There was concern about writing a user requiring two 
&gt; writes, opening up the potential of leaving behind orphan keys in the event 
&gt; of 
&gt; failure. I've started to come to the conclusion that this is acceptable, and 
&gt; you just need to sweep it up later. The above-described scheme will work 
&gt; fine 
&gt; even if you leave dirty keys all over the place and never resolve siblings. 
&gt; You'll never have half of a user or some dangling index or anything. The 
&gt; validity checks at read-time ensure this. But, some periodically run task 
&gt; that cleans up your DB with MapReduce operations would be smart.
&gt; 
&gt; Maybe there's a better way to do this, but I thought I'd share.
&gt; 
&gt; Justin
&gt; 
&gt; On Monday, November 07, 2011 06:19:48 PM Nate Lawson wrote:
&gt;&gt; Ok, then 2I will work fine for what you're doing if you're going to keep
&gt;&gt; adding fields like that. You can just use the binary type for the 2I keys
&gt;&gt; (\\_bin).
&gt;&gt; 
&gt;&gt; http://basho.com/blog/technical/2011/09/14/Secondary-Indexes-in-Riak/
&gt;&gt; 
&gt;&gt; You first said you were concerned about the query performance due to the
&gt;&gt; covering set access and that you only had 2 unique fields. That's why I
&gt;&gt; suggested you create a manual index (either email or userid) to lookup the
&gt;&gt; other key. But with this additional info you mention, I think you should
&gt;&gt; just use 2I since that's one of its main purposes.
&gt;&gt; 
&gt;&gt; -Nate
&gt;&gt; 
&gt;&gt; On Nov 7, 2011, at 6:11 PM, Greg Pascale wrote:
&gt;&gt;&gt; Hi Nate,
&gt;&gt;&gt; 
&gt;&gt;&gt; There are only 2 secondary keys for now (in addition to the primary key),
&gt;&gt;&gt; but this number will grow to 5 or more pretty soon.
&gt;&gt;&gt; 
&gt;&gt;&gt; I think when you say "insert each separately", you mean create 2
&gt;&gt;&gt; duplicate objects, one keyed by username and one keyed by email. Or do
&gt;&gt;&gt; you mean create one object keyed by username, and then another object
&gt;&gt;&gt; containing the username and keyed by email (a manual index if you will)?
&gt;&gt;&gt; Code complexity is the main reason I'd like to avoid a solution like
&gt;&gt;&gt; this. Suddenly a user create operation requires n writes to be
&gt;&gt;&gt; considered a success. If one fails, I need to delete the others, etcâ€¦ It
&gt;&gt;&gt; quickly becomes a pain.
&gt;&gt;&gt; 
&gt;&gt;&gt; I don't know what you mean by "some relationship between the keys"
&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; On Nov 7, 2011, at 5:45 PM, Greg Pascale wrote:
&gt;&gt;&gt;&gt;&gt; Hi,
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; I'm thinking about using 2i for a certain piece of my system, but I'm
&gt;&gt;&gt;&gt;&gt; worried that the document-based partitioning may make it suboptimal.
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; The issue is that the secondary fields I want to query over (email and
&gt;&gt;&gt;&gt;&gt; username) are unique, so each will only ever map to one value. Since
&gt;&gt;&gt;&gt;&gt; 2i queries a coverage set, but I'm only ever looking for one result,
&gt;&gt;&gt;&gt;&gt; it's going to be hitting n-1 machines needlessly.
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; So, what I'm looking to understand is how much overhead a single-result
&gt;&gt;&gt;&gt;&gt; 2i lookup like this will incur vs. a primary-key lookup, or even vs.
&gt;&gt;&gt;&gt;&gt; search. Search doesn't intuitively feel like the right tool here, but
&gt;&gt;&gt;&gt;&gt; I wonder if it may actually be preferable since it uses term-based
&gt;&gt;&gt;&gt;&gt; partitioning.
&gt;&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt;&gt; Thanks,
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; If it's only 2 keys, why not insert each separately? You will double
&gt;&gt;&gt;&gt; your total number of keys in the db. But both search and 2I are
&gt;&gt;&gt;&gt; creating extra keys anyway in their private indices, so it has the same
&gt;&gt;&gt;&gt; or worse effect on total storage as doubling your primary keys. And
&gt;&gt;&gt;&gt; query efficiency is worse, as you point out.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 2I and search are more useful where there's some relationship between
&gt;&gt;&gt;&gt; the keys, not when they're fully independent as you point out.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; -Nate


\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

