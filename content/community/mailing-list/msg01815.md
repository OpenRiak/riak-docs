---
title: "Re: Riak Recap for Dec. 13 - 14"
description: ""
project: community
lastmod: 2010-12-17T04:27:50-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg01815"
mailinglist_parent_id: "msg01813"
author_name: "Jeremiah Peschka"
project_section: "mailinglistitem"
sent_date: 2010-12-17T04:27:50-08:00
---


Daniel -

You should be using some kind of load balancer to point clients at the Riak
cluster since all nodes can handle communication.

There's information in the wiki about how the keys are hashed:
http://wiki.basho.com/pages/viewpage.action?pageId=1245320 and there is also
info about how routing an object to a set of partitions works:
http://wiki.basho.com/display/RIAK/Replication#Replication-Routinganobjecttoasetofpartitions

Key hashing is done through SHA1, so I'd assume that you would be able to
figure out which node is responsible for that chunk o' keyspace.

Jeremiah Peschka
Microsoft SQL Server MVP
MCITP: Database Developer, DBA


On Fri, Dec 17, 2010 at 3:54 AM, Daniel Woo  wrote:

&gt; Thanks for your patience :-)
&gt;
&gt; So, when the client makes a call, it can just be round-robin since all
&gt; nodes can gossip? But it should be better to guess where the partition is
&gt; before making the call, right? Where can I find the documentation about how
&gt; the partition distributions are shared between nodes and how client decides
&gt; which node to ask?
&gt;
&gt; Regards,
&gt; Daniel
&gt;
&gt;
&gt; On Fri, Dec 17, 2010 at 12:24 PM, Dan Reverri  wrote:
&gt;
&gt;&gt; Yes.
&gt;&gt;
&gt;&gt; Daniel Reverri
&gt;&gt; Developer Advocate
&gt;&gt; Basho Technologies, Inc.
&gt;&gt; d...@basho.com
&gt;&gt;
&gt;&gt;
&gt;&gt; On Thu, Dec 16, 2010 at 7:56 PM, Daniel Woo wrote:
&gt;&gt;
&gt;&gt;&gt; Hi Daniel,
&gt;&gt;&gt;
&gt;&gt;&gt; So, the hashing algorithm is still consistent, but partitions (vnodes)
&gt;&gt;&gt; are re-distributed to nodes when new nodes are added, and the nodes gossip
&gt;&gt;&gt; and share the knowledge of the partition distribution, right? So the client
&gt;&gt;&gt; can query any of the nodes, if the node doesn't know where the partition
&gt;&gt;&gt; located it will query or gossip with adjacent nodes, right?
&gt;&gt;&gt;
&gt;&gt;&gt; Thanks,
&gt;&gt;&gt; Daniel
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; On Fri, Dec 17, 2010 at 10:42 AM, Dan Reverri  wrote:
&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Hi Daniel,
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Clients do not specify the partition when making a request. A client can
&gt;&gt;&gt;&gt; request any key from any node in the cluster and Riak will return the
&gt;&gt;&gt;&gt; associated value.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Thanks,
&gt;&gt;&gt;&gt; Dan
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Daniel Reverri
&gt;&gt;&gt;&gt; Developer Advocate
&gt;&gt;&gt;&gt; Basho Technologies, Inc.
&gt;&gt;&gt;&gt; d...@basho.com
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; On Thu, Dec 16, 2010 at 6:01 PM, Daniel Woo wrote:
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; Hi Mark,
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; Thanks for your explanation, so in this case the partitions would be
&gt;&gt;&gt;&gt;&gt; re-distributed
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; \\*from\\*
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; Node1: p1 ~ p16
&gt;&gt;&gt;&gt;&gt; Node2: p17 ~ p32
&gt;&gt;&gt;&gt;&gt; Node3: p33 ~ p48
&gt;&gt;&gt;&gt;&gt; Node4: p49 ~ p64
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; \\*to \\*
&gt;&gt;&gt;&gt;&gt; Node1: p1 ~ p13 (remove 3 partitions)
&gt;&gt;&gt;&gt;&gt; Node2: p17 ~ p29 (remove 3 partitions)
&gt;&gt;&gt;&gt;&gt; Node3: p33 ~ p45 (remove 3 partitions)
&gt;&gt;&gt;&gt;&gt; Node4: p49 ~ p61 (remove 3 partitions)
&gt;&gt;&gt;&gt;&gt; Node5: p14, 15, 16, 30, 31, 32, 46, 47, 48, 62, 63, 64 (aprox 1/5
&gt;&gt;&gt;&gt;&gt; partitions will be transferred to this new node)
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; Since there is no centralized node in Riak, how do we know the
&gt;&gt;&gt;&gt;&gt; partition 32 is moved to node 5 from the client caller? Cassandra seems
&gt;&gt;&gt;&gt;&gt; break half of the adjacent node's data into the new node, that will be 
&gt;&gt;&gt;&gt;&gt; easy
&gt;&gt;&gt;&gt;&gt; for the client to search for the datum just around the node-circle
&gt;&gt;&gt;&gt;&gt; clockwise, although it causes unbalanced data distribution and you have to
&gt;&gt;&gt;&gt;&gt; move them by command lines. Riak seems to have this solved by moving
&gt;&gt;&gt;&gt;&gt; partitions into new nodes equally, that's very interesting, how do you 
&gt;&gt;&gt;&gt;&gt; guys
&gt;&gt;&gt;&gt;&gt; make it? If the client caller queries for partition 32 which was 
&gt;&gt;&gt;&gt;&gt; originally
&gt;&gt;&gt;&gt;&gt; on node 2, how do the client know it's on a new node now?
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; Thanks,
&gt;&gt;&gt;&gt;&gt; Daniel
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; On Fri, Dec 17, 2010 at 6:56 AM, Mark Phillips  wrote:
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt; Hey Daniel,
&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt; [snip]
&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt; &gt; So, I guess Riak would have to re-hash the whole partitions into all
&gt;&gt;&gt;&gt;&gt;&gt; the 5
&gt;&gt;&gt;&gt;&gt;&gt; &gt; nodes, right? Is this done lazily when the node finds the requested
&gt;&gt;&gt;&gt;&gt;&gt; data is
&gt;&gt;&gt;&gt;&gt;&gt; &gt; missing?
&gt;&gt;&gt;&gt;&gt;&gt; &gt; Or is there a way to handle this with consistent re-hashing so we
&gt;&gt;&gt;&gt;&gt;&gt; can avoid
&gt;&gt;&gt;&gt;&gt;&gt; &gt; moving data around when new nodes added?
&gt;&gt;&gt;&gt;&gt;&gt; &gt;
&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt; Riak won't rehash all the partitions in the ring when new nodes are
&gt;&gt;&gt;&gt;&gt;&gt; added. When you go from 4 -&gt; 5 nodes, for example, approx. 1/5 of the
&gt;&gt;&gt;&gt;&gt;&gt; existing partitions are transferred to the new node. The other 4/5s of
&gt;&gt;&gt;&gt;&gt;&gt; the partitions will remain unchanged. As far as moving data around
&gt;&gt;&gt;&gt;&gt;&gt; when new nodes are added, this is impossible to avoid. Data needs to
&gt;&gt;&gt;&gt;&gt;&gt; be handed off to be spread around the ring.
&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt; Hope that helps.
&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt; Mark
&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; --
&gt;&gt;&gt;&gt;&gt; Thanks & Regards,
&gt;&gt;&gt;&gt;&gt; Daniel
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt;&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; --
&gt;&gt;&gt; Thanks & Regards,
&gt;&gt;&gt; Daniel
&gt;&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;
&gt;
&gt; --
&gt; Thanks & Regards,
&gt; Daniel
&gt;
&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;
&gt;
\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

