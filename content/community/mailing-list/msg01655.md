---
title: "Re: Whole cluster times out if one node is gone"
description: ""
project: community
lastmod: 2010-11-29T10:28:05-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg01655"
mailinglist_parent_id: "msg01653"
author_name: "Jay Adkisson"
project_section: "mailinglistitem"
sent_date: 2010-11-29T10:28:05-08:00
---


Hm, that's curious. Are you rebooting the physical machine? When you
reboot one of the nodes, what happens to HTTP calls to that node? Do they
immediately error, or do they hang indefinitely?

In the meanwhile, I'll add some logging so I can see whether I'm timing out
on the writes as well, and I'll see what happens with different keys.

Thanks,
--Jay

On Mon, Nov 29, 2010 at 10:02 AM, Dan Reverri  wrote:

&gt; Hi Jay,
&gt;
&gt; I'm not able to reproduce the behavior you are seeing. Here is what I am
&gt; doing to try to reproduce the issue:
&gt; 1. Setup a 4 node cluster
&gt; 2. Continuously write a new object to Riak every 0.5 second
&gt; 3. Continuously read a known object (GET riak/test/1) from Riak every 0.5
&gt; second
&gt; 4. Reboot one of the nodes
&gt;
&gt; The reads and writes continue working normally when rebooting the node.
&gt;
&gt; Do you see timeouts while writing objects to Riak?
&gt; Can you try reading other objects from Riak during the reboot (i.e.
&gt; different keys)?
&gt;
&gt; Thanks,
&gt; Dan
&gt;
&gt; Daniel Reverri
&gt; Developer Advocate
&gt; Basho Technologies, Inc.
&gt; d...@basho.com
&gt;
&gt;
&gt; On Mon, Nov 29, 2010 at 9:39 AM, Jay Adkisson  wrote:
&gt;
&gt;&gt; Hey Dan/Sean,
&gt;&gt;
&gt;&gt; Thanks for the response. sasl-error.log on node A is completely empty,
&gt;&gt; and I see this pattern in erlang.log:
&gt;&gt;
&gt;&gt; ===== ALIVE Tue Nov 23 12:46:57 PST 2010
&gt;&gt;
&gt;&gt; ===== Tue Nov 23 12:57:36 PST 2010
&gt;&gt;
&gt;&gt; =ERROR REPORT==== 23-Nov-2010::12:57:36 ===
&gt;&gt; \\*\\* Node 'riak@' not responding \\*\\*
&gt;&gt; \\*\\* Removing (timedout) connection \\*\\*
&gt;&gt;
&gt;&gt; =INFO REPORT==== 23-Nov-2010::12:58:41 ===
&gt;&gt; Starting handoff of partition riak\\_kv\\_vnode
&gt;&gt; 251195593916248939066258330623111144003363405824 to 'riak@'
&gt;&gt;
&gt;&gt; =INFO REPORT==== 23-Nov-2010::12:58:41 ===
&gt;&gt; Handoff of partition riak\\_kv\\_vnode
&gt;&gt; 251195593916248939066258330623111144003363405824 to 'riak@'
&gt;&gt; completed: sent 1 objects in 0.02 seconds
&gt;&gt; =INFO REPORT==== 23-Nov-2010::12:59:18 ===
&gt;&gt; Starting handoff of partition riak\\_kv\\_vnode
&gt;&gt; 707914855582156101004909840846949587645842325504 to 'riak@'
&gt;&gt;
&gt;&gt; =INFO REPORT==== 23-Nov-2010::12:59:18 ===
&gt;&gt; Handoff of partition riak\\_kv\\_vnode
&gt;&gt; 707914855582156101004909840846949587645842325504 to 'riak@'
&gt;&gt; completed: sent 5 objects in 0.03 seconds
&gt;&gt; =INFO REPORT==== 23-Nov-2010::12:59:20 ===
&gt;&gt; Starting handoff of partition riak\\_kv\\_vnode
&gt;&gt; 525227150915793236229449236757414210188850757632 to 'riak@'
&gt;&gt;
&gt;&gt; 
&gt;&gt;
&gt;&gt; This is my testing process: I'm doing an initial load into riak of small
&gt;&gt; image files between 1 and 150K, throttled to two images per second, with
&gt;&gt; W=1. In a different terminal, I'm running a wget every second against node
&gt;&gt; A of one particular image I already know to be in the cluster, again with
&gt;&gt; R=1. I'm using R,W=1 because I figured that would reduce the chance of
&gt;&gt; timing out, and with my data pattern, nothing I write to the cluster will
&gt;&gt; ever change, so I really don't need to wait for a quorum.
&gt;&gt;
&gt;&gt; In response to Sean,
&gt;&gt;
&gt;&gt;&gt; 1) Riak detects node outage the same way any Erlang system does - when a
&gt;&gt;&gt; message fails to deliver, or the heartbeat maintained by epmd fails. The
&gt;&gt;&gt; default timeout in epmd is 1 minute, which is probably why you're seeing it
&gt;&gt;&gt; take 1 minute to be detected.
&gt;&gt;&gt;
&gt;&gt; Thanks, this is enlightening.
&gt;&gt;
&gt;&gt; 2) If it takes too long (the vnode is overloaded, perhaps, or is just
&gt;&gt;&gt; starting up as a hint partition) to retrieve from any node, the request can
&gt;&gt;&gt; time out.
&gt;&gt;&gt;
&gt;&gt; That makes sense, but I still wonder why this happens even when the quorum
&gt;&gt; is already met by the machines that are responding normally?
&gt;&gt;
&gt;&gt;
&gt;&gt;&gt; 3) You could probably configure epmd to timeout sooner, but then you
&gt;&gt;&gt; become more vulnerable to temporary partitions. YMMV
&gt;&gt;&gt;
&gt;&gt; I may try that - it might be a good fit with my data pattern.
&gt;&gt;
&gt;&gt; Thanks again,
&gt;&gt; --Jay
&gt;&gt;
&gt;&gt;
&gt;&gt; On Mon, Nov 29, 2010 at 4:44 AM, David Smith  wrote:
&gt;&gt;
&gt;&gt;&gt; On Tue, Nov 23, 2010 at 3:33 PM, Jay Adkisson  wrote:
&gt;&gt;&gt; &gt; (many profuse apologies to Dan - hit "reply" instead of "reply all")
&gt;&gt;&gt; &gt; Alrighty, I've done a little more digging. When I throttle the writes
&gt;&gt;&gt; &gt; heavily (2/sec) and set R and W to 1 all around, the cluster works just
&gt;&gt;&gt; fine
&gt;&gt;&gt; &gt; after I restart the node for about 15-20 seconds. Then the read
&gt;&gt;&gt; request
&gt;&gt;&gt; &gt; hangs for about a minute, until node D disappears from connected\\_nodes
&gt;&gt;&gt; in
&gt;&gt;&gt; &gt; riak-admin status, at which point it returns the desired value
&gt;&gt;&gt; (although
&gt;&gt;&gt; &gt; sometimes I get a 503):
&gt;&gt;&gt;
&gt;&gt;&gt; Are you seeing any error messages in log/erlang.log.\\* or
&gt;&gt;&gt; log/sasl-error.log?
&gt;&gt;&gt;
&gt;&gt;&gt; Can you expound on your use case a little -- are you doing a large
&gt;&gt;&gt; insert, or just a random read/write mix? Did you pre-populate the
&gt;&gt;&gt; dataset? Why are you using r=1, instead of relying on quorom for
&gt;&gt;&gt; reads?
&gt;&gt;&gt;
&gt;&gt;&gt; How are you running the riak-admin status to measure the 15-20 seconds?
&gt;&gt;&gt;
&gt;&gt;&gt; Thanks.
&gt;&gt;&gt;
&gt;&gt;&gt; D.
&gt;&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;
