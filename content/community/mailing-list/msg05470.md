---
title: "Re: Severe problems when adding a new node"
description: ""
project: community
lastmod: 2011-11-08T02:36:12-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg05470"
mailinglist_parent_id: "msg05353"
author_name: "John Axel Eriksson"
project_section: "mailinglistitem"
sent_date: 2011-11-08T02:36:12-08:00
---


Thanks for the emails detailing this issue - private and to the list. I've got 
a question for the list on our situation:

As stated we did an upgrade from 0.14.2 to 1.0.1 and after that we added a new 
node to our cluster. This
really messed things up and nodes started crashing. In the end I opted to 
remove the added node and after
quite a short while things settled down. The cluster is responding again. What 
we see now are corrupted files.

We've tried to determine how many of them there are but it's been a bit 
difficult. What we know is that there ARE
corrupted files(or at least returned in an inconsistent state). I was wondering 
if there is anything we can do to get
the cluster in a proper state again without having to manually delete 
everything that's corrupted? Is it possible that
the data is actually there but not returned in a proper state by riak? I think 
it's only the larger files stored in luwak
that have this problem.

John


29 okt 2011 kl. 01:03 skrev John Axel Eriksson:

&gt; I've got the utmost respect for developers such as yourselves(Basho) and 
&gt; we've had great success using Riak - we have been using it
&gt; in production since 0.11. We've had our share of problems with it during this 
&gt; whole time but none as big as this. I can't understand why
&gt; this wasn't posted somewhere using the blink tag and big red bold text. I 
&gt; mean if I try to fsck a mounted disk in use in Linux I get:
&gt; 
&gt; "WARNING!!! The filesystem is mounted. If you continue you \\*\\*\\*WILL\\*\\*\\*
&gt; cause \\*\\*\\*SEVERE\\*\\*\\* filesystem damage."
&gt; 
&gt; I understand why I don't get a warning like that when trying to run 
&gt; "riak-admin join r...@my.node.com" on Riak 1.0.1 but something similar to
&gt; it happens.
&gt; 
&gt; It goes against the whole idea of Riak being an ops-dream, distributed, 
&gt; fault-tolerant system having a bug such as this without disclosing it
&gt; more openly than an entry in a bug tracking system. I don't want to be afraid 
&gt; of adding nodes to my cluster but that is the result of this bug and
&gt; the lack of communication of same bug. The 1.0.1 release should have been 
&gt; pulled in my opinion.
&gt; 
&gt; To sum it up, this was a nightmare for us, I didn't get much sleep last night 
&gt; and I woke up in hell. All that, corrupted data, downtime and lost customer
&gt; confidence could have been avoided by better communication.
&gt; 
&gt; I don't want to be too hard on you fine people of Basho and you provide a 
&gt; really great system in Riak and I understand what you're aiming for, but if
&gt; anything as bad as this ever happens in the future you might want to 
&gt; communicate it better and consider pulling the release.
&gt; 
&gt; Thanks,
&gt; John
&gt; 
&gt; 
&gt; 28 okt 2011 kl. 17:51 skrev Kelly McLaughlin:
&gt; 
&gt;&gt; John,
&gt;&gt; 
&gt;&gt; It appears you've run into a race condition with adding and leaving nodes 
&gt;&gt; that's present in 1.0.1. The problem happens during handoff and can cause 
&gt;&gt; bitcask directories to be unexpectedly deleted. We have identified the issue 
&gt;&gt; and we are in the process of correcting it, testing, and generating a new 
&gt;&gt; point release containing the fix. In the meantime, we apologize for the 
&gt;&gt; inconvenience and irritation this has caused. 
&gt;&gt; 
&gt;&gt; Kelly
&gt;&gt; 
&gt;&gt; 
&gt;&gt; On Oct 28, 2011, at 9:14 AM, John Axel Eriksson wrote:
&gt;&gt; 
&gt;&gt;&gt; Last night we did two things. First we upgraded our entire cluster from 
&gt;&gt;&gt; riak-search 0.14.2 to 1.0.1. This process went
&gt;&gt;&gt; pretty well and the cluster was responding correctly after this was 
&gt;&gt;&gt; completed.
&gt;&gt;&gt; 
&gt;&gt;&gt; In our cluster we have around 40 000 files stored in Luwak (we also have 
&gt;&gt;&gt; about the same amount of keys, or more, in riak which is mostly
&gt;&gt;&gt; the metadata for the files in Luwak). The files are in sizes ranging from 
&gt;&gt;&gt; around 50K to around 400MB, most of the files are pretty small though. I
&gt;&gt;&gt; think we're up to a total of around 30GB now.
&gt;&gt;&gt; 
&gt;&gt;&gt; Anyway, upon adding a new node to the now 1.0.1 cluster I saw the beam.smp 
&gt;&gt;&gt; processes on all the servers, including the new one, taking
&gt;&gt;&gt; up almost all available cpu. It stayed in this state for around an hour and 
&gt;&gt;&gt; the cluster was slow to respond and occasionally timed out. During the
&gt;&gt;&gt; process Riak crashed on random nodes from time to time and I had to restart 
&gt;&gt;&gt; it. After about an hour things settled down. I added this
&gt;&gt;&gt; new node to our load-balancer so it too could serve requests. When testing 
&gt;&gt;&gt; our apps against the cluster we still got lots of timeouts and something
&gt;&gt;&gt; seemed very very wrong.
&gt;&gt;&gt; 
&gt;&gt;&gt; After a while I did a "riak-admin leave" on the node that was added (kind 
&gt;&gt;&gt; of a panic move I guess). Around 20 minutes after I did this, the cluster 
&gt;&gt;&gt; started
&gt;&gt;&gt; responding correctly again. All was not well though - files seemed to be 
&gt;&gt;&gt; corrupted(not sure what percentage but could be 1 % or more). I have no 
&gt;&gt;&gt; idea how
&gt;&gt;&gt; that could happen but files that we had accessed before now contained 
&gt;&gt;&gt; garbage. I haven't thoroughly researched exactly WHAT garbage they contain 
&gt;&gt;&gt; but
&gt;&gt;&gt; they're not in a usable state anymore. Is this something that could happen 
&gt;&gt;&gt; under any circumstances in Riak?
&gt;&gt;&gt; 
&gt;&gt;&gt; I'm afraid of adding a node at all now since it resulted in downtime and 
&gt;&gt;&gt; corruption when I tried it. I checked and rechecked the configuration files 
&gt;&gt;&gt; and really - they're
&gt;&gt;&gt; the same on all the nodes (except for vm.args where they have different 
&gt;&gt;&gt; names of course). Has anyone ever seen anything like this? Could it somehow 
&gt;&gt;&gt; be related to
&gt;&gt;&gt; the fact that I did an upgrade from 0.14.2 to 1.0.1 and maybe an hour later 
&gt;&gt;&gt; added a new 1.0.1 node?
&gt;&gt;&gt; 
&gt;&gt;&gt; Thanks for any input!
&gt;&gt;&gt; 
&gt;&gt;&gt; John

&gt;&gt; 
&gt; 
