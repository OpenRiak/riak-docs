---
title: "Re: have a new node take over the role of a downed,	unrecoverable node?"
description: ""
project: community
lastmod: 2010-10-16T16:01:31-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg01315"
mailinglist_parent_id: "msg01314"
author_name: "Sean Cribbs"
project_section: "mailinglistitem"
sent_date: 2010-10-16T16:01:31-07:00
---


Sorry, I wasn't completely clear. You can make any node "leave" from the 
console. e.g.

riak\\_core\\_gossip:remove\\_from\\_cluster('r...@some-host.com').

Sean Cribbs 
Developer Advocate
Basho Technologies, Inc.
http://basho.com/

On Oct 16, 2010, at 5:05 PM, Alexander Sicular wrote:

&gt; This has come up before. "Leave" is what is currently available and
&gt; needs to be run on the node that wants to leave. This, of course,
&gt; means the node needs to be available. What you really want is a kick
&gt; like "remove" or something that doesn't exist yet, afaik. I think
&gt; there is a ticket open.
&gt; 
&gt; -alexander
&gt; 
&gt; On 2010-10-16, Jesse Newland  wrote:
&gt;&gt; The description of leave on the wiki mentions that it "causes the node to
&gt;&gt; leave the cluster it participates in" - I assume "the node" refers to the
&gt;&gt; node this command is run on? How would I "leave" a node that I can't run
&gt;&gt; this command on anymore?
&gt;&gt; 
&gt;&gt; Regards -
&gt;&gt; 
&gt;&gt; Jesse Newland
&gt;&gt; ---
&gt;&gt; je...@railsmachine.com
&gt;&gt; 404.216.1093
&gt;&gt; 
&gt;&gt; On Oct 16, 2010, at 3:16 PM, Sean Cribbs wrote:
&gt;&gt; 
&gt;&gt;&gt; `leave` is exactly what you want to do then. Once the old node has left
&gt;&gt;&gt; (use `ringready` to track its exit), add the new neode.
&gt;&gt;&gt; 
&gt;&gt;&gt; If the EBS volume containing the node's data was not lost, you could mount
&gt;&gt;&gt; it onto the new node to save some recovery time, and then reip. However,
&gt;&gt;&gt; you'll need to reip on all machines.
&gt;&gt;&gt; 
&gt;&gt;&gt; Sean Cribbs 
&gt;&gt;&gt; Developer Advocate
&gt;&gt;&gt; Basho Technologies, Inc.
&gt;&gt;&gt; http://basho.com/
&gt;&gt;&gt; 
&gt;&gt;&gt; On Oct 16, 2010, at 2:54 PM, Jesse Newland wrote:
&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; I'm running through some disaster scenarios before bringing a riak
&gt;&gt;&gt;&gt; cluster into production, and have run into a scenario that I can't work
&gt;&gt;&gt;&gt; through the proper resolution for just yet:
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Say an ec2 instance that was a part of a ring went away quickly, and data
&gt;&gt;&gt;&gt; from it was unrecoverable.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; How might I go about telling the rest of the ring that a new instance
&gt;&gt;&gt;&gt; that I've brought up should take over the vnodes that were on that old
&gt;&gt;&gt;&gt; instance? This sounds like a job for `riak-admin reip`, but after running
&gt;&gt;&gt;&gt; `reip downed\\_node new\\_node`, `riak-admin ringready` still shows that the
&gt;&gt;&gt;&gt; old nodes are a part of the ring and down. I guess what I'd like to do is
&gt;&gt;&gt;&gt; a posthumeous `leave`?
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Thoughts?
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Regards -
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Jesse Newland
&gt;&gt;&gt;&gt; ---
&gt;&gt;&gt;&gt; je...@railsmachine.com
&gt;&gt;&gt;&gt; 404.216.1093
&gt;&gt;&gt;&gt; 
&gt; 
&gt; -- 
&gt; Sent from my mobile device
