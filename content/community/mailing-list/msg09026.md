---
title: "Re: Is Riak suitable for s small-record write-intensive	billion-records application?"
description: ""
project: community
lastmod: 2012-10-23T10:50:42-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg09026"
mailinglist_parent_id: "msg08979"
author_name: "Yassen Damyanov"
project_section: "mailinglistitem"
sent_date: 2012-10-23T10:50:42-07:00
---


Thanks Jens, Joshua, Sean, Jared; I'll surely give Couchbase a try (no offense).

CARP works for machines, not for services, right. I'll think on that aspect.

Again, any suggestion for a \\*storage backend\\* for our use case?

Quoting myself,

&gt; We consider a NoSQL DB deployment for a mission-critical application where we 
&gt; need to store several hundreds of millions of data records, each record 
&gt; consisting of about 6 string fields, record total length is 160 bytes. There 
&gt; is a unique key in each record that seems suitable for hashing (20+ bytes 
&gt; string, e.g. "cle01\\_tpls01\\_2105328884").
&gt;
&gt; The application should be able to write several hundreds of new records per 
&gt; second, but first check if the unique key already exists. Writing is to be 
&gt; done only if it is not there. If it is, the app needs to retrieve the whole 
&gt; record and return it to the client and no writing is done in this case.
&gt;
&gt; We need to have a cluster of at least 2-3 nodes, which must be able to grow 
&gt; easily if a need be.

Thank you!

\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

