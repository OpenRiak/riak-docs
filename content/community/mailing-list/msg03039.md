---
title: "Re: Large ring_creation_size"
description: ""
project: community
lastmod: 2011-04-14T10:48:58-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg03039"
mailinglist_parent_id: "msg03024"
author_name: "Greg Nelson"
project_section: "mailinglistitem"
sent_date: 2011-04-14T10:48:58-07:00
---


We have a exact idea of the amount of data we'll be storing, and the kinds of 
machines we'll be storing them on. The simple math of (total data we'll be 
storing 6 months from now) / (total capacity of a single node) \\* (number of 
duplicates of each datum we'd like to store for redundancy) gives us a number a 
concrete number of machines we'll need, whether we're using Riak or something 
else...

However, that's a little off track from the question I'm trying to answer. 
Which is: 

Why is it when I start two nodes with a large-ish ring\\_creation\\_size -- as soon 
as I join the second node to the first, CPU and memory usage of both nodes goes 
through the roof? No data stored yet. This even happens with a 
ring\\_creation\\_size I wouldn't consider huge, like 4096.

So that is the small configuration I'm starting with and that is the limit I'm 
hitting.

I realize that the feasibility of building out a single 1000 node cluster is a 
larger question. But I can tell you we'll get there; having to shard across 10 
100-node clusters is an option. Regardless, I'd like to have an understanding 
of what the resource overhead of each vnode is...
On Thursday, April 14, 2011 at 6:38 AM, Sean Cribbs wrote:
Good points, Dave.
&gt; 
&gt; Also, it's worth mentioning that we've seen that many customers and 
&gt; open-source users think they will need many more nodes than they actually do. 
&gt; Many are able to start with 5 nodes and are happy for quite a while. The only 
&gt; way to tell what you actually need is to start with a baseline configuration 
&gt; and simulate some percentage above your current load. Once you've figured out 
&gt; what size that initial cluster is, start with (number of nodes) \\* 50 as the 
&gt; ring\\_creation\\_size (rounded to the nearest power of 2 of course). This gives 
&gt; you a growth factor of about 5 before you need to consider changing.
&gt; 
&gt; As well, there's some ops "common sense" that says the lifetime of any single 
&gt; architecture is 18 months or less. That doesn't necessarily mean that you'll 
&gt; be building a new cluster with a larger ring size in 18 months, but just that 
&gt; your needs will be different at that time and are hard to predict. Plan for 
&gt; now, worry about the 1000 node cluster when you actually need it.
&gt; 
&gt; Sean Cribbs 
&gt; Developer Advocate
&gt; Basho Technologies, Inc.
&gt; http://basho.com/
&gt; 
&gt; 
&gt; On Apr 14, 2011, at 9:09 AM, Dave Barnes wrote:
&gt; &gt; Sorry I feel compelled to chime in.
&gt; &gt; 
&gt; &gt; Maybe you could assess your physical node limits and start with a small 
&gt; &gt; configuration, then increase it and increase it until you hit a limit.
&gt; &gt; 
&gt; &gt; Work small to large.
&gt; &gt; 
&gt; &gt; Once you find the pain point, lets us know what resource ran out.
&gt; &gt; 
&gt; &gt; You will learn a lot along the way on how your servers behave and we'll 
&gt; &gt; discover a lot when you share the results.
&gt; &gt; 
&gt; &gt; Thanks for digging in,
&gt; &gt; 
&gt; &gt; Dave
&gt; &gt; 
&gt; &gt; On Wed, Apr 13, 2011 at 5:11 PM, Greg Nelson  wrote:
&gt; &gt; &gt; Ok, how about in this case I described? It runs out of memory with a 
&gt; &gt; &gt; single pair of nodes...
&gt; &gt; &gt; 
&gt; &gt; &gt; (Or did you mean there's a connection between each pair of vnodes?) 
&gt; &gt; &gt; On Wednesday, April 13, 2011 at 1:56 PM, Jon Meredith wrote:
&gt; &gt; &gt; &gt; Hi Greg et al,
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; As you say largest known is not largest possible. Internally within 
&gt; &gt; &gt; &gt; Basho, the largest cluster we've experimented with so far had 50 nodes. 
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; Going beyond that it's speculation from me about pain points. 
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; 1) It is true that you need enough file descriptors to start up all 
&gt; &gt; &gt; &gt; partitions when a node restarts - Riak checks if there is any handoff 
&gt; &gt; &gt; &gt; data pending for each partition. We have work scheduled to address that 
&gt; &gt; &gt; &gt; in the medium term. The plan is to only spin up partitions the node 
&gt; &gt; &gt; &gt; owns and any that have been started as fallbacks that handoff has not 
&gt; &gt; &gt; &gt; completed for. Until that work is done you will need a high ulimit with 
&gt; &gt; &gt; &gt; large ring sizes. 
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; 2) It is also true that Erlang runs a fully connected network, so there 
&gt; &gt; &gt; &gt; will be connections between each node pair in the cluster. We haven't 
&gt; &gt; &gt; &gt; determined the point at which it becomes a problem. 
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; So it looks like you'll be pushing the known limits. Basho will do our 
&gt; &gt; &gt; &gt; very best to help overcome any obstacles as you encounter them.
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; Jon Meredith
&gt; &gt; &gt; &gt; Basho Technologies.
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; On Wed, Apr 13, 2011 at 1:41 PM, Greg Nelson  wrote:
&gt; &gt; &gt; &gt; &gt; The largest known riak cluster != the largest possible riak cluster. 
&gt; &gt; &gt; &gt; &gt; ;-)
&gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; &gt; The inter node communication of the cluster depends on the data set 
&gt; &gt; &gt; &gt; &gt; and usage pattern, doesn't it? Or is there some constant overhead 
&gt; &gt; &gt; &gt; &gt; that tops out at a few hundred nodes? I should point out that we'll 
&gt; &gt; &gt; &gt; &gt; have big data, but not a huge number of keys. 
&gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; &gt; The number of vnodes in the cluster should be equal to the 
&gt; &gt; &gt; &gt; &gt; ring\\_creation\\_size under normal circumstances, shouldn't it? So when 
&gt; &gt; &gt; &gt; &gt; I have a one node cluster, that node is running ring\\_creation\\_size 
&gt; &gt; &gt; &gt; &gt; vnodes... File descriptors probably isn't a problem -- these machines 
&gt; &gt; &gt; &gt; &gt; won't be doing anything else, and the limits are set to 65536. 
&gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; &gt; Thinking about the internode communication you mentioned, that's 
&gt; &gt; &gt; &gt; &gt; probably where the resource hog is.. socket buffers, etc.
&gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; &gt; Anyway, I'd also love to hear more from basho. :)
&gt; &gt; &gt; &gt; &gt; On Wednesday, April 13, 2011 at 12:33 PM, sicul...@gmail.com wrote:
&gt; &gt; &gt; &gt; &gt; &gt; Ill just chime in and say that this is not practical for a few 
&gt; &gt; &gt; &gt; &gt; &gt; reasons. The largest known riak cluster has like 50 or 60 nodes. 
&gt; &gt; &gt; &gt; &gt; &gt; Afaik, inter node communication of erlang clusters top out at a few 
&gt; &gt; &gt; &gt; &gt; &gt; hundred nodes. I'm also under the impression that each physical 
&gt; &gt; &gt; &gt; &gt; &gt; node has to have enough file descriptors to accommodate every 
&gt; &gt; &gt; &gt; &gt; &gt; virtual node in the cluster. 
&gt; &gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; &gt; &gt; I'd love to hear more from basho. 
&gt; &gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; &gt; &gt; -alexander 
&gt; &gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; &gt; &gt; Sent from my Verizon Wireless BlackBerry
&gt; &gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; &gt; &gt; -----Original Message-----
&gt; &gt; &gt; &gt; &gt; &gt; From: Greg Nelson 
&gt; &gt; &gt; &gt; &gt; &gt; Sender: riak-users-boun...@lists.basho.com
&gt; &gt; &gt; &gt; &gt; &gt; Date: Wed, 13 Apr 2011 12:13:34 
&gt; &gt; &gt; &gt; &gt; &gt; To: 
&gt; &gt; &gt; &gt; &gt; &gt; Subject: Large ring\\_creation\\_size
&gt; &gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; &gt; &gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; &gt; &gt; &gt; &gt; &gt; riak-users mailing list
&gt; &gt; &gt; &gt; &gt; &gt; riak-users@lists.basho.com
&gt; &gt; &gt; &gt; &gt; &gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt; &gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; &gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; &gt; &gt; &gt; &gt; riak-users mailing list
&gt; &gt; &gt; &gt; &gt; riak-users@lists.basho.com
&gt; &gt; &gt; &gt; &gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; 
&gt; &gt; &gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; &gt; &gt; riak-users mailing list
&gt; &gt; &gt; riak-users@lists.basho.com
&gt; &gt; &gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt; &gt; &gt; 
&gt; &gt; 
&gt; &gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; &gt; riak-users mailing list
&gt; &gt; riak-users@lists.basho.com
&gt; &gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt; 
&gt; 
\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

