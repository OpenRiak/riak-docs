---
title: "Re: Scaling up or out"
description: ""
project: community
lastmod: 2012-12-05T21:22:28-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg09448"
mailinglist_parent_id: "msg09447"
author_name: "Sean Carey"
project_section: "mailinglistitem"
sent_date: 2012-12-05T21:22:28-08:00
---


So Ken, 
Fair amount? &lt; 5% or &gt; 20% 

If there's iowait and memory issues, adding nodes could alleviate that. If 
there's almost no iowait or minimal iowait, adding memory will help. Also, 
tuning vm.dirty on linux might get you more memory and less iowait. Or at least 
more consistent iowait.


Which linux distro are you on and which scheduler are you using? 


-Sean 






On Thursday, December 6, 2012 at 12:15 AM, Ken Perkins wrote:

&gt; VMs, not the same host, rackspace has VM affinity to protect against that. We 
&gt; do see a fair amount of IO Wait.
&gt; 
&gt; Rackspace has a new affinity based SSD block device service that I plan to 
&gt; evaluate, but I'm not ready for that in production. 
&gt; 
&gt; 
&gt; On Wed, Dec 5, 2012 at 7:45 PM, Sean Carey  (mailto:ca...@basho.com)&gt; wrote:
&gt; &gt; Ken, 
&gt; &gt; Are your vms on different bare metal? Could they potentially be on the same 
&gt; &gt; bare metal? 
&gt; &gt; 
&gt; &gt; Are you seeing any io contention? 
&gt; &gt; 
&gt; &gt; 
&gt; &gt; Sean Carey
&gt; &gt; @densone
&gt; &gt; 
&gt; &gt; 
&gt; &gt; On Wednesday, December 5, 2012 at 20:41, Ken Perkins wrote:
&gt; &gt; 
&gt; &gt; &gt; Yes, we're thrashing on all of the boxes, due to disk access when looking 
&gt; &gt; &gt; through merge\\_index. It's not noisy neighbors, given how consistent the 
&gt; &gt; &gt; thrashing is. We had a box with a corrupted index (we had to remove 
&gt; &gt; &gt; merge\\_index and rebuild) and that machine instantly went to 0% thrashing. 
&gt; &gt; &gt; So we have a pretty good indication of the source.
&gt; &gt; &gt; 
&gt; &gt; &gt; The cost for 10 8GB VMs is roughly equivalent to 5 16GB ones.
&gt; &gt; &gt; 
&gt; &gt; &gt; Thanks for your input Michael!
&gt; &gt; &gt; 
&gt; &gt; &gt; Ken
&gt; &gt; &gt; 
&gt; &gt; &gt; 
&gt; &gt; &gt; On Wed, Dec 5, 2012 at 4:47 PM, Michael Johnson  &gt; &gt; (mailto:m...@mediatemple.net)&gt; wrote:
&gt; &gt; &gt; &gt; There are a lot of things that go into this, but I would tend to 
&gt; &gt; &gt; &gt; suggest in a hosted VM senario, upping the ram is likely the right 
&gt; &gt; &gt; &gt; solution.
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; You mention thrashing, but what is that thrashing coming from? I 
&gt; &gt; &gt; &gt; assume all the boxes are thrashing and not just one or two of them? Is 
&gt; &gt; &gt; &gt; it due to swapping or is it just the raw disk access? Maybe you 
&gt; &gt; &gt; &gt; logging too aggressively? 
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; Perhaps your are suffering from a bad neighbor effect. If this is the 
&gt; &gt; &gt; &gt; case, increasing the amount of ram will likely put you on a physical 
&gt; &gt; &gt; &gt; host with few customers and thus you would be less likely to have a bad 
&gt; &gt; &gt; &gt; neighbor. 
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; Cost-wise in the VM world, you might be better off adding a few nodes 
&gt; &gt; &gt; &gt; rather than increasing the ram in your existing vm's.
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; But then we are talking VMs and thus it should be fairly painless to 
&gt; &gt; &gt; &gt; experiment. I would try adding ram first and if that doesn't work, add 
&gt; &gt; &gt; &gt; a few nodes. Someone else my have a different opinion, but that is my 
&gt; &gt; &gt; &gt; two cents. 
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; On Wed, Dec 5, 2012 at 4:33 PM, Ken Perkins  &gt; &gt; &gt; (mailto:k...@clipboard.com)&gt; wrote:
&gt; &gt; &gt; &gt; &gt; Hello all,
&gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; &gt; We're seeing enough thrashing and low-memory on our production ring 
&gt; &gt; &gt; &gt; &gt; that we've decided to upgrade our hardware. The real question is 
&gt; &gt; &gt; &gt; &gt; should we scale up or out.
&gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; &gt; Currently our ring is 512 partitions. We know that it's a sub-optimal 
&gt; &gt; &gt; &gt; &gt; size but we can't easily solve that now. We're currently running a 
&gt; &gt; &gt; &gt; &gt; search-heavy app on 5 8GB VMs. I'm debating between moving the VMs up 
&gt; &gt; &gt; &gt; &gt; to 16GB, or adding a few more 8GB VMs. 
&gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; &gt; Some of the talk in #riak has pushed me towards adding more machines 
&gt; &gt; &gt; &gt; &gt; (thus lowering the per node number of partitions) but I wanted to do 
&gt; &gt; &gt; &gt; &gt; a quick sanity check here with folks that it's better than scaling up 
&gt; &gt; &gt; &gt; &gt; my current machines. 
&gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; &gt; Thanks!
&gt; &gt; &gt; &gt; &gt; Ken Perkins
&gt; &gt; &gt; &gt; &gt; clipboard.com (http://clipboard.com)
&gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; &gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; &gt; &gt; &gt; &gt; riak-users mailing list
&gt; &gt; &gt; &gt; &gt; riak-users@lists.basho.com (mailto:riak-users@lists.basho.com)
&gt; &gt; &gt; &gt; &gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; 
&gt; &gt; &gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; &gt; &gt; riak-users mailing list
&gt; &gt; &gt; riak-users@lists.basho.com (mailto:riak-users@lists.basho.com)
&gt; &gt; &gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt; &gt; &gt; 
&gt; &gt; &gt; 
&gt; &gt; &gt; 
&gt; &gt; 
&gt; &gt; 
&gt; 

\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

