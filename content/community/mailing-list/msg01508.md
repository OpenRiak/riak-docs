---
title: "Re: Understanding Riaks rebalancing and handoff behaviour"
description: ""
project: community
lastmod: 2010-11-09T07:30:53-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg01508"
mailinglist_parent_id: "msg01510"
author_name: "Alexander Sicular"
project_section: "mailinglistitem"
sent_date: 2010-11-09T07:30:53-08:00
---


Mainly, I'm of the impression that you should join/leave a cluster one
node at a time.

-Alexander.

On 2010-11-09, Sven Riedel  wrote:
&gt; Hi,
&gt; I'm currently assessing how well riak fits our needs as a large scale data
&gt; store.
&gt;
&gt; In the course of testing riak, I've set up a cluster in Amazons with 6 nodes
&gt; across two EC2 instances (m2.xlarge). After seeing surprisingly a
&gt; surprisingly bad write performance (which I'll write more on in a separate
&gt; post once I've finished my tests), I wanted to migrate the cluster to
&gt; instances with a better IO performance.
&gt;
&gt; Lets call the original EC2 instances A and B. The plan was to migrate the
&gt; cluster to new EC2 instances called C and D. During the following actions no
&gt; other processes were reading/writing from/to the cluster. All instances are
&gt; in the same availability zone.
&gt;
&gt; What I did so far was to tell all riak nodes on B to leave the ring and let
&gt; the ring re-stabilize. One surprising behaviour here was that the riak nodes
&gt; on A suddenly all went into deep sleep mode (process state D) for about 30
&gt; minutes, and all riak-admin status/transfer calls claimed all nodes were
&gt; down when in fact they weren't and were quite busy. But left to themselves
&gt; they sorted everything out in the end.
&gt;
&gt; Then I set up 3 new riak nodes on C and told them to join the cluster.
&gt;
&gt; So far everything went well. riak-admin transfers showed me that both the
&gt; nodes on A and the nodes on C were waiting on/for handoffs. However, the
&gt; handoffs didn't start. I gave the cluster an hour, but no data transfer got
&gt; initiated to the new nodes.
&gt;
&gt; Since I didn't find any way to manually trigger the handoff, I told all the
&gt; nodes on A (riak01, riak02 and riak03) to leave the cluster and after the
&gt; last node on A left the ring, the handoffs started.
&gt; After all the data in riak01 got moved to the nodes on C, the master process
&gt; shut down and the handoff for the remaining data from riak02 and riak03
&gt; stopped. I tried restarting riak01 manually, however riak-admin ringready
&gt; claims that riak01 and riak04 (on C) disagree on the partition owners.
&gt; riak-admin transfers still lists the same amount of partitions awaiting
&gt; handoff as when the the handoff to the nodes on C started.
&gt;
&gt; My current data distribution is as follows (via du -c):
&gt; On A:
&gt; 1780 riak01/data
&gt; 188948 riak02/data
&gt; 3766736 riak03/data
&gt;
&gt; On B:
&gt; 13215908 riak04/data
&gt; 1855584 riak05/data
&gt; 5745076 riak06/data
&gt;
&gt; riak04 and riak05 are awaiting the handoff of 341 partitions, riak06 of 342
&gt; partitions.
&gt;
&gt; The ring\\_creation\\_size is 512, n\\_val for the bucket is 3, w is set to 1.
&gt;
&gt; My questions at this point are:
&gt; 1. What would normally trigger a rebalancing of the nodes?
&gt; 2. Is there a way to manually trigger a rebalancing?
&gt; 3. Did I do anything wrong with the procedure described above to be left in
&gt; the current odd state by riak?
&gt; 4. How would I rectify this situation in a production environment?
&gt;
&gt; Regards,
&gt; Sven
&gt;
&gt; ------------------------------------------
&gt; Scoreloop AG, Brecherspitzstrasse 8, 81541 Munich, Germany,
&gt; www.scoreloop.com
&gt; sven.rie...@scoreloop.com
&gt;
&gt; Sitz der Gesellschaft: München, Registergericht: Amtsgericht München, HRB
&gt; 174805
&gt; Vorstand: Dr. Marc Gumpinger (Vorsitzender), Dominik Westner, Christian van
&gt; der Leeden, Vorsitzender des Aufsichtsrates: Olaf Jacobi
&gt;
&gt;


-- 
Sent from my mobile device

