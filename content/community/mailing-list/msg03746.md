---
title: "Re: speeding up riaksearch precommit indexing"
description: ""
project: community
lastmod: 2011-06-18T16:55:16-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg03746"
mailinglist_parent_id: "msg03745"
author_name: "John D. Rowell"
project_section: "mailinglistitem"
sent_date: 2011-06-18T16:55:16-07:00
---


The "real" queues like HornetQ and others can take care of this without a
single point of failure but it's a pain (in my opinion) to set them up that
way, and usually with all the cluster and failover features active they get
quite slow for writes.We use Redis for this because it's simpler and
lightweight. The problem is that there is no real clustering option for
Redis today, even thought there are some hacks that get close. When we
cannot afford a single point of failure or any downtime, we tend to use
MongoDB for simple queues. It has full cluster support and the performance
is pretty close to what you get with Redis in this use case.

OTOH you could keep it all Riak and setup a separate small cluster with a
RAM backend and use that as a queue, probably with similar performance. The
idea here is that you can scale these clusters (the "queue" and the indexed
production data) independently in response to your load patterns, and have
optimum hardware and I/O specs for the different cluster nodes.

-jd

2011/6/18 Les Mikesell 

&gt; Is there a good way to handle something like this with redundancy all the
&gt; way through? On simple key/value items you could have two readers write the
&gt; same things to riak and let bitcask cleanup eventually discard one, but with
&gt; indexing you probably need to use some sort of failover approach up front.
&gt; Do any of those queue managers handle that without adding their own single
&gt; point of failure? Assuming there are unique identifiers in the items being
&gt; written, you might use the CAS feature of redis to arbitrate writes into its
&gt; queue, but what happens when the redis node fails?
&gt;
&gt; -Les
&gt;
&gt;
&gt;
&gt; On 6/17/11 11:48 PM, John D. Rowell wrote:
&gt;
&gt;&gt; Why not decouple the twitter stream processing from the indexing? More
&gt;&gt; than
&gt;&gt; likely you have a single process consuming the spritzer stream, so you can
&gt;&gt; put
&gt;&gt; the fetched results in a queue (hornetq, beanstalk, or even a simple Redis
&gt;&gt; queue) and then have workers pull from the queue and insert into Riak. You
&gt;&gt; could
&gt;&gt; run one worker per node and thus insert in parallel into all nodes. If you
&gt;&gt; need
&gt;&gt; free CPU (e.g. for searches), just throttle the workers to some sane
&gt;&gt; level. If
&gt;&gt; you see the queue getting bigger, add another Riak node (and thus another
&gt;&gt; local
&gt;&gt; worker).
&gt;&gt;
&gt;&gt; -jd
&gt;&gt;
&gt;&gt; 2011/6/13 Steve Webb &gt;
&gt;&gt;
&gt;&gt;
&gt;&gt; Ok, I've changed my two VMs to each have:
&gt;&gt;
&gt;&gt; 3 CPUs, 1GB ram, 120GB disk
&gt;&gt;
&gt;&gt; I'm ingesting the twitter spritzer stream (about 10-20 tweets per
&gt;&gt; second,
&gt;&gt; approx 2k of data per tweet). One bucket is storing the non-indexed
&gt;&gt; tweets
&gt;&gt; in full. Another bucket is storing the indexed tweet string, id, date
&gt;&gt; and
&gt;&gt; username. A maximum of 20 clients can be hitting the 'cluster' at any
&gt;&gt; one time.
&gt;&gt;
&gt;&gt; I'm using n\\_val=2 so there is replication going on behind the scenes.
&gt;&gt;
&gt;&gt; I'm using a hardware load-balancer to distribute the work amongst the
&gt;&gt; two
&gt;&gt; nodes and now I'm seeing about 75% CPU usage as opposed to 100% on one
&gt;&gt; node
&gt;&gt; and 50% on the replicating-only node.
&gt;&gt;
&gt;&gt; I've monitored the VM over the last few days and it seems to be mostly
&gt;&gt; CPU-bound. The disk I/O is low. The Network I/O is low.
&gt;&gt;
&gt;&gt; Q: Can I change the pre-commit to a post-commit trigger or something
&gt;&gt; perhaps
&gt;&gt; or will that make any difference at all? I'm ok if the tweet stuff
&gt;&gt; doesn't
&gt;&gt; get indexed immediately and there's a slight lag in indexing if it
&gt;&gt; saves on CPU.
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\*\\*\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt; riak-users mailing list
&gt;&gt; riak-users@lists.basho.com
&gt;&gt; http://lists.basho.com/\\*\\*mailman/listinfo/riak-users\\_\\*\\*lists.basho.com
&gt;&gt;
&gt;
&gt;
&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\*\\*\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/\\*\\*mailman/listinfo/riak-users\\_\\*\\*lists.basho.com
&gt;
\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

