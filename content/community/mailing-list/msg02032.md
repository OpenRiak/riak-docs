---
title: "Bucket size limitations."
description: ""
project: community
lastmod: 2011-01-20T09:59:05-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg02032"
author_name: "Christopher Rueber"
project_section: "mailinglistitem"
sent_date: 2011-01-20T09:59:05-08:00
---


This seems like a question that would be answered in some of the docs, but I
can't find the details...

What kind of upper limitations are on large buckets? Millions of entries?
Billions? Directly correlating to the amount of disk space available to it?
Is there any kind of performance degradation of using one massive bucket,
over cutting things down in to more digestable chunks and splitting them in
to different buckets (which pulls more from a relational database mindset)?

Clearly there will be a map/reduce implication of having to iterate over
millions of entries, but is there an appreciable difference in the
read/write speed that Riak performs at, when its buckets get quite large?
