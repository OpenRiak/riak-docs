---
title: "Re: High volume data series storage and queries"
description: ""
project: community
lastmod: 2011-08-11T07:49:24-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg04345"
mailinglist_parent_id: "msg04314"
author_name: "Paul O"
project_section: "mailinglistitem"
sent_date: 2011-08-11T07:49:24-07:00
---


Hi Ciprian, first of all thanks a lot for the detailed explanations
and references. A lot of food for thought here and I'm not sure I grok
it all just yet. I'll inline my comments.

On Wed, Aug 10, 2011 at 9:14 AM, Ciprian Dorin Craciun
 wrote:
[...]
&gt;    So the first problem I see with any solution would be the 1
&gt; billion records. I would try to find an alternative solution for
&gt; "historical-data", like dumping it in flat files, and just keeping in
&gt; the database recent data. By recent I don't have a specific timeframe
&gt; in mind, but whatever you determine it works in your particular case.

Well this is kind of the crux of the problem, in the end. Regardless
of how I store the older data, I still need a predictable query time
for it. So if I dump it in flat files, for instance bundles as
described in my initial solution that I pre-index then I'm by
definition back to the starting point. And if that (1 index check + 3
bundles read + quick range chop off) is quick enough, then everything
else is caching for more recent info. I'm not saying that to discount
the solution, just saying that I can't ignore the historical-data
(actually I might have to degrade historical data performance in the
end, I'm just saying that if the degraded solution is good enough, it
could serve for recent info, too.)


&gt;    Now about the solution: I'm not proposing any batching or
&gt; partitioning based on either timestamp or source; neither as
&gt; individual files on the file system or stored inside the database as
&gt; opaque values.
&gt;
&gt;    Instead I suggest to just put every data point in the same "table"
&gt; or "database" as described below:
&gt;    \\* the first requirement for the embedded database is to support
&gt; lexicographically sorted keys and cursors which can be pointed to
&gt; start from a particular (or greater) key;
&gt;    \\* the key of the record would be the binary representation of the
&gt; source id padded with 0's to an exact length, concatenated with the
&gt; timestamp which follows the same rules;
&gt;
&gt;    Now about your concerns about executing range queries without
&gt; considering the whole data is already solved by the database which
&gt; doesn't need to look at the whole data but only search for the first
&gt; data point matching your query. Actually behind scenes it does
&gt; something similar -- but even better -- than what you're proposing
&gt; with the MaxN records opaque value.
&gt;
&gt;    Furthermore, to back my claims I've put on Github the old code
&gt; which I've used to benchmark different databases -- Riak or other
&gt; key-value stores are not included except BerkeleyDB, but I've included
&gt; others like Hypertable which could be a good choice -- at the
&gt; following address (you can easily extend the "framework" to include
&gt; new backends):
&gt;        https://github.com/cipriancraciun/sds-benchmark
&gt;
&gt;    Also there is a "so-called" report I've put up at that time in the
&gt; same place at -- the second link is the content of the wkipage and
&gt; there are also images which GitHub doesn't display in the page:
&gt;        https://github.com/cipriancraciun/sds-benchmark/tree/master/results
&gt;        
&gt; https://github.com/cipriancraciun/sds-benchmark/blob/master/results/results.mediawiki
&gt;
&gt;    For the record we've tested up to 100m records on some
&gt; data-stores, but on some other (like PostgreSQL and MySQL we've
&gt; stopped at 10 million as the insertion rate dropped tremendously).
&gt;
&gt;    My conclusion from this experiment was: any database which is
&gt; backed by a tree-like data structure (almost all use B trees or
&gt; derivate) will get to a grinding halt in insert speed if the
&gt; clustering keys (clientid + timpstamp in this case) don't exhibit any
&gt; kind of locality (as is your case). See the quote from my report:
&gt; ~~~~
&gt; \\* this section applies to Postgres and SQLite, as MonetDB behaved Ok;
&gt; \\* it seems that the initial insert speed is good for the first couple
&gt; million records;
&gt; \\* as the data accumulates and new inserts are done, the indices start
&gt; to be rewritten, and consume the entire disk bandwidth; (the initial
&gt; good speed is due the fact that the indices fit into the RAM memory;)
&gt; \\* if the inserts are done without any indices defined, the insert
&gt; speed is incredible (600k in the case of Postgres), but the scan speed
&gt; is under 100;
&gt; \\* maybe, it is possible, that this behavior is specific to any
&gt; database which uses trees (like BerkeleyDB?);
&gt; ~~~~
&gt;
&gt;    Hope this helps you,

This is very good information and thanks a lot for this, I was not
expecting such in depth benchmarks and am wondering why yours didn't
turn up in my Google searches.

However, I'm still a bit in the dark regarding some aspects of storing
everything in one big database. Are these records stored in an "append
only" fashion? If they are, then how can the range query be done
without considering potentially a huge number of records? If they are
not "append only" then the DB might have to shuffle data around which
would degrade performance or store an index.

Anyway, I'll read some more on this, it's probably just my ignorance
regarding how those databases work, but if the DBs can be append-only
then I need one DB per data source and even then I have to contend
with data insertions in the past (a requirement which I hope I made
explicit in previous emails.) And if I have to use multiple DBs are
these solutions able to juggle multiple DBs, open/close them quickly,
etc.?

I hope I'm not too off-list-topic with this and hope at least some
other people find the discussion useful.

Regards,

Paul

