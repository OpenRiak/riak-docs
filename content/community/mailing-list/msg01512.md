---
title: "Re: Slow performance using linkwalk, help wanted"
description: ""
project: community
lastmod: 2010-11-09T07:18:43-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg01512"
mailinglist_parent_id: "msg01502"
author_name: "Kevin Smith"
project_section: "mailinglistitem"
sent_date: 2010-11-09T07:18:43-08:00
---

On Nov 9, 2010, at 5:01 AM, Karsten Thygesen wrote:

&gt; Hi
&gt; 
&gt; OK, we will use a larger ringsize next time and will consider a data reload.
&gt; 
&gt; Regarding the metrics: the servers are dedicated to Riak use and it not used 
&gt; for anything else. They are new HP servers with 8 cores each and 4x146GB 10K 
&gt; RPM SAS disks in a contatenated mirror setup. We use Solaris with ZFS as 
&gt; filesystem and I have turned off atime update in the data partition.
&gt; 
&gt; The pool is built as such:
&gt; 
&gt; pool: pool01
&gt; state: ONLINE
&gt; scrub: scrub completed after 0h0m with 0 errors on Tue Oct 26 21:25:05 2010
&gt; config:
&gt; 
&gt; NAME STATE READ WRITE CKSUM
&gt; pool01 ONLINE 0 0 0
&gt; mirror-0 ONLINE 0 0 0
&gt; c0t0d0s7 ONLINE 0 0 0
&gt; c0t1d0s7 ONLINE 0 0 0
&gt; mirror-1 ONLINE 0 0 0
&gt; c0t2d0 ONLINE 0 0 0
&gt; c0t3d0 ONLINE 0 0 0
&gt; 
&gt; errors: No known data errors
&gt; 
&gt; so it is as fast as possible. 
&gt; 
&gt; However - we use the ZFS default blocksize, which is 128Kb - is that optimal 
&gt; with bitcask as backend? It is rather large, but what is optimal with bitcask?

I don't have much experience tuning Solaris or ZFS for Riak. This is a question 
best asked of Ryan and I will make sure he sees this.

&gt; 
&gt; The cluster is 4 servers with gigabit connection located in the same 
&gt; datacenter on the same switch. The loadbalancer is a Zeus ZTM, which does 
&gt; quote a few http optimizations including extended reuse of http connections 
&gt; and we usually see far better response times using the loadbalancer than 
&gt; using a node directly.

Hmmm. Can you share what the performance times are like for direct cluster 
access?

&gt; 
&gt; When we run the test, each riak node is only about 100% cpu loaded (which on 
&gt; solaris means, that it only uses one of the 8 cores). We have seen spikes in 
&gt; the 160% area, but everything below 800% is not cpu bound. So all-in-all, the 
&gt; cpuload is between 5 and 10%.

Can you send me the code you're using for the performance test? I'd like to run 
the exact code on my test hardware and see if that reveals anything.

Also, low CPU usage might indicate you are IO bound. Do you know if Riak 
processes are spending much time waiting for IO to complete?

--Kevin

