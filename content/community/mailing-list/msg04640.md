---
title: "Re: Riak Clustering Changes in 1.0"
description: ""
project: community
lastmod: 2011-09-09T15:02:10-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg04640"
mailinglist_parent_id: "msg04639"
author_name: "Mark Phillips"
project_section: "mailinglistitem"
sent_date: 2011-09-09T15:02:10-07:00
---


Extremely relevant to this conversation:

http://blog.basho.com/2011/09/09/Riak-Cluster-Membership-Overview/

Buy Joe a beer next time you see him :)

Mark

On Fri, Sep 9, 2011 at 2:37 PM, Jeff Pollard  wrote:
&gt;&gt; Data is first transferred to new partition owners before handing over
&gt;&gt; partition ownership. This change fixes numerous bugs, such
&gt;&gt; as 404s/not\\_founds during ownership changes. The Ring/Pending columns
&gt;&gt; in [riak admin member\\_status] visualize this at a high-level, and the full
&gt;&gt; transfer status in [riak-admin ring\\_status] provide additional insight.
&gt;
&gt; At present (0.14.x series) my understanding is that when a new node is added
&gt; to the cluster, it claims a portion of the ring and services requests for
&gt; that portion before all the data is actually present on the node.  Is that
&gt; correct?  If so, as long as you're able to meet the R value of a read (i.e.
&gt; R=2, N=3) by servicing reads from nodes with replicas of the same data you
&gt; shouldn't see any 404s.  Is that also correct?
&gt;
&gt; I should add that we're planning on adding our first node to our production
&gt; cluster soon and wanted to make sure we had our story straight :)  That
&gt; said, I'm very excited to see all the clustering improvements in 1.0 and
&gt; hoping we can upgrade before adding a new node.
&gt;
&gt; On Fri, Sep 9, 2011 at 3:10 AM, Jens Rantil  wrote:
&gt;&gt;
&gt;&gt; Thanks for very well written answer. I appreciate it, mate.
&gt;&gt;
&gt;&gt; Jens
&gt;&gt;
&gt;&gt; -----Ursprungligt meddelande-----
&gt;&gt; Från: Joseph Blomstedt [mailto:j...@basho.com]
&gt;&gt; Skickat: den 8 september 2011 17:42
&gt;&gt; Till: Jens Rantil
&gt;&gt; Kopia: riak-users@lists.basho.com
&gt;&gt; Ämne: Re: Riak Clustering Changes in 1.0
&gt;&gt;
&gt;&gt; &gt; Out of curiousity, what was the reason for the 'join' command
&gt;&gt; &gt; behaviour to change?
&gt;&gt;
&gt;&gt; 1. Existing bugs/limitations. For example, joining two entire clusters
&gt;&gt; together was not an entirely safe operation. In some cases, the newly formed
&gt;&gt; cluster would not correctly converge, leaving the ring/cluster in flux.
&gt;&gt; Likewise, we realized that many users were often joining two clusters
&gt;&gt; together by accident and would prefer additional safety. In particular,
&gt;&gt; joining two clusters together with overlapping data but no common vector
&gt;&gt; clock relationship could result in data loss as unintended siblings were
&gt;&gt; reconciled.
&gt;&gt;
&gt;&gt; 2. It was necessary consequence of how the new cluster code works. In the
&gt;&gt; new cluster, the cluster state / ring is only ever mutated by a single node
&gt;&gt; at a time. This is done by having a cluster-wide claimant, as mentioned in
&gt;&gt; my original email. Given the claimant approach, all cluster state / ring
&gt;&gt; changes are totally ordered. When a new node joins an existing cluster, it
&gt;&gt; throws away it's existing ring and replaces it with a copy of the ring from
&gt;&gt; the target cluster, thus joining into the same cluster history. If you were
&gt;&gt; to join two clusters together, we would need to deterministically merge two
&gt;&gt; independent cluster histories and elect a single new claimant for the new
&gt;&gt; cluster. This is easy in cases where there are no node failures or
&gt;&gt; net-splits during joining, but less trivial when there are errors. The
&gt;&gt; entire new cluster code was heavily modeled before implementation, and in
&gt;&gt; the modeling work several corner cases related to failures were found that
&gt;&gt; were hard to address in a cluster/cluster join but easy to fix in a
&gt;&gt; node/cluster join. Thus, I went with the simple and correct approach.
&gt;&gt;
&gt;&gt; -Joe
&gt;&gt;
&gt;&gt; --
&gt;&gt; Joseph Blomstedt 
&gt;&gt; Software Engineer
&gt;&gt; Basho Technologies, Inc.
&gt;&gt; http://www.basho.com/
&gt;&gt;
&gt;&gt; On Thu, Sep 8, 2011 at 5:19 AM, Jens Rantil 
&gt;&gt; wrote:
&gt;&gt; &gt; Out of curiousity, what was the reason for the 'join' command
&gt;&gt; &gt; behaviour to change?
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt; &gt; Regards,
&gt;&gt; &gt;
&gt;&gt; &gt; Jens
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt; &gt; -----------------------------------------------------------
&gt;&gt; &gt;
&gt;&gt; &gt; Date: Wed, 7 Sep 2011 18:12:40 -0600
&gt;&gt; &gt;
&gt;&gt; &gt; From: Joseph Blomstedt 
&gt;&gt; &gt;
&gt;&gt; &gt; To: riak-users Users 
&gt;&gt; &gt;
&gt;&gt; &gt; Subject: Riak Clustering Changes in 1.0
&gt;&gt; &gt;
&gt;&gt; &gt; Message-ID:
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt; &gt; 
&gt;&gt; &gt;
&gt;&gt; &gt; Content-Type: text/plain; charset=ISO-8859-1
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt; &gt; Given that 1.0 prerelease packages are now available, I wanted to
&gt;&gt; &gt;
&gt;&gt; &gt; mention some changes to Riak's clustering capabilities in 1.0. In
&gt;&gt; &gt;
&gt;&gt; &gt; particular, there are some subtle semantic differences in the
&gt;&gt; &gt;
&gt;&gt; &gt; riak-admin commands. More complete docs will be updated in the near
&gt;&gt; &gt;
&gt;&gt; &gt; future, but I hope a quick email suffices for now.
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt; &gt; [nodeB/riak-admin join nodeA] is now strictly one-way. It joins nodeB
&gt;&gt; &gt;
&gt;&gt; &gt; to the cluster that nodeA is a member of. This is semantically
&gt;&gt; &gt;
&gt;&gt; &gt; different than pre-1.0 Riak in which join essentially joined clusters
&gt;&gt; &gt;
&gt;&gt; &gt; together rather than joined a node to a cluster. As part of this
&gt;&gt; &gt;
&gt;&gt; &gt; change, the joining node (nodeB in this case) must be a singleton
&gt;&gt; &gt;
&gt;&gt; &gt; (1-node) cluster.
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt; &gt; In pre-1.0, leave and remove were essentially the same operation, with
&gt;&gt; &gt;
&gt;&gt; &gt; leave just being an alias for 'remove this-node'. This has changed.
&gt;&gt; &gt;
&gt;&gt; &gt; Leave and remove are now very different operations.
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt; &gt; [nodeB/riak-admin leave] is the only safe way to have a node leave the
&gt;&gt; &gt;
&gt;&gt; &gt; cluster, and it must be executed by the node that you want to remove.
&gt;&gt; &gt;
&gt;&gt; &gt; In this case, nodeB will start leaving the cluster, and will not leave
&gt;&gt; &gt;
&gt;&gt; &gt; the cluster until after it has handed off all its data. Even if nodeB
&gt;&gt; &gt;
&gt;&gt; &gt; is restarted (crashed/shutdown/whatever), it will remain in the leave
&gt;&gt; &gt;
&gt;&gt; &gt; state and continue handing off partitions until done. After handoff,
&gt;&gt; &gt;
&gt;&gt; &gt; it will leave the cluster, and eventually shutdown.
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt; &gt; [nodeA/riak-admin remove nodeB] immediately removes nodeB from the
&gt;&gt; &gt;
&gt;&gt; &gt; cluster, without handing off its data. All replicas held by nodeB are
&gt;&gt; &gt;
&gt;&gt; &gt; therefore lost, and will need to be re-generated through read-repair.
&gt;&gt; &gt;
&gt;&gt; &gt; Use this command carefully. It's intended for nodes that are
&gt;&gt; &gt;
&gt;&gt; &gt; permanently unrecoverable and therefore for which handoff doesn't make
&gt;&gt; &gt;
&gt;&gt; &gt; sense. By the final 1.0 release, this command may be renamed
&gt;&gt; &gt;
&gt;&gt; &gt; "force-remove" just to make the distinction clear.
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt; &gt; There are now two new commands that provide additional insight into
&gt;&gt; &gt;
&gt;&gt; &gt; the cluster. [riak-admin member\\_status] and [riak-admin ring\\_status].
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt; &gt; Underneath, the clustering protocol has been mostly re-written. The
&gt;&gt; &gt;
&gt;&gt; &gt; new approach has the following advantages:
&gt;&gt; &gt;
&gt;&gt; &gt; 1. It is no longer necessary to wait on [riak-admin ringready] in
&gt;&gt; &gt;
&gt;&gt; &gt; between adding/removing nodes from the cluster, and adding/removing is
&gt;&gt; &gt;
&gt;&gt; &gt; also much more sound/graceful. Starting up 16 nodes and issuing
&gt;&gt; &gt;
&gt;&gt; &gt; [nodeX: riak-admin join node1] for X=1:16 should just work.
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt; &gt; 2. Data is first transferred to new partition owners before handing
&gt;&gt; &gt;
&gt;&gt; &gt; over partition ownership. This change fixes numerous bugs, such as
&gt;&gt; &gt;
&gt;&gt; &gt; 404s/not\\_founds during ownership changes. The Ring/Pending columns in
&gt;&gt; &gt;
&gt;&gt; &gt; [riak-admin member\\_status] visualize this at a high-level, and the
&gt;&gt; &gt;
&gt;&gt; &gt; full transfer status in [riak-admin ring\\_status] provide additional
&gt;&gt; &gt;
&gt;&gt; &gt; insight.
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt; &gt; 3. All partition ownership decisions are now made by a single node in
&gt;&gt; &gt;
&gt;&gt; &gt; the cluster (the claimant). Any node can be the claimant, and the duty
&gt;&gt; &gt;
&gt;&gt; &gt; is automatically taken over if the previous claimant is removed from
&gt;&gt; &gt;
&gt;&gt; &gt; the cluster. [riak-admin member\\_status] will list the current
&gt;&gt; &gt;
&gt;&gt; &gt; claimant.
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt; &gt; 4. Handoff related to ownership changes can now occur under load;
&gt;&gt; &gt;
&gt;&gt; &gt; hinted handoff still only occurs when a vnode is inactive. This change
&gt;&gt; &gt;
&gt;&gt; &gt; allows a cluster to scale up/down under load, although this needs to
&gt;&gt; &gt;
&gt;&gt; &gt; be further benchmarked and tuned before 1.0.
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt; &gt; To support all of the above, a new limitation has been introduced.
&gt;&gt; &gt;
&gt;&gt; &gt; Cluster changes (member addition/removal, ring rebalance, etc) can
&gt;&gt; &gt;
&gt;&gt; &gt; only occur when all nodes are up and reachable. [riak-admin
&gt;&gt; &gt;
&gt;&gt; &gt; ring\\_status] will complain when this is not the case. If a node is
&gt;&gt; &gt;
&gt;&gt; &gt; down, you must issue [riak-admin down ] to mark the node as
&gt;&gt; &gt;
&gt;&gt; &gt; down, and the remaining nodes will then proceed to converge as usual.
&gt;&gt; &gt;
&gt;&gt; &gt; Once the down node comes back online, it will automatically
&gt;&gt; &gt;
&gt;&gt; &gt; re-integrate into the cluster. However, there is nothing preventing
&gt;&gt; &gt;
&gt;&gt; &gt; client requests being served by a down node before it re-integrates.
&gt;&gt; &gt;
&gt;&gt; &gt; Before issuing [down ], make sure to update your load balancers
&gt;&gt; &gt;
&gt;&gt; &gt; / connection pools to not include this node. Future releases of Riak
&gt;&gt; &gt;
&gt;&gt; &gt; may make offlining a node an automatic operation, but it's a
&gt;&gt; &gt;
&gt;&gt; &gt; user-initiated action in 1.0.
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt; &gt; -Joe
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt; &gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt;&gt; &gt; riak-users mailing list
&gt;&gt; &gt; riak-users@lists.basho.com
&gt;&gt; &gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;
&gt;
