---
title: "Re: How do I improve Level DB performance?"
description: ""
project: community
lastmod: 2012-05-11T08:00:39-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg07433"
mailinglist_parent_id: "msg07428"
author_name: "Tim Haines"
project_section: "mailinglistitem"
sent_date: 2012-05-11T08:00:39-07:00
---


David,

I ran the benchmark again for 9 hours overnight, just doing puts.
 Performance fell steadily from 400 puts/s to 250 puts/s.

Graph: http://twitpic.com/9jtjmu/full

Cheers,

Tim.

On Thu, May 10, 2012 at 3:01 PM, David Smith  wrote:

&gt; On Thu, May 10, 2012 at 2:33 PM, Tim Haines  wrote:
&gt;
&gt; &gt; I've set up a new cluster, and have been doing pre-deployment benchmarks
&gt; on
&gt; &gt; it. The benchmark I was running slowly sunk from 1000 TPS to 250 TPS over
&gt; &gt; the course of the single 8 hour benchmark doing 1 read+1 update using 1k
&gt; &gt; values. I'm wondering if anyone might have suggestions on how I can
&gt; improve
&gt; &gt; this.
&gt;
&gt; Generally, this suggests that you are becoming seek-time bound. The
&gt; test config, as specified, will generate a pretty huge number of
&gt; not\\_founds which are (currently) crazy expensive w/ LevelDB,
&gt; particularly as the dataset grows.
&gt;
&gt; Assuming you start with an empty database, a sample of this test will
&gt; generate operations like so:
&gt;
&gt; Key 1000 - get -&gt; not\\_found
&gt; Key 1001 - update -&gt; not\\_found + write
&gt; Key 1002 - get -&gt; not\\_found
&gt; etc..
&gt;
&gt; I.e. the leveldb cache never gets a chance to be useful, because
&gt; you're always writing new values and the cost of writing each new
&gt; value goes up, since you have to thrash the cache to determine if
&gt; you're ever seen the key that doesn't exist. :)
&gt;
&gt; The root problem here is going to be the key\\_generator --
&gt; partitioned\\_sequential\\_int will just run through all the ints in order
&gt; and never revisit a key.
&gt;
&gt;
&gt; &gt; {write\\_buffer\\_size, 16777216},
&gt; &gt; {max\\_open\\_files, 100},
&gt; &gt; {block\\_size, 262144},
&gt; &gt; {cache\\_size, 168430088}
&gt;
&gt; I strongly recommend not changing write\\_buffer\\_size; it can have
&gt; unexpected latency side-effects when LevelDB compaction occurs.
&gt; Smaller == more predictable.
&gt;
&gt; Does that help?
&gt;
&gt; D.
&gt;
&gt; --
&gt; Dave Smith
&gt; VP, Engineering
&gt; Basho Technologies, Inc.
&gt; diz...@basho.com
&gt;
\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

