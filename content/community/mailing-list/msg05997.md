---
title: "Re: standby cluster experiment"
description: ""
project: community
lastmod: 2011-12-19T12:05:15-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg05997"
mailinglist_parent_id: "msg05907"
author_name: "Jeremy Raymond"
project_section: "mailinglistitem"
sent_date: 2011-12-19T12:05:15-08:00
---


Hi John,

I'm curious if you ever figured out what was going on?

--
Jeremy


On Fri, Dec 9, 2011 at 2:53 PM, John Loehrer wrote:

&gt; I am currently evaluating riak. I'd like to be able to do periodic
&gt; snapshots of /var/lib/riak using LVM without stopping the node. According
&gt; to a response on this ML you should be able to copy the data directory for
&gt; eleveldb backend.
&gt;
&gt; http://comments.gmane.org/gmane.comp.db.riak.user/5202
&gt;
&gt;
&gt; If I cycle through each node and do `riak stop` before taking a snapshot
&gt; everything works fine.
&gt; But if I don't shut down the node before copying, I run into problems.
&gt; Since I access the http interface of the cluster through an haproxy
&gt; load-balancer, once the node turns off it is taken out of the pool almost
&gt; immediately. But for a millisecond or two before haproxy detects the node
&gt; is down there might be some bad responses. I can live with it and build
&gt; better retries into my client, but would rather avoid it if I can.
&gt;
&gt; More details below.
&gt;
&gt; Thanks for help!
&gt;
&gt; ~ John Loehrer
&gt; Gaia Interactive INC
&gt;
&gt;
&gt; DETAILS
&gt; ------------------
&gt; I am playing with the idea of being able to bring up an standby cluster on
&gt; an alternate port on the same server pointing at an hourly snapshot of my
&gt; choosing, so that I can go back in time and review the data for recovery
&gt; and repair purposes.
&gt;
&gt; Here's what I have so far.
&gt;
&gt; I have a small cluster of 4 nodes on centos 5.4 using the eleveldb backend
&gt; so I can take advantage of 2i (very cool feature, btw).
&gt;
&gt; Steps for installation:
&gt;
&gt;
&gt; ----
&gt; # install the riak rpm ...
&gt; yum install riak-1.0.2-1.el5.x86\\_64.rpm
&gt;
&gt; # get the ip address out of ifconfig
&gt; IPADDR=`ifconfig eth0 | grep "inet addr" | awk '{ print $2 }' | awk 'BEGIN
&gt; { FS=":" } { print $2 }'`
&gt;
&gt; # replace the loopback ip address in app.config and vm.args with the
&gt; machine's ip
&gt; perl -pi -e s/127.0.0.1/$IPADDR/g /etc/riak/\\*
&gt;
&gt; # change the storage backend to eleveldb
&gt; perl -pi -e 's/riak\\_kv\\_bitcask\\_backend/riak\\_kv\\_eleveldb\\_backend/g'
&gt; /etc/riak/app.config
&gt; ----
&gt;
&gt; We also mount an lvm partition at /var/lib/riak so we can snapshot the
&gt; data directory and back it up using rsnapshot once per hour. It uses
&gt; hardlinks on all the files from the initial snapshot of the data making for
&gt; very efficient storage. The append-only storage approach of the leveldb and
&gt; bitcask backends mean that once a file is closed it is immutable. rsnapshot
&gt; only has to rsync over files that have changed since the previous snapshot.
&gt; Hourly snapshots of take up only a little bit more storage space as the
&gt; original even if i populate the cluster with hundreds of millions of keys
&gt; over the course of a 24 hour period. The backup operation takes only a few
&gt; seconds even for 50GB of data. Now i can copy the data in the hourly
&gt; snapshot directory to my standby riak node, reip, and start up a standby
&gt; cluster on the same machines. pointing to an hourly snapshot and starting
&gt; up the node takes only a second or two as well.
&gt;
&gt; Steps for creating the standby node on the same machine:
&gt;
&gt; ----
&gt;
&gt; # make the root directory of the standby node in the snapshots directory
&gt; # so that we can hard-link to the hourly snapshots dir for a which restore.
&gt; mkdir /.snapshots/riak-standby
&gt;
&gt; # create a handy symlink for the standby node root dir ...
&gt; # we'll use /riak-standby from now on.
&gt; ln -s /.snapshots/riak-standby /riak-standby
&gt;
&gt; # create the default directory structure
&gt; mkdir -p /riak-standby/bin/
&gt; mkdir -p /riak-standby/etc/
&gt; mkdir -p /riak-standby/data
&gt;
&gt; # we are going to use the same libraries, so symlink that in place.
&gt; ln -s /usr/lib64/riak/\\* /riak-standby/
&gt;
&gt; # copy the app.config and vm.args files from the live node
&gt; cp ~/etc/riak/app.config /riak-standby/etc/app.config
&gt; cp ~/etc/riak/vm.args /riak-standby/etc/vm.args
&gt;
&gt; # now, we need to make the app.config file work for the standby node.
&gt; # change /var/lib/riak to ./data
&gt; perl -pi -e 's/\\/var\\/lib\\/riak/.\\/data/g' /riak-standby/etc/app.config
&gt;
&gt; # change /usr/sbin to ./bin
&gt; perl -pi -e 's/\\/usr\\/sbin/.\\/bin/g' /riak-standby/etc/app.config
&gt;
&gt; # change /usr/lib64/riak to ./lib
&gt; perl -pi -e 's/\\/usr\\/lib64\\/riak/.\\/lib/g' /riak-standby/etc/app.config
&gt;
&gt; # change /var/log/riak to ./log
&gt; perl -pi -e 's/\\/var\\/log\\/riak/.\\/log/g' /riak-standby/etc/app.config
&gt;
&gt; # change all the ports from 80\\*\\* to 81\\*\\*
&gt; perl -pi -e 's/80/81/g' /riak-standby/etc/app.config
&gt;
&gt; # change the cookie and node names in vm.args
&gt; perl -pi -e 's/riak@/stby@/g' /riak-standby/etc/app.config
&gt; perl -pi -e 's/setcookie riak/setcookie stby/g'
&gt; /riak-standby/etc/app.config
&gt;
&gt; # fix any permission issues.
&gt; chown -R riak:riak /.snapshots/riak-standby
&gt;
&gt; The riak script in /riak-standby/bin/riak is almost the same as the
&gt; default one installed in /usr/sbin/riak:
&gt;
&gt; diff /usr/sbin/riak /riak-standby/bin/riak
&gt; 3a4
&gt; &gt; ## MANAGED BY PUPPET.
&gt; 5c6
&gt; &lt; RUNNER\\_SCRIPT\\_DIR=/usr/sbin
&gt; ---
&gt; &gt; RUNNER\\_SCRIPT\\_DIR=$(cd ${0%/\\*} && pwd)
&gt; 8,11c9,12
&gt; &lt; RUNNER\\_BASE\\_DIR=/usr/lib64/riak
&gt; &lt; RUNNER\\_ETC\\_DIR=/etc/riak
&gt; &lt; RUNNER\\_LOG\\_DIR=/var/log/riak
&gt; &lt; PIPE\\_DIR=/var/run/riak/
&gt; ---
&gt; &gt; RUNNER\\_BASE\\_DIR=${RUNNER\\_SCRIPT\\_DIR%/\\*}
&gt; &gt; RUNNER\\_ETC\\_DIR=$RUNNER\\_BASE\\_DIR/etc
&gt; &gt; RUNNER\\_LOG\\_DIR=$RUNNER\\_BASE\\_DIR/log
&gt; &gt; PIPE\\_DIR=/tmp/$RUNNER\\_BASE\\_DIR/
&gt; 13c14
&gt; &lt; PLATFORM\\_DATA\\_DIR=/var/lib/riak
&gt; ---
&gt; &gt; PLATFORM\\_DATA\\_DIR=./data
&gt;
&gt;
&gt; Same is true of the riak-admin script for the standby node:
&gt;
&gt; diff /usr/sbin/riak-admin /riak-standby/bin/riak-admin
&gt; 1a2
&gt; &gt; ## MANAGED BY PUPPET.
&gt; 3c4
&gt; &lt; RUNNER\\_SCRIPT\\_DIR=/usr/sbin
&gt; ---
&gt; &gt; RUNNER\\_SCRIPT\\_DIR=$(cd ${0%/\\*} && pwd)
&gt; 6,8c7,9
&gt; &lt; RUNNER\\_BASE\\_DIR=/usr/lib64/riak
&gt; &lt; RUNNER\\_ETC\\_DIR=/etc/riak
&gt; &lt; RUNNER\\_LOG\\_DIR=/var/log/riak
&gt; ---
&gt; &gt; RUNNER\\_BASE\\_DIR=${RUNNER\\_SCRIPT\\_DIR%/\\*}
&gt; &gt; RUNNER\\_ETC\\_DIR=$RUNNER\\_BASE\\_DIR/etc
&gt; &gt; RUNNER\\_LOG\\_DIR=$RUNNER\\_BASE\\_DIR/log
&gt;
&gt;
&gt; After that, i expected to be able to just copy the data from snapshots,
&gt; reip, and start up my standby cluster.
&gt;
&gt;
&gt; rm -rf /riak-standby/data && cp -al /.snapshots/hourly.0/riak/
&gt; /riak-standby/data
&gt; /riak-standby/bin/riak-admin reip riak@ stby@
&gt; /riak-standby/bin/riak-admin reip riak@ stby@
&gt; /riak-standby/bin/riak-admin reip riak@ stby@
&gt; /riak-standby/bin/riak-admin reip riak@ stby@
&gt; /riak-standby/bin/riak start
&gt;
&gt;
&gt; But when I did, /riak-standby/bin/riak-admin ring\\_status showed the
&gt; claimant as riak@ not stby@ as I expected.
&gt;
&gt; Instead of doing reip, i did a binary safe replacement of riak@ with
&gt; stby@:
&gt;
&gt; perl -pi -e 's/riak@/stby@/g'
&gt; /riak-standby/data/ring/riak\\_core\\_ring.default.\\*
&gt;
&gt; When the nodes start up, the claimant looks correct and all the nodes join
&gt; together just fine.
&gt;
&gt; But I still have the problem where the data directory fills up even though
&gt; nothing is being written actively to the standby cluster. I left it alone
&gt; for 5 or 6 hours and it eventually filled up an entire TB of storage.
&gt;
&gt; I noticed that riak-admin transfers starts off showing waiting to hand off
&gt; 1 partition.
&gt;
&gt; /riak-standby/bin/riak-admin transfers
&gt; 'stby@' waiting to handoff 1 partitions
&gt;
&gt; This usually clears up after a minute or so. Not sure it if is related.
&gt;
&gt;
&gt; No clues in the console log. They all look something like:
&gt;
&gt; 2011-12-09 19:10:33.371 [info] &lt;0.7.0&gt; Application bitcask started on node
&gt; 'stby@192.168.3.94'
&gt; 2011-12-09 19:10:33.388 [info] &lt;0.7.0&gt; Application riak\\_kv started on node
&gt; 'stby@192.168.3.94'
&gt; 2011-12-09 19:10:33.388 [info] &lt;0.7.0&gt; Application skerl started on node '
&gt; stby@192.168.3.94'
&gt; 2011-12-09 19:10:33.391 [info] &lt;0.7.0&gt; Application luwak started on node '
&gt; stby@192.168.3.94'
&gt; 2011-12-09 19:10:33.402 [info] &lt;0.7.0&gt; Application merge\\_index started on
&gt; node 'stby@192.168.3.94'
&gt; 2011-12-09 19:10:33.405 [info] &lt;0.7.0&gt; Application riak\\_search started on
&gt; node 'stby@192.168.3.94'
&gt; 2011-12-09 19:10:33.405 [info] &lt;0.7.0&gt; Application basho\\_stats started on
&gt; node 'stby@192.168.3.94'
&gt; 2011-12-09 19:10:33.419 [info] &lt;0.7.0&gt; Application runtime\\_tools started
&gt; on node 'stby@192.168.3.94'
&gt; 2011-12-09 19:10:33.419 [info] &lt;0.7.0&gt; Application public\\_key started on
&gt; node 'stby@192.168.3.94'
&gt; 2011-12-09 19:10:33.447 [info] &lt;0.7.0&gt; Application ssl started on node '
&gt; stby@192.168.3.94'
&gt;
&gt;
&gt;
&gt; If I turn off the node before taking the snapshot everything works fine.
&gt;
&gt; /etc/init.d/riak stop
&gt; .... do backup here
&gt; /etc/init.d/riak start
&gt;
&gt;
&gt; But the standby data directory starts filling up at the rate of about 500
&gt; MB a second on some of the nodes if I do a copy without first stopping
&gt; riak. I know this is not a supported approach, but I was curious if someone
&gt; might be able to shed some light on what might be happening.
&gt;
&gt;
&gt; Ideas?
&gt;
&gt;
&gt; Thanks for any insight.
&gt;
&gt;
&gt;
&gt;
&gt;
&gt;

