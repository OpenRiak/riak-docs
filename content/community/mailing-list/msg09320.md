---
title: "Re: mysterious Riak problems"
description: ""
project: community
lastmod: 2012-11-14T08:51:30-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg09320"
mailinglist_parent_id: "msg09244"
author_name: "Matthew Von-Maszewski"
project_section: "mailinglistitem"
sent_date: 2012-11-14T08:51:30-08:00
---


Dave,

Just getting my head back into the game. Was away for a few days. Random 
thought, maybe there is a hard drive with a read problem. That can cause 
issues similar to this. 1.2.1 does NOT percolate the read errors seen in 
leveldb to riak-admin (yes, that should start to happen in 1.3).

I will assume your leveldb "data\\_root" is "/var/db/riak/leveldb" for this 
script. Please substitute your appropriate path from app.config and try these 
two commands on each physical server (node):

sort /var/db/riak/leveldb/\\*/LOG\\* | grep corrupted
sort /var/db/riak/leveldb/\\*/LOG\\* | grep checksum

If you get hits on either, we have found the problem.

The "LOCK file unavailable" is more of a statement about the internal condition 
of the code instead of an error. The message is saying that the first attempt 
to re-open a vnode failed because the prior instance is still closing (or more 
likely waiting for Erlang's garbage collection to finish destroying things). 
This message is new to 1.2 code base.

Matthew


On Nov 7, 2012, at 6:56 PM, David Lowell wrote:

&gt; After further thought, I want to add more color to this issue. I hypothesize 
&gt; the symptoms I described here were continued fallout of an earlier crash. So 
&gt; I've waded further back into to the logs to try to shed light on how the Riak 
&gt; process was doing prior to this time. It was unhappy. It appears to have 
&gt; failed and stopped several times in the 1/2 day prior. The pattern revealed 
&gt; in the logs looks something like this:
&gt; 
&gt; - Following a clean startup, the service runs for a while
&gt; 
&gt; - Usually the first log entry of any kind near the beginning of a long block 
&gt; of error logs is "alarm\\_handler: {set,{system\\_memory\\_high\\_watermark,[]}}" 
&gt; ( Is this indicating excessive memory use? )
&gt; 
&gt; - Within a few minutes we see several log messages warning about "long\\_gc", 
&gt; which I assume is an indication that garbage collection took longer than some 
&gt; threshold
&gt; 
&gt; - Within the next minute or two, we start to see the legions of errors, 
&gt; "riak\\_kv\\_vnode worker pool crashed", and "gen\\_fsm" having some sort of 
&gt; timeout when trying to communicate with the eleveldb backend
&gt; 
&gt; - Eventually we see a log record indicating "Failed to start 
&gt; riak\\_kv\\_eleveldb" because of a leveldb LOCK file being temporarily unavailable
&gt; 
&gt; - Then Riak starts to exit: riak:stop:46 "backend module failed to start." 
&gt; 
&gt; 
&gt; So, not really knowing the Riak internals, it's a little difficult to piece 
&gt; together the story here. Could be we're running low on memory. Hard to know 
&gt; why riak\\_kv\\_workers are failing, or why this leveldb LOCK file is 
&gt; unavailable. To those more learned, do these log records tell a story?
&gt; 
&gt; For the record, we're using the default 8 MB of leveldb cache per vnode, so 
&gt; that ought to cap cache for our 512 vnodes at 4 GB. Our host has 32 GB of 
&gt; physical memory. Are there other pieces of Riak that can use a lot of memory 
&gt; that we need to look out for?
&gt; 
&gt; I'll include a few of the actual log records for reference, below.
&gt; 
&gt; Dave
&gt; 
&gt; --
&gt; Dave Lowell
&gt; d...@connectv.com
&gt; 
&gt; Representative logs, many similar ones deleted for brevity:
&gt; 
&gt; 2012-11-07 01:41:02.395 [info] &lt;0.51.0&gt; alarm\\_handler: 
&gt; {set,{system\\_memory\\_high\\_watermark,[]}}
&gt; 2012-11-07 01:55:50.517 [info] 
&gt; &lt;0.73.0&gt;@riak\\_core\\_sysmon\\_handler:handle\\_event:85 monitor long\\_gc 
&gt; &lt;0.18585.32&gt; 
&gt; [{initial\\_call,{riak\\_core\\_coverage\\_fsm,init,1}},{almost\\_current\\_function,{riak\\_core\\_coverage\\_plan,'-next\\_vnode/2-fun-0-',2}},{message\\_queue\\_len,0}]
&gt; 
&gt; [{timeout,219},{old\\_heap\\_block\\_size,0},{heap\\_block\\_size,28657},{mbuf\\_size,0},{stack\\_size,48},{old\\_heap\\_size,0},{heap\\_size,11430}]
&gt; 2012-11-07 01:56:20.303 [error] &lt;0.9231.0&gt;@riak\\_core\\_vnode:handle\\_info:510 
&gt; 1258832464966656615093408225054454710289582522368 riak\\_kv\\_vnode worker pool 
&gt; crashed 
&gt; {timeout,{gen\\_fsm,sync\\_send\\_event,[&lt;0.9234.0&gt;,{checkout,false,5000},5000]}}
&gt; 2012-11-07 01:56:22.382 [error] &lt;0.10002.0&gt; gen\\_fsm &lt;0.10002.0&gt; in state 
&gt; ready terminated with reason: 
&gt; {timeout,{gen\\_server,call,[&lt;0.10005.0&gt;,{work,&lt;0.10003.0&gt;,{fold,#Fun,#Fun},{fsm,{40916762,{1398702738851840683437120250060505233655091691520,'riak@10.0.3.11'}},&lt;0.18429.32&gt;}}]}}
&gt; 2012-11-07 01:57:11.833 [error] &lt;0.19755.32&gt;@riak\\_kv\\_vnode:init:265 Failed to 
&gt; start riak\\_kv\\_eleveldb\\_backend Reason: {db\\_open,"IO error: lock 
&gt; /var/data/ctv/riak/leveldb/1258832464966656615093408225054454710289582522368/LOCK:
&gt; Resource temporarily unavailable"}
&gt; 2012-11-07 01:57:27.425 [info] 
&gt; &lt;0.73.0&gt;@riak\\_core\\_sysmon\\_handler:handle\\_event:85 monitor long\\_gc 
&gt; &lt;0.19181.32&gt; 
&gt; [{initial\\_call,{riak\\_core\\_coverage\\_fsm,init,1}},{almost\\_current\\_function,{gen\\_fsm,loop,7}},{message\\_queue\\_len,0}]
&gt; 
&gt; [{timeout,109},{old\\_heap\\_block\\_size,0},{heap\\_block\\_size,28657},{mbuf\\_size,0},{stack\\_size,47},{old\\_heap\\_size,0},{heap\\_size,9171}]
&gt; 2012-11-07 01:57:51.109 [error] &lt;0.10002.0&gt; CRASH REPORT Process &lt;0.10002.0&gt; 
&gt; with 0 neighbours exited with reason: 
&gt; {timeout,{gen\\_server,call,[&lt;0.10005.0&gt;,{work,&lt;0.10003.0&gt;,{fold,#Fun,#Fun},{fsm,{40916762,{1398702738851840683437120250060505233655091691520,'riak@10.0.3.11'}},&lt;0.18429.32&gt;}}]}}
&gt; in gen\\_fsm:terminate/7 line 
&gt; 
&gt; 
&gt; On Nov 7, 2012, at 11:34 AM, David Lowell wrote:
&gt; 
&gt;&gt; Hello Riak Folks,
&gt;&gt; 
&gt;&gt; The last three days, we've been having a string of problems with Riak. An 
&gt;&gt; otherwise healthy server running our full application stack will suddenly 
&gt;&gt; start throwing a bunch of errors in the logs. Although the Riak processes 
&gt;&gt; stay up, most or all requests to Riak fail during these periods.
&gt;&gt; 
&gt;&gt; The errors in the logs are predominantly describing "riak\\_kv\\_vnode worker 
&gt;&gt; pool crashed" and timeout conditions. This morning, we had this crashy 
&gt;&gt; behavior start immediately after a clean Riak startup, and making a single 
&gt;&gt; call to our API, so the logs are quite free of other noise. I've summarized 
&gt;&gt; those logs below for curious parties, and can attach the full set of logs if 
&gt;&gt; needed.
&gt;&gt; 
&gt;&gt; I forgot to check this morning, but during a similar outage on Monday, the 
&gt;&gt; Riak server was refusing connections to new clients.
&gt;&gt; 
&gt;&gt; Interestingly, after giving Riak a while with no traffic at all today (like 
&gt;&gt; 15-30 minutes), it appears to have recovered without a restart. We've had 
&gt;&gt; similar recoveries during other "outages" of this type since Sunday evening. 
&gt;&gt; Facilitating this sort of recovery does seem to require shutting down all 
&gt;&gt; application KV requests for a while.
&gt;&gt; 
&gt;&gt; We're suspicious of some kind of corruption in the eleveldb on-disk files, 
&gt;&gt; because in past outages of this type, we've observed that the condition 
&gt;&gt; persists over reboots. But we don't have much more evidence than that. Is 
&gt;&gt; there a command we can run that will check over eleveldb files for 
&gt;&gt; corruption or inconsistency?
&gt;&gt; 
&gt;&gt; Other than that, what can cause "worker pool crashed" events? What do you 
&gt;&gt; know about the "timeouts" that are in these logs?
&gt;&gt; 
&gt;&gt; For the record, we're running Riak 1.2.0 on Ubuntu 10.04, eleveldb backend 
&gt;&gt; with 512 partitions. We're running predominantly in a single-node 
&gt;&gt; configuration on a bunch of isolated dev boxes at the moment, on our way to 
&gt;&gt; spreading out our 512 vnodes onto 5 hosts in production.
&gt;&gt; 
&gt;&gt; Thanks for your help,
&gt;&gt; 
&gt;&gt; Dave
&gt;&gt; 
&gt;&gt; --
&gt;&gt; Dave Lowell
&gt;&gt; d...@connectv.com
&gt;&gt; 
&gt;&gt; 
&gt;&gt; 2012-11-07 18:11:03.398 [info] &lt;0.7.0&gt; Application lager started on node 
&gt;&gt; 'riak@10.0.3.11'
&gt;&gt; 
&gt;&gt; ... normal startup messages ...
&gt;&gt; 
&gt;&gt; 2012-11-07 18:11:50.109 [info] 
&gt;&gt; &lt;0.10582.0&gt;@riak\\_core:wait\\_for\\_application:419 Wait complete for application 
&gt;&gt; riak\\_search (0 seconds)
&gt;&gt; 2012-11-07 18:22:18.509 [error] &lt;0.2897.0&gt;@riak\\_core\\_vnode:handle\\_info:510 
&gt;&gt; 105616329260241031198313161739262640092323250176 riak\\_kv\\_vnode worker pool 
&gt;&gt; crashed 
&gt;&gt; {timeout,{gen\\_server,call,[&lt;0.2902.0&gt;,{work,&lt;0.2900.0&gt;,{fold,#Fun,#Fun},{fsm,{96247562,{105616329260241031198313161739262640092323250176,'riak@10.0.3.11'}},&lt;0.15324.0&gt;}}]}}
&gt;&gt; 2012-11-07 18:22:18.509 [error] &lt;0.2899.0&gt; gen\\_fsm &lt;0.2899.0&gt; in state ready 
&gt;&gt; terminated with reason: 
&gt;&gt; {timeout,{gen\\_server,call,[&lt;0.2902.0&gt;,{work,&lt;0.2900.0&gt;,{fold,#Fun,#Fun},{fsm,{96247562,{105616329260241031198313161739262640092323250176,'riak@10.0.3.11'}},&lt;0.15324.0&gt;}}]}}
&gt;&gt; 
&gt;&gt; ... 13 more "riak\\_kv\\_vnode worker pool crashed" messages...
&gt;&gt; 
&gt;&gt; 2012-11-07 18:22:21.245 [error] &lt;0.2899.0&gt; CRASH REPORT Process &lt;0.2899.0&gt; 
&gt;&gt; with 0 neighbours exited with reason: 
&gt;&gt; {timeout,{gen\\_server,call,[&lt;0.2902.0&gt;,{work,&lt;0.2900.0&gt;,{fold,#Fun,#Fun},{fsm,{96247562,{105616329260241031198313161739262640092323250176,'riak@10.0.3.11'}},&lt;0.15324.0&gt;}}]}}
&gt;&gt; in gen\\_fsm:terminate/7 line 611
&gt;&gt; 2012-11-07 18:22:21.844 [error] &lt;0.2944.0&gt; gen\\_fsm &lt;0.2944.0&gt; in state ready 
&gt;&gt; terminated with reason: 
&gt;&gt; {timeout,{gen\\_server,call,[&lt;0.2947.0&gt;,{work,&lt;0.2945.0&gt;,{fold,#Fun,#Fun},{fsm,{96247562,{114179815416476790484662877555959610910619729920,'riak@10.0.3.11'}},&lt;0.15324.0&gt;}}]}}
&gt;&gt; 
&gt;&gt; ... 13 more "CRASH REPORT Process  with 0 neighbours exited with reason" 
&gt;&gt; and "gen\\_fsm &lt;0.2989.0&gt; in state ready terminated with reason" message pairs
&gt;&gt; 
&gt;&gt; 2012-11-07 18:23:21.427 [error] &lt;0.15322.0&gt; gen\\_server &lt;0.15322.0&gt; 
&gt;&gt; terminated with reason: 
&gt;&gt; {error,{case\\_clause,{error,timeout,[[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[&lt;&lt;"1352256943.4983411"&gt;&gt;],[],[],[],...]}},...}
&gt;&gt; 2012-11-07 18:23:21.495 [error] &lt;0.15322.0&gt; CRASH REPORT Process &lt;0.15322.0&gt; 
&gt;&gt; with 0 neighbours exited with reason: 
&gt;&gt; {error,{case\\_clause,{error,timeout,[[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[&lt;&lt;"1352256943.4983411"&gt;&gt;],[],[],[],...]}},...}
&gt;&gt; in gen\\_server:terminate/6 line 747
&gt;&gt; 2012-11-07 18:23:21.525 [error] &lt;0.10590.0&gt; Supervisor riak\\_api\\_pb\\_sup had 
&gt;&gt; child undefined started with {riak\\_api\\_pb\\_server,start\\_link,undefined} at 
&gt;&gt; &lt;0.15322.0&gt; exit with reason 
&gt;&gt; {error,{case\\_clause,{error,timeout,[[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[&lt;&lt;"1352256943.4983411"&gt;&gt;],[],[],[],...]}},...}
&gt;&gt; in context child\\_terminated
&gt; 

