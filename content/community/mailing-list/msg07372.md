---
title: "Re: Reip(ing) riak node created two copies in the cluster"
description: ""
project: community
lastmod: 2012-05-02T08:51:15-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg07372"
mailinglist_parent_id: "msg07371"
author_name: "Nitish Sharma"
project_section: "mailinglistitem"
sent_date: 2012-05-02T08:51:15-07:00
---


Hi Jon,
Thanks for your input. I've already started working on that lines. 
I stopped all the nodes, moved ring directory from one node, brought that one 
up, and issued join command to one other node (after moving the ring directory) 
- node2. While they were busy re-distributing the partitions, I started another 
node (node3) and issued join command (before risk\\_kv was running, since it 
takes some time to load existing data).
But after this, data handoffs are occurring only between node1 and node2. 
"member\\_status" says that node 3 owns 0% of the ring and 0% are pending.
We have a lot of data - each node serves around 200 million documents. Riak 
cluster is running 1.1.2.
Any suggestions?

Cheers
Nitish
On May 2, 2012, at 5:31 PM, Jon Meredith wrote:

&gt; Hi Nitish,
&gt; 
&gt; If you rebuild the cluster with the same ring size, the data will eventually 
&gt; get back to the right place. While the rebuild is taking place you may have 
&gt; notfounds for gets until the data has been handed off to the newly assigned 
&gt; owner (as it will be secondary handoff, not primary ownership handoff to get 
&gt; teh data back). If you don't have a lot of data stored in the cluster it 
&gt; shouldn't take too long.
&gt; 
&gt; The process would be to stop all nodes, move the files out of the ring 
&gt; directory to a safe place, start all nodes and rejoin. If you're using 1.1.x 
&gt; and you have capacity in your hardware you may want to increase 
&gt; handoff\\_concurrency to something like 4 to permit more transfers to happen 
&gt; across the cluster.
&gt; 
&gt; 
&gt; Jon.
&gt; 
&gt; 
&gt; 
&gt; On Wed, May 2, 2012 at 9:05 AM, Nitish Sharma  
&gt; wrote:
&gt; Hi,
&gt; We have a 12-node Riak cluster. Until now we were naming every new node as 
&gt; riak@. We then decided to rename the all the nodes to 
&gt; riak@, which makes troubleshooting easier.
&gt; After issuing reip command to two nodes, we noticed in the "status" that 
&gt; those 2 nodes were now appearing in the cluster with the old name as well as 
&gt; the new name. Other nodes were trying to handoff partitions to the "new" 
&gt; nodes, but apparently they were not able to. After this the whole cluster 
&gt; went down and completely stopped responding to any read/write requests.
&gt; member\\_status displayed old Riak name in "legacy" mode. Since this is our 
&gt; production cluster, we are desperately looking for some quick remedies. 
&gt; Issuing "force-remove" to the old names, restarting all the nodes, changing 
&gt; the riak names back to the old ones - none of it helped.
&gt; Currently, we are hosting limited amount of data. Whats an elegant way to 
&gt; recover from this mess? Would shutting off all the nodes, deleting the ring 
&gt; directory, and again forming the cluster work?
&gt; 
&gt; Cheers
&gt; Nitish
&gt; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
&gt; riak-users mailing list
&gt; riak-users@lists.basho.com
&gt; http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com
&gt; 
&gt; 
&gt; 
&gt; -- 
&gt; Jon Meredith
&gt; Platform Engineering Manager
&gt; Basho Technologies, Inc.
&gt; jmered...@basho.com
&gt; 

\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\\_lists.basho.com

