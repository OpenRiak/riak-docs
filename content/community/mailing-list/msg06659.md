---
title: "Re: Is Riak a good solution for this problem?"
description: ""
project: community
lastmod: 2012-02-20T12:46:35-08:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg06659"
mailinglist_parent_id: "msg06593"
author_name: "Marco Monteiro"
project_section: "mailinglistitem"
sent_date: 2012-02-20T12:46:35-08:00
---


Thank you to everyone that answered my first message.

I got ideas from every answer. Most helpful.

We decided to go ahead with a few more tests.

We are storing each page hit has a new object. Some of the queries we are
doing need to visit many objects,
and doing it in real time would not work. So we have implemented a system
that every hour goes through all
the objects of the previous hour and creates summary objects in some other
bucket. We then map-reduce
this new set of object to query in real time.

We started with about 1 request every 2 seconds, and all was working fine.
Then we went to 50 requests
per second and it stopped working. When doing the hourly map-reduce I get
error messages like

{ [Error: HTTP error 500:
{"phase":0,"error":"[preflist\\_exhausted]","input":"{ok,{r\\_object,&lt;&lt;\\"whs2\\"&gt;&gt;,&lt;&lt;\\"E4cWJZs2mZtnZqtv2xXz957lnY4\\"&gt;&gt;,[{r\\_content,{dict,6,16,16,8,80,48,{[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]},{{[],[],[[&lt;&lt;\\"Links\\"&gt;&gt;]],[],[],[],[],[],[],[],[[&lt;&lt;\\"content-type\\"&gt;&gt;,97,112,112,108,105,99,97,116,105,111,110,47,106,115,111,110],[&lt;&lt;\\"X-Riak-VTag\\"&gt;&gt;,50,121,57,75,51,109,84,103,90,74,87,108,53,67,83,89,86,88,97,82,65,52]],[[&lt;&lt;\\"index\\"&gt;&gt;,{&lt;&lt;\\"htime\\_int\\"&gt;&gt;,1329642026123},{&lt;&lt;\\"hua\\_bin\\"&gt;&gt;,&lt;&lt;\\"IE\\"&gt;&gt;},{&lt;&lt;\\"pid\\_bin\\"&gt;&gt;,&lt;&lt;\\"56a710af68f46d1a\\"&gt;&gt;},{&lt;&lt;\\"sid\\_bin\\"&gt;&gt;,&lt;&lt;\\"95...\\"&gt;&gt;}]],...}}},...}],...},...}","type":"forward\\_preflist","stack":"[]"}]
statusCode: 500 }

for every single map-reduce query, even some that use secondary indexes
that should only get a few dozen
objects. We are using 3 large instances in ec2. I'm sure that all instances
are running.

We are using eleveldb (because of the secondary indexes) and Riak1.1.0rc2-1
is running on Ubuntu systems.

Thanks for the help.

Cheers,
Marco


On 13 February 2012 23:59, Jeffrey Massung  wrote:

&gt; Marco,
&gt;
&gt; Thanks for stopping to take a look at Riak. Here are a few thoughts for
&gt; you to consider and try:
&gt;
&gt; We want the page view count by day for registered and unregistered users.
&gt; We are storing session
&gt; documents. Each document has a session identifier as it's key and a list
&gt; of page views as the value
&gt; (and a few additional properties we can ignore). This document structure
&gt; comes from CouchDB,
&gt;
&gt;
&gt; The "few additional properties we can ignore" actually can't be ignored.
&gt; The reason is because the SpiderMonkey JS VMs that are running on each node
&gt; still have to parse the JSON data for each document. If your document is 1K
&gt; of JSON text, but you only care about a fraction of that data, the VM still
&gt; has to parse it to give you - what essentially boils down to - a length
&gt; value. This is going to end up being a time sync (you would need to profile
&gt; to know how much). So, ...
&gt;
&gt; Would it be better to store each page hit under a new key,
&gt;
&gt;
&gt; Yes.
&gt;
&gt; Also, will updating the session documents be a problem for Riak?
&gt;
&gt;
&gt; You may want to consider using "links" in your documents to help speed
&gt; this up: http://wiki.basho.com/Links.html. The data you are talking about
&gt; storing off is (from what little I know) largely immutable. The list of
&gt; pages a user visited during their time on the site isn't going to change
&gt; once the session is done. So, you should be able to make a really
&gt; small object that is nothing more than a key of \\_page\\_count and
&gt; the value being the count of the pages visited. You can then make a link
&gt; from the original session object to the cached count object with a link.
&gt; Once in place, you can then use a post-commit hook (
&gt; http://wiki.basho.com/Commit-Hooks.html#Post-Commit-Hooks) so that
&gt; whenever the session log is updated, the cached page count object is also
&gt; updated, keeping the data in-sync. This will seriously cut down on the time
&gt; spent in the JS VMs.
&gt;
&gt; Next, without knowing what else is in the database (is it 100% logging
&gt; data or do your logs make up only 10% of the total data?), it's worth
&gt; taking a moment to point out secondary indexes:
&gt; http://wiki.basho.com/Secondary-Indexes.html. Right now, your map is
&gt; traversing over all the objects in the database. Buckets are \\*not\\* physical
&gt; namespaces (like directories in a file system). The bucket name is merely a
&gt; quick "early out" for the map phase before your Javascript code is even
&gt; executed. It's still fast, but maybe not fast enough. CouchDB views make a
&gt; tradeoff to gain performance at the cost of disk space, and you can use
&gt; secondary indices the same way. If you give all your page count objects a
&gt; 2i index field, you can then pass it as an input to map/reduce query, you
&gt; are now instantly limiting which objects are getting scanned to only those
&gt; with the 2i field. This has the added benefit of allowing you to range
&gt; query (e.g. if your field was a UTC timestamp, you could look at only the
&gt; page hits for sessions over the last week, month, day, minute, â€¦).
&gt;
&gt; Hope this helps, if you have the time/ability to try the above and give
&gt; feedback on the results, I'd be very interested in learning them and
&gt; helping further.
&gt;
&gt; --
&gt; Jeffrey Massung
&gt; j...@basho.com
&gt;
&gt; On Feb 12, 2012, at 4:27 AM, Marco Monteiro wrote:
&gt;
&gt; Hello!
&gt;
&gt; I'm considering Riak for the statistics of a site that is approaching a
&gt; billion page views per month.
&gt; The plan is to log a little information about each the page view and then
&gt; to query that data.
&gt;
&gt; I'm very new to Riak. I've gone over the documentation on the wiki, and I
&gt; know about map-reduce,
&gt; secondary indexes and Riak search. I've installed Riak on a single node
&gt; and made a test with the
&gt; default configuration. The results were a little bellow what I expected.
&gt; For the test is used the following
&gt; requirement.
&gt;
&gt; We want the page view count by day for registered and unregistered users.
&gt; We are storing session
&gt; documents. Each document has a session identifier as it's key and a list
&gt; of page views as the value
&gt; (and a few additional properties we can ignore). This document structure
&gt; comes from CouchDB,
&gt; where I organised things like this to be able to more easily query the
&gt; database. I've done a basic
&gt; javascript map-reduce query for this. I just map over each session (every
&gt; k/v in a bucket) returning
&gt; the length of the page views array for either the registered or
&gt; unregistered field (the other is zero), and
&gt; the day of the request. In the reduce I collect them by hashing the day
&gt; and summing the two number
&gt; of page views. Then I have a second reduce to sort the list by day.
&gt;
&gt; This is very slow on a single machine setup with default Riak
&gt; configuration. 1.000 sessions takes
&gt; 6 seconds. 10.000 sessions takes more that 2 minutes (timeout). We want to
&gt; handle 10.000.000
&gt; sessions, at least. Is there a way, maybe with secondary indexes, to make
&gt; this go faster using only Riak?
&gt; Or must I use some kind of persistent cache to store this info as time
&gt; goes by? Or can I make Riak
&gt; run 100 times faster by tweaking the config? I don't want to have 1000
&gt; machines for making this work.
&gt;
&gt; Also, will updating the session documents be a problem for Riak? Would it
&gt; be better to store each
&gt; page hit under a new key, to not update the the session document. Because
&gt; of the "multilevel" map
&gt; reduce this ca work on Riak, where it didn't work on CouchDB, because its
&gt; view system limitation.
&gt; Unfortunately, with the update of documents the CouchDB database was
&gt; growing way too fast for it
&gt; to be a feasible solution.
&gt;
&gt;
&gt; Any advice to make Riak work for this problem is greatly appreciated.
&gt;
&gt; Thanks,
&gt; Marco

